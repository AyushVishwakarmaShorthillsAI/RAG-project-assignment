# ğŸ§  RAG-LLM: Multi-Source Q&A System with Evaluation

## âœ¨ Project Overview

This project implements an end-to-end Retrieval-Augmented Generation (RAG) system designed to generate high-quality answers from multiple authoritative sources including Wikipedia, NASA, National Geographic, History.com, and Britannica. The solution leverages state-of-the-art tools and models to build a production-ready pipeline, supporting everything from data scraping to question-answer evaluation.

### ğŸ”‘ Key Features:

- ğŸ“š **Comprehensive Data Collection**: Scraping and processing 100 URLs from multiple sources:
  - Wikipedia pages (Science, History, Mathematics)
  - NASA articles (Space, Astronomy, Technology)
  - National Geographic content (Environment, History, Science)
  - History.com articles (Historical Events, Civilizations)
  - Britannica entries (Science, History, Culture)
- ğŸ” **Advanced Text Processing**: Efficient chunking and cleaning of content
- ğŸ¤– **State-of-the-art Embedding**: Using sentence-transformers/all-MiniLM-L6-v2 model
- ğŸ’¾ **Efficient Storage**: FAISS vector database for fast similarity search
- ğŸ§  **Smart Generation**: Ollama LLM for high-quality answer generation
- ğŸ¨ **User-friendly Interface**: Streamlit-based UI with real-time interaction
- ğŸ“Š **Robust Evaluation**: Comprehensive testing and evaluation framework
- ğŸ“ **Detailed Logging**: Complete interaction history and performance tracking
- âš¡ **Performance Optimization**: Fast retrieval and response times

## ğŸ“ Folder Structure

```
Assignment/
â”œâ”€â”€ app/                      # Main application code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ all_Urls.py          # URL management
â”‚   â”œâ”€â”€ data_processing.py   # Data processing utilities
â”‚   â”œâ”€â”€ embeddings.pkl       # Stored embeddings
â”‚   â”œâ”€â”€ faiss_index.bin      # FAISS vector index
â”‚   â”œâ”€â”€ llm.py              # Language model related code
â”‚   â”œâ”€â”€ rag_pipeline.py     # RAG pipeline implementation
â”‚   â”œâ”€â”€ scraper.py          # Web scraping functionality
â”‚   â”œâ”€â”€ scraped_data.txt    # Scraped data storage
â”‚   â”œâ”€â”€ texts.pkl           # Processed text data
â”‚   â””â”€â”€ vector_store.py     # Vector store implementation
â”‚
â”œâ”€â”€ testing/                # Testing related files
â”‚   â”œâ”€â”€ test_rag.py        # RAG pipeline testing
â”‚   â”œâ”€â”€ test_unit.py       # Unit testing
â”‚  
â”œâ”€â”€ test_cases/           # Test cases
â”œâ”€â”€ logs/                 # Log files
â”œâ”€â”€ chroma_db_store/     # ChromaDB storage
â”œâ”€â”€ rag_env/             # Virtual environment
â”œâ”€â”€ main.py              # Main entry point
â”œâ”€â”€ requirements.txt     # Project dependencies
â””â”€â”€ .gitignore          # Git ignore file
```

## ğŸ—ï¸ Architecture Overview

The system follows a standard RAG pipeline:

1. **Data Collection**: Scraping Wikipedia content
2. **Text Processing**: Cleaning and chunking the content
3. **Embedding**: Converting text to vectors using sentence-transformers
4. **Storage**: FAISS vector database for efficient similarity search
5. **Retrieval**: Finding relevant context for questions
6. **Generation**: Using Ollama LLM to generate answers 
7. **Evaluation**: Comprehensive testing and evaluation framework

## ğŸ”„ Workflow

### 1ï¸âƒ£ Data Collection Phase
- Initialize scrapers for multiple sources
- Define target URLs from diverse domains:
  - Science and Technology (NASA, Wikipedia)
  - History and Culture (History.com, Britannica)
  - Environment and Nature (National Geographic)
  - Mathematics and Physics (Wikipedia, Britannica)
- Extract and clean content from each source
- Store raw data in structured format

### 2ï¸âƒ£ Processing Phase
- Text cleaning and normalization
- Content chunking with overlap
- Metadata extraction
- Quality checks and validation

### 3ï¸âƒ£ Embedding Phase
- Load embedding model
- Generate vector representations
- Optimize for semantic search
- Store embeddings in FAISS

### 4ï¸âƒ£ Query Processing
- User question input
- Query embedding generation
- Similarity search in FAISS
- Context retrieval and ranking

### 5ï¸âƒ£ Answer Generation
- Context preparation
- LLM prompt engineering
- Answer generation
- Response formatting

### 6ï¸âƒ£ QA Test Case Generation
- Manual creation of 1000 diverse Q&A pairs using gemini.
- Categorization of questions by complexity
- Validation of answers against source content
- Documentation of test cases in structured format
- Creation of ground truth dataset for evaluation

### 7ï¸âƒ£ Evaluation Phase
- Automated testing
- Performance metrics calculation
- Quality assessment
- Logging and reporting

## ğŸŒ Scraping Pipeline

### ğŸ“¦ Core Module: scraper.py

- Uses custom Wikipedia scraper to extract clean text
- Saves content in structured format
- Handles error cases and logging

### ğŸ”„ Workflow:

1. Initialize scraper with target URLs
2. Extract and clean content from each page
3. Save processed content for further use
4. Log all operations for debugging

## âš™ï¸ Chunking, Embedding & FAISS Storage

### ğŸ” Embedding:

- Model Used: sentence-transformers/all-MiniLM-L6-v2
- Optimized for semantic search
- Efficient vector operations

### ğŸ’¾ Storage:

- FAISS for fast similarity search
- Persistent storage of embeddings and texts
- Efficient retrieval for real-time Q&A

## ğŸ–¥ï¸ Streamlit UI Chatbot

### âœ¨ Features:

- Clean, intuitive interface
- Real-time question answering
- Interaction history display
- Response time tracking

### ğŸ¯ UI Components:

- Question input form
- Answer display
- Response time metrics
- Sidebar with history
- Error handling and user feedback

## ğŸ“ˆ Evaluation Strategy

### ğŸ“Š Evaluation Components:

1. **QA Evaluation**
   - Automated testing framework
   - Multiple(1000+) QA test cases
   - Performance metrics:
     - **BERT F1 Score (0.911)**: Measures semantic similarity between generated and reference answers
     - **Cosine Similarity (0.747)**: Evaluates vector space similarity between answers
     - **BLEU Score (0.222)**: Measures n-gram overlap between generated and reference answers
     - **ROUGE Score (0.488)**: Evaluates recall of n-grams between answers
     - **METEOR Score (0.566)**: Considers word alignment and synonym matching
     - **Entailment Analysis**:
       - Contradiction Rate: 46.08% (411 cases)
       - Entailment Rate: 43.83% (391 cases)
       - Neutral Rate: 10.09% (90 cases)

2. **Unit Testing**
   - Comprehensive test suite for core components:
     - Scraper functionality testing
     - Vector store operations (FAISS)
     - LLM integration and response generation
     - RAG pipeline processing
     - Data processing and storage
   - Edge case handling:
     - Large batch processing
     - Special character handling
     - Empty input scenarios
   - Integration testing between components
   - Mock testing for external dependencies

3. **Logging and Monitoring**
   - Detailed interaction logs
   - Performance tracking
   - Error monitoring

## ğŸ› ï¸ Improvements and Enhancements

- Efficient text processing pipeline
- Using more Quick Vector Storage and Retrieval
- High performance Text Embedding LLM
- Enchanced User-friendly interface
- Performance optimization by chaning
- Asynchronous scraping and test case evaluation

## ğŸš€ How to Run

1. Install dependencies:
```bash
pip install -r requirements.txt
```

2. Run the main application:
```bash
python main.py
```

3. Access the Streamlit UI in your browser

## ğŸ”® Future Work

- Enhanced evaluation metrics
- Support for more document types
- Improved chunking strategies
- Advanced reranking techniques
- Multilingual support

## ğŸ“œ License

MIT License

## ğŸ™ Acknowledgements

- HuggingFace for Transformers
- Meta for FAISS
- Streamlit for UI framework
- Wikipedia for content 