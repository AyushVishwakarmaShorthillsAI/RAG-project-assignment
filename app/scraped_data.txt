Photosynthesis(/ˌfoʊtəˈsɪnθəsɪs/FOH-tə-SINTH-ə-sis)[1]is asystemofbiological processesby whichphotosynthetic organisms, such as most plants,algae, andcyanobacteria, convertlight energy, typically from sunlight, into thechemical energynecessary to fuel theirmetabolism.Photosynthesisusually refers tooxygenic photosynthesis, a process that produces oxygen. Photosynthetic organisms store the chemical energy so produced within intracellularorganic compounds(compounds containing carbon) like sugars,glycogen,celluloseandstarches. To use this stored chemical energy, an organism's cells metabolize the organic compounds throughcellular respiration. Photosynthesis plays a critical role in producing and maintaining theoxygen contentof the Earth's atmosphere, and it supplies most of thebiological energynecessary forcomplex lifeon Earth.[2]
Somebacteriaalso performanoxygenic photosynthesis, which usesbacteriochlorophyllto splithydrogen sulfideas areductantinstead of water, producingsulfurinstead of oxygen.Archaeasuch asHalobacteriumalso perform a type of non-carbon-fixinganoxygenic photosynthesis, where the simplerphotopigmentretinaland itsmicrobial rhodopsinderivativesare used to absorb green light and powerproton pumpsto directly synthesizeadenosine triphosphate(ATP), the "energy currency" of cells. Such archaeal photosynthesis might have been the earliest form of photosynthesis that evolved on Earth, as far back as thePaleoarchean, preceding that ofcyanobacteria(seePurple Earth hypothesis).
While the details may differ betweenspecies, the process always begins when light energy is absorbed by thereaction centers, proteins that containphotosynthetic pigmentsorchromophores. In plants, these pigments arechlorophylls(aporphyrinderivative that absorbs the red and bluespectrumsof light, thus reflecting green) held insidechloroplasts, abundant inleafcells. In bacteria, they are embedded in theplasma membrane. In these light-dependent reactions, some energy is used to stripelectronsfrom suitable substances, such as water, producing oxygen gas. Thehydrogenfreed by the splitting of water is used in the creation of two important molecules that participate in energetic processes: reducednicotinamide adenine dinucleotide phosphate(NADPH) and ATP.
In plants,algae, andcyanobacteria, sugars are synthesized by a subsequent sequence oflight-independentreactions called theCalvin cycle. In this process, atmospheric carbon dioxide is incorporated into already existing organic compounds, such asribulose bisphosphate(RuBP).[3]Using the ATP and NADPH produced by the light-dependent reactions, the resulting compounds are thenreducedand removed to form further carbohydrates, such asglucose. In other bacteria, different mechanisms like thereverse Krebs cycleare used to achieve the same end.
The first photosynthetic organisms probablyevolvedearly in theevolutionary history of lifeusingreducing agentssuch as hydrogen or hydrogen sulfide, rather than water, as sources of electrons.[4]Cyanobacteria appeared later; theexcess oxygenthey produced contributed directly to theoxygenation of the Earth,[5]which rendered the evolution of complex life possible. The average rate of energy captured by global photosynthesis is approximately 130terawatts,[6][7][8]which is about eight times the totalpower consumption of human civilization.[9]Photosynthetic organisms also convert around 100–115 billiontons(91–104 Pgpetagrams, or billions of metric tons), of carbon intobiomassper year.[10][11]Photosynthesis was discovered in 1779 byJan Ingenhouszwho showed that plants need light, not just soil and water.
Most photosynthetic organisms arephotoautotrophs, which means that they are able tosynthesizefood directly fromcarbon dioxideandwaterusingenergyfrom light. However, not all organisms use carbon dioxide as a source of carbon atoms to carry out photosynthesis;photoheterotrophsuse organic compounds, rather than carbon dioxide, as a source of carbon.[2]
Inplants,algae, andcyanobacteria, photosynthesis releases oxygen. Thisoxygenic photosynthesisis by far the most common type of photosynthesis used by living organisms. Some shade-loving plants (sciophytes) produce such low levels of oxygen during photosynthesis that they use all of it themselves instead of releasing it to the atmosphere.[12]
Although there are some differences between oxygenic photosynthesis in plants, algae, and cyanobacteria, the overall process is quite similar in these organisms. There are also many varieties ofanoxygenic photosynthesis, used mostly by bacteria, which consume carbon dioxide but do not release oxygen or which produce elemental sulfur instead of molecular oxygen.[13][14]
Carbon dioxide is converted into sugars in a process calledcarbon fixation; photosynthesis captures energy from sunlight to convert carbon dioxide intocarbohydrates. Carbon fixation is anendothermicredoxreaction. In general outline, photosynthesis is the opposite ofcellular respiration: while photosynthesis is a process of reduction of carbon dioxide to carbohydrates, cellular respiration is the oxidation of carbohydrates or othernutrientsto carbon dioxide. Nutrients used in cellular respiration include carbohydrates, amino acids and fatty acids. These nutrients are oxidized to produce carbon dioxide and water, and to release chemical energy to drive the organism'smetabolism.
Photosynthesis and cellular respiration are distinct processes, as they take place through different sequences of chemical reactions and in differentcellular compartments(cellular respiration inmitochondria).[15][16]
The generalequationfor photosynthesis as first proposed byCornelis van Nielis:[17]
Since water is used as the electron donor in oxygenic photosynthesis, the equation for this process is:
This equation emphasizes that water is both a reactant in thelight-dependent reactionand a product of thelight-independent reaction, but cancelingnwater molecules from each side gives the net equation:
Other processes substitute other compounds (such asarsenite) for water in the electron-supply role; for example some microbes use sunlight to oxidize arsenite toarsenate:[18]The equation for this reaction is:
Photosynthesis occurs in two stages. In the first stage,light-dependent reactionsorlight reactionscapture the energy of light and use it to make the hydrogen carrierNADPHand the energy-storage moleculeATP. During the second stage, thelight-independent reactionsuse these products to capture and reduce carbon dioxide.
Most organisms that use oxygenic photosynthesis usevisible lightfor the light-dependent reactions, although at least three use shortwaveinfraredor, more specifically, far-red radiation.[20]
Some organisms employ even more radical variants of photosynthesis. Somearchaeause a simpler method that employs a pigment similar to those used for vision in animals. Thebacteriorhodopsinchanges its configuration in response to sunlight, acting as a proton pump. This produces a proton gradient more directly, which is then converted to chemical energy. The process does not involve carbon dioxide fixation and does not release oxygen, and seems to have evolved separately from the more common types of photosynthesis.[21]
In photosynthetic bacteria, the proteins that gather light for photosynthesis are embedded incell membranes. In its simplest form, this involves the membrane surrounding the cell itself.[22]However, the membrane may be tightly folded into cylindrical sheets calledthylakoids,[23]or bunched up into roundvesiclescalledintracytoplasmic membranes.[24]These structures can fill most of the interior of a cell, giving the membrane a very large surface area and therefore increasing the amount of light that the bacteria can absorb.[23]
In plants and algae, photosynthesis takes place inorganellescalledchloroplasts. A typicalplant cellcontains about 10 to 100 chloroplasts. The chloroplast is enclosed by a membrane. This membrane is composed of a phospholipid inner membrane, a phospholipid outer membrane, and an intermembrane space. Enclosed by the membrane is an aqueous fluid called the stroma. Embedded within the stroma are stacks of thylakoids (grana), which are the site of photosynthesis. The thylakoids appear as flattened disks. The thylakoid itself is enclosed by the thylakoid membrane, and within the enclosed volume is a lumen or thylakoid space. Embedded in the thylakoid membrane are integral andperipheral membrane proteincomplexes of the photosynthetic system.
Plants absorb light primarily using thepigmentchlorophyll. The green part of the light spectrum is not absorbed but is reflected, which is the reason that most plants have a green color. Besides chlorophyll, plants also use pigments such ascarotenesandxanthophylls.[25]Algae also use chlorophyll, but various other pigments are present, such asphycocyanin,carotenes, andxanthophyllsingreen algae,phycoerythrininred algae(rhodophytes) andfucoxanthininbrown algaeanddiatomsresulting in a wide variety of colors.
These pigments are embedded in plants and algae in complexes called antenna proteins. In such proteins, the pigments are arranged to work together. Such a combination of proteins is also called alight-harvesting complex.[26]
Although all cells in the green parts of a plant have chloroplasts, the majority of those are found in specially adapted structures calledleaves. Certain species adapted to conditions of strong sunlight andaridity, such as manyEuphorbiaandcactusspecies, have their main photosynthetic organs in their stems. The cells in the interior tissues of a leaf, called themesophyll, can contain between 450,000 and 800,000 chloroplasts for every square millimeter of leaf. The surface of the leaf is coated with a water-resistantwaxycuticlethat protects the leaf from excessiveevaporationof water and decreases the absorption ofultravioletorbluelightto minimizeheating. The transparentepidermislayer allows light to pass through to thepalisademesophyll cells where most of the photosynthesis takes place.
In thelight-dependent reactions, onemoleculeof the pigmentchlorophyllabsorbs onephotonand loses oneelectron. This electron is taken up by a modified form of chlorophyll calledpheophytin, which passes the electron to aquinonemolecule, starting the flow of electrons down anelectron transport chainthat leads to the ultimatereductionofNADPtoNADPH. In addition, this creates aproton gradient(energy gradient) across thechloroplast membrane, which is used byATP synthasein the synthesis ofATP. The chlorophyll molecule ultimately regains the electron it lost when awatermolecule is split in a process calledphotolysis, which releasesoxygen.
The overall equation for the light-dependent reactions under the conditions of non-cyclic electron flow in green plants is:[27]
Not allwavelengthsoflightcan support photosynthesis. The photosyntheticaction spectrumdepends on the type ofaccessory pigmentspresent. For example, ingreen plants, the action spectrum resembles theabsorption spectrumforchlorophyllsandcarotenoidswith absorption peaks in violet-blue and red light. Inred algae, the action spectrum is blue-green light, which allows thesealgaeto use the blue end of the spectrum to grow in the deeper waters that filter out the longer wavelengths (red light) used by above-ground green plants. The non-absorbed part of thelight spectrumis what givesphotosynthetic organismstheircolor(e.g., green plants, red algae,purple bacteria) and is the least effective for photosynthesis in the respectiveorganisms.
Inplants,light-dependent reactionsoccur in thethylakoid membranesof thechloroplastswhere they drive the synthesis ofATPandNADPH. The light-dependent reactions are of two forms:cyclic and non-cyclic.
In the non-cyclic reaction, the photons are captured in the light-harvestingantenna complexesofphotosystem IIbychlorophylland otheraccessory pigments(see diagram "Z-scheme"). The absorption of a photon by the antenna complex loosens an electron by a process calledphotoinduced charge separation. The antenna system is at the core of thechlorophyllmolecule of the photosystem II reaction center. That loosened electron is taken up by the primaryelectron-acceptormolecule,pheophytin. As the electrons are shuttled through anelectron transport chain(the so-calledZ-schemeshown in the diagram), achemiosmotic potentialis generated by pumpingproton cations(H+) across themembraneand into thethylakoid space. An ATP synthaseenzymeuses thatchemiosmotic potentialto make ATP duringphotophosphorylation, whereasNADPHis a product of the terminalredoxreaction in theZ-scheme. The electron enters a chlorophyllmoleculeinPhotosystem I. There it is further excited by thelightabsorbed by thatphotosystem. The electron is then passed along a chain ofelectron acceptorsto which it transfers some of itsenergy. The energy delivered to the electron acceptors is used to movehydrogen ionsacross the thylakoid membrane into thelumen. The electron is eventually used toreducethe coenzymeNADPwith anH+to NADPH (which has functions in the light-independent reaction); at that point, the path of that electron ends.
The cyclic reaction is similar to that of the non-cyclic but differs in that it generates only ATP, and no reduced NADP (NADPH) is created. The cyclic reaction takes place only at photosystem I. Once the electron is displaced from the photosystem, the electron is passed down the electron acceptor molecules and returns to photosystem I, from where it was emitted, hence the namecyclic reaction.
Linear electron transportthrough a photosystem will leave thereaction centerof that photosystemoxidized. Elevating another electron will first require re-reduction of the reaction center. The excited electrons lost from the reaction center (P700) ofphotosystem Iare replaced by transfer fromplastocyanin, whose electrons come from electron transport throughphotosystem II. Photosystem II, as the first step of theZ-scheme, requires an external source of electrons to reduce its oxidizedchlorophyllareaction center. The source of electrons for photosynthesis in green plants andcyanobacteriais water. Two water molecules are oxidized by the energy of four successive charge-separation reactions of photosystem II to yield a molecule ofdiatomicoxygen and fourhydrogenions. The electrons yielded are transferred to a redox-activetyrosineresidue that is oxidized by the energy ofP680+. This resets the ability of P680 to absorb another photon and release anotherphoto-dissociatedelectron. The oxidation of water iscatalyzedin photosystem II by a redox-active structure that contains fourmanganeseions and acalcium ion; thisoxygen-evolving complexbinds twowater moleculesand contains the four oxidizing equivalents that are used to drive the water-oxidizing reaction (Kok's S-state diagrams). The hydrogen ions are released in thethylakoid lumenand therefore contribute to the transmembrane chemiosmotic potential that leads toATP synthesis. Oxygen is awaste productof light-dependent reactions, but the majority of organisms onEarthuse oxygen and its energy forcellular respiration, includingphotosynthetic organisms.[28][29]
In thelight-independent(or "dark") reactions, the enzymeRuBisCOcapturesCO2from theatmosphereand, in aprocesscalled theCalvin cycle, uses the newly formedNADPHand releasesthree-carbon sugars, which are latercombinedto formsucroseandstarch. The overall equation for the light-independent reactions ingreen plantsis[27]: 128
Carbon fixationproduces thethree-carbon sugar intermediate, which is then converted into the finalcarbohydrateproducts. Thesimple carbon sugarsphotosynthesis produces are then used to form otherorganic compounds, such as the building materialcellulose, theprecursorsforlipidandamino acidbiosynthesis, or as a fuel incellular respiration. The latter occurs not only inplantsbut also inanimalswhen thecarbonandenergyfrom plants is passed through afood chain.
Thefixationorreductionofcarbon dioxideis a process in which carbon dioxide combines with afive-carbon sugar,ribulose 1,5-bisphosphate, toyieldtwomoleculesof a three-carbon compound,glycerate 3-phosphate, also known as 3-phosphoglycerate. Glycerate 3-phosphate, in the presence ofATPandNADPHproduced during the light-dependent stages, is reduced toglyceraldehyde 3-phosphate. Thisproductis also referred to as 3-phosphoglyceraldehyde (PGAL) or, more generically, astriosephosphate. Most (five out of six molecules) of the glyceraldehyde 3-phosphate produced are used to regenerate ribulose 1,5-bisphosphate so the process can continue. The triose phosphates not thus "recycled" often condense to formhexosephosphates, which ultimately yieldsucrose,starch, andcellulose, as well asglucoseandfructose. Thesugarsproduced during carbonmetabolismyieldcarbon skeletonsthat can be used for othermetabolic reactionslike the production ofamino acidsandlipids.
Inhot and dry conditions, plants close theirstomatato prevent water loss. Under these conditions, CO2will decrease and oxygengas, produced by thelight reactionsof photosynthesis, will increase, causing an increase ofphotorespirationby theoxygenaseactivity ofribulose-1,5-bisphosphate carboxylase/oxygenase(RuBisCO) and decrease in carbon fixation. Some plants haveevolvedmechanisms to increase the CO2concentration in the leaves under these conditions.[30]
Plants that use theC4carbon fixationprocess chemically fix carbon dioxide in thecellsof themesophyllby adding it to the three-carbon moleculephosphoenolpyruvate(PEP), a reactioncatalyzedby anenzymecalledPEP carboxylase, creating the four-carbon organic acidoxaloacetic acid. Oxaloacetic acid ormalatesynthesized by this process is thentranslocatedto specializedbundle sheathcells where the enzymeRuBisCOand other Calvin cycle enzymes are located, and where CO2released bydecarboxylationof the four-carbon acids is then fixed by RuBisCO activity to the three-carbon3-phosphoglyceric acids. The physical separation of RuBisCO from the oxygen-generating light reactions reduces photorespiration and increases CO2fixation and, thus, thephotosynthetic capacityof theleaf.[31]C4plantscan produce more sugar thanC3plantsin conditions of high light andtemperature. Many importantcrop plantsare C4plants, includingmaize,sorghum,sugarcane, andmillet. Plants that do not use PEP-carboxylase in carbon fixation are called C3plants because the primarycarboxylation reaction, catalyzed by RuBisCO, produces the three-carbon 3-phosphoglyceric acids directly in theCalvin-Benson cycle. Over 90% of plants use C3carbon fixation, compared to 3% that use C4carbon fixation;[32]however, the evolution of C4in over sixty plant lineages makes it a striking example ofconvergent evolution.[30]C2photosynthesis, which involves carbon-concentration by selective breakdown of photorespiratory glycine, is both an evolutionary precursor to C4and a usefulcarbon-concentrating mechanismin its own right.[33]
Xerophytes, such ascactiand mostsucculents, also use PEP carboxylase to capture carbon dioxide in a process calledCrassulacean acid metabolism(CAM). In contrast to C4metabolism, whichspatiallyseparates the CO2fixation to PEP from the Calvin cycle, CAMtemporallyseparates these two processes. CAM plants have a differentleaf anatomyfrom C3plants, and fix the CO2at night, when their stomata are open. CAM plants store the CO2mostly in the form ofmalic acidvia carboxylation ofphosphoenolpyruvatetooxaloacetate, which is then reduced to malate. Decarboxylation of malate during the day releases CO2inside the leaves, thus allowing carbon fixation to 3-phosphoglycerate by RuBisCO. CAM is used by 16,000speciesof plants.[34]
Calcium-oxalate-accumulating plants, such asAmaranthus hybridusandColobanthus quitensis,show a variation of photosynthesis where calcium oxalatecrystalsfunction as dynamiccarbon pools, supplying carbon dioxide (CO2) to photosynthetic cells when stomata are partially or totally closed. This process was namedalarm photosynthesis. Understressconditions (e.g.,water deficit),oxalatereleased from calcium oxalate crystals is converted to CO2by anoxalate oxidaseenzyme, and the produced CO2can support theCalvin cyclereactions. Reactivehydrogen peroxide(H2O2), thebyproductof oxalate oxidase reaction, can beneutralizedbycatalase. Alarm photosynthesis represents a photosynthetic variant to be added to the well-known C4 and CAM pathways. However, alarm photosynthesis, in contrast to these pathways, operates as a biochemical pump that collects carbon from the organ interior (or from thesoil) and not from the atmosphere.[35][36]
Cyanobacteriapossesscarboxysomes, which increase the concentration of CO2around RuBisCO to increase the rate of photosynthesis. An enzyme,carbonic anhydrase, located within the carboxysome, releases CO2from dissolvedhydrocarbonate ions(HCO−3). Before the CO2can diffuse out, RuBisCO concentrated within the carboxysome quickly sponges it up. HCO−3ions are made from CO2outside the cell by another carbonic anhydrase and are actively pumped into the cell by a membrane protein. They cannot cross the membrane as they are charged, and within the cytosol they turn back into CO2very slowly without the help of carbonic anhydrase. This causes the HCO−3ions to accumulate within the cell from where they diffuse into the carboxysomes.[37]Pyrenoidsinalgaeandhornwortsalso act to concentrate CO2around RuBisCO.[38][39]
The overallprocessof photosynthesis takes place in four stages:[11]
Plantsusuallyconvert light into chemical energywith aphotosynthetic efficiencyof 3–6%.[40][41]Absorbed light that is unconverted isdissipatedprimarily asheat, with a smallfraction(1–2%) reemitted aschlorophyll fluorescenceat longer (redder)wavelengths. This fact allowsmeasurementof thelight reactionof photosynthesis by using chlorophyllfluorometers.[42]
Actual plants' photosynthetic efficiency varies with thefrequency of the lightbeing converted,light intensity,temperature, and proportion ofcarbon dioxide in the atmosphere, and can vary from 0.1% to 8%.[43]By comparison,solar panelsconvert light intoelectric energyat an efficiency of approximately 6–20% formass-producedpanels, and above 40% inlaboratorydevices.Scientistsare studying photosynthesis in hopes of developing plants with increasedyield.[41]
The efficiency of both light and dark reactions can be measured, but the relationship between the two can be complex. For example, thelight reactioncreatesATPandNADPHenergymolecules, whichC3plantscan use forcarbon fixationorphotorespiration.[44]Electronsmay also flow to other electron sinks.[45][46][47]For this reason, it is not uncommon forauthorsto differentiate between work done undernon-photorespiratory conditions and under photorespiratory conditions.[48][49][50]
Chlorophyll fluorescenceofphotosystem IIcan measure the light reaction, andinfrared gas analyzerscan measure thedark reaction.[51]An integrated chlorophyllfluorometerandgas exchange systemcan investigate both light and dark reactions when researchers use the two separatesystemstogether.[52]Infrared gas analyzers and somemoisture sensorsare sensitive enough to measure thephotosynthetic assimilationofCO2and ofΔH2O usingreliable methods. CO2is commonly measured inμmols/(m2/s),parts permillion, or volume per million; andH2Ois commonly measured inmmols/(m2/s) or inmbars. By measuringCO2assimilation, ΔH2O, leaf temperature,barometric pressure, leaf area, andphotosynthetically active radiation(PAR), it becomes possible to estimate, "A" or carbon assimilation, "E" ortranspiration, "gs" orstomatal conductance, and "Ci" or intracellular CO2.[53]However, it is more common to use chlorophyll fluorescence forplant stress measurement, where appropriate, because the most commonly used parametersFV/FMandY(II) or F/FM'can be measured in a few seconds, allowing the investigation of larger plant populations.[50]
Gas exchange systemsthat offer control of CO2levels, above and belowambient, allow the common practice of measurement of A/Ci curves, at different CO2levels, to characterize a plant's photosynthetic response.[53]
Integrated chlorophyll fluorometer – gas exchange systems allow a moreprecisemeasure of photosynthetic response and mechanisms.[51][52]While standard gas exchange photosynthesis systems can measure Ci, or substomatal CO2levels, the addition of integrated chlorophyll fluorescence measurements allows a more precise measurement of CC,the estimation of CO2concentration at the site ofcarboxylationin the chloroplast, to replace Ci.[52][54]CO2concentration in the chloroplast becomes possible to estimate with the measurement of mesophyll conductance or gmusing an integrated system.[51][52][55]
Photosynthesis measurement systems are not designed to directly measure the amount of light the leaf absorbs, but analysis ofchlorophyll fluorescence,P700- and P515-absorbance, andgas exchangemeasurements reveal detailed information about, e.g., thephotosystems,quantum efficiencyand the CO2assimilation rates. With some instruments, even wavelength dependency of the photosynthetic efficiency can beanalyzed.[56]
Aphenomenonknown asquantum walkincreases the efficiency of the energy transport of light significantly. In the photosynthetic cell of analga,bacterium, or plant, there are light-sensitive molecules calledchromophoresarranged in an antenna-shaped structure called a photocomplex. When aphotonis absorbed by a chromophore, it is converted into aquasiparticlereferred to as anexciton, which jumps from chromophore to chromophore towards the reaction center of the photocomplex, a collection of molecules that traps its energy in a chemical form accessible to the cell's metabolism. The exciton's wave properties enable it to cover a wider area and try out several possible paths simultaneously, allowing it to instantaneously "choose" the most efficient route, where it will have the highest probability of arriving at its destination in the minimum possible time.
Because that quantum walking takes place at temperatures far higher than quantum phenomena usually occur, it is only possible over very short distances. Obstacles in the form of destructive interference cause the particle to lose its wave properties for an instant before it regains them once again after it is freed from its locked position through a classic "hop". The movement of the electron towards the photo center is therefore covered in a series of conventional hops and quantum walks.[57][58][59]
Fossilsof what are thought to befilamentousphotosyntheticorganismshave been dated at 3.4 billion years old.[60][61]More recentstudiesalso suggest that photosynthesis may have begun about 3.4 billion years ago,[62][63]though the first directevidenceof photosynthesis comes fromthylakoid membranespreserved in 1.75-billion-year-oldcherts.[64]
Oxygenic photosynthesisis the main source ofoxygenin theEarth's atmosphere, and its earliest appearance is sometimes referred to as theoxygen catastrophe.Geologicalevidence suggests that oxygenic photosynthesis, such as that incyanobacteria, became important during thePaleoproterozoicera around two billion years ago. Modern photosynthesis inplantsand most photosyntheticprokaryotesis oxygenic, usingwateras anelectron donor, which isoxidizedto molecular oxygen in thephotosynthetic reaction center.
Several groups ofanimalshave formedsymbioticrelationships with photosyntheticalgae. These are most common incorals,sponges, andsea anemones.Scientistspresume that this is due to the particularly simplebody plansand largesurface areasof these animals compared to theirvolumes.[65]In addition, a few marinemollusks, such asElysia viridisandElysia chlorotica,also maintain a symbiotic relationship withchloroplaststhey capture from the algae intheir dietand then store in their bodies (seeKleptoplasty). This allows the mollusks to survive solely by photosynthesis for several months at a time.[66][67]Some of thegenesfrom the plantcell nucleushave even been transferred to theslugs, so that the chloroplasts can be supplied withproteinsthey need to survive.[68]
An even closer form of symbiosis may explain the origin of chloroplasts. Chloroplasts have many similarities with photosyntheticbacteria, including a circularchromosome, prokaryotic-typeribosome, and similarproteins in the photosynthetic reaction center.[69][70]Theendosymbiotic theorysuggests that photosynthetic bacteria were acquired (byendocytosis) by earlyeukaryoticcells to form the first plant cells. Therefore, chloroplasts may be photosynthetic bacteria that adapted to life inside plant cells. Likemitochondria, chloroplasts possess their ownDNA, separate from thenuclear DNAof their plant host cells and the genes in this chloroplast DNA resemble those found incyanobacteria.[71]DNA in chloroplasts codes forredoxproteins such as those found in the photosynthetic reaction centers. TheCoRR Hypothesisproposes that this co-location of genes with their gene products is required for redox regulation ofgene expression, and accounts for the persistence of DNA in bioenergeticorganelles.[72]
Except for the euglenids, which are found within theExcavata, all of these belong to theDiaphoretickes. Archaeplastida and the photosynthetic Paulinella got their plastids, which are surrounded by two membranes, through primaryendosymbiosisin two separate events, by engulfing a cyanobacterium. The plastids in all the other groups have either a red or green algal origin, and are referred to as the "red lineages" and the "green lineages". The only known exception is the ciliatePseudoblepharisma tenue, which in addition to its plastids that originated from green algae also has apurple sulfur bacteriumas symbiont. In dinoflagellates and euglenids the plastids are surrounded by three membranes, and in the remaining lines by four. Anucleomorph, remnants of the original algal nucleus located between the inner and outer membranes of the plastid, is present in the cryptophytes (from a red alga) and chlorarachniophytes (from a green alga).[73]Some dinoflagellates that lost their photosynthetic ability later regained it again through new endosymbiotic events with different algae.
While able to perform photosynthesis, many of these eukaryotic groups aremixotrophsand practiceheterotrophyto various degrees.
Early photosynthetic systems, such as those ingreenandpurple sulfurandgreenandpurple nonsulfur bacteria, are thought to have beenanoxygenic, and used various other molecules than water aselectron donors. Green and purple sulfur bacteria are thought to have usedhydrogenandsulfuras electron donors. Green nonsulfur bacteria used variousaminoand otherorganic acidsas electron donors. Purple nonsulfur bacteria used a variety of nonspecific organic molecules. The use of these molecules is consistent with the geological evidence that Earth's early atmosphere was highlyreducingatthat time.[74]
With a possible exception ofHeimdallarchaeota, photosynthesis is not found inarchaea.[75]Haloarchaeaarephotoheterotrophic; they can absorb energy from the sun, but do not harvest carbon from the atmosphere and are therefore not photosynthetic.[76]Instead of chlorophyll they use rhodopsins, which convert light-energy to ion gradients but cannot mediate electron transfer reactions.[77][78]
Inbacteriaeight photosynthetic lineages are currently known:[79][80][81][82]
The biochemical capacity to use water as the source for electrons in photosynthesis evolved once, in acommon ancestorof extantcyanobacteria(formerly called blue-green algae). The geological record indicates that this transforming event took place early in Earth's history, at least 2450–2320 million years ago (Ma), and, it is speculated, much earlier.[83][84]Because the Earth's atmosphere contained almost no oxygen during the estimated development of photosynthesis, it is believed that the first photosynthetic cyanobacteria did not generate oxygen.[85]Available evidence from geobiological studies ofArchean(>2500 Ma)sedimentary rocksindicates that life existed 3500 Ma, but the question of when oxygenic photosynthesis evolved is still unanswered. A clear paleontological window on cyanobacterialevolutionopened about 2000 Ma, revealing an already-diverse biota of cyanobacteria. Cyanobacteria remained the principalprimary producersof oxygen throughout theProterozoic Eon(2500–543 Ma), in part because the redox structure of the oceans favored photoautotrophs capable ofnitrogen fixation.[86][87]Green algaejoined cyanobacteria as the major primary producers of oxygen oncontinental shelvesnear the end of theProterozoic, but only with theMesozoic(251–66 Ma) radiations of dinoflagellates, coccolithophorids, and diatoms did theprimary productionof oxygen in marine shelf waters take modern form. Cyanobacteria remain critical tomarine ecosystemsasprimary producers of oxygenin oceanic gyres, as agents of biological nitrogen fixation, and, in modified form, as theplastidsof marine algae.[88]
Although some of the steps in photosynthesis are still not completely understood, the overall photosynthetic equation has been known since the 19th century.
Jan van Helmontbegan theresearchof theprocessin the mid-17th century when he carefully measured themassof thesoilaplantwas using and the mass of the plant as it grew. After noticing that the soil mass changed very little, hehypothesizedthat the mass of thegrowingplant must come from thewater, the onlysubstancehe added to the potted plant. His hypothesis was partiallyaccurate– much of the gained mass comes fromcarbon dioxideas well as water. However, this was a signaling point to the idea that the bulk of a plant'sbiomasscomes from the inputs of photosynthesis, not the soil itself.
Joseph Priestley, achemistandminister, discovered that when he isolated avolumeof air under an invertedjarand burned acandlein it (which gave offCO2), the candle would burn out very quickly, much before it ran out ofwax. He further discovered that amousecould similarly"injure"air. He then showed that a plant could restore the air the candle and the mouse had "injured."[89]
In 1779,Jan Ingenhouszrepeated Priestley'sexperiments. He discovered that it was the influence ofsunlighton the plant that could cause it to revive a mouse in a matter of hours.[89][90]
In 1796,Jean Senebier, a Swisspastor,botanist, andnaturalist,demonstratedthatgreen plantsconsume carbon dioxide and release oxygen under the influence oflight. Soon afterward,Nicolas-Théodore de Saussureshowed that the increase in mass of the plant as it grows could not be due only to uptake of CO2but also to the incorporation of water. Thus, the basicreactionby whichorganismsuse photosynthesis to producefood(such asglucose) was outlined.[91]
Cornelis Van Nielmade key discoveries explaining thechemistryof photosynthesis. By studyingpurple sulfur bacteriaandgreen bacteria, he was the first to demonstrate that photosynthesis is a light-dependentredox reactionin which hydrogenreduces(donates itsatomsaselectronsandprotonsto) carbon dioxide.
Robert Emersondiscovered two light reactions by testing plant productivity using different wavelengths of light. With the red alone, the light reactions were suppressed. When blue and red were combined, the output was much more substantial. Thus, there were two photosystems, one absorbing up to 600 nm wavelengths, the other up to 700 nm. The former is known as PSII, the latter is PSI. PSI contains only chlorophyll "a", PSII contains primarily chlorophyll "a" with most of the available chlorophyll "b", among other pigments. These include phycobilins, which are the red and blue pigments of red and blue algae, respectively, and fucoxanthol for brown algae and diatoms. The process is most productive when the absorption of quanta is equal in both PSII and PSI, assuring that input energy from the antenna complex is divided between the PSI and PSII systems, which in turn powers the photochemistry.[11]
Robert Hillthought that a complex of reactions consisted of an intermediate to cytochrome b6(now a plastoquinone), and that another was from cytochrome f to a step in the carbohydrate-generating mechanisms. These are linked by plastoquinone, which does require energy to reduce cytochrome f. Further experiments to prove that the oxygen developed during the photosynthesis of green plants came from water were performed by Hill in 1937 and 1939. He showed that isolatedchloroplastsgive off oxygen in the presence of unnatural reducing agents likeironoxalate,ferricyanideorbenzoquinoneafter exposure to light. In the Hill reaction:[92]
A is the electron acceptor. Therefore, in light, the electron acceptor is reduced and oxygen is evolved.Samuel RubenandMartin Kamenusedradioactive isotopesto determine that the oxygen liberated in photosynthesis came from the water.
Melvin CalvinandAndrew Benson, along withJames Bassham, elucidated the path of carbon assimilation (the photosynthetic carbon reduction cycle) in plants. The carbon reduction cycle is known as theCalvin cycle, but many scientists refer to it as the Calvin-Benson, Benson-Calvin, or even Calvin-Benson-Bassham (or CBB) Cycle.
Nobel Prize–winning scientistRudolph A. Marcuswas later able to discover the function and significance of the electron transport chain.
Otto Heinrich WarburgandDean Burkdiscovered the I-quantum photosynthesis reaction that splits CO2, activated by the respiration.[93]
In 1950, first experimental evidence for the existence ofphotophosphorylationin vivowas presented byOtto Kandlerusing intactChlorellacells and interpreting his findings as light-dependentATPformation.[94]In 1954,Daniel I. Arnonet al. discovered photophosphorylationin vitroin isolatedchloroplastswith the help of P32.[95][96]
Louis N. M. DuysensandJan Ameszdiscovered that chlorophyll "a" will absorb one light, oxidize cytochrome f, while chlorophyll "a" (and other pigments) will absorb another light but will reduce this same oxidized cytochrome, stating the two light reactions are in series.
In 1893, the American botanistCharles Reid Barnesproposed two terms,photosyntaxandphotosynthesis, for the biological process ofsynthesis of complex carbon compounds out of carbonic acid, in the presence of chlorophyll, under the influence of light. The termphotosynthesisis derived from theGreekphōs(φῶς, gleam) andsýnthesis(σύνθεσις, arranging together),[97][98][99]while another word that he designated wasphotosyntax, fromsýntaxis(σύνταξις, configuration). Over time, the termphotosynthesiscame into common usage. Later discovery of anoxygenic photosynthetic bacteria and photophosphorylation necessitated redefinition of the term.[100]
In the late 1940s at theUniversity of California, Berkeley, the details of photosynthetic carbon metabolism were sorted out by the chemistsMelvin Calvin, Andrew Benson, James Bassham and a score of students and researchers utilizing the carbon-14 isotope and paper chromatography techniques.[101]The pathway of CO2fixation by the algaeChlorellain a fraction of a second in light resulted in a three carbon molecule called phosphoglyceric acid (PGA). For that original and ground-breaking work, aNobel Prize in Chemistrywas awarded to Melvin Calvin in 1961. In parallel, plant physiologists studied leaf gas exchanges using the new method of infrared gas analysis and a leaf chamber where the net photosynthetic rates ranged from 10 to 13 μmol CO2·m−2·s−1, with the conclusion that all terrestrial plants have the same photosynthetic capacities, that are light saturated at less than 50% of sunlight.[102][103]
Later in 1958–1963 atCornell University, field grownmaizewas reported to have much greater leaf photosynthetic rates of 40 μmol CO2·m−2·s−1and not be saturated at near full sunlight.[104][105]This higher rate in maize was almost double of those observed in other species such as wheat and soybean, indicating that large differences in photosynthesis exist among higher plants. At the University of Arizona, detailed gas exchange research on more than 15 species ofmonocotsanddicotsuncovered for the first time that differences in leaf anatomy are crucial factors in differentiating photosynthetic capacities among species.[106][107]In tropical grasses, including maize, sorghum, sugarcane, Bermuda grass and in the dicot amaranthus, leaf photosynthetic rates were around 38−40 μmol CO2·m−2·s−1, and the leaves have two types of green cells, i.e. outer layer of mesophyll cells surrounding a tightly packed cholorophyllous vascular bundle sheath cells. This type of anatomy was termed Kranz anatomy in the 19th century by the botanistGottlieb Haberlandtwhile studying leaf anatomy of sugarcane.[108]Plant species with the greatest photosynthetic rates and Kranz anatomy showed no apparent photorespiration, very low CO2compensation point, high optimum temperature, high stomatal resistances and lower mesophyll resistances for gas diffusion and rates never saturated at full sun light.[109]The research at Arizona was designated a Citation Classic in 1986.[107]These species were later termed C4 plants as the first stable compound of CO2fixation in light has four carbons as malate and aspartate.[110][111][112]Other species that lack Kranz anatomy were termed C3 type such as cotton and sunflower, as the first stable carbon compound is the three-carbon PGA. At 1000 ppm CO2in measuring air, both the C3 and C4 plants had similar leaf photosynthetic rates around 60 μmol CO2·m−2·s−1indicating the suppression of photorespiration in C3 plants.[106][107]
There are four main factors influencing photosynthesis and several corollary factors. The four main are:[113]
Total photosynthesis is limited by a range of environmental factors. These include the amount of light available, the amount ofleafarea a plant has to capture light (shading by other plants is a major limitation of photosynthesis), the rate at which carbon dioxide can be supplied to thechloroplaststo support photosynthesis, the availability of water, and the availability of suitable temperatures for carrying out photosynthesis.[114]
The process of photosynthesis provides the main input of free energy into the biosphere, and is one of four main ways in which radiation is important for plant life.[115]
The radiation climate within plant communities is extremely variable, in both time and space.
In the early 20th century,Frederick BlackmanandGabrielle Matthaeiinvestigated the effects of light intensity (irradiance) and temperature on the rate of carbon assimilation.
These two experiments illustrate several important points: First, it is known that, in general,photochemicalreactions are not affected bytemperature. However, these experiments clearly show that temperature affects the rate of carbon assimilation, so there must be two sets of reactions in the full process of carbon assimilation. These are the light-dependent 'photochemical' temperature-independent stage, and the light-independent, temperature-dependent stage. Second, Blackman's experiments illustrate the concept oflimiting factors. Another limiting factor is the wavelength of light. Cyanobacteria, which reside several meters underwater, cannot receive the correct wavelengths required to cause photoinduced charge separation in conventional photosynthetic pigments. To combat this problem, Cyanobacteria have a light-harvesting complex calledPhycobilisome.[116]This complex is made up of a series of proteins with different pigments which surround the reaction center.
As carbon dioxide concentrations rise, the rate at which sugars are made by the light-independent reactions increases until limited by other factors.RuBisCO, the enzyme that captures carbon dioxide in the light-independent reactions, has a binding affinity for both carbon dioxide and oxygen. When the concentration of carbon dioxide is high, RuBisCO will fix carbon dioxide. However, if the carbon dioxide concentration is low, RuBisCO will bind oxygen instead of carbon dioxide. This process, calledphotorespiration, uses energy, but does not produce sugars.
RuBisCO oxygenase activity is disadvantageous to plants for several reasons:
The salvaging pathway for the products of RuBisCO oxygenase activity is more commonly known as photorespiration, since it is characterized by light-dependent oxygen consumption and the release of carbon dioxide.
TheRenaissance(UK:/rɪˈneɪsəns/rin-AY-sənss,US:/ˈrɛnəsɑːns/ⓘREN-ə-sahnss)[1][2][a]is aperiod of historyand a Europeancultural movementcovering the 15th and 16th centuries. It marked the transition from theMiddle Agestomodernityand was characterized by an effort to revive and surpass the ideas and achievements ofclassical antiquity. Associated with greatsocial changein most fields and disciplines, includingart,architecture, politics,literature,explorationandscience, the Renaissance was first centered in theRepublic of Florence, then spread to therest of Italyand later throughout Europe. The termrinascita("rebirth") first appeared inLives of the Artists(c.1550) byGiorgio Vasari, while the corresponding French wordrenaissancewas adopted into English as the term for this period during the 1830s.[4][b]
The Renaissance's intellectual basis was founded in its version ofhumanism, derived from the concept of Romanhumanitasand the rediscovery ofclassical Greek philosophy, such as that ofProtagoras, who said that "man is the measure of all things". Although the invention ofmetal movable typesped the dissemination of ideas from the later 15th century, the changes of the Renaissance were not uniform across Europe: the first traces appear in Italy as early as the late 13th century, in particular with the writings ofDanteand the paintings ofGiotto.
As a cultural movement, the Renaissance encompassed innovative flowering ofliterary Latinand an explosion ofvernacular literatures, beginning with the 14th-century resurgence of learning based on classical sources, which contemporaries credited toPetrarch; the development of linear perspective and other techniques of rendering a more natural reality in painting; and gradual but widespreadeducational reform. It saw myriad artistic developments and contributions from suchpolymathsasLeonardo da VinciandMichelangelo, who inspired the term "Renaissance man".[5][6]In politics, the Renaissance contributed to the development of the customs and conventions of diplomacy, and in science to an increased reliance on observation andinductive reasoning. The period also saw revolutions in other intellectual andsocial scientificpursuits, as well as the introduction of modern banking and the field of accounting.[7]
The Renaissance period started during thecrisis of the Late Middle Agesand conventionally ends with the waning ofhumanism, and the advents of theReformationandCounter-Reformation, and in art, theBaroqueperiod. It had a different period and characteristics in different regions, such as the Italian Renaissance, theNorthern Renaissance, theSpanish Renaissance, etc.
In addition to the standard periodization, proponents of a "long Renaissance" may put its beginning in the 14th century and its end in the 17th century.[c]
The traditional view focuses more on the Renaissance'searly modernaspects and argues that it was a break from the past, but many historians today focus more on its medieval aspects and argue that it was an extension of the Middle Ages.[11][12]
The beginnings of the period—the early Renaissance of the 15th century and the ItalianProto-Renaissancefrom around 1250 or 1300—overlap considerably with theLate Middle Ages, conventionally dated toc.1350–1500, and the Middle Ages themselves were a long period filled with gradual changes, like the modern age; as a transitional period between both, the Renaissance has close similarities to both, especially the late and early sub-periods of either.
The Renaissance began inFlorence, one of the many states ofItaly.[13]The Italian Renaissance concluded in 1527 whenHoly Roman Emperor Charles Vlaunched anassault on Romeduring  thewar of the League of Cognac. Nevertheless, its impact endured in the art of renowned Italian painters likeTintoretto,Sofonisba Anguissola, andPaolo Veronese, who continued their work during the mid-to-late 16th century.[14]
Various theories have been proposed to account for its origins and characteristics, focusing on a variety of factors, including Florence's social and civic peculiarities at the time: its political structure, the patronage of its dominant family, theMedici,[15]and the migration ofGreek scholarsand their texts to Italy following thefall of Constantinopleto theOttoman Empire.[16][17][18]Other major centers wereVenice,Genoa,Milan,Romeduring theRenaissance Papacy, andNaples. From Italy, the Renaissance spread throughout Europe and also to American, African and Asian territories ruled by the European colonial powers of the time or where Christian missionaries were active.
The Renaissance has a long and complexhistoriography, and in line with general skepticism of discrete periodizations, there has been much debate among historians reacting to the 19th-century glorification of the "Renaissance" and individual cultural heroes as "Renaissance men", questioning the usefulness ofRenaissanceas a term and as a historical delineation.[19]
Some observers have questioned whether the Renaissance was a cultural "advance" from the Middle Ages, instead seeing it as a period of pessimism andnostalgiaforclassical antiquity,[20]while social and economic historians, especially of thelongue durée, have instead focused onthe continuitybetween the two eras,[21]which are linked, asPanofskyobserved, "by a thousand ties".[22][d]
The word has also been extended to other historical and cultural movements, such as theCarolingian Renaissance(8th and 9th centuries),Ottonian Renaissance(10th and 11th century), and theRenaissance of the 12th century.[24]
The Renaissance was a cultural movement that profoundly affected European intellectual life in theearly modern period. Beginning in Italy, and spreading to the rest of Europe by the 16th century, its influence was felt inart,architecture,philosophy,literature,music,science,technology, politics, religion, and other aspects of intellectual inquiry. Renaissance scholars employed the humanist method in study, and searched for realism and human emotion in art.[25]
Renaissance humanistssuch asPoggio Bracciolinisought out in Europe's monastic libraries the Latin literary, historical, and oratorical texts ofantiquity, while thefall of Constantinople(1453) generated a wave ofémigréGreek scholarsbringing precious manuscripts inancient Greek, many of which had fallen into obscurity in the West. It was in their new focus on literary and historical texts that Renaissance scholars differed so markedly from the medieval scholars of theRenaissance of the 12th century, who had focused on studyingGreekandArabicworks of natural sciences, philosophy, and mathematics, rather than on such cultural texts.[citation needed]
In the revival ofneoplatonism, Renaissance humanists did not rejectChristianity; on the contrary, many of the Renaissance's greatest works were devoted to it, and the Church patronized many works of Renaissance art.[citation needed]But a subtle shift took place in the way that intellectuals approached religion that was reflected in many other areas of cultural life.[26][better source needed]In addition, many Greek Christian works, including the Greek New Testament, were brought back fromByzantiumto Western Europe and engaged Western scholars for the first time since late antiquity. This new engagement with Greek Christian works, and particularly the return to the original Greek of the New Testament promoted by humanistsLorenzo VallaandErasmus, helped pave the way for theReformation.[citation needed]
Well after the first artistic return toclassicismhad been exemplified in the sculpture ofNicola Pisano, Florentine painters led byMasacciostrove to portray the human form realistically, developing techniques to renderperspectiveand light more naturally.Political philosophers, most famouslyNiccolò Machiavelli, sought to describe political life as it really was, that is to understand it rationally. A critical contribution to Italian Renaissance humanism,Giovanni Pico della MirandolawroteDe hominis dignitate(Oration on the Dignity of Man, 1486), a series of theses on philosophy, natural thought, faith, and magic defended against any opponent on the grounds of reason. In addition to studying classical Latin and Greek, Renaissance authors also began increasingly to use vernacular languages; combined with the introduction of theprinting press, this allowed many more people access to books, especially the Bible.[27]
In all, the Renaissance can be viewed as an attempt by intellectuals to study and improve the secular and worldly, both through the revival of ideas from antiquity and through novel approaches to thought. Political philosopherHans Kohndescribes it as an age where "Men looked for new foundations"; some likeErasmusandThomas Moreenvisioned new reformed spiritual foundations, others. in the words ofMachiavelli,una lunga sperienza delle cose moderne ed una continua lezione delle antiche(a long experience with modern life and a continuous learning from antiquity).[28]
SociologistRodney Starkplays down the Renaissance in favor of the earlier innovations of theItalian city-statesin theHigh Middle Ages, which married responsive government, Christianity and the birth ofcapitalism.[29]This analysis argues that, whereas the great European states (France and Spain) wereabsolute monarchies, and others were under direct Church control, the independentcity-republicsof Italy took over the principles of capitalism invented on monastic estates and set off a vast unprecedentedCommercial Revolutionthat preceded and financed the Renaissance.[citation needed]
HistorianLeon Poliakovoffers a critical view in his seminal study of European racist thought:The Aryan Myth. According to Poliakov, the use of ethnic origin myths are first used by Renaissance humanists "in the service of a new born chauvinism".[30][31]
Many argue that the ideas characterizing the Renaissance had their origin inFlorenceat the turn of the 13th and 14th centuries, in particular with the writings ofDante Alighieri(1265–1321) andPetrarch(1304–1374), as well as the paintings ofGiotto di Bondone(1267–1337). Some writers date the Renaissance quite precisely; one proposed starting point is 1401, when the rival geniusesLorenzo GhibertiandFilippo Brunelleschicompeted for the contract to build the bronze doors for theBaptisteryof theFlorence Cathedral(Ghiberti won).[32]Others see more general competition between artists and polymaths such as Brunelleschi, Ghiberti,Donatello, andMasacciofor artistic commissions as sparking the creativity of the Renaissance.
Yet it remains much debated why the Renaissance began in Italy, and why it began when it did. Accordingly, several theories have been put forward to explain its origins. Peter Rietbergen posits that various influential Proto-Renaissance movements started from roughly 1300 onwards across many regions ofEurope.[33]
In stark contrast to theHigh Middle Ages, when Latin scholars focused almost entirely on studying Greek and Arabic works of natural science, philosophy and mathematics,[e]Renaissance scholars were most interested in recovering and studying Latin and Greek literary, historical, and oratorical texts. Broadly speaking, this began in the 14th century with a Latin phase, when Renaissance scholars such asPetrarch,Coluccio Salutati(1331–1406),Niccolò de' Niccoli(1364–1437), andPoggio Bracciolini(1380–1459) scoured the libraries of Europe in search of works by such Latin authors asCicero,Lucretius,Livy, andSeneca.[34]By the early 15th century, the bulk of the surviving such Latin literature had been recovered; the Greek phase of Renaissance humanism was under way, as Western European scholars turned to recovering ancient Greek literary, historical, oratorical and theological texts.[35]
Unlike with Latin texts, which had been preserved and studied in Western Europe since late antiquity, the study of ancient Greek texts was very limited in medieval Western Europe. Ancient Greek works on science, mathematics, and philosophy had been studied since theHigh Middle Agesin Western Europe and in theIslamic Golden Age(normally in translation), but Greek literary, oratorical and historical works (such asHomer, the Greek dramatists,DemosthenesandThucydides) were not studied in either the Latin or medievalIslamic worlds; in the Middle Ages these sorts of texts were only studied by Byzantine scholars. Some argue that theTimurid RenaissanceinSamarkandandHerat, whose magnificence toned with Florence as the center of a cultural rebirth,[36][37]were linked to theOttoman Empire, whose conquests led to the migration ofGreek scholarsto Italian cities.[38][full citation needed][39][full citation needed][16][40]One of the greatest achievements of Renaissance scholars was to bring this entire class of Greek cultural works back into Western Europe for the first time since late antiquity.
Muslimlogicians, most notablyAvicennaandAverroes, had inherited Greek ideas after they had invaded and conqueredEgyptand theLevant. Their translations and commentaries on these ideas worked their way through the Arab West intoIberiaandSicily, which became important centers for this transmission of ideas. Between the 11th and 13th centuries, many schools dedicated to the translation of philosophical and scientific works fromClassical ArabictoMedieval Latinwere established in Iberia, most notably theToledo School of Translators. This work of translation from Islamic culture, though largely unplanned and disorganized, constituted one of the greatest transmissions of ideas in history.[41]
The movement to reintegrate the regular study of Greek literary, historical, oratorical, and theological texts back into the Western European curriculum is usually dated to the 1396 invitation from Coluccio Salutati to the Byzantine diplomat and scholarManuel Chrysoloras(c. 1355–1415) to teach Greek in Florence.[42]This legacy was continued by a number of expatriate Greek scholars, fromBasilios BessariontoLeo Allatius.
The unique political structures ofItalyduring theLate Middle Ageshave led some to theorize that its unusual social climate allowed the emergence of a rare cultural efflorescence. Italy did not exist as apolitical entityin the early modern period. Instead, it was divided into smallercity-statesand territories: theNeapolitanscontrolled the south, theFlorentinesand theRomansat the center, theMilaneseand theGenoeseto the north and west respectively, and theVenetiansto the north east. 15th-century Italy was one of the mosturbanizedareas in Europe.[43]Many of its cities stood among the ruins of ancient Roman buildings; it seems likely that the classical nature of the Renaissance was linked to its origin in the Roman Empire's heartland.[44]
Historian and political philosopherQuentin Skinnerpoints out thatOtto of Freising(c. 1114–1158), a German bishop visiting north Italy during the 12th century, noticed a widespread new form of political and social organization, observing that Italy appeared to have exited fromfeudalismso that its society was based on merchants and commerce. Linked to this was anti-monarchical thinking, represented in the famous early RenaissancefrescocycleThe Allegory of Good and Bad GovernmentbyAmbrogio Lorenzetti(painted 1338–1340), whose strong message is about the virtues of fairness, justice, republicanism and good administration. Holding both Church andEmpireat bay, these city republics were devoted to notions of liberty. Skinner reports that there were many defences of liberty such as theMatteo Palmieri(1406–1475) celebration of Florentine genius not only in art, sculpture and architecture, but "the remarkable efflorescence of moral, social and political philosophy that occurred in Florence at the same time".[45]
Even cities and states beyond central Italy, such as the Republic of Florence at this time, were also notable for theirmerchant republics, especially the Republic of Venice. Although in practice these wereoligarchical, and bore little resemblance to a moderndemocracy, they did have democratic features and were responsive states, with forms of participation in governance and belief in liberty.[45][46][47]The relative political freedom they afforded was conducive to academic and artistic advancement.[48]Likewise, the position of Italian cities such as Venice as great trading centres made them intellectual crossroads.Merchantsbrought with them ideas from far corners of the globe, particularly theLevant. Venice was Europe's gateway to trade with the East, and a producer offine glass, while Florence was a capital of textiles. The wealth such business brought to Italy meant large public and private artistic projects could be commissioned and individuals had more leisure time for study.[48]
One theory that has been advanced is that the devastation inFlorencecaused by theBlack Death, which hit Europe between 1348 and 1350, resulted in a shift in the world view of people in 14th century Italy.Italywas particularly badly hit by the plague, and it has been speculated that the resulting familiarity with death caused thinkers to dwell more on their lives on Earth, rather than onspiritualityand theafterlife.[49]It has also been argued that the Black Death prompted a new wave of piety, manifested in thesponsorshipof religious works of art.[50]However, this does not fully explain why the Renaissance occurred specifically in Italy in the 14th century. The Black Death was apandemicthat affected all of Europe in the ways described, not only Italy. The Renaissance's emergence in Italy was most likely the result of the complex interaction of the above factors.[19]
The plague was carried by fleas on sailing vessels returning from the ports of Asia, spreading quickly due to lack of proper sanitation: the population ofEngland, then about 4.2 million, lost 1.4 million people to thebubonic plague. Florence's population was nearly halved in the year 1348. As a result of the decimation in the populace the value of the working class increased, and commoners came to enjoy more freedom. To answer the increased need for labor, workers traveled in search of the most favorable position economically.[51]
The demographic decline due to the plague had economic consequences: the prices of food dropped and land values declined by 30–40% in most parts of Europe between 1350 and 1400.[52]Landholders faced a great loss, but for ordinary men and women it was a windfall. The survivors of the plague found not only that the prices of food were cheaper but also that lands were more abundant, and many of them inherited property from their dead relatives.
The spread of disease was significantly more rampant in areas of poverty.Epidemicsravaged cities, particularly children. Plagues were easily spread by lice, unsanitary drinking water, armies, or by poor sanitation. Children were hit the hardest because many diseases, such astyphusandcongenital syphilis, target the immune system, leaving young children without a fighting chance. Children in city dwellings were more affected by the spread of disease than the children of the wealthy.[53]
The Black Death caused greater upheaval to Florence's social and political structure than later epidemics. Despite a significant number of deaths among members of the ruling classes, the government of Florence continued to function during this period. Formal meetings of elected representatives were suspended during the height of the epidemic due to the chaotic conditions in the city, but a small group of officials was appointed to conduct the affairs of the city, which ensured continuity of government.[54]
It has long been a matter of debate why the Renaissance began inFlorence, and not elsewhere in Italy. Scholars have noted several features unique to Florentine cultural life that may have caused such a cultural movement. Many have emphasized the role played by theMedici, a banking family and laterducal ruling house, in patronizing and stimulating the arts. Some historians have postulated that Florence was the birthplace of the Renaissance as a result of luck, i.e., because "Great Men" were born there by chance:[55]Leonardo, Botticelli and Michelangelo were all born inTuscany. Arguing that such chance seems improbable, other historians have contended that these "Great Men" were only able to rise to prominence because of the prevailing cultural conditions at the time.[56]
Lorenzo de' Medici(1449–1492) was the catalyst for an enormous amount of arts patronage, encouraging his countrymen to commission works from the leading artists of Florence, includingLeonardo da Vinci,Sandro Botticelli, andMichelangelo Buonarroti.[15]Works byNeri di Bicci, Botticelli, Leonardo, andFilippino Lippihad been commissioned additionally by the Convent of San Donato in Scopeto in Florence.[57]
The Renaissance was certainly underway before Lorenzo de' Medici came to power – indeed, before the Medici family itself achieved hegemony in Florentine society.
In some ways,Renaissance humanismwas not a philosophy but a method of learning. In contrast to the medievalscholasticmode, which focused on resolving contradictions between authors, Renaissance humanists would study ancient texts in their original languages and appraise them through a combination of reasoning andempirical evidence. Humanist education was based on the programme ofStudia Humanitatis, the study of five humanities:poetry,grammar,history,moral philosophy, andrhetoric. Although historians have sometimes struggled to define humanism precisely, most have settled on "a middle of the road definition... the movement to recover, interpret, and assimilate the language, literature, learning and values of ancient Greece and Rome".[58]Above all, humanists asserted "the genius of man ... the unique and extraordinary ability of the human mind".[59]
Humanist scholars shaped the intellectual landscape throughout the early modern period. Political philosophers such as Niccolò Machiavelli andThomas Morerevived the ideas of Greek and Roman thinkers and applied them in critiques of contemporary government, following the Islamic steps ofIbn Khaldun.[61][62]Pico della Mirandolawrote the "manifesto" of the Renaissance, theOration on the Dignity of Man, a vibrant defence of thinking.[citation needed]Matteo Palmieri(1406–1475), another humanist, is most known for his workDella vita civile("On Civic Life"; printed 1528), which advocatedcivic humanism, and for his influence in refining theTuscan vernacularto the same level as Latin. Palmieri drew on Roman philosophers and theorists, especiallyCicero, who, like Palmieri, lived an active public life as a citizen and official, as well as a theorist and philosopher and alsoQuintilian. Perhaps the most succinct expression of his perspective on humanism is in a 1465 poetic workLa città di vita, but an earlier work,Della vita civile, is more wide-ranging. Composed as a series of dialogues set in a country house in the Mugello countryside outside Florence during the plague of 1430, Palmieri expounds on the qualities of the ideal citizen. The dialogues include ideas about how children develop mentally and physically, how citizens can conduct themselves morally, how citizens and states can ensure probity in public life, and an important debate on the difference between that which is pragmatically useful and that which is honest.[citation needed]
The humanists believed that it is important to transcend to the afterlife with a perfect mind and body, which could be attained with education. The purpose of humanism was to create a universal man whose person combined intellectual and physical excellence and who was capable of functioning honorably in virtually any situation.[63]This ideology was referred to as theuomo universale, an ancient Greco-Roman ideal. Education during the Renaissance was mainly composed of ancient literature and history as it was thought that the classics provided moral instruction and an intensive understanding of human behavior.
A unique characteristic of some Renaissance libraries is that they were open to the public. These libraries were places where ideas were exchanged and where scholarship and reading were considered both pleasurable and beneficial to the mind and soul. As freethinking was a hallmark of the age, many libraries contained a wide range of writers. Classical texts could be found alongside humanist writings. These informal associations of intellectuals profoundly influenced Renaissance culture. An essential tool of Renaissance librarianship was the catalog that listed, described, and classified a library's books.[64]Some of the richest "bibliophiles" built libraries as temples to books and knowledge. A number of libraries appeared as manifestations of immense wealth joined with a love of books. In some cases, cultivated library builders were also committed to offering others the opportunity to use their collections. Prominent aristocrats and princes of the Church created great libraries for the use of their courts, called "court libraries", and were housed in lavishly designed monumental buildings decorated with ornate woodwork, and the walls adorned with frescoes (Murray, Stuart A.P.).
Renaissance art marks a cultural rebirth at the close of the Middle Ages and rise of the Modern world. One of the distinguishing features of Renaissance art was its development of highly realistic linear perspective.Giotto di Bondone(1267–1337) is credited with first treating a painting as a window into space, but it was not until the demonstrations of architectFilippo Brunelleschi(1377–1446) and the subsequent writings ofLeon Battista Alberti(1404–1472) that perspective was formalized as an artistic technique.[65]
The development ofperspectivewas part of a wider trend towardrealismin the arts.[66]Painters developed other techniques, studying light, shadow, and, famously in the case ofLeonardo da Vinci,human anatomy. Underlying these changes in artistic method was a renewed desire to depict the beauty of nature and to unravel the axioms ofaesthetics, with the works of Leonardo,MichelangeloandRaphaelrepresenting artistic pinnacles that were much imitated by other artists.[67]Other notable artists includeSandro Botticelli, working for the Medici in Florence,Donatello, another Florentine, andTitianin Venice, among others.
In theLow Countries, a particularly vibrant artistic culture developed. The work ofHugo van der GoesandJan van Eyckwas particularly influential on the development of painting in Italy, both technically with the introduction ofoil paintand canvas, and stylistically in terms of naturalism in representation. Later, the work ofPieter Brueghel the Elderwould inspire artists to depict themes of everyday life.[68]
In architecture, Filippo Brunelleschi was foremost in studying the remains of ancient classical buildings. With rediscovered knowledge from the 1st-century writerVitruviusand the flourishing discipline of mathematics, Brunelleschi formulated the Renaissance style that emulated and improved on classical forms. His major feat of engineering was building the dome ofFlorence Cathedral.[69]Another building demonstrating this style is theBasilica of Sant'Andrea, Mantua, built by Alberti. The outstanding architectural work of theHigh Renaissancewas the rebuilding ofSt. Peter's Basilica, combining the skills ofBramante, Michelangelo, Raphael,SangalloandMaderno.
During the Renaissance, architects aimed to use columns,pilasters, andentablaturesas an integrated system. The Roman orders types of columns are used:TuscanandComposite. These can either be structural, supporting an arcade or architrave, or purely decorative, set against a wall in the form of pilasters. One of the first buildings to use pilasters as an integrated system was in the Old Sacristy (1421–1440) by Brunelleschi.[70]Arches, semi-circular or (in theManneriststyle) segmental, are often used in arcades, supported on piers or columns with capitals. There may be a section of entablature between the capital and the springing of the arch. Alberti was one of the first to use the arch on a monumental. Renaissance vaults do not have ribs; they are semi-circular or segmental and on a square plan, unlike theGothicvault, which is frequently rectangular.
Renaissance artists were not pagans, although they admired antiquity and kept some ideas and symbols of the medieval past.Nicola Pisano(c. 1220 – c. 1278) imitated classical forms by portraying scenes from the Bible. HisAnnunciation, from thePisa Baptistry, demonstrates that classical models influenced Italian art before the Renaissance took root as a literary movement.[71]
Applied innovation extended to commerce. At the end of the 15th century,Luca Paciolipublished the first work onbookkeeping, making him the founder ofaccounting.[7]
The rediscovery of ancient texts and the invention of theprinting pressin about 1440 democratized learning and allowed a faster propagation of more widely distributed ideas. In the first period of theItalian Renaissance, humanists favored the study ofhumanitiesovernatural philosophyorapplied mathematics, and their reverence for classical sources further enshrined theAristotelianandPtolemaicviews of the universe. Writing around 1450,Nicholas of Cusaanticipated theheliocentricworldview ofCopernicus, but in a philosophical fashion.
Science and art were intermingled in the early Renaissance, with polymath artists such asLeonardo da Vincimaking observational drawings of anatomy and nature. Leonardo set up controlled experiments in water flow, medical dissection, and systematic study of movement and aerodynamics, and he devised principles of research method that ledFritjof Caprato classify him as the "father of modern science".[g]Other examples of Da Vinci's contribution during this period include machines designed to saw marbles and lift monoliths, and new discoveries in acoustics, botany, geology, anatomy, and mechanics.[74]
A suitable environment had developed to question classical scientific doctrine. Thediscoveryin 1492 of theNew WorldbyChristopher Columbuschallenged the classical worldview. The works of Ptolemy (in geography) andGalen(in medicine) were found to not always match everyday observations. As the Reformation andCounter-Reformationclashed, theNorthern Renaissanceshowed a decisive shift in focus from Aristotelean natural philosophy to chemistry and the biological sciences (botany, anatomy, and medicine).[75]The willingness to question previously held truths and search for new answers resulted in a period of major scientific advancements.
Some view this as a "scientific revolution", heralding the beginning of the modern age,[76]others as an acceleration of a continuous process stretching from the ancient world to the present day.[77]Significant scientific advances were made during this time byGalileo Galilei,Tycho Brahe, andJohannes Kepler.[78]Copernicus, inDe revolutionibus orbium coelestium(On the Revolutions of the Heavenly Spheres), posited that the Earth moved around the Sun.De humani corporis fabrica(On the Workings of the Human Body) byAndreas Vesalius, gave a new confidence to the role ofdissection, observation, and themechanisticview of anatomy.[79]
Another important development was in theprocessfor discovery, thescientific method,[79]focusing onempirical evidenceand the importance ofmathematics, while discarding much of Aristotelian science. Early and influential proponents of these ideas included Copernicus, Galileo, andFrancis Bacon.[80][81]The new scientific method led to great contributions in the fields of astronomy, physics, biology, and anatomy.[h][82]
During the Renaissance, extending from 1450 to 1650,[83]every continent was visited and mostly mapped by Europeans, except the south polar continent now known asAntarctica. This development is depicted in the large world mapNova Totius Terrarum Orbis Tabulamade by the Dutch cartographerJoan Blaeuin 1648 to commemorate thePeace of Westphalia.
In 1492,Christopher Columbussailed across the Atlantic Ocean from Spain seeking a direct route to India of theDelhi Sultanate. He accidentally stumbled upon the Americas, but believed he had reached theEast Indies.
In 1606, the Dutch navigatorWillem Janszoonsailed from the East Indies in theDutch East India CompanyshipDuyfkenand landed inAustralia. He charted about 300 km of the west coast ofCape York Peninsulain Queensland. More than thirty Dutch expeditions followed, mapping sections of the north, west, and south coasts. In 1642–1643,Abel Tasmancircumnavigated the continent, proving that it was not joined to the imagined south polar continent.
By 1650, Dutch cartographers had mapped most of the coastline of the continent, which they namedNew Holland, except the east coast which was charted in 1770 byJames Cook.
The long-imagined south polar continent was eventually sighted in 1820. Throughout the Renaissance it had been known asTerra Australis, or 'Australia' for short. However, after that name was transferred to New Holland in the nineteenth century, the new name of 'Antarctica' was bestowed on the south polar continent.[84]
From this changing society emerged a common, unifying musical language, in particular thepolyphonicstyle of theFranco-Flemishschool. The development ofprintingmade distribution of music possible on a wide scale. Demand for music as entertainment and as an activity for educated amateurs increased with the emergence of a bourgeois class. Dissemination ofchansons,motets, andmassesthroughout Europe coincided with the unification of polyphonic practice into the fluid style that culminated in the second half of the sixteenth century in the work of composers such asGiovanni Pierluigi da Palestrina,Orlande de Lassus,Tomás Luis de Victoria, andWilliam Byrd.
The new ideals of humanism, although more secular in some aspects, developed against a Christian backdrop, especially in theNorthern Renaissance. Much, if not most, of the new art was commissioned by or in dedication to theRoman Catholic Church.[26]However, the Renaissance had a profound effect on contemporarytheology, particularly in the way people perceived the relationship between man and God.[26]Many of the period's foremost theologians were followers of the humanist method, includingErasmus,Huldrych Zwingli,Thomas More,Martin Luther, andJohn Calvin.
The Renaissance began in times of religious turmoil. The Late Middle Ages was a period of political intrigue surrounding thePapacy, culminating in theWestern Schism, in which three men simultaneously claimed to be trueBishopofRome.[85]While the schism was resolved by theCouncil of Constance(1414), a resulting reform movement known asConciliarismsought to limit the power of the pope. Although the papacy eventually emerged supreme in ecclesiastical matters by theFifth Council of the Lateran(1511), it was dogged by continued accusations of corruption, most famously in the person ofPope Alexander VI, who was accused variously ofsimony,nepotism, andfathering children(most of whom were married off, presumably for the consolidation of power) while acardinal.[86]
Churchmen such as Erasmus and Luther proposed reform to the Church, often based on humanisttextual criticismof theNew Testament.[26]In October 1517, Luther published theNinety-five Theses, challenging papal authority and criticizing its perceived corruption, particularly with regard to instances of soldindulgences.[i]The 95 Theses led to theReformation, a break with the Roman Catholic Church that previously claimed hegemony inWestern Europe. Humanism and the Renaissance therefore played a direct role in sparking the Reformation, as well as in many other contemporaneous religious debates and conflicts.
Pope Paul IIIcame to the papal throne (1534–1549) after thesack of Rome in 1527, with uncertainties prevalent in the Catholic Church following the Reformation. Nicolaus Copernicus dedicatedDe revolutionibus orbium coelestium(On the Revolutions of the Celestial Spheres) to Paul III, who became the grandfather ofAlessandro Farnese, who had paintings byTitian,Michelangelo, andRaphael, as well as an important collection of drawings, and who commissioned the masterpiece ofGiulio Clovio, arguably the last majorilluminated manuscript, theFarnese Hours.
By the 15th century, writers, artists, and architects in Italy were well aware of the transformations that were taking place and were using phrases such asmodi antichi(in the antique manner) oralle romana et alla antica(in the manner of the Romans and the ancients) to describe their work. In the 1330sPetrarchreferred to pre-Christian times asantiqua(ancient) and to the Christian period asnova(new).[87]From Petrarch's Italian perspective, this new period (which included his own time) was an age of national eclipse.[87]Leonardo Bruniwas the first to use tripartiteperiodizationin hisHistory of the Florentine People(1442).[88]Bruni's first two periods were based on those of Petrarch, but he added a third period because he believed that Italy was no longer in a state of decline.Flavio Biondoused a similar framework inDecades of History from the Deterioration of the Roman Empire(1439–1453).
Humanist historians argued that contemporary scholarship restored direct links to the classical period, thus bypassing the Medieval period, which they then named for the first time the "Middle Ages". The term first appears in Latin in 1469 asmedia tempestas(middle times).[89]The termrinascita(rebirth) first appeared, however, in its broad sense inGiorgio Vasari'sLives of the Artists, 1550, revised 1568.[90][91]Vasari divides the age into three phases: the first phase containsCimabue,Giotto, andArnolfo di Cambio; the second phase containsMasaccio,Brunelleschi, andDonatello; the third centers onLeonardo da Vinciand culminates withMichelangelo. It was not just the growing awareness of classical antiquity that drove this development, according to Vasari, but also the growing desire to study and imitate nature.[92]
In the 15th century, the Renaissance spread rapidly from its birthplace in Florence to the rest of Italy and soon to the rest of Europe. The invention of theprinting pressby German printerJohannes Gutenbergallowed the rapid transmission of these new ideas. As it spread, its ideas diversified and changed, being adapted to local culture. In the 20th century, scholars began to break the Renaissance into regional and national movements.
TheElizabethan erain the second half of the 16th century is usually regarded as the height of the English Renaissance. Many scholars see its beginnings in the early 16th century during the reign ofHenry VIII.[93]
The English Renaissance is different from theItalian Renaissancein several ways. The dominant art forms of the English Renaissance wereliteratureandmusic, which had a rich flowering.[94]Visual artsin the English Renaissance were much less significant than in the Italian Renaissance. The English Renaissance period in art began far later than the Italian, which had moved intoMannerismby the 1530s.[95]
In literature the later part of the 16th century saw the flowering ofElizabethan literature, with poetry heavily influenced byItalian Renaissance literaturebutElizabethan theatrea distinctive native style. Writers includeWilliam Shakespeare(1564–1616),Christopher Marlowe(1564–1593),Edmund Spenser(1552–1599), SirThomas More(1478–1535), and SirPhilip Sidney(1554–1586).English Renaissance musiccompeted with that in Europe with composers such asThomas Tallis(1505–1585),John Taverner(1490–1545), andWilliam Byrd(1540–1623).Elizabethan architectureproduced the largeprodigy housesof courtiers, and in the next centuryInigo Jones(1573–1652), who introducedPalladian architectureto England.[96]
Elsewhere, SirFrancis Bacon(1561–1626) was the pioneer of modern scientific thought, and is commonly regarded as one of the founders of theScientific Revolution.[97][98]
The word "Renaissance" is borrowed from the French language, where it means "re-birth". It was first used in the eighteenth century and was later popularized by FrenchhistorianJules Michelet(1798–1874) in his 1855 work,Histoire de France(History of France).[99][100]
In 1495 theItalian Renaissancearrived in France, imported by KingCharles VIIIafter his invasion of Italy. A factor that promoted the spread of secularism was the inability of the Church to offer assistance against theBlack Death.Francis Iimported Italian art and artists, includingLeonardo da Vinci,Primaticcio,Rosso Fiorentino,Niccolò dell'AbbateandBenvenuto Celliniand built ornate palaces at great expense, like thePalace of Fontainebleauand thecastle of Chambord. Writers such asFrançois Rabelais,Pierre de Ronsard,Joachim du Bellay, andMichel de Montaigne, painters such asJean ClouetandFrançois Clouet, and musicians such asJean Moutonalso borrowed from the spirit of the Renaissance. French Renaissance sculptors includeMichel Colombe,Jean Goujon,Pierre Bontemps,Ligier RichierandGermain Pilonwhile important architects of the time werePierre Lescot, who built the Henri II aisle of theLouvre,Philibert DelormeandJacques I Androuet du Cerceau.
In 1533, a fourteen-year-oldCatherine de' Medici(1519–1589), born in Florence toLorenzo de' Medici, Duke of UrbinoandMadeleine de La Tour d'Auvergne, marriedHenry II of France, second son of King Francis I and QueenClaude. Though she became famous and infamous for her role in theFrench Wars of Religion, she made a direct contribution in bringing arts, sciences, and music (including the origins ofballet) to the French court from her native Florence.
In the second half of the 15th century, the Renaissance spirit spread toGermanyand theLow Countries, where the development of the printing press (ca. 1450) and Renaissance artists such asAlbrecht Dürer(1471–1528) predated the influence from Italy. In the early Protestant areas of the countryhumanismbecame closely linked to the turmoil of the Reformation, and the art and writing of theGerman Renaissancefrequently reflected this dispute.[101]However, theGothic styleand medieval scholastic philosophy remained exclusively until the turn of the 16th century. EmperorMaximilian IofHabsburg(ruling 1493–1519) was the first truly Renaissance monarch of theHoly Roman Empire.
After Italy,Hungarywas the first European country where the Renaissance appeared.[102]The Renaissance style came directly from Italy during theQuattrocento(1400s) to Hungary first in the Central European region, thanks to the development of early Hungarian-Italian relationships — not only in dynastic connections, but also in cultural, humanistic and commercial relations – growing in strength from the 14th century. The relationship between Hungarian and Italian Gothic styles was a second reason – exaggerated breakthrough of walls is avoided, preferring clean and light structures. Large-scale building schemes provided ample and long term work for the artists, for example, the building of the Friss (New)Castle in Buda, the castles of Visegrád,Tata, and Várpalota. InSigismund's court there were patrons such asPippo Spano, a descendant of the Scolari family of Florence, who invited Manetto Ammanatini andMasolino da Pannicaleto Hungary.[103]
The new Italian trend combined with existing national traditions to create a particular local Renaissance art. Acceptance of Renaissance art was furthered by the continuous arrival of humanist thought in the country. Many young Hungarians studying at Italian universities came closer to theFlorentinehumanist center, so a direct connection with Florence evolved. The growing number of Italian traders moving to Hungary, specially toBuda, helped this process. New thoughts were carried by the humanist prelates, among themVitéz János, archbishop ofEsztergom, one of the founders of Hungarian humanism.[104]During the long reign of Emperor Sigismund of Luxemburg the Royal Castle of Buda became probably the largestGothicpalace of the lateMiddle Ages. KingMatthias Corvinus(r. 1458–1490) rebuilt the palace in early Renaissance style and further expanded it.[105][106]
After the marriage in 1476 of King Matthias toBeatrice of Naples,Budabecame one of the most important artistic centers of the Renaissance north of theAlps.[107]The most important humanists living in Matthias' court wereAntonio Bonfiniand the famous Hungarian poetJanus Pannonius.[107]András Hessset up a printing press in Buda in 1472. Matthias Corvinus's library, theBibliotheca Corviniana, was Europe's greatest collections of secular books: historical chronicles, philosophic and scientific works in the 15th century. His library was second only in size to theVatican Library. (However, the Vatican Library mainly contained Bibles and religious materials.)[108]In 1489, Bartolomeo della Fonte of Florence wrote that Lorenzo de' Medici founded his own Greek-Latin library encouraged by the example of the Hungarian king. Corvinus's library is part of UNESCO World Heritage.[109]
Matthias started at least two major building projects.[110]The works in Buda andVisegrádbegan in about 1479.[111]Two new wings and ahanging gardenwere built at the royal castle of Buda, and the palace at Visegrád was rebuilt in Renaissance style.[111][112]Matthias appointed the ItalianChimenti Camiciaand the DalmatianGiovanni Dalmatato direct these projects.[111]Matthias commissioned the leading Italian artists of his age to embellish his palaces: for instance, the sculptorBenedetto da Majanoand the paintersFilippino LippiandAndrea Mantegnaworked for him.[113]A copy of Mantegna's portrait of Matthias survived.[114]Matthias also hired the Italian military engineerAristotele Fioravantito direct the rebuilding of the forts along the southern frontier.[115]He had new monasteries built inLate Gothicstyle for theFranciscansin Kolozsvár,Szegedand Hunyad, and for thePaulinesin Fejéregyháza.[116][117]In the spring of 1485,Leonardo da Vincitravelled toHungaryon behalf of Sforza to meet King Matthias Corvinus, and was commissioned by him to paint aMadonna.[118]
Matthias enjoyed the company of Humanists and had lively discussions on various topics with them.[119]The fame of his magnanimity encouraged many scholars—mostly Italian—to settle in Buda.[120]Antonio Bonfini,Pietro Ranzano, Bartolomeo Fonzio, andFrancesco Bandinispent many years in Matthias's court.[121][119]This circle of educated men introduced the ideas ofNeoplatonismto Hungary.[122][123]Like all intellectuals of his age, Matthias was convinced that the movements and combinations of the stars and planets exercised influence on individuals' life and on the history of nations.[124]Martius Galeottidescribed him as "king and astrologer", and Antonio Bonfini said Matthias "never did anything without consulting the stars".[125]Upon his request, the famous astronomers of the age,Johannes RegiomontanusandMarcin Bylica, set up an observatory in Buda and installed it withastrolabesandcelestial globes.[126]Regiomontanus dedicated his book on navigation that was used byChristopher Columbusto Matthias.[120]
Other important figures of Hungarian Renaissance includeBálint Balassi(poet),Sebestyén Tinódi Lantos(poet),Bálint Bakfark(composer and lutenist), andMaster MS(fresco painter).
Culture in the Netherlands at the end of the 15th century was influenced by the Italian Renaissance through trade viaBruges, which made Flanders wealthy. Its nobles commissioned artists who became known across Europe.[127]In science, theanatomistAndreas Vesaliusled the way; incartography,Gerardus Mercator's map assisted explorers and navigators. In art,Dutch and Flemish Renaissance paintingranged from the strange work ofHieronymus Bosch[128]to the everyday life depictions ofPieter Brueghel the Elder.[127]
Erasmus was arguably the Netherlands' best known humanist and Catholic intellectual during the Renaissance.[33]
The Renaissance in Northern Europe has been termed the "Northern Renaissance". While Renaissance ideas were moving north from Italy, there was a simultaneous southward spread of some areas of innovation, particularly inmusic.[129]The music of the 15th-centuryBurgundian Schooldefined the beginning of the Renaissance in music, and thepolyphonyof theNetherlanders, as it moved with the musicians themselves into Italy, formed the core of the first true international style inmusicsince the standardization ofGregorian Chantin the 9th century.[129]The culmination of the Netherlandish school was in the music of the ItaliancomposerGiovanni Pierluigi da Palestrina. At the end of the 16th century Italy again became a center of musical innovation, with the development of the polychoral style of theVenetian School, which spread northward into Germany around 1600. InDenmark, the Renaissance sparked the translation of the works ofSaxo GrammaticusintoDanishas well asFrederick IIandChristian IVordering the redecoration or construction of several important works of architecture, i.e.Kronborg,RosenborgandBørsen.[130]Danish astronomerTycho Brahegreatly contributed to turn astronomy into the firstmodern scienceand also helped launch theScientific Revolution.[131][132]
The paintings of the Italian Renaissance differed from those of the Northern Renaissance. Italian Renaissance artists were among the first to paint secular scenes, breaking away from the purely religious art of medieval painters. Northern Renaissance artists initially remained focused on religious subjects, such as the contemporary religious upheaval portrayed byAlbrecht Dürer. Later, the works ofPieter Bruegel the Elderinfluenced artists to paint scenes of daily life rather than religious or classical themes. It was also during the Northern Renaissance thatFlemishbrothersHubertandJan van Eyckperfected theoil paintingtechnique, which enabled artists to produce strong colors on a hard surface that could survive for centuries.[133]A feature of the Northern Renaissance was its use of the vernacular in place of Latin or Greek, which allowed greater freedom of expression. This movement had started in Italy with the decisive influence ofDante Alighierion the development of vernacular languages; in fact the focus on writing in Italian has neglected a major source of Florentine ideas expressed in Latin.[134]The spread of the printing press technology boosted the Renaissance in Northern Europe as elsewhere, with Venice becoming a world center of printing.
The Polish Renaissance lasted from the late 15th to the late 16th century and was theGolden AgeofPolish culture. Ruled by theJagiellonian dynasty, theKingdom of Poland(from 1569 known as thePolish–Lithuanian Commonwealth) actively participated in the broad European Renaissance. An early Italian humanist who came to Poland in the mid-15th century wasFilippo Buonaccorsi, who was employed as royal advisor and councillor. The tomb ofJohn I Albert, completed in 1505 byFrancesco Fiorentino, is the first example of a Renaissance composition in the country.[135][136]Many Italian artists subsequently came to Poland withBona SforzaofMilan, when she married KingSigismund Iin 1518.[137]This was supported by temporarily strengthened monarchies in both areas, as well as by newly established universities.[138]
The Renaissance was a period when the multi-national Polish state experienced a substantial period of cultural growth thanks in part to a century without major wars, aside from conflicts in the sparsely populatedeastern and southern borderlands. Architecture became more refined and decorative.Mannerismplayed an important part in shaping what is now considered to be the truly Polish architectural style – highatticsabove thecornicewith pinnacles andpilasters.[139]It was also the time when the first major works ofPolish literaturewere published, particularly those ofMikołaj ReyandJan Kochanowski, and thePolish languagebecame thelingua francaof East-Central Europe.[140]TheJagiellonian Universitytransformed into a major institution of higher education for the region and hosted many notable scholars, chieflyNicolaus CopernicusandConrad Celtes. Three more academies were founded atKönigsberg(1544),Vilnius(1579), andZamość(1594). The Reformation spread peacefully throughout the country, giving rise to theNontrinitarianPolish Brethren.[141]Living conditions improved, cities grew, and exports of agricultural products enriched the population, especially the nobility (szlachta) andmagnates. The nobles gained dominance in the new political system ofGolden Liberty, a counterweight tomonarchicalabsolutism.[142]
Although Italian Renaissance had a modest impact in Portuguese arts,Portugalwas influential in broadening the European worldview,[143]stimulating humanist inquiry. Renaissance arrived through the influence of wealthy Italian and Flemish merchants who invested in the profitable commerce overseas. As the pioneer headquarters of European exploration,Lisbonflourished in the late 15th century, attracting experts who made several breakthroughs in mathematics, astronomy and naval technology, includingPedro Nunes,João de Castro,Abraham Zacuto, andMartin Behaim. CartographersPedro Reinel,Lopo Homem,Estêvão Gomes, andDiogo Ribeiromade crucial advances in mapping the world. ApothecaryTomé Piresand physiciansGarcia de Ortaand Cristóvão da Costa collected and published works on plants and medicines, soon translated by Flemish pioneer botanistCarolus Clusius.
In architecture, the huge profits of thespice tradefinanced a sumptuous composite style in the first decades of the 16th century, theManueline, incorporating maritime elements.[144]The primary painters wereNuno Gonçalves,Gregório Lopes, andVasco Fernandes. In music,Pedro de EscobarandDuarte Loboproduced four songbooks, including theCancioneiro de Elvas.
In literature,Luís de Camõesinscribed the Portuguese feats overseas in the epic poemOs Lusíadas.Sá de Mirandaintroduced Italian forms of verse andBernardim Ribeirodevelopedpastoral romance, while plays byGil Vicentefused it with popular culture, reporting the changing times.Travel literatureespecially flourished:João de Barros,Fernão Lopes de Castanheda,António Galvão,Gaspar Correia,Duarte Barbosa, andFernão Mendes Pinto, among others, described new lands and were translated and spread with the new printing press.[143]After joining the Portuguese exploration of Brazil in 1500,Amerigo Vespuccicoined the termNew World,[145]in his letters toLorenzo di Pierfrancesco de' Medici.
The intense international exchange produced several cosmopolitan humanist scholars, includingFrancisco de Holanda,André de Resende, andDamião de Góis, a friend of Erasmus who wrote with rare independence on the reign of KingManuel I.Diogo de GouveiaandAndré de Gouveiamade relevant teaching reforms via France. Foreign news and products in the PortuguesefactoryinAntwerpattracted the interest ofThomas More[146]andAlbrecht Dürerto the wider world.[147]There, profits and know-how helped nurture theDutch RenaissanceandGolden Age, especially after the arrival of the wealthy cultured Jewish communityexpelled from Portugal.
The Renaissance arrived in the Iberian peninsula through the Mediterranean possessions of theCrown of Aragonand the city ofValencia. Many early Spanish Renaissance writers come from the Crown of Aragon, includingAusiàs MarchandJoanot Martorell. In theCrown of Castile, the early Renaissance was heavily influenced by the Italian humanism, starting with writers and poets such asÍñigo López de Mendoza, marqués de Santillana, who introduced the new Italian poetry to Spain in the early 15th century. Other writers, such asJorge Manrique,Fernando de Rojas,Juan del Encina,Juan Boscán Almogáver, andGarcilaso de la Vega, kept a close resemblance to the Italian canon.Miguel de Cervantes'smasterpieceDon Quixoteis credited as the first Western novel. Renaissance humanism flourished in the early 16th century, with influential writers such as philosopherJuan Luis Vives, grammarianAntonio de Nebrijaand natural historianPedro de Mexía. The poet and philosopherLuisa de Medrano, celebrated among her Renaissance contemporaries as one of thepuellae doctae("learned girls"), was the first female professor in Europe at theUniversity of Salamanca.
Later Spanish Renaissance tended toward religious themes and mysticism, with poets such asLuis de León,Teresa of Ávila, andJohn of the Cross, and treated issues related to the exploration of theNew World, with chroniclers and writers such asInca Garcilaso de la VegaandBartolomé de las Casas, giving rise to a body of work, now known asSpanish Renaissance literature. The late Renaissance in Spain produced political and religious authors such asTomás Fernández de Medranoand artists such asEl Grecoand composers such asTomás Luis de VictoriaandAntonio de Cabezón.
The Italian artist and criticGiorgio Vasari(1511–1574) first used the termrinascitain his bookThe Lives of the Artists(published 1550). In the book Vasari attempted to define what he described as a break with the barbarities ofGothic art: the arts (he held) had fallen into decay with the collapse of theRoman Empireand only theTuscanartists, beginning withCimabue(1240–1301) andGiotto(1267–1337) began to reverse this decline in the arts. Vasari saw ancient art as central to the rebirth of Italian art.[148]
However, only in the 19th century did the French wordrenaissanceachieve popularity in describing the self-conscious cultural movement based on revival of Roman models that began in the late 13th century. FrenchhistorianJules Michelet(1798–1874) defined "The Renaissance" in his 1855 workHistoire de Franceas an entire historical period, whereas previously it had been used in a more limited sense.[24]For Michelet, the Renaissance was more a development in science than in art and culture. He asserted that it spanned the period fromColumbustoCopernicustoGalileo; that is, from the end of the 15th century to the middle of the 17th century.[99]Moreover, Michelet distinguished between what he called, "the bizarre and monstrous" quality of the Middle Ages and thedemocraticvalues that he, as a vocalRepublican, chose to see in its character.[19]A French nationalist, Michelet also sought to claim the Renaissance as a French movement.[19]
TheSwisshistorianJacob Burckhardt(1818–1897) in hisThe Civilization of the Renaissance in Italy(1860), by contrast, defined the Renaissance as the period betweenGiottoandMichelangeloin Italy, that is, the 14th to mid-16th centuries. He saw in the Renaissance the emergence of the modern spirit ofindividuality, which the Middle Ages had stifled.[149]His book was widely read and became influential in the development of the modern interpretation of theItalian Renaissance.[150]
More recently, some historians have been much less keen to define the Renaissance as a historical age, or even as a coherent cultural movement. The historian Randolph Starn, of theUniversity of California Berkeley, stated in 1998:
Rather than a period with definitive beginnings and endings and consistent content in between, the Renaissance can be (and occasionally has been) seen as a movement of practices and ideas to which specific groups and identifiable persons variously responded in different times and places. It would be in this sense a network of diverse, sometimes converging, sometimes conflicting cultures, not a single, time-bound culture.[21]
There is debate about the extent to which the Renaissance improved on the culture of the Middle Ages. Both Michelet and Burckhardt were keen to describe the progress made in the Renaissance toward themodern age. Burckhardt likened the change to a veil being removed from man's eyes, allowing him to see clearly.[55]
In the Middle Ages both sides of human consciousness – that which was turned within as that which was turned without – lay dreaming or half awake beneath a common veil. The veil was woven of faith, illusion, and childish prepossession, through which the world and history were seen clad in strange hues.[151]
On the other hand, many historians now point out that most of the negative social factors popularly associated with the medieval period – poverty, warfare, religious and political persecution, for example – seem to have worsened in this era, which saw the rise ofMachiavellian politics, theWars of Religion, the corruptBorgiaPopes, and the intensifiedwitch-huntsof the 16th century. Many people who lived during the Renaissance did not view it as the "golden age" imagined by certain 19th-century authors, but were concerned by these social maladies.[152]Significantly, though, the artists, writers, and patrons involved in the cultural movements in question believed they were living in a new era that was a clean break from the Middle Ages.[90]SomeMarxist historiansprefer to describe the Renaissance in material terms, holding the view that the changes in art, literature, and philosophy were part of a general economic trend fromfeudalismtowardcapitalism, resulting in abourgeoisclass with leisure time to devote to the arts.[153]
Johan Huizinga(1872–1945) acknowledged the existence of the Renaissance but questioned whether it was a positive change. In his bookThe Autumn of the Middle Ages, he argued that the Renaissance was a period of decline from theHigh Middle Ages, destroying much that was important.[20]TheMedieval Latinlanguage, for instance, had evolved greatly from the classical period and was still a living language used in the church and elsewhere. The Renaissance obsession with classical purity halted its further evolution and sawLatinrevert to its classical form. This view is however somewhat contested byrecent studies. Robert S. Lopez has contended that it was a period of deepeconomic recession.[154]Meanwhile,George SartonandLynn Thorndikehave both argued thatscientificprogress was perhaps less original than has traditionally been supposed.[155]Finally,Joan Kellyargued that the Renaissance led to greater gender dichotomy, lessening the agency women had had during the Middle Ages.[156]
Some historians have begun to consider the wordRenaissanceto be unnecessarily loaded, implying an unambiguously positive rebirth from the supposedly more primitive "Dark Ages", the Middle Ages. Most political and economic historians now prefer to use the term "early modern" for this period (and a considerable period afterwards), a designation intended to highlight the period as a transitional one between the Middle Ages and the modern era.[157]Others such as Roger Osborne have come to consider the Italian Renaissance as a repository of the myths and ideals of western history in general, and instead of rebirth of ancient ideas as a period of great innovation.[158]
Theart historianErwin Panofskyobserved of this resistance to the concept of "Renaissance":
It is perhaps no accident that the factuality of theItalian Renaissancehas been most vigorously questioned by those who are not obliged to take a professional interest in the aesthetic aspects of civilization – historians of economic and social developments, political and religious situations, and, most particularly, natural science – but only exceptionally by students of literature and hardly ever by historians of Art.[159]
The termRenaissancehas also been used to define periods outside of the 15th and 16th centuries in the earlierMedieval period.Charles H. Haskins(1870–1937), for example, made a case for aRenaissance of the 12th century.[160]Other historians have argued for aCarolingian Renaissancein the 8th and 9th centuries,Ottonian Renaissancein the 10th century and for theTimurid Renaissanceof the 14th century. TheIslamic Golden Agehas been also sometimes termed with the Islamic Renaissance.[161]TheMacedonian Renaissanceis a term used for a period in the Roman Empire in the 9th-11th centuries CE.
Other periods of cultural rebirth inModern timeshave also been termed "renaissances", such as theBengal Renaissance,Tamil Renaissance,Nepal Bhasa renaissance,al-Nahdaor theHarlem Renaissance. The term can also be used in cinema. In animation, theDisney Renaissanceis a period that spanned the years from 1989 to 1999 which saw the studio return to the level of quality not witnessed since their Golden Age of Animation. TheSan Francisco Renaissancewas a vibrant period of exploratory poetry and fiction writing inSan Franciscoin the mid-20th century.
Rapid accumulation of knowledge, which has characterized the development of science since the 17th century, had never occurred before that time. The new kind of scientific activity emerged only in a few countries of Western Europe, and it was restricted to that small area for about two hundred years. (Since the 19th century, scientific knowledge has been assimilated by the rest of the world).
Deoxyribonucleic acid(/diːˈɒksɪˌraɪboʊnjuːˌkliːɪk,-ˌkleɪ-/ⓘ;[1]DNA) is apolymercomposed of twopolynucleotidechains that coil around each other to form adouble helix. The polymer carriesgeneticinstructions for the development, functioning, growth andreproductionof all knownorganismsand manyviruses. DNA andribonucleic acid(RNA) arenucleic acids. Alongsideproteins,lipidsand complex carbohydrates (polysaccharides), nucleic acids are one of the four major types ofmacromoleculesthat are essential for all known forms oflife.
The two DNA strands are known as polynucleotides as they are composed of simplermonomericunits callednucleotides.[2][3]Each nucleotide is composed of one of fournitrogen-containingnucleobases(cytosine[C],guanine[G],adenine[A] orthymine[T]), asugarcalleddeoxyribose, and aphosphate group. The nucleotides are joined to one another in a chain bycovalent bonds(known as thephosphodiester linkage) between the sugar of one nucleotide and the phosphate of the next, resulting in an alternatingsugar-phosphate backbone. The nitrogenous bases of the two separate polynucleotide strands are bound together, according tobase pairingrules (A with T and C with G), withhydrogen bondsto make double-stranded DNA. The complementary nitrogenous bases are divided into two groups, the single-ringedpyrimidinesand the double-ringedpurines. In DNA, the pyrimidines are thymine and cytosine; the purines are adenine and guanine.
Both strands of double-stranded DNA store the samebiological information. This information isreplicatedwhen the two strands separate. A large part of DNA (more than 98% for humans) isnon-coding, meaning that these sections do not serve as patterns forprotein sequences. The two strands of DNA run in opposite directions to each other and are thusantiparallel. Attached to each sugar is one of four types of nucleobases (orbases). It is thesequenceof these four nucleobases along the backbone that encodes genetic information. RNA strands are created using DNA strands as a template in a process calledtranscription, where DNA bases are exchanged for their corresponding bases except in the case of thymine (T), for which RNA substitutesuracil(U).[4]Under thegenetic code, these RNA strands specify the sequence ofamino acidswithin proteins in a process calledtranslation.
Within eukaryotic cells, DNA is organized into long structures calledchromosomes. Before typicalcell division, these chromosomes are duplicated in the process of DNA replication, providing a complete set of chromosomes for each daughter cell.Eukaryotic organisms(animals,plants,fungiandprotists) store most of their DNA inside thecell nucleusasnuclear DNA, and some in themitochondriaasmitochondrial DNAor inchloroplastsaschloroplast DNA.[5]In contrast,prokaryotes(bacteriaandarchaea) store their DNA only in thecytoplasm, incircular chromosomes. Within eukaryotic chromosomes,chromatinproteins, such ashistones, compact and organize DNA. These compacting structures guide the interactions between DNA and other proteins, helping control which parts of the DNA are transcribed.
DNA is a longpolymermade from repeating units callednucleotides.[6][7]The structure of DNA is dynamic along its length, being capable of coiling into tight loops and other shapes.[8]In all species it is composed of two helical chains, bound to each other byhydrogen bonds. Both chains are coiled around the same axis, and have the samepitchof 34ångströms(3.4nm). The pair of chains have a radius of 10 Å (1.0 nm).[9]According to another study, when measured in a different solution, the DNA chain measured 22–26 Å (2.2–2.6 nm) wide, and one nucleotide unit measured 3.3 Å (0.33 nm) long.[10]The buoyant density of most DNA is 1.7g/cm3.[11]
DNA does not usually exist as a single strand, but instead as a pair of strands that are held tightly together.[9][12]These two long strands coil around each other, in the shape of adouble helix. The nucleotide contains both a segment of thebackboneof the molecule (which holds the chain together) and anucleobase(which interacts with the other DNA strand in the helix). A nucleobase linked to a sugar is called anucleoside, and a base linked to a sugar and to one or more phosphate groups is called anucleotide. Abiopolymercomprising multiple linked nucleotides (as in DNA) is called apolynucleotide.[13]
The backbone of the DNA strand is made from alternatingphosphateandsugargroups.[14]The sugar in DNA is2-deoxyribose, which is apentose(five-carbon) sugar. The sugars are joined by phosphate groups that formphosphodiester bondsbetween the third and fifth carbonatomsof adjacent sugar rings. These are known as the3′-end(three prime end), and5′-end(five prime end) carbons, the prime symbol being used to distinguish these carbon atoms from those of the base to which the deoxyribose forms aglycosidic bond.[12]
Therefore, any DNA strand normally has one end at which there is a phosphate group attached to the 5′ carbon of a ribose (the 5′ phosphoryl) and another end at which there is a free hydroxyl group attached to the 3′ carbon of a ribose (the 3′ hydroxyl). The orientation of the 3′ and 5′ carbons along the sugar-phosphate backbone confersdirectionality(sometimes called polarity) to each DNA strand. In anucleic acid double helix, the direction of the nucleotides in one strand is opposite to their direction in the other strand: the strands areantiparallel. The asymmetric ends of DNA strands are said to have a directionality of five prime end (5′ ), and three prime end (3′), with the 5′ end having a terminal phosphate group and the 3′ end a terminal hydroxyl group. One major difference between DNA andRNAis the sugar, with the 2-deoxyribose in DNA being replaced by the related pentose sugarribosein RNA.[12]
The DNA double helix is stabilized primarily by two forces:hydrogen bondsbetween nucleotides andbase-stackinginteractions amongaromaticnucleobases.[16]The four bases found in DNA areadenine(A),cytosine(C),guanine(G) andthymine(T). These four bases are attached to the sugar-phosphate to form the complete nucleotide, as shown foradenosine monophosphate. Adenine pairs with thymine and guanine pairs with cytosine, formingA-TandG-Cbase pairs.[17][18]
The nucleobases are classified into two types: thepurines,AandG, which are fused five- and six-memberedheterocyclic compounds, and thepyrimidines, the six-membered ringsCandT.[12]A fifth pyrimidine nucleobase,uracil(U), usually takes the place of thymine in RNA and differs from thymine by lacking amethyl groupon its ring. In addition to RNA and DNA, many artificialnucleic acid analogueshave been created to study the properties of nucleic acids, or for use in biotechnology.[19]
Modified bases occur in DNA. The first of these recognized was5-methylcytosine, which was found in thegenomeofMycobacterium tuberculosisin 1925.[20]The reason for the presence of these noncanonical bases in bacterial viruses (bacteriophages) is to avoid therestriction enzymespresent in bacteria. This enzyme system acts at least in part as a molecular immune system protecting bacteria from infection by viruses.[21]Modifications of the bases cytosine and adenine, the more common and modified DNA bases, play vital roles in theepigeneticcontrol of gene expression in plants and animals.[22]
A number of noncanonical bases are known to occur in DNA.[23]Most of these are modifications of the canonical bases plus uracil.
Twin helical strands form the DNA backbone. Another double helix may be found tracing the spaces, or grooves, between the strands. These voids are adjacent to the base pairs and may provide abinding site. As the strands are not symmetrically located with respect to each other, the grooves are unequally sized. The major groove is 22 ångströms (2.2 nm) wide, while the minor groove is 12 Å (1.2 nm) in width.[24]Due to the larger width of the major groove, the edges of the bases are more accessible in the major groove than in the minor groove. As a result, proteins such astranscription factorsthat can bind to specific sequences in double-stranded DNA usually make contact with the sides of the bases exposed in the major groove.[25]This situation varies in unusual conformations of DNA within the cell(see below), but the major and minor grooves are always named to reflect the differences in width that would be seen if the DNA was twisted back into the ordinaryB form.
In a DNA double helix, each type of nucleobase on one strand bonds with just one type of nucleobase on the other strand. This is calledcomplementarybase pairing. Purines formhydrogen bondsto pyrimidines, with adenine bonding only to thymine in two hydrogen bonds, and cytosine bonding only to guanine in three hydrogen bonds. This arrangement of two nucleotides binding together across the double helix (from six-carbon ring to six-carbon ring) is called a Watson-Crick base pair. DNA with highGC-contentis more stable than DNA with lowGC-content. AHoogsteen base pair(hydrogen bonding the 6-carbon ring to the 5-carbon ring) is a rare variation of base-pairing.[26]As hydrogen bonds are notcovalent, they can be broken and rejoined relatively easily. The two strands of DNA in a double helix can thus be pulled apart like a zipper, either by a mechanical force or hightemperature.[27]As a result of this base pair complementarity, all the information in the double-stranded sequence of a DNA helix is duplicated on each strand, which is vital in DNA replication. This reversible and specific interaction between complementary base pairs is critical for all the functions of DNA in organisms.[7]
Most DNA molecules are actually two polymer strands, bound together in a helical fashion by noncovalent bonds; this double-stranded (dsDNA) structure is maintained largely by the intrastrand base stacking interactions, which are strongest forG,Cstacks. The two strands can come apart—a process known as melting—to form two single-stranded DNA (ssDNA) molecules. Melting occurs at high temperatures, low salt and highpH(low pH also melts DNA, but since DNA is unstable due to acid depurination, low pH is rarely used).
The stability of the dsDNA form depends not only on theGC-content (%G,Cbasepairs) but also on sequence (since stacking is sequence specific) and also length (longer molecules are more stable). The stability can be measured in various ways; a common way is themelting temperature(also calledTmvalue), which is the temperature at which 50% of the double-strand molecules are converted to single-strand molecules; melting temperature is dependent on ionic strength and the concentration of DNA. As a result, it is both the percentage ofGCbase pairs and the overall length of a DNA double helix that determines the strength of the association between the two strands of DNA. Long DNA helices with a highGC-content have more strongly interacting strands, while short helices with highATcontent have more weakly interacting strands.[28]In biology, parts of the DNA double helix that need to separate easily, such as theTATAATPribnow boxin somepromoters, tend to have a highATcontent, making the strands easier to pull apart.[29]
In the laboratory, the strength of this interaction can be measured by finding the melting temperatureTmnecessary to break half of the hydrogen bonds. When all the base pairs in a DNA double helix melt, the strands separate and exist in solution as two entirely independent molecules. These single-stranded DNA molecules have no single common shape, but some conformations are more stable than others.[30]
In humans, the total femalediploidnuclear genomeper cell extends for 6.37 Gigabase pairs (Gbp), is 208.23 cm long and weighs 6.51 picograms (pg).[31]Male values are 6.27 Gbp, 205.00 cm, 6.41 pg.[31]Each DNA polymer can contain hundreds of millions of nucleotides, such as inchromosome 1. Chromosome 1 is the largest humanchromosomewith approximately 220 millionbase pairs, and would be85 mmlong if straightened.[32]
Ineukaryotes, in addition tonuclear DNA, there is alsomitochondrial DNA(mtDNA) which encodes certain proteins used by the mitochondria. The mtDNA is usually relatively small in comparison to the nuclear DNA. For example, thehuman mitochondrial DNAforms closed circular molecules, each of which contains 16,569[33][34]DNA base pairs,[35]with each such molecule normally containing a full set of the mitochondrial genes. Each human mitochondrion contains, on average, approximately 5 such mtDNA molecules.[35]Each humancellcontains approximately 100 mitochondria, giving a total number of mtDNA molecules per human cell of approximately 500.[35]However, the amount of mitochondria per cell also varies by cell type, and anegg cellcan contain 100,000 mitochondria, corresponding to up to 1,500,000 copies of the mitochondrial genome (constituting up to 90% of the DNA of the cell).[36]
ADNA sequenceis called a "sense" sequence if it is the same as that of amessenger RNAcopy that is translated into protein.[37]The sequence on the opposite strand is called the "antisense" sequence. Both sense and antisense sequences can exist on different parts of the same strand of DNA (i.e. both strands can contain both sense and antisense sequences). In both prokaryotes and eukaryotes, antisense RNA sequences are produced, but the functions of these RNAs are not entirely clear.[38]One proposal is that antisense RNAs are involved in regulatinggene expressionthrough RNA-RNA base pairing.[39]
A few DNA sequences in prokaryotes and eukaryotes, and more inplasmidsandviruses, blur the distinction between sense and antisense strands by havingoverlapping genes.[40]In these cases, some DNA sequences do double duty, encoding one protein when read along one strand, and a second protein when read in the opposite direction along the other strand. Inbacteria, this overlap may be involved in the regulation of gene transcription,[41]while in viruses, overlapping genes increase the amount of information that can be encoded within the small viral genome.[42]
DNA can be twisted like a rope in a process calledDNA supercoiling. With DNA in its "relaxed" state, a strand usually circles the axis of the double helix once every 10.4 base pairs, but if the DNA is twisted the strands become more tightly or more loosely wound.[43]If the DNA is twisted in the direction of the helix, this is positive supercoiling, and the bases are held more tightly together. If they are twisted in the opposite direction, this is negative supercoiling, and the bases come apart more easily. In nature, most DNA has slight negative supercoiling that is introduced byenzymescalledtopoisomerases.[44]These enzymes are also needed to relieve the twisting stresses introduced into DNA strands during processes such astranscriptionandDNA replication.[45]
DNA exists in many possibleconformationsthat includeA-DNA,B-DNA, andZ-DNAforms, although only B-DNA and Z-DNA have been directly observed in functional organisms.[14]The conformation that DNA adopts depends on the hydration level, DNA sequence, the amount and direction of supercoiling, chemical modifications of the bases, the type and concentration of metalions, and the presence ofpolyaminesin solution.[46]
The first published reports of A-DNAX-ray diffraction patterns—and also B-DNA—used analyses based onPatterson functionsthat provided only a limited amount of structural information for oriented fibers of DNA.[47][48]An alternative analysis was proposed by Wilkinset al.in 1953 for thein vivoB-DNA X-ray diffraction-scattering patterns of highly hydrated DNA fibers in terms of squares ofBessel functions.[49]In the same journal,James WatsonandFrancis Crickpresented theirmolecular modelinganalysis of the DNA X-ray diffraction patterns to suggest that the structure was a double helix.[9]
Although theB-DNA formis most common under the conditions found in cells,[50]it is not a well-defined conformation but a family of related DNA conformations[51]that occur at the high hydration levels present in cells. Their corresponding X-ray diffraction and scattering patterns are characteristic of molecularparacrystalswith a significant degree of disorder.[52][53]
Compared to B-DNA, the A-DNA form is a widerright-handedspiral, with a shallow, wide minor groove and a narrower, deeper major groove. The A form occurs under non-physiological conditions in partly dehydrated samples of DNA, while in the cell it may be produced in hybrid pairings of DNA and RNA strands, and in enzyme-DNA complexes.[54][55]Segments of DNA where the bases have been chemically modified bymethylationmay undergo a larger change in conformation and adopt theZ form. Here, the strands turn about the helical axis in a left-handed spiral, the opposite of the more common B form.[56]These unusual structures can be recognized by specific Z-DNA binding proteins and may be involved in the regulation of transcription.[57]
For many years,exobiologistshave proposed the existence of ashadow biosphere, a postulated microbialbiosphereof Earth that uses radically different biochemical and molecular processes than currently known life. One of the proposals was the existence of lifeforms that usearsenic instead of phosphorus in DNA. A report in 2010 of the possibility in thebacteriumGFAJ-1was announced,[58][59]though the research was disputed,[59][60]and evidence suggests the bacterium actively prevents the incorporation of arsenic into the DNA backbone and other biomolecules.[61]
At the ends of the linear chromosomes are specialized regions of DNA calledtelomeres. The main function of these regions is to allow the cell to replicate chromosome ends using the enzymetelomerase, as the enzymes that normally replicate DNA cannot copy the extreme 3′ ends of chromosomes.[63]These specialized chromosome caps also help protect the DNA ends, and stop theDNA repairsystems in the cell from treating them as damage to be corrected.[64]Inhuman cells, telomeres are usually lengths of single-stranded DNA containing several thousand repeats of a simple TTAGGG sequence.[65]
These guanine-rich sequences may stabilize chromosome ends by forming structures of stacked sets of four-base units, rather than the usual base pairs found in other DNA molecules. Here, four guanine bases, known as aguanine tetrad, form a flat plate. These flat four-base units then stack on top of each other to form a stableG-quadruplexstructure.[66]These structures are stabilized by hydrogen bonding between the edges of the bases andchelationof a metal ion in the centre of each four-base unit.[67]Other structures can also be formed, with the central set of four bases coming from either a single strand folded around the bases, or several different parallel strands, each contributing one base to the central structure.
In addition to these stacked structures, telomeres also form large loop structures called telomere loops, or T-loops. Here, the single-stranded DNA curls around in a long circle stabilized by telomere-binding proteins.[68]At the very end of the T-loop, the single-stranded telomere DNA is held onto a region of double-stranded DNA by the telomere strand disrupting the double-helical DNA and base pairing to one of the two strands. Thistriple-strandedstructure is called a displacement loop orD-loop.[66]
In DNA,frayingoccurs when non-complementary regions exist at the end of an otherwise complementary double-strand of DNA. However, branched DNA can occur if a third strand of DNA is introduced and contains adjoining regions able to hybridize with the frayed regions of the pre-existing double-strand. Although the simplest example of branched DNA involves only three strands of DNA, complexes involving additional strands and multiple branches are also possible.[69]Branched DNA can be used innanotechnologyto construct geometric shapes, see the section onuses in technologybelow.
Several artificial nucleobases have been synthesized, and successfully incorporated in the eight-base DNA analogue namedHachimoji DNA. Dubbed S, B, P, and Z, these artificial bases are capable of bonding with each other in a predictable way (S–B and P–Z), maintain the double helix structure of DNA, and be transcribed to RNA. Their existence could be seen as an indication that there is nothing special about the four natural nucleobases that evolved on Earth.[70][71]On the other hand, DNA is tightly related toRNAwhich does not only act as a transcript of DNA but also performs as molecular machines many tasks in cells. For this purpose it has to fold into a structure. It has been shown that to allow to create all possible structures at least four bases are required for the correspondingRNA,[72]while a higher number is also possible but this would be against the naturalprinciple of least effort.
The phosphate groups of DNA give it similaracidicproperties tophosphoric acidand it can be considered as astrong acid. It will be fully ionized at a normal cellular pH, releasingprotonswhich leave behind negative charges on the phosphate groups. These negative charges protect DNA from breakdown byhydrolysisby repellingnucleophileswhich could hydrolyze it.[73]
Pure DNA extracted from cells forms white, stringy clumps.[74]
The expression of genes is influenced by how the DNA is packaged in chromosomes, in a structure calledchromatin. Base modifications can be involved in packaging, with regions that have low or no gene expression usually containing high levels ofmethylationofcytosinebases. DNA packaging and its influence on gene expression can also occur by covalent modifications of thehistoneprotein core around which DNA is wrapped in the chromatin structure or else by remodeling carried out by chromatin remodeling complexes (seeChromatin remodeling). There is, further,crosstalkbetween DNA methylation and histone modification, so they can coordinately affect chromatin and gene expression.[75]
For one example, cytosine methylation produces5-methylcytosine, which is important forX-inactivationof chromosomes.[76]The average level of methylation varies between organisms—the wormCaenorhabditis eleganslacks cytosine methylation, whilevertebrateshave higher levels, with up to 1% of their DNA containing 5-methylcytosine.[77]Despite the importance of 5-methylcytosine, it candeaminateto leave a thymine base, so methylated cytosines are particularly prone tomutations.[78]Other base modifications include adenine methylation in bacteria, the presence of5-hydroxymethylcytosinein thebrain,[79]and theglycosylationof uracil to produce the "J-base" inkinetoplastids.[80][81]
DNA can be damaged by many sorts ofmutagens, which change theDNA sequence. Mutagens includeoxidizing agents,alkylating agentsand also high-energyelectromagnetic radiationsuch asultravioletlight andX-rays. The type of DNA damage produced depends on the type of mutagen. For example, UV light can damage DNA by producingthymine dimers, which are cross-links between pyrimidine bases.[83]On the other hand, oxidants such asfree radicalsorhydrogen peroxideproduce multiple forms of damage, including base modifications, particularly of guanosine, and double-strand breaks.[84]A typical human cell contains about 150,000 bases that have suffered oxidative damage.[85]Of these oxidative lesions, the most dangerous are double-strand breaks, as these are difficult to repair and can producepoint mutations,insertions,deletionsfrom the DNA sequence, andchromosomal translocations.[86]These mutations can causecancer. Because of inherent limits in the DNA repair mechanisms, if humans lived long enough, they would all eventually develop cancer.[87][88]DNA damages that arenaturally occurring, due to normal cellular processes that produce reactive oxygen species, the hydrolytic activities of cellular water, etc., also occur frequently. Although most of these damages are repaired, in any cell some DNA damage may remain despite the action of repair processes. These remaining DNA damages accumulate with age in mammalian postmitotic tissues. This accumulation appears to be an important underlying cause of aging.[89][90][91]
Many mutagens fit into the space between two adjacent base pairs, this is calledintercalation. Most intercalators arearomaticand planar molecules; examples includeethidium bromide,acridines,daunomycin, anddoxorubicin. For an intercalator to fit between base pairs, the bases must separate, distorting the DNA strands by unwinding of the double helix. This inhibits both transcription and DNA replication, causing toxicity and mutations.[92]As a result, DNA intercalators may becarcinogens, and in the case of thalidomide, ateratogen.[93]Others such asbenzo[a]pyrene diol epoxideandaflatoxinform DNA adducts that induce errors in replication.[94]Nevertheless, due to their ability to inhibit DNA transcription and replication, other similar toxins are also used inchemotherapyto inhibit rapidly growingcancercells.[95]
DNA usually occurs as linearchromosomesineukaryotes, andcircular chromosomesinprokaryotes. The set of chromosomes in a cell makes up itsgenome; thehuman genomehas approximately 3 billion base pairs of DNA arranged into 46 chromosomes.[96]The information carried by DNA is held in thesequenceof pieces of DNA calledgenes.Transmissionof genetic information in genes is achieved via complementary base pairing. For example, in transcription, when a cell uses the information in a gene, the DNA sequence is copied into a complementary RNA sequence through the attraction between the DNA and the correct RNA nucleotides. Usually, this RNA copy is then used to make a matchingprotein sequencein a process calledtranslation, which depends on the same interaction between RNA nucleotides. In an alternative fashion, a cell may copy its genetic information in a process calledDNA replication. The details of these functions are covered in other articles; here the focus is on the interactions between DNA and other molecules that mediate the function of the genome.
Genomic DNA is tightly and orderly packed in the process calledDNA condensation, to fit the small available volumes of the cell. In eukaryotes, DNA is located in thecell nucleus, with small amounts inmitochondriaandchloroplasts. In prokaryotes, the DNA is held within an irregularly shaped body in the cytoplasm called thenucleoid.[97]The genetic information in a genome is held within genes, and the complete set of this information in an organism is called itsgenotype. A gene is a unit ofheredityand is a region of DNA that influences a particular characteristic in an organism. Genes contain anopen reading framethat can be transcribed, andregulatory sequencessuch aspromotersandenhancers, which control transcription of the open reading frame.
In manyspecies, only a small fraction of the total sequence of thegenomeencodes protein. For example, only about 1.5% of the human genome consists of protein-codingexons, with over 50% of human DNA consisting of non-codingrepetitive sequences.[98]The reasons for the presence of so muchnoncoding DNAin eukaryotic genomes and the extraordinary differences ingenome size, orC-value, among species, represent a long-standing puzzle known as the "C-value enigma".[99]However, some DNA sequences that do not code protein may still encode functionalnon-coding RNAmolecules, which are involved in theregulation of gene expression.[100]
Some noncoding DNA sequences play structural roles in chromosomes.Telomeresandcentromerestypically contain few genes but are important for the function and stability of chromosomes.[64][102]An abundant form of noncoding DNA in humans arepseudogenes, which are copies of genes that have been disabled by mutation.[103]These sequences are usually just molecularfossils, although they can occasionally serve as rawgenetic materialfor the creation of new genes through the process ofgene duplicationanddivergence.[104]
A gene is a sequence of DNA that contains genetic information and can influence thephenotypeof an organism. Within a gene, the sequence of bases along a DNA strand defines amessenger RNAsequence, which then defines one or more protein sequences. The relationship between the nucleotide sequences of genes and theamino-acidsequences of proteins is determined by the rules oftranslation, known collectively as thegenetic code. The genetic code consists of three-letter 'words' calledcodonsformed from a sequence of three nucleotides (e.g. ACT, CAG, TTT).
In transcription, the codons of a gene are copied into messenger RNA byRNA polymerase. This RNA copy is then decoded by aribosomethat reads the RNA sequence by base-pairing the messenger RNA totransfer RNA, which carries amino acids. Since there are 4 bases in 3-letter combinations, there are 64 possible codons (43combinations). These encode the twentystandard amino acids, giving most amino acids more than one possible codon. There are also three 'stop' or 'nonsense' codons signifying the end of the coding region; these are the TAG, TAA, and TGA codons, (UAG, UAA, and UGA on the mRNA).
Cell divisionis essential for an organism to grow, but, when a cell divides, it must replicate the DNA in its genome so that the two daughter cells have the same genetic information as their parent. The double-stranded structure of DNA provides a simple mechanism forDNA replication. Here, the two strands are separated and then each strand'scomplementary DNAsequence is recreated by anenzymecalledDNA polymerase. This enzyme makes the complementary strand by finding the correct base through complementary base pairing and bonding it onto the original strand. As DNA polymerases can only extend a DNA strand in a 5′ to 3′ direction, different mechanisms are used to copy the antiparallel strands of the double helix.[105]In this way, the base on the old strand dictates which base appears on the new strand, and the cell ends up with a perfect copy of its DNA.
Naked extracellular DNA (eDNA), most of it released by cell death, is nearly ubiquitous in the environment. Its concentration in soil may be as high as 2 μg/L, and its concentration in natural aquatic environments may be as high at 88 μg/L.[106]Various possible functions have been proposed for eDNA: it may be involved inhorizontal gene transfer;[107]it may provide nutrients;[108]and it may act as a buffer to recruit or titrate ions or antibiotics.[109]Extracellular DNA acts as a functional extracellular matrix component in thebiofilmsof several bacterial species. It may act as a recognition factor to regulate the attachment and dispersal of specific cell types in the biofilm;[110]it may contribute to biofilm formation;[111]and it may contribute to the biofilm's physical strength and resistance to biological stress.[112]
Cell-free fetal DNAis found in the blood of the mother, and can be sequenced to determine a great deal of information about the developing fetus.[113]
Under the name ofenvironmental DNAeDNA has seen increased use in the natural sciences as a survey tool forecology, monitoring the movements and presence of species in water, air, or on land, and assessing an area's biodiversity.[114][115]
Neutrophil extracellular traps (NETs) are networks of extracellular fibers, primarily composed of DNA, which allowneutrophils, a type of white blood cell, to kill extracellular pathogens while minimizing damage to the host cells.
All the functions of DNA depend on interactions with proteins. Theseprotein interactionscan be non-specific, or the protein can bind specifically to a single DNA sequence. Enzymes can also bind to DNA and of these, the polymerases that copy the DNA base sequence in transcription and DNA replication are particularly important.
Structural proteins that bind DNA are well-understood examples of non-specific DNA-protein interactions. Within chromosomes, DNA is held in complexes with structural proteins. These proteins organize the DNA into a compact structure calledchromatin. In eukaryotes, this structure involves DNA binding to a complex of small basic proteins calledhistones, while in prokaryotes multiple types of proteins are involved.[116][117]The histones form a disk-shaped complex called anucleosome, which contains two complete turns of double-stranded DNA wrapped around its surface. These non-specific interactions are formed through basic residues in the histones, makingionic bondsto the acidic sugar-phosphate backbone of the DNA, and are thus largely independent of the base sequence.[118]Chemical modifications of these basic amino acid residues includemethylation,phosphorylation, andacetylation.[119]These chemical changes alter the strength of the interaction between the DNA and the histones, making the DNA more or less accessible totranscription factorsand changing the rate of transcription.[120]Other non-specific DNA-binding proteins in chromatin include the high-mobility group proteins, which bind to bent or distorted DNA.[121]These proteins are important in bending arrays of nucleosomes and arranging them into the larger structures that make up chromosomes.[122]
A distinct group of DNA-binding proteins is the DNA-binding proteins that specifically bind single-stranded DNA. In humans, replicationprotein Ais the best-understood member of this family and is used in processes where the double helix is separated, including DNA replication, recombination, and DNA repair.[123]These binding proteins seem to stabilize single-stranded DNA and protect it from formingstem-loopsor being degraded bynucleases.
In contrast, other proteins have evolved to bind to particular DNA sequences. The most intensively studied of these are the varioustranscription factors, which are proteins that regulate transcription. Each transcription factor binds to one particular set of DNA sequences and activates or inhibits the transcription of genes that have these sequences close to their promoters. The transcription factors do this in two ways. Firstly, they can bind the RNA polymerase responsible for transcription, either directly or through other mediator proteins; this locates the polymerase at the promoter and allows it to begin transcription.[125]Alternatively, transcription factors can bindenzymesthat modify the histones at the promoter. This changes the accessibility of the DNA template to the polymerase.[126]
As these DNA targets can occur throughout an organism's genome, changes in the activity of one type of transcription factor can affect thousands of genes.[127]Consequently, these proteins are often the targets of thesignal transductionprocesses that control responses to environmental changes orcellular differentiationand development. The specificity of these transcription factors' interactions with DNA come from the proteins making multiple contacts to the edges of the DNA bases, allowing them to "read" the DNA sequence. Most of these base-interactions are made in the major groove, where the bases are most accessible.[25]
Nucleasesareenzymesthat cut DNA strands by catalyzing thehydrolysisof thephosphodiester bonds. Nucleases that hydrolyse nucleotides from the ends of DNA strands are calledexonucleases, whileendonucleasescut within strands. The most frequently used nucleases inmolecular biologyare therestriction endonucleases, which cut DNA at specific sequences. For instance, the EcoRV enzyme shown to the left recognizes the 6-base sequence 5′-GATATC-3′ and makes a cut at the horizontal line. In nature, these enzymes protectbacteriaagainstphageinfection by digesting the phage DNA when it enters the bacterial cell, acting as part of therestriction modification system.[129]In technology, these sequence-specific nucleases are used inmolecular cloningandDNA fingerprinting.
Enzymes calledDNA ligasescan rejoin cut or broken DNA strands.[130]Ligases are particularly important inlagging strandDNA replication, as they join the short segments of DNA produced at thereplication forkinto a complete copy of the DNA template. They are also used inDNA repairandgenetic recombination.[130]
Topoisomerasesare enzymes with both nuclease and ligase activity. These proteins change the amount ofsupercoilingin DNA. Some of these enzymes work by cutting the DNA helix and allowing one section to rotate, thereby reducing its level of supercoiling; the enzyme then seals the DNA break.[44]Other types of these enzymes are capable of cutting one DNA helix and then passing a second strand of DNA through this break, before rejoining the helix.[131]Topoisomerases are required for many processes involving DNA, such as DNA replication and transcription.[45]
Helicasesare proteins that are a type ofmolecular motor. They use the chemical energy innucleoside triphosphates, predominantlyadenosine triphosphate(ATP), to break hydrogen bonds between bases and unwind the DNA double helix into single strands.[132]These enzymes are essential for most processes where enzymes need to access the DNA bases.
Polymerasesareenzymesthat synthesize polynucleotide chains fromnucleoside triphosphates. The sequence of their products is created based on existing polynucleotide chains—which are calledtemplates. These enzymes function by repeatedly adding a nucleotide to the 3′hydroxylgroup at the end of the growing polynucleotide chain. As a consequence, all polymerases work in a 5′ to 3′ direction.[133]In theactive siteof these enzymes, the incoming nucleoside triphosphate base-pairs to the template: this allows polymerases to accurately synthesize the complementary strand of their template. Polymerases are classified according to the type of template that they use.
In DNA replication, DNA-dependentDNA polymerasesmake copies of DNA polynucleotide chains. To preserve biological information, it is essential that the sequence of bases in each copy are precisely complementary to the sequence of bases in the template strand. Many DNA polymerases have aproofreadingactivity. Here, the polymerase recognizes the occasional mistakes in the synthesis reaction by the lack of base pairing between the mismatched nucleotides. If a mismatch is detected, a 3′ to 5′exonucleaseactivity is activated and the incorrect base removed.[134]In most organisms, DNA polymerases function in a large complex called thereplisomethat contains multiple accessory subunits, such as theDNA clamporhelicases.[135]
RNA-dependent DNA polymerases are a specialized class of polymerases that copy the sequence of an RNA strand into DNA. They includereverse transcriptase, which is aviralenzyme involved in the infection of cells byretroviruses, andtelomerase, which is required for the replication of telomeres.[63][136]For example, HIV reverse transcriptase is an enzyme for AIDS virus replication.[136]Telomerase is an unusual polymerase because it contains its own RNA template as part of its structure. It synthesizestelomeresat the ends of chromosomes. Telomeres prevent fusion of the ends of neighboring chromosomes and protect chromosome ends from damage.[64]
Transcription is carried out by a DNA-dependentRNA polymerasethat copies the sequence of a DNA strand into RNA. To begin transcribing a gene, the RNA polymerase binds to a sequence of DNA called a promoter and separates the DNA strands. It then copies the gene sequence into amessenger RNAtranscript until it reaches a region of DNA called theterminator, where it halts and detaches from the DNA. As with human DNA-dependent DNA polymerases,RNA polymerase II, the enzyme that transcribes most of the genes in the human genome, operates as part of a largeprotein complexwith multiple regulatory and accessory subunits.[137]
A DNA helix usually does not interact with other segments of DNA, and in human cells, the different chromosomes even occupy separate areas in the nucleus called "chromosome territories".[139]This physical separation of different chromosomes is important for the ability of DNA to function as a stable repository for information, as one of the few times chromosomes interact is inchromosomal crossoverwhich occurs duringsexual reproduction, whengenetic recombinationoccurs. Chromosomal crossover is when two DNA helices break, swap a section and then rejoin.
Recombination allows chromosomes to exchange genetic information and produces new combinations of genes, which increases the efficiency ofnatural selectionand can be important in the rapid evolution of new proteins.[140]Genetic recombination can also be involved in DNA repair, particularly in the cell's response to double-strand breaks.[141]
The most common form of chromosomal crossover ishomologous recombination, where the two chromosomes involved share very similar sequences.Non-homologous recombinationcan be damaging to cells, as it can producechromosomal translocationsand genetic abnormalities. The recombination reaction is catalyzed by enzymes known asrecombinases, such asRAD51.[142]The first step in recombination is a double-stranded break caused by either anendonucleaseor damage to the DNA.[143]A series of steps catalyzed in part by the recombinase then leads to joining of the two helices by at least oneHolliday junction, in which a segment of a single strand in each helix is annealed to the complementary strand in the other helix. The Holliday junction is a tetrahedral junction structure that can be moved along the pair of chromosomes, swapping one strand for another. The recombination reaction is then halted by cleavage of the junction and re-ligation of the released DNA.[144]Only strands of like polarity exchange DNA during recombination. There are two types of cleavage: east-west cleavage and north–south cleavage. The north–south cleavage nicks both strands of DNA, while the east–west cleavage has one strand of DNA intact. The formation of a Holliday junction during recombination makes it possible for genetic diversity, genes to exchange on chromosomes, and expression of wild-type viral genomes.
DNA contains the genetic information that allows all forms of life to function, grow and reproduce. However, it is unclear how long in the 4-billion-yearhistory of lifeDNA has performed this function, as it has been proposed that the earliest forms of life may have used RNA as their genetic material.[145][146]RNA may have acted as the central part of earlycell metabolismas it can both transmit genetic information and carry outcatalysisas part ofribozymes.[147]This ancientRNA worldwhere nucleic acid would have been used for both catalysis and genetics may have influenced theevolutionof the current genetic code based on four nucleotide bases. This would occur, since the number of different bases in such an organism is a trade-off between a small number of bases increasing replication accuracy and a large number of bases increasing the catalytic efficiency of ribozymes.[148]However, there is no direct evidence of ancient genetic systems, as recovery of DNA from most fossils is impossible because DNA survives in the environment for less than one million years, and slowly degrades into short fragments in solution.[149]Claims for older DNA have been made, most notably a report of the isolation of a viable bacterium from a salt crystal 250 million years old,[150]but these claims are controversial.[151][152]
Building blocks of DNA (adenine,guanine, and relatedorganic molecules) may have been formed extraterrestrially inouter space.[153][154][155]Complex DNA andRNAorganic compoundsoflife, includinguracil,cytosine, andthymine, have also been formed in the laboratory under conditions mimicking those found inouter space, using starting chemicals, such aspyrimidine, found inmeteorites. Pyrimidine, likepolycyclic aromatic hydrocarbons(PAHs), the most carbon-rich chemical found in theuniverse, may have been formed inred giantsor in interstellarcosmic dustand gas clouds.[156]
Ancient DNAhas been recovered from ancient organisms at a timescale where genome evolution can be directly observed, including from extinct organisms up to millions of years old, such as thewoolly mammoth.[157][158]
Methods have been developed to purify DNA from organisms, such asphenol-chloroform extraction, and to manipulate it in the laboratory, such asrestriction digestsand thepolymerase chain reaction. Modernbiologyandbiochemistrymake intensive use of these techniques in recombinant DNA technology.Recombinant DNAis a man-made DNA sequence that has been assembled from other DNA sequences. They can betransformedinto organisms in the form ofplasmidsor in the appropriate format, by using aviral vector.[159]Thegenetically modifiedorganisms produced can be used to produce products such as recombinantproteins, used inmedical research,[160]or be grown inagriculture.[161][162]
Forensic scientistscan use DNA inblood,semen,skin,salivaorhairfound at acrime sceneto identify a matching DNA of an individual, such as a perpetrator.[163]This process is formally termedDNA profiling, also calledDNA fingerprinting. In DNA profiling, the lengths of variable sections of repetitive DNA, such asshort tandem repeatsandminisatellites, are compared between people. This method is usually an extremely reliable technique for identifying a matching DNA.[164]However, identification can be complicated if the scene is contaminated with DNA from several people.[165]DNA profiling was developed in 1984 by British geneticist SirAlec Jeffreys,[166]and first used in forensic science to convict Colin Pitchfork in the 1988Enderby murderscase.[167]
The development of forensic science and the ability to now obtain genetic matching on minute samples of blood, skin, saliva, or hair has led to re-examining many cases. Evidence can now be uncovered that was scientifically impossible at the time of the original examination. Combined with the removal of thedouble jeopardylaw in some places, this can allow cases to be reopened where prior trials have failed to produce sufficient evidence to convince a jury. People charged with serious crimes may be required to provide a sample of DNA for matching purposes. The most obvious defense to DNA matches obtained forensically is to claim that cross-contamination of evidence has occurred. This has resulted in meticulous strict handling procedures with new cases of serious crime.
DNA profiling is also used successfully to positively identify victims of mass casualty incidents,[168]bodies or body parts in serious accidents, and individual victims in mass war graves, via matching to family members.
DNA profiling is also used inDNA paternity testingto determine if someone is the biological parent or grandparent of a child with the probability of parentage is typically 99.99% when the alleged parent is biologically related to the child. NormalDNA sequencingmethods happen after birth, but there are new methods to test paternity while a mother is still pregnant.[169]
Deoxyribozymes, also called DNAzymes or catalytic DNA, were first discovered in 1994.[170]They are mostly single stranded DNA sequences isolated from a large pool of random DNA sequences through a combinatorial approach calledin vitroselection orsystematic evolution of ligands by exponential enrichment(SELEX). DNAzymes catalyze variety of chemical reactions including RNA-DNA cleavage, RNA-DNA ligation, amino acids phosphorylation-dephosphorylation, carbon-carbon bond formation, etc. DNAzymes can enhance catalytic rate of chemical reactions up to 100,000,000,000-fold over the uncatalyzed reaction.[171]The most extensively studied class of DNAzymes is RNA-cleaving types which have been used to detect different metal ions and designing therapeutic agents. Several metal-specific DNAzymes have been reported including the GR-5 DNAzyme (lead-specific),[170]the CA1-3 DNAzymes (copper-specific),[172]the 39E DNAzyme (uranyl-specific) and the NaA43 DNAzyme (sodium-specific).[173]The NaA43 DNAzyme, which is reported to be more than 10,000-fold selective for sodium over other metal ions, was used to make a real-time sodium sensor in cells.
Bioinformaticsinvolves the development of techniques to store,data mine, search and manipulate biological data, including DNAnucleic acid sequencedata. These have led to widely applied advances incomputer science, especiallystring searching algorithms,machine learning, anddatabase theory.[174]String searching or matching algorithms, which find an occurrence of a sequence of letters inside a larger sequence of letters, were developed to search for specific sequences of nucleotides.[175]The DNA sequence may bealignedwith other DNA sequences to identifyhomologous sequencesand locate the specificmutationsthat make them distinct. These techniques, especiallymultiple sequence alignment, are used in studyingphylogeneticrelationships and protein function.[176]Data sets representing entire genomes' worth of DNA sequences, such as those produced by theHuman Genome Project, are difficult to use without the annotations that identify the locations of genes and regulatory elements on each chromosome. Regions of DNA sequence that have the characteristic patterns associated with protein- or RNA-coding genes can be identified bygene findingalgorithms, which allow researchers to predict the presence of particulargene productsand their possible functions in an organism even before they have been isolated experimentally.[177]Entire genomes may also be compared, which can shed light on the evolutionary history of particular organism and permit the examination of complex evolutionary events.
DNA nanotechnology uses the uniquemolecular recognitionproperties of DNA and other nucleic acids to create self-assembling branched DNA complexes with useful properties.[179]DNA is thus used as a structural material rather than as a carrier of biological information. This has led to the creation of two-dimensional periodic lattices (both tile-based and using theDNA origamimethod) and three-dimensional structures in the shapes ofpolyhedra.[180]Nanomechanical devicesandalgorithmic self-assemblyhave also been demonstrated,[181]and these DNA structures have been used to template the arrangement of other molecules such asgold nanoparticlesandstreptavidinproteins.[182]DNA and other nucleic acids are the basis ofaptamers, synthetic oligonucleotide ligands for specific target molecules used in a range of biotechnology and biomedical applications.[183]
Because DNA collects mutations over time, which are then inherited, it contains historical information, and, by comparing DNA sequences, geneticists can infer the evolutionary history of organisms, theirphylogeny.[184]This field of phylogenetics is a powerful tool inevolutionary biology. If DNA sequences within a species are compared,population geneticistscan learn the history of particular populations. This can be used in studies ranging fromecological geneticstoanthropology.
DNA as astorage devicefor information has enormous potential since it has much higherstorage densitycompared to electronic devices. However, high costs, slow read and write times (memory latency), and insufficientreliabilityhas prevented its practical use.[185][186]
DNA was first isolated by the Swiss physicianFriedrich Miescherwho, in 1869, discovered a microscopic substance in thepusof discarded surgical bandages. As it resided in the nuclei of cells, he called it "nuclein".[187][188]In 1878,Albrecht Kosselisolated the non-protein component of "nuclein", nucleic acid, and later isolated its five primarynucleobases.[189][190]
In 1909,Phoebus Leveneidentified the base, sugar, and phosphate nucleotide unit of RNA (then named "yeast nucleic acid").[191][192][193]In 1929, Levene identified deoxyribose sugar in "thymus nucleic acid" (DNA).[194]Levene suggested that DNA consisted of a string of four nucleotide units linked together through the phosphate groups ("tetranucleotide hypothesis"). Levene thought the chain was short and the bases repeated in a fixed order. In 1927,Nikolai Koltsovproposed that inherited traits would be inherited via a "giant hereditary molecule" made up of "two mirror strands that would replicate in a semi-conservative fashion using each strand as a template".[195][196]In 1928,Frederick Griffithin hisexperimentdiscovered thattraitsof the "smooth" form ofPneumococcuscould be transferred to the "rough" form of the same bacteria by mixing killed "smooth" bacteria with the live "rough" form.[197][198]This system provided the first clear suggestion that DNA carries genetic information.
In 1933, while studying virginsea urchineggs,Jean Brachetsuggested that DNA is found in thecell nucleusand thatRNAis present exclusively in thecytoplasm. At the time, "yeast nucleic acid" (RNA) was thought to occur only in plants, while "thymus nucleic acid" (DNA) only in animals. The latter was thought to be a tetramer, with the function of buffering cellular pH.[199][200]
In 1937,William Astburyproduced the first X-ray diffraction patterns that showed that DNA had a regular structure.[201]
In 1943,Oswald Avery, along with co-workersColin MacLeodandMaclyn McCarty, identified DNA as thetransforming principle, supporting Griffith's suggestion (Avery–MacLeod–McCarty experiment).[202]Erwin Chargaffdeveloped and published observations now known asChargaff's rules, stating that in DNA from any species of any organism, the amount ofguanineshould be equal tocytosineand the amount ofadenineshould be equal tothymine.[203][204]
Late in 1951,Francis Crickstarted working withJames Watsonat theCavendish Laboratorywithin theUniversity of Cambridge. DNA's role inhereditywas confirmed in 1952 whenAlfred HersheyandMartha Chasein theHershey–Chase experimentshowed that DNA is thegenetic materialof theenterobacteria phage T2.[205]
In May 1952,Raymond Gosling, a graduate student working under the supervision ofRosalind Franklin, took anX-ray diffractionimage, labeled as "Photo 51",[206]at high hydration levels of DNA. This photo was given to Watson and Crick byMaurice Wilkinsand was critical to their obtaining the correct structure of DNA. Franklin told Crick and Watson that the backbones had to be on the outside. Before then, Linus Pauling, and Watson and Crick, had erroneous models with the chains inside and the bases pointing outwards. Franklin's identification of thespace groupfor DNA crystals revealed to Crick that the two DNA strands wereantiparallel.[207]In February 1953,Linus PaulingandRobert Coreyproposed a model for nucleic acids containing three intertwined chains, with the phosphates near the axis, and the bases on the outside.[208]Watson and Crick completed their model, which is now accepted as the first correct model of the double helix ofDNA. On 28 February 1953 Crick interrupted patrons' lunchtime atThe Eaglepubin Cambridge, England to announce that he and Watson had "discovered the secret of life".[209]
The 25 April 1953 issue of the journalNaturepublished a series of five articles giving the Watson and Crick double-helix structure DNA and evidence supporting it.[210]The structure was reported in a letter titled "MOLECULAR STRUCTURE OF NUCLEIC ACIDS A Structure for Deoxyribose Nucleic Acid", in which they said, "It has not escaped our notice that the specific pairing we have postulated immediately suggests a possible copying mechanism for the genetic material."[9]This letter was followed by a letter from Franklin and Gosling, which was the first publication of their own X-ray diffraction data and of their original analysis method.[48][211]Then followed a letter by Wilkins and two of his colleagues, which contained an analysis ofin vivoB-DNA X-ray patterns, and which supported the presencein vivoof the Watson and Crick structure.[49]
In April 2023, scientists, based on new evidence, concluded that Rosalind Franklin was a contributor and "equal player" in the discovery process of DNA, rather than otherwise, as may have been presented subsequently after the time of the discovery.[212][213][214]
In 1962, after Franklin's death, Watson, Crick, and Wilkins jointly received theNobel Prize in Physiology or Medicine.[215]Nobel Prizes are awarded only to living recipients. A debate continues about who should receive credit for the discovery.[216]
In an influential presentation in 1957, Crick laid out thecentral dogma of molecular biology, which foretold the relationship between DNA, RNA, and proteins, and articulated the "adaptor hypothesis".[217]Final confirmation of the replication mechanism that was implied by the double-helical structure followed in 1958 through theMeselson–Stahl experiment.[218]Further work by Crick and co-workers showed that the genetic code was based on non-overlapping triplets of bases, calledcodons, allowingHar Gobind Khorana,Robert W. Holley, andMarshall Warren Nirenbergto decipher the genetic code.[219]These findings represent the birth ofmolecular biology.[220]
In 1986, DNA analysis was first used in a criminal investigation when police in the UK requestedAlec Jeffreysof the University of Leicester to prove or disprove the involvement in a particular case of a suspect who claimed innocence in the matter. Although the suspect had already confessed to committing a recent rape-murder, he was denying any involvement in a similar crime committed three years earlier. Yet the details of the two cases were so alike that the police concluded both crimes had been committed by the same person. However, all charges against the suspect were dropped when Jeffreys' DNA testing exonerated the suspect — from both the earlier murder and the one to which he'd confessed. But further such DNA profiling ultimately led to positive identification of another suspect who, in 1988, was found guilty of both rape-murders.[221][222]
Quantum mechanicsis the fundamental physicaltheorythat describes the behavior of matter and of light; its unusual characteristics typically occur at and below the scale ofatoms.[2]: 1.1It is the foundation of allquantum physics, which includesquantum chemistry,quantum field theory,quantum technology, andquantum information science.
Quantum mechanics can describe many systems thatclassical physicscannot. Classical physics can describe many aspects of nature at an ordinary (macroscopicand(optical) microscopic) scale, but is not sufficient for describing them at very smallsubmicroscopic(atomic andsubatomic) scales. Classical mechanics can be derived from quantum mechanics as an approximation that is valid at ordinary scales.[3]
Quantum systems haveboundstates that arequantizedtodiscrete valuesofenergy,momentum,angular momentum, and other quantities, in contrast to classical systems where these quantities can be measured continuously. Measurements of quantum systems show characteristics of bothparticlesandwaves(wave–particle duality), and there are limits to how accurately the value of a physical quantity can be predicted prior to its measurement, given a complete set of initial conditions (theuncertainty principle).
Quantum mechanicsarose graduallyfrom theories to explain observations that could not be reconciled with classical physics, such asMax Planck's solution in 1900 to theblack-body radiationproblem, and the correspondence between energy and frequency inAlbert Einstein's1905 paper, which explained thephotoelectric effect. These early attempts to understand microscopic phenomena, now known as the "old quantum theory", led to the full development of quantum mechanics in the mid-1920s byNiels Bohr,Erwin Schrödinger,Werner Heisenberg,Max Born,Paul Diracand others. The modern theory is formulated in variousspecially developed mathematical formalisms. In one of them, a mathematical entity called thewave functionprovides information, in the form ofprobability amplitudes, about what measurements of a particle's energy, momentum, and other physical properties may yield.
Quantum mechanics allows the calculation of properties and behaviour ofphysical systems. It is typically applied to microscopic systems:molecules,atomsandsubatomic particles. It has been demonstrated to hold for complex molecules with thousands of atoms,[4]but its application to human beings raises philosophical problems, such asWigner's friend, and its application to the universe as a whole remains speculative.[5]Predictions of quantum mechanics have been verified experimentally to an extremely high degree ofaccuracy. For example, the refinement of quantum mechanics for the interaction of light and matter, known asquantum electrodynamics(QED), has beenshown to agree with experimentto within 1 part in 1012when predicting the magnetic properties of an electron.[6]
A fundamental feature of the theory is that it usually cannot predict with certainty what will happen, but only give probabilities. Mathematically, a probability is found by taking the square of the absolute value of acomplex number, known as a probability amplitude. This is known as theBorn rule, named after physicistMax Born. For example, a quantum particle like anelectroncan be described by a wave function, which associates to each point in space a probability amplitude. Applying the Born rule to these amplitudes gives aprobability density functionfor the position that the electron will be found to have when an experiment is performed to measure it. This is the best the theory can do; it cannot say for certain where the electron will be found. TheSchrödinger equationrelates the collection of probability amplitudes that pertain to one moment of time to the collection of probability amplitudes that pertain to another.[7]: 67–87
One consequence of the mathematical rules of quantum mechanics is a tradeoff in predictability between measurable quantities. The most famous form of thisuncertainty principlesays that no matter how a quantum particle is prepared or how carefully experiments upon it are arranged, it is impossible to have a precise prediction for a measurement of its position and also at the same time for a measurement of itsmomentum.[7]: 427–435
Another consequence of the mathematical rules of quantum mechanics is the phenomenon ofquantum interference, which is often illustrated with thedouble-slit experiment. In the basic version of this experiment, acoherent light source, such as alaserbeam, illuminates a plate pierced by two parallel slits, and the light passing through the slits is observed on a screen behind the plate.[8]: 102–111[2]: 1.1–1.8The wave nature of light causes the light waves passing through the two slits tointerfere, producing bright and dark bands on the screen – a result that would not be expected if light consisted of classical particles.[8]However, the light is always found to be absorbed at the screen at discrete points, as individual particles rather than waves; the interference pattern appears via the varying density of these particle hits on the screen. Furthermore, versions of the experiment that include detectors at the slits find that each detectedphotonpasses through one slit (as would a classical particle), and not through both slits (as would a wave).[8]: 109[9][10]However,such experimentsdemonstrate that particles do not form the interference pattern if one detects which slit they pass through.  This behavior is known aswave–particle duality. In addition to light,electrons,atoms, andmoleculesare all found to exhibit the same dual behavior when fired towards a double slit.[2]
Another non-classical phenomenon predicted by quantum mechanics isquantum tunnelling: a particle that goes up against apotential barriercan cross it, even if its kinetic energy is smaller than the maximum of the potential.[11]In classical mechanics this particle would be trapped. Quantum tunnelling has several important consequences, enablingradioactive decay,nuclear fusionin stars, and applications such asscanning tunnelling microscopy,tunnel diodeandtunnel field-effect transistor.[12][13]
When quantum systems interact, the result can be the creation ofquantum entanglement: their properties become so intertwined that a description of the whole solely in terms of the individual parts is no longer possible. Erwin Schrödinger called entanglement "...thecharacteristic trait of quantum mechanics, the one that enforces its entire departure from classical lines of thought".[14]Quantum entanglement enablesquantum computingand is part of quantum communication protocols, such asquantum key distributionandsuperdense coding.[15]Contrary to popular misconception, entanglement does not allow sending signalsfaster than light, as demonstrated by theno-communication theorem.[15]
Another possibility opened by entanglement is testing for "hidden variables", hypothetical properties more fundamental than the quantities addressed in quantum theory itself, knowledge of which would allow more exact predictions than quantum theory provides. A collection of results, most significantlyBell's theorem, have demonstrated that broad classes of such hidden-variable theories are in fact incompatible with quantum physics. According to Bell's theorem, if nature actually operates in accord with any theory oflocalhidden variables, then the results of aBell testwill be constrained in a particular, quantifiable way. Many Bell tests have been performed and they have shown results incompatible with the constraints imposed by local hidden variables.[16][17]
It is not possible to present these concepts in more than a superficial way without introducing the mathematics involved; understanding quantum mechanics requires not only manipulating complex numbers, but alsolinear algebra,differential equations,group theory, and other more advanced subjects.[18][19]Accordingly, this article will present a mathematical formulation of quantum mechanics and survey its application to some useful and oft-studied examples.
In the mathematically rigorous formulation of quantum mechanics, the state of a quantum mechanical system is a vectorψ{\displaystyle \psi }belonging to a (separable) complexHilbert spaceH{\displaystyle {\mathcal {H}}}. This vector is postulated to be normalized under the Hilbert space inner product, that is, it obeys⟨ψ,ψ⟩=1{\displaystyle \langle \psi ,\psi \rangle =1}, and it is well-defined up to a complex number of modulus 1 (the global phase), that is,ψ{\displaystyle \psi }andeiαψ{\displaystyle e^{i\alpha }\psi }represent the same physical system. In other words, the possible states are points in theprojective spaceof a Hilbert space, usually called thecomplex projective space. The exact nature of this Hilbert space is dependent on the system – for example, for describing position and momentum the Hilbert space is the space of complexsquare-integrablefunctionsL2(C){\displaystyle L^{2}(\mathbb {C} )}, while the Hilbert space for thespinof a single proton is simply the space of two-dimensional complex vectorsC2{\displaystyle \mathbb {C} ^{2}}with the usual inner product.
Physical quantities of interest – position, momentum, energy, spin – are represented by observables, which areHermitian(more precisely,self-adjoint) linearoperatorsacting on the Hilbert space. A quantum state can be aneigenvectorof an observable, in which case it is called aneigenstate, and the associatedeigenvaluecorresponds to the value of the observable in that eigenstate. More generally, a quantum state will be a linear combination of the eigenstates, known as aquantum superposition. When an observable is measured, the result will be one of its eigenvalues with probability given by theBorn rule: in the simplest case the eigenvalueλ{\displaystyle \lambda }is non-degenerate and the probability is given by|⟨λ→,ψ⟩|2{\displaystyle |\langle {\vec {\lambda }},\psi \rangle |^{2}}, whereλ→{\displaystyle {\vec {\lambda }}}is its associated unit-length eigenvector. More generally, the eigenvalue is degenerate and the probability is given by⟨ψ,Pλψ⟩{\displaystyle \langle \psi ,P_{\lambda }\psi \rangle }, wherePλ{\displaystyle P_{\lambda }}is the projector onto its associated eigenspace. In the continuous case, these formulas give instead theprobability density.
After the measurement, if resultλ{\displaystyle \lambda }was obtained, the quantum state is postulated tocollapsetoλ→{\displaystyle {\vec {\lambda }}}, in the non-degenerate case, or toPλψ/⟨ψ,Pλψ⟩{\textstyle P_{\lambda }\psi {\big /}\!{\sqrt {\langle \psi ,P_{\lambda }\psi \rangle }}}, in the general case. Theprobabilisticnature of quantum mechanics thus stems from the act of measurement. This is one of the most difficult aspects of quantum systems to understand. It was the central topic in the famousBohr–Einstein debates, in which the two scientists attempted to clarify these fundamental principles by way ofthought experiments. In the decades after the formulation of quantum mechanics, the question of what constitutes a "measurement" has been extensively studied. Newerinterpretations of quantum mechanicshave been formulated that do away with the concept of "wave function collapse" (see, for example, themany-worlds interpretation). The basic idea is that when a quantum system interacts with a measuring apparatus, their respective wave functions becomeentangledso that the original quantum system ceases to exist as an independent entity (seeMeasurement in quantum mechanics[20]).
The time evolution of a quantum state is described by the Schrödinger equation:iℏ∂∂tψ(t)=Hψ(t).{\displaystyle i\hbar {\frac {\partial }{\partial t}}\psi (t)=H\psi (t).}HereH{\displaystyle H}denotes theHamiltonian, the observable corresponding to thetotal energyof the system, andℏ{\displaystyle \hbar }is the reducedPlanck constant. The constantiℏ{\displaystyle i\hbar }is introduced so that the Hamiltonian is reduced to theclassical Hamiltonianin cases where the quantum system can be approximated by a classical system; the ability to make such an approximation in certain limits is called thecorrespondence principle.
The solution of this differential equation is given byψ(t)=e−iHt/ℏψ(0).{\displaystyle \psi (t)=e^{-iHt/\hbar }\psi (0).}The operatorU(t)=e−iHt/ℏ{\displaystyle U(t)=e^{-iHt/\hbar }}is known as the time-evolution operator, and has the crucial property that it isunitary. This time evolution isdeterministicin the sense that – given an initial quantum stateψ(0){\displaystyle \psi (0)}– it makes a definite prediction of what the quantum stateψ(t){\displaystyle \psi (t)}will be at any later time.[21]
Some wave functions produce probability distributions that are independent of time, such aseigenstatesof the Hamiltonian.[7]: 133–137Many systems that are treated dynamically in classical mechanics are described by such "static" wave functions. For example, a single electron in an unexcited atom is pictured classically as a particle moving in a circular trajectory around theatomic nucleus, whereas in quantum mechanics, it is described by a static wave function surrounding the nucleus. For example, the electron wave function for an unexcited hydrogen atom is a spherically symmetric function known as ansorbital(Fig. 1).
Analytic solutions of the Schrödinger equation are known forvery few relatively simple model Hamiltoniansincluding thequantum harmonic oscillator, theparticle in a box, thedihydrogen cation, and thehydrogen atom. Even theheliumatom – which contains just two electrons – has defied all attempts at a fully analytic treatment, admitting no solution inclosed form.[22][23][24]
However, there are techniques for finding approximate solutions. One method, calledperturbation theory, uses the analytic result for a simple quantum mechanical model to create a result for a related but more complicated model by (for example) the addition of a weakpotential energy.[7]: 793Another approximation method applies to systems for which quantum mechanics produces only small deviations from classical behavior. These deviations can then be computed based on the classical motion.[7]: 849
One consequence of the basic quantum formalism is the uncertainty principle. In its most familiar form, this states that no preparation of a quantum particle can imply simultaneously precise predictions both for a measurement of its position and for a measurement of its momentum.[25][26]Both position and momentum are observables, meaning that they are represented byHermitian operators. The position operatorX^{\displaystyle {\hat {X}}}and momentum operatorP^{\displaystyle {\hat {P}}}do not commute, but rather satisfy thecanonical commutation relation:[X^,P^]=iℏ.{\displaystyle [{\hat {X}},{\hat {P}}]=i\hbar .}Given a quantum state, the Born rule lets us compute expectation values for bothX{\displaystyle X}andP{\displaystyle P}, and moreover for powers of them. Defining the uncertainty for an observable by astandard deviation, we haveσX=⟨X2⟩−⟨X⟩2,{\displaystyle \sigma _{X}={\textstyle {\sqrt {\left\langle X^{2}\right\rangle -\left\langle X\right\rangle ^{2}}}},}and likewise for the momentum:σP=⟨P2⟩−⟨P⟩2.{\displaystyle \sigma _{P}={\sqrt {\left\langle P^{2}\right\rangle -\left\langle P\right\rangle ^{2}}}.}The uncertainty principle states thatσXσP≥ℏ2.{\displaystyle \sigma _{X}\sigma _{P}\geq {\frac {\hbar }{2}}.}Either standard deviation can in principle be made arbitrarily small, but not both simultaneously.[27]This inequality generalizes to arbitrary pairs of self-adjoint operatorsA{\displaystyle A}andB{\displaystyle B}. Thecommutatorof these two operators is[A,B]=AB−BA,{\displaystyle [A,B]=AB-BA,}and this provides the lower bound on the product of standard deviations:σAσB≥12|⟨[A,B]⟩|.{\displaystyle \sigma _{A}\sigma _{B}\geq {\tfrac {1}{2}}\left|{\bigl \langle }[A,B]{\bigr \rangle }\right|.}
Another consequence of the canonical commutation relation is that the position and momentum operators areFourier transformsof each other, so that a description of an object according to its momentum is the Fourier transform of its description according to its position. The fact that dependence in momentum is the Fourier transform of the dependence in position means that the momentum operator is equivalent (up to ani/ℏ{\displaystyle i/\hbar }factor) to taking the derivative according to the position, since in Fourier analysisdifferentiation corresponds to multiplication in the dual space. This is why in quantum equations in position space, the momentumpi{\displaystyle p_{i}}is replaced by−iℏ∂∂x{\displaystyle -i\hbar {\frac {\partial }{\partial x}}}, and in particular in thenon-relativistic Schrödinger equation in position spacethe momentum-squared term is replaced with a Laplacian times−ℏ2{\displaystyle -\hbar ^{2}}.[25]
When two different quantum systems are considered together, the Hilbert space of the combined system is thetensor productof the Hilbert spaces of the two components. For example, letAandBbe two quantum systems, with Hilbert spacesHA{\displaystyle {\mathcal {H}}_{A}}andHB{\displaystyle {\mathcal {H}}_{B}}, respectively. The Hilbert space of the composite system is thenHAB=HA⊗HB.{\displaystyle {\mathcal {H}}_{AB}={\mathcal {H}}_{A}\otimes {\mathcal {H}}_{B}.}If the state for the first system is the vectorψA{\displaystyle \psi _{A}}and the state for the second system isψB{\displaystyle \psi _{B}}, then the state of the composite system isψA⊗ψB.{\displaystyle \psi _{A}\otimes \psi _{B}.}Not all states in the joint Hilbert spaceHAB{\displaystyle {\mathcal {H}}_{AB}}can be written in this form, however, because the superposition principle implies that linear combinations of these "separable" or "product states" are also valid. For example, ifψA{\displaystyle \psi _{A}}andϕA{\displaystyle \phi _{A}}are both possible states for systemA{\displaystyle A}, and likewiseψB{\displaystyle \psi _{B}}andϕB{\displaystyle \phi _{B}}are both possible states for systemB{\displaystyle B}, then12(ψA⊗ψB+ϕA⊗ϕB){\displaystyle {\tfrac {1}{\sqrt {2}}}\left(\psi _{A}\otimes \psi _{B}+\phi _{A}\otimes \phi _{B}\right)}is a valid joint state that is not separable. States that are not separable are calledentangled.[28][29]
If the state for a composite system is entangled, it is impossible to describe either component systemAor systemBby a state vector. One can instead definereduced density matricesthat describe the statistics that can be obtained by making measurements on either component system alone. This necessarily causes a loss of information, though: knowing the reduced density matrices of the individual systems is not enough to reconstruct the state of the composite system.[28][29]Just as density matrices specify the state of a subsystem of a larger system, analogously,positive operator-valued measures(POVMs) describe the effect on a subsystem of a measurement performed on a larger system. POVMs are extensively used in quantum information theory.[28][30]
As described above, entanglement is a key feature of models of measurement processes in which an apparatus becomes entangled with the system being measured. Systems interacting with the environment in which they reside generally become entangled with that environment, a phenomenon known asquantum decoherence. This can explain why, in practice, quantum effects are difficult to observe in systems larger than microscopic.[31]
There are many mathematically equivalent formulations of quantum mechanics. One of the oldest and most common is the "transformation theory" proposed byPaul Dirac, which unifies and generalizes the two earliest formulations of quantum mechanics –matrix mechanics(invented byWerner Heisenberg) and wave mechanics (invented byErwin Schrödinger).[32]An alternative formulation of quantum mechanics isFeynman'spath integral formulation, in which a quantum-mechanical amplitude is considered as a sum over all possible classical and non-classical paths between the initial and final states. This is the quantum-mechanical counterpart of theaction principlein classical mechanics.[33]
The HamiltonianH{\displaystyle H}is known as thegeneratorof time evolution, since it defines a unitary time-evolution operatorU(t)=e−iHt/ℏ{\displaystyle U(t)=e^{-iHt/\hbar }}for each value oft{\displaystyle t}. From this relation betweenU(t){\displaystyle U(t)}andH{\displaystyle H}, it follows that any observableA{\displaystyle A}that commutes withH{\displaystyle H}will beconserved: its expectation value will not change over time.[7]: 471This statement generalizes, as mathematically, any Hermitian operatorA{\displaystyle A}can generate a family of unitary operators parameterized by a variablet{\displaystyle t}. Under the evolution generated byA{\displaystyle A}, any observableB{\displaystyle B}that commutes withA{\displaystyle A}will be conserved. Moreover, ifB{\displaystyle B}is conserved by evolution underA{\displaystyle A}, thenA{\displaystyle A}is conserved under the evolution generated byB{\displaystyle B}. This implies a quantum version of the result proven byEmmy Noetherin classical (Lagrangian) mechanics: for everydifferentiablesymmetryof a Hamiltonian, there exists a correspondingconservation law.
The simplest example of a quantum system with a position degree of freedom is a free particle in a single spatial dimension. A free particle is one which is not subject to external influences, so that its Hamiltonian consists only of its kinetic energy:H=12mP2=−ℏ22md2dx2.{\displaystyle H={\frac {1}{2m}}P^{2}=-{\frac {\hbar ^{2}}{2m}}{\frac {d^{2}}{dx^{2}}}.}The general solution of the Schrödinger equation is given byψ(x,t)=12π∫−∞∞ψ^(k,0)ei(kx−ℏk22mt)dk,{\displaystyle \psi (x,t)={\frac {1}{\sqrt {2\pi }}}\int _{-\infty }^{\infty }{\hat {\psi }}(k,0)e^{i(kx-{\frac {\hbar k^{2}}{2m}}t)}\mathrm {d} k,}which is a superposition of all possibleplane wavesei(kx−ℏk22mt){\displaystyle e^{i(kx-{\frac {\hbar k^{2}}{2m}}t)}}, which are eigenstates of the momentum operator with momentump=ℏk{\displaystyle p=\hbar k}. The coefficients of the superposition areψ^(k,0){\displaystyle {\hat {\psi }}(k,0)}, which is the Fourier transform of the initial quantum stateψ(x,0){\displaystyle \psi (x,0)}.
It is not possible for the solution to be a single momentum eigenstate, or a single position eigenstate, as these are not normalizable quantum states.[note 1]Instead, we can consider a Gaussianwave packet:ψ(x,0)=1πa4e−x22a{\displaystyle \psi (x,0)={\frac {1}{\sqrt[{4}]{\pi a}}}e^{-{\frac {x^{2}}{2a}}}}which has Fourier transform, and therefore momentum distributionψ^(k,0)=aπ4e−ak22.{\displaystyle {\hat {\psi }}(k,0)={\sqrt[{4}]{\frac {a}{\pi }}}e^{-{\frac {ak^{2}}{2}}}.}We see that as we makea{\displaystyle a}smaller the spread in position gets smaller, but the spread in momentum gets larger. Conversely, by makinga{\displaystyle a}larger we make the spread in momentum smaller, but the spread in position gets larger. This illustrates the uncertainty principle.
As we let the Gaussian wave packet evolve in time, we see that its center moves through space at a constant velocity (like a classical particle with no forces acting on it). However, the wave packet will also spread out as time progresses, which means that the position becomes more and more uncertain. The uncertainty in momentum, however, stays constant.[34]
The particle in a one-dimensional potential energy box is the most mathematically simple example where restraints lead to the quantization of energy levels. The box is defined as having zero potential energy everywhereinsidea certain region, and therefore infinite potential energy everywhereoutsidethat region.[25]: 77–78For the one-dimensional case in thex{\displaystyle x}direction, the time-independent Schrödinger equation may be written−ℏ22md2ψdx2=Eψ.{\displaystyle -{\frac {\hbar ^{2}}{2m}}{\frac {d^{2}\psi }{dx^{2}}}=E\psi .}
With the differential operator defined byp^x=−iℏddx{\displaystyle {\hat {p}}_{x}=-i\hbar {\frac {d}{dx}}}the previous equation is evocative of theclassic kinetic energy analogue,12mp^x2=E,{\displaystyle {\frac {1}{2m}}{\hat {p}}_{x}^{2}=E,}with stateψ{\displaystyle \psi }in this case having energyE{\displaystyle E}coincident with the kinetic energy of the particle.
The general solutions of the Schrödinger equation for the particle in a box areψ(x)=Aeikx+Be−ikxE=ℏ2k22m{\displaystyle \psi (x)=Ae^{ikx}+Be^{-ikx}\qquad \qquad E={\frac {\hbar ^{2}k^{2}}{2m}}}or, fromEuler's formula,ψ(x)=Csin⁡(kx)+Dcos⁡(kx).{\displaystyle \psi (x)=C\sin(kx)+D\cos(kx).\!}
The infinite potential walls of the box determine the values ofC,D,{\displaystyle C,D,}andk{\displaystyle k}atx=0{\displaystyle x=0}andx=L{\displaystyle x=L}whereψ{\displaystyle \psi }must be zero. Thus, atx=0{\displaystyle x=0},ψ(0)=0=Csin⁡(0)+Dcos⁡(0)=D{\displaystyle \psi (0)=0=C\sin(0)+D\cos(0)=D}andD=0{\displaystyle D=0}. Atx=L{\displaystyle x=L},ψ(L)=0=Csin⁡(kL),{\displaystyle \psi (L)=0=C\sin(kL),}in whichC{\displaystyle C}cannot be zero as this would conflict with the postulate thatψ{\displaystyle \psi }has norm 1. Therefore, sincesin⁡(kL)=0{\displaystyle \sin(kL)=0},kL{\displaystyle kL}must be an integer multiple ofπ{\displaystyle \pi },k=nπLn=1,2,3,….{\displaystyle k={\frac {n\pi }{L}}\qquad \qquad n=1,2,3,\ldots .}
This constraint onk{\displaystyle k}implies a constraint on the energy levels, yieldingEn=ℏ2π2n22mL2=n2h28mL2.{\displaystyle E_{n}={\frac {\hbar ^{2}\pi ^{2}n^{2}}{2mL^{2}}}={\frac {n^{2}h^{2}}{8mL^{2}}}.}
Afinite potential wellis the generalization of the infinite potential well problem to potential wells having finite depth. The finite potential well problem is mathematically more complicated than the infinite particle-in-a-box problem as the wave function is not pinned to zero at the walls of the well. Instead, the wave function must satisfy more complicated mathematical boundary conditions as it is nonzero in regions outside the well. Another related problem is that of therectangular potential barrier, which furnishes a model for thequantum tunnelingeffect that plays an important role in the performance of modern technologies such asflash memoryandscanning tunneling microscopy.
As in the classical case, the potential for the quantum harmonic oscillator is given by[7]: 234V(x)=12mω2x2.{\displaystyle V(x)={\frac {1}{2}}m\omega ^{2}x^{2}.}
This problem can either be treated by directly solving the Schrödinger equation, which is not trivial, or by using the more elegant "ladder method" first proposed by Paul Dirac. Theeigenstatesare given byψn(x)=12nn!⋅(mωπℏ)1/4⋅e−mωx22ℏ⋅Hn(mωℏx),{\displaystyle \psi _{n}(x)={\sqrt {\frac {1}{2^{n}\,n!}}}\cdot \left({\frac {m\omega }{\pi \hbar }}\right)^{1/4}\cdot e^{-{\frac {m\omega x^{2}}{2\hbar }}}\cdot H_{n}\left({\sqrt {\frac {m\omega }{\hbar }}}x\right),\qquad }n=0,1,2,….{\displaystyle n=0,1,2,\ldots .}whereHnare theHermite polynomialsHn(x)=(−1)nex2dndxn(e−x2),{\displaystyle H_{n}(x)=(-1)^{n}e^{x^{2}}{\frac {d^{n}}{dx^{n}}}\left(e^{-x^{2}}\right),}and the corresponding energy levels areEn=ℏω(n+12).{\displaystyle E_{n}=\hbar \omega \left(n+{1 \over 2}\right).}
This is another example illustrating the discretization of energy forbound states.
TheMach–Zehnder interferometer(MZI) illustrates the concepts of superposition and interference with linear algebra in dimension 2, rather than differential equations. It can be seen as a simplified version of the double-slit experiment, but it is of interest in its own right, for example in thedelayed choice quantum eraser, theElitzur–Vaidman bomb tester, and in studies of quantum entanglement.[35][36]
We can model a photon going through the interferometer by considering that at each point it can be in a superposition of only two paths: the "lower" path which starts from the left, goes straight through both beam splitters, and ends at the top, and the "upper" path which starts from the bottom, goes straight through both beam splitters, and ends at the right. The quantum state of the photon is therefore a vectorψ∈C2{\displaystyle \psi \in \mathbb {C} ^{2}}that is a superposition of the "lower" pathψl=(10){\displaystyle \psi _{l}={\begin{pmatrix}1\\0\end{pmatrix}}}and the "upper" pathψu=(01){\displaystyle \psi _{u}={\begin{pmatrix}0\\1\end{pmatrix}}}, that is,ψ=αψl+βψu{\displaystyle \psi =\alpha \psi _{l}+\beta \psi _{u}}for complexα,β{\displaystyle \alpha ,\beta }. In order to respect the postulate that⟨ψ,ψ⟩=1{\displaystyle \langle \psi ,\psi \rangle =1}we require that|α|2+|β|2=1{\displaystyle |\alpha |^{2}+|\beta |^{2}=1}.
Bothbeam splittersare modelled as the unitary matrixB=12(1ii1){\displaystyle B={\frac {1}{\sqrt {2}}}{\begin{pmatrix}1&i\\i&1\end{pmatrix}}}, which means that when a photon meets the beam splitter it will either stay on the same path with a probability amplitude of1/2{\displaystyle 1/{\sqrt {2}}}, or be reflected to the other path with a probability amplitude ofi/2{\displaystyle i/{\sqrt {2}}}. The phase shifter on the upper arm is modelled as the unitary matrixP=(100eiΔΦ){\displaystyle P={\begin{pmatrix}1&0\\0&e^{i\Delta \Phi }\end{pmatrix}}}, which means that if the photon is on the "upper" path it will gain a relative phase ofΔΦ{\displaystyle \Delta \Phi }, and it will stay unchanged if it is in the lower path.
A photon that enters the interferometer from the left will then be acted upon with a beam splitterB{\displaystyle B}, a phase shifterP{\displaystyle P}, and another beam splitterB{\displaystyle B}, and so end up in the stateBPBψl=ieiΔΦ/2(−sin⁡(ΔΦ/2)cos⁡(ΔΦ/2)),{\displaystyle BPB\psi _{l}=ie^{i\Delta \Phi /2}{\begin{pmatrix}-\sin(\Delta \Phi /2)\\\cos(\Delta \Phi /2)\end{pmatrix}},}and the probabilities that it will be detected at the right or at the top are given respectively byp(u)=|⟨ψu,BPBψl⟩|2=cos2⁡ΔΦ2,{\displaystyle p(u)=|\langle \psi _{u},BPB\psi _{l}\rangle |^{2}=\cos ^{2}{\frac {\Delta \Phi }{2}},}p(l)=|⟨ψl,BPBψl⟩|2=sin2⁡ΔΦ2.{\displaystyle p(l)=|\langle \psi _{l},BPB\psi _{l}\rangle |^{2}=\sin ^{2}{\frac {\Delta \Phi }{2}}.}One can therefore use the Mach–Zehnder interferometer to estimate thephase shiftby estimating these probabilities.
It is interesting to consider what would happen if the photon were definitely in either the "lower" or "upper" paths between the beam splitters. This can be accomplished by blocking one of the paths, or equivalently by removing the first beam splitter (and feeding the photon from the left or the bottom, as desired). In both cases, there will be no interference between the paths anymore, and the probabilities are given byp(u)=p(l)=1/2{\displaystyle p(u)=p(l)=1/2}, independently of the phaseΔΦ{\displaystyle \Delta \Phi }. From this we can conclude that the photon does not take one path or another after the first beam splitter, but rather that it is in a genuine quantum superposition of the two paths.[37]
Quantum mechanics has had enormous success in explaining many of the features of our universe, with regard to small-scale and discrete quantities and interactions which cannot be explained byclassical methods.[note 2]Quantum mechanics is often the only theory that can reveal the individual behaviors of the subatomic particles that make up all forms of matter (electrons,protons,neutrons,photons, and others).Solid-state physicsandmaterials scienceare dependent upon quantum mechanics.[38]
In many aspects, modern technology operates at a scale where quantum effects are significant. Important applications of quantum theory includequantum chemistry,quantum optics,quantum computing,superconducting magnets,light-emitting diodes, theoptical amplifierand the laser, thetransistorandsemiconductorssuch as themicroprocessor,medical and research imagingsuch asmagnetic resonance imagingandelectron microscopy.[39]Explanations for many biological and physical phenomena are rooted in the nature of the chemical bond, most notably the macro-moleculeDNA.
The rules of quantum mechanics assert that the state space of a system is a Hilbert space and that observables of the system are Hermitian operators acting on vectors in that space – although they do not tell us which Hilbert space or which operators. These can be chosen appropriately in order to obtain a quantitative description of a quantum system, a necessary step in making physical predictions. An important guide for making these choices is thecorrespondence principle, a heuristic which states that the predictions of quantum mechanics reduce to those ofclassical mechanicsin the regime of largequantum numbers.[40]One can also start from an established classical model of a particular system, and then try to guess the underlying quantum model that would give rise to the classical model in the correspondence limit. This approach is known asquantization.[41]: 299[42]
When quantum mechanics was originally formulated, it was applied to models whose correspondence limit wasnon-relativisticclassical mechanics. For instance, the well-known model of thequantum harmonic oscillatoruses an explicitly non-relativistic expression for thekinetic energyof the oscillator, and is thus a quantum version of theclassical harmonic oscillator.[7]: 234
Complications arise withchaotic systems, which do not have good quantum numbers, andquantum chaosstudies the relationship between classical and quantum descriptions in these systems.[41]: 353
Quantum decoherenceis a mechanism through which quantum systems losecoherence, and thus become incapable of displaying many typically quantum effects:quantum superpositionsbecome simply probabilistic mixtures, and quantum entanglement becomes simply classical correlations.[7]: 687–730Quantum coherence is not typically evident at macroscopic scales, though at temperatures approachingabsolute zeroquantum behavior may manifest macroscopically.[note 3]
Many macroscopic properties of a classical system are a direct consequence of the quantum behavior of its parts. For example, the stability of bulk matter (consisting of atoms andmoleculeswhich would quickly collapse under electric forces alone), the rigidity of solids, and the mechanical, thermal, chemical, optical and magnetic properties of matter are all results of the interaction ofelectric chargesunder the rules of quantum mechanics.[43]
Early attempts to merge quantum mechanics withspecial relativityinvolved the replacement of the Schrödinger equation with a covariant equation such as theKlein–Gordon equationor theDirac equation. While these theories were successful in explaining many experimental results, they had certain unsatisfactory qualities stemming from their neglect of the relativistic creation and annihilation of particles. A fully relativistic quantum theory required the development of quantum field theory, which applies quantization to a field (rather than a fixed set of particles). The first complete quantum field theory,quantum electrodynamics, provides a fully quantum description of theelectromagnetic interaction. Quantum electrodynamics is, along withgeneral relativity, one of the most accurate physical theories ever devised.[44][45]
The full apparatus of quantum field theory is often unnecessary for describing electrodynamic systems. A simpler approach, one that has been used since the inception of quantum mechanics, is to treatchargedparticles as quantum mechanical objects being acted on by a classicalelectromagnetic field. For example, the elementary quantum model of thehydrogen atomdescribes theelectric fieldof the hydrogen atom using a classical−e2/(4πϵ0r){\displaystyle \textstyle -e^{2}/(4\pi \epsilon _{_{0}}r)}Coulomb potential.[7]: 285Likewise, in aStern–Gerlach experiment, a charged particle is modeled as a quantum system, while the background magnetic field is described classically.[41]: 26This "semi-classical" approach fails if quantum fluctuations in the electromagnetic field play an important role, such as in the emission of photons bycharged particles.
Quantum fieldtheories for thestrong nuclear forceand theweak nuclear forcehave also been developed. The quantum field theory of the strong nuclear force is calledquantum chromodynamics, and describes the interactions of subnuclear particles such asquarksandgluons. The weak nuclear force and the electromagnetic force were unified, in their quantized forms, into a single quantum field theory (known aselectroweak theory), by the physicistsAbdus Salam,Sheldon GlashowandSteven Weinberg.[46]
Even though the predictions of both quantum theory and general relativity have been supported by rigorous and repeatedempirical evidence, their abstract formalisms contradict each other and they have proven extremely difficult to incorporate into one consistent, cohesive model. Gravity is negligible in many areas of particle physics, so that unification between general relativity and quantum mechanics is not an urgent issue in those particular applications. However, the lack of a correct theory ofquantum gravityis an important issue inphysical cosmologyand the search by physicists for an elegant "Theory of Everything" (TOE). Consequently, resolving the inconsistencies between both theories has been a major goal of 20th- and 21st-century physics. This TOE would combine not only the models of subatomic physics but also derive the four fundamental forces of nature from a single force or phenomenon.[47]
One proposal for doing so isstring theory, which posits that thepoint-like particlesofparticle physicsare replaced byone-dimensionalobjects calledstrings. String theory describes how these strings propagate through space and interact with each other. On distance scales larger than the string scale, a string looks just like an ordinary particle, with itsmass,charge, and other properties determined by thevibrationalstate of the string. In string theory, one of the many vibrational states of the string corresponds to thegraviton, a quantum mechanical particle that carries gravitational force.[48][49]
Another popular theory isloop quantum gravity(LQG), which describes quantum properties of gravity and is thus a theory ofquantum spacetime. LQG is an attempt to merge and adapt standard quantum mechanics and standard general relativity. This theory describes space as an extremely fine fabric "woven" of finite loops calledspin networks. The evolution of a spin network over time is called aspin foam. The characteristic length scale of a spin foam is thePlanck length, approximately 1.616×10−35m, and so lengths shorter than the Planck length are not physically meaningful in LQG.[50]
Since its inception, the many counter-intuitive aspects and results of quantum mechanics have provoked strongphilosophicaldebates and manyinterpretations. The arguments centre on the probabilistic nature of quantum mechanics, the difficulties withwavefunction collapseand the relatedmeasurement problem, andquantum nonlocality. Perhaps the only consensus that exists about these issues is that there is no consensus.Richard Feynmanonce said, "I think I can safely say that nobody understands quantum mechanics."[51]According toSteven Weinberg, "There is now in my opinion no entirely satisfactory interpretation of quantum mechanics."[52]
The views ofNiels Bohr, Werner Heisenberg and other physicists are often grouped together as the "Copenhagen interpretation".[53][54]According to these views, the probabilistic nature of quantum mechanics is not atemporaryfeature which will eventually be replaced by a deterministic theory, but is instead afinalrenunciation of the classical idea of "causality". Bohr in particular emphasized that any well-defined application of the quantum mechanical formalism must always make reference to the experimental arrangement, due to thecomplementarynature of evidence obtained under different experimental situations. Copenhagen-type interpretations were adopted by Nobel laureates in quantum physics, including Bohr,[55]Heisenberg,[56]Schrödinger,[57]Feynman,[2]and Zeilinger[58]as well as 21st-century researchers in quantum foundations.[59]
Albert Einstein, himself one of the founders ofquantum theory, was troubled by its apparent failure to respect some cherished metaphysical principles, such asdeterminismandlocality. Einstein's long-running exchanges with Bohr about the meaning and status of quantum mechanics are now known as theBohr–Einstein debates. Einstein believed that underlying quantum mechanics must be a theory that explicitly forbidsaction at a distance. He argued that quantum mechanics was incomplete, a theory that was valid but not fundamental, analogous to howthermodynamicsis valid, but the fundamental theory behind it isstatistical mechanics. In 1935, Einstein and his collaboratorsBoris PodolskyandNathan Rosenpublished an argument that the principle of locality implies the incompleteness of quantum mechanics, athought experimentlater termed theEinstein–Podolsky–Rosen paradox.[note 4]In 1964,John Bellshowed that EPR's principle of locality, together with determinism, was actually incompatible with quantum mechanics: they implied constraints on the correlations produced by distance systems, now known asBell inequalities, that can be violated by entangled particles.[64]Since thenseveral experimentshave been performed to obtain these correlations, with the result that they do in fact violate Bell inequalities, and thus falsify the conjunction of locality with determinism.[16][17]
Bohmian mechanicsshows that it is possible to reformulate quantum mechanics to make it deterministic, at the price of making it explicitly nonlocal. It attributes not only a wave function to a physical system, but in addition a real position, that evolves deterministically under a nonlocal guiding equation. The evolution of a physical system is given at all times by the Schrödinger equation together with the guiding equation; there is never a collapse of the wave function. This solves the measurement problem.[65]
Everett'smany-worlds interpretation, formulated in 1956, holds thatallthe possibilities described by quantum theorysimultaneouslyoccur in a multiverse composed of mostly independent parallel universes.[66]This is a consequence of removing the axiom of the collapse of the wave packet. All possible states of the measured system and the measuring apparatus, together with the observer, are present in a real physical quantum superposition. While the multiverse is deterministic, we perceive non-deterministic behavior governed by probabilities, because we do not observe the multiverse as a whole, but only one parallel universe at a time. Exactly how this is supposed to work has been the subject of much debate. Several attempts have been made to make sense of this and derive the Born rule,[67][68]with no consensus on whether they have been successful.[69][70][71]
Relational quantum mechanicsappeared in the late 1990s as a modern derivative of Copenhagen-type ideas,[72]andQuantum Bayesianismwas developed some years later.[73]
Quantum mechanics was developed in the early decades of the 20th century, driven by the need to explain phenomena that, in some cases, had been observed in earlier times. Scientific inquiry into the wave nature of light began in the 17th and 18th centuries, when scientists such asRobert Hooke,Christiaan HuygensandLeonhard Eulerproposed a wave theory of light based on experimental observations.[74]In 1803 EnglishpolymathThomas Youngdescribed the famousdouble-slit experiment.[75]This experiment played a major role in the general acceptance of thewave theory of light.
During the early 19th century,chemicalresearch byJohn DaltonandAmedeo Avogadrolent weight to theatomic theoryof matter, an idea thatJames Clerk Maxwell,Ludwig Boltzmannand others built upon to establish thekinetic theory of gases. The successes of kinetic theory gave further credence to the idea that matter is composed of atoms, yet the theory also had shortcomings that would only be resolved by the development of quantum mechanics.[76]While the early conception of atoms fromGreek philosophyhad been that they were indivisible units – the word "atom" deriving from theGreekfor 'uncuttable' – the 19th century saw the formulation of hypotheses about subatomic structure. One important discovery in that regard wasMichael Faraday's 1838 observation of a glow caused by an electrical discharge inside a glass tube containing gas at low pressure.Julius Plücker,Johann Wilhelm HittorfandEugen Goldsteincarried on and improved upon Faraday's work, leading to the identification ofcathode rays, whichJ. J. Thomsonfound to consist of subatomic particles that would be called electrons.[77][78]
Theblack-body radiationproblem was discovered byGustav Kirchhoffin 1859. In 1900, Max Planck proposed the hypothesis that energy is radiated and absorbed in discrete "quanta" (or energy packets), yielding a calculation that precisely matched the observed patterns of black-body radiation.[79]The wordquantumderives from theLatin, meaning "how great" or "how much".[80]According to Planck, quantities of energy could be thought of as divided into "elements" whose size (E) would be proportional to theirfrequency(ν):E=hν{\displaystyle E=h\nu \ },
wherehis thePlanck constant. Planck cautiously insisted that this was only an aspect of the processes of absorption and emission of radiation and was not thephysical realityof the radiation.[81]In fact, he considered his quantum hypothesis a mathematical trick to get the right answer rather than a sizable discovery.[82]However, in 1905 Albert Einstein interpreted Planck's quantum hypothesisrealisticallyand used it to explain thephotoelectric effect, in which shining light on certain materials can eject electrons from the material. Niels Bohr then developed Planck's ideas about radiation into amodel of the hydrogen atomthat successfully predicted thespectral linesof hydrogen.[83]Einstein further developed this idea to show that anelectromagnetic wavesuch as light could also be described as a particle (later called the photon), with a discrete amount of energy that depends on its frequency.[84]In his paper "On the Quantum Theory of Radiation", Einstein expanded on the interaction between energy and matter to explain the absorption and emission of energy by atoms. Although overshadowed at the time by his general theory of relativity, this paper articulated the mechanism underlying the stimulated emission of radiation,[85]which became the basis of the laser.[86]
This phase is known as theold quantum theory. Never complete or self-consistent, the old quantum theory was rather a set ofheuristiccorrections to classical mechanics.[87][88]The theory is now understood as asemi-classical approximationto modern quantum mechanics.[89][90]Notable results from this period include, in addition to the work of Planck, Einstein and Bohr mentioned above, Einstein andPeter Debye's work on thespecific heatof solids, Bohr andHendrika Johanna van Leeuwen'sproofthat classical physics cannot account fordiamagnetism, andArnold Sommerfeld's extension of the Bohr model to include special-relativistic effects.[87][91]
In the mid-1920s quantum mechanics was developed to become the standard formulation for atomic physics. In 1923, the French physicistLouis de Broglieput forward his theory of matter waves by stating that particles can exhibit wave characteristics and vice versa. Building on de Broglie's approach, modern quantum mechanics was born in 1925, when the German physicists Werner Heisenberg, Max Born, andPascual Jordan[92][93]developedmatrix mechanicsand the Austrian physicist Erwin Schrödinger inventedwave mechanics. Born introduced the probabilistic interpretation of Schrödinger's wave function in July 1926.[94]Thus, the entire field of quantum physics emerged, leading to its wider acceptance at the FifthSolvay Conferencein 1927.[95]
By 1930, quantum mechanics had been further unified and formalized byDavid Hilbert, Paul Dirac andJohn von Neumann[96]with greater emphasis onmeasurement, the statistical nature of our knowledge of reality, andphilosophical speculation about the 'observer'. It has since permeated many disciplines, including quantum chemistry,quantum electronics,quantum optics, andquantum information science. It also provides a useful framework for many features of the modernperiodic table of elements, and describes the behaviors ofatomsduringchemical bondingand the flow of electrons in computersemiconductors, and therefore plays a crucial role in many modern technologies. While quantum mechanics was constructed to describe the world of the very small, it is also needed to explain somemacroscopicphenomena such assuperconductors[97]andsuperfluids.[98]
The following titles, all by working physicists, attempt to communicate quantum theory to lay people, using a minimum of technical apparatus:
World War II[b]or theSecond World War(1 September 1939 – 2 September 1945) was aglobal conflictbetween two coalitions: theAlliesand theAxis powers.Nearly all of the world's countriesparticipated, with many nations mobilising all resources in pursuit oftotal war.Tanksandaircraft played major roles, enabling thestrategic bombingof cities and delivery of thefirst and only nuclear weaponsever used in war. World War II was thedeadliest conflictin history, resulting in70 to 85 million deaths, more than half of which were civilians. Millions died ingenocides, includingthe Holocaust, and by massacres, starvation, and disease. After the Allied victory,Germany,Austria,Japan, andKoreawere occupied, and German and Japanese leaders were tried forwar crimes.
Thecauses of World War IIincluded unresolved tensions in theaftermath of World War Iand the rises offascism in Europeandmilitarism in Japan. Key events preceding the war includedJapan's invasion of Manchuriain 1931, theSpanish Civil War, the outbreak of theSecond Sino-Japanese Warin 1937, and Germany'sannexations of Austriaandthe Sudetenland. World War II is generally considered to have begun on 1 September 1939, whenNazi Germany, underAdolf Hitler,invaded Poland, after which theUnited KingdomandFrancedeclared war on Germany. Poland was divided between Germany and theSoviet Unionunder theMolotov–Ribbentrop Pact. In 1940, the Sovietsannexed the Baltic statesandparts of FinlandandRomania. After thefall of Francein June 1940, the war continued mainly between Germany and theBritish Empire, with fighting in theBalkans,Mediterranean, and Middle East, the aerialBattle of Britainandthe Blitz, and navalBattle of the Atlantic. Through campaigns and treaties, Germany gained control of much ofcontinental Europeandformed the Axis alliancewithItaly,Japan, and other countries. In June 1941, Germany ledan invasion of the Soviet Union, opening theEastern Frontand initially making large territorial gains.
In December 1941, Japan attacked American and British territoriesin Asia and the Pacific, including atPearl Harbor in Hawaii, leading theUnited Statesto enter the war against Japan and Germany. Japan conquered much of coastal China and Southeast Asia, but its advances in the Pacific were halted in June 1942 at theBattle of Midway. In late 1942, Axis forces were defeatedin North Africaandat Stalingradin the Soviet Union, and in 1943 their continued defeats on the Eastern Front, anAllied invasion of Italy, and Allied offensives in the Pacific forced them into retreat on all fronts. In 1944, the Western Alliesinvaded France at Normandyas the Soviet Unionrecaptured its pre-war territoryand the U.S. crippled Japan's navy andcaptured key Pacific islands. The war in Europe concluded with the liberation ofGerman-occupied territories;invasions of Germany by the Western Alliesand the Soviet Union, which culminated in thefall of Berlinto Soviet troops; andGermany's unconditional surrenderon8 May 1945. On 6 and 9 August, the U.S.dropped atomic bombsonHiroshimaandNagasakiin Japan. Faced with animminent Allied invasion, the prospect of further atomic bombings, and a Sovietdeclaration of warandinvasion of Manchuria, Japan announcedits unconditional surrenderon 15 August, and signeda surrender documenton2 September 1945.
World War II transformed the political, economic, and social structures of the world, and established the foundation of international relations for the rest of the 20th century and into the 21st century. TheUnited Nationswas created to foster international cooperation and prevent future conflicts, with the victoriousgreat powers—China, France, the Soviet Union, the UK, and the U.S.—becomingthe permanent membersofits security council. The Soviet Union and U.S. emerged as rival globalsuperpowers, setting the stage for the half-centuryCold War. In the wake of Europe's devastation, the influence of its great powers waned, triggering thedecolonisation of AfricaandAsia. Many countries whose industries had been damaged moved towardseconomic recovery and expansion.
World War II began in Europe on 1 September 1939[1][2]with theGerman invasion of Polandand theUnited KingdomandFrance's declaration of war on Germany two days later on 3 September 1939. Dates for the beginning of thePacific Warinclude the start of theSecond Sino-Japanese Waron 7 July 1937,[3][4]or the earlierJapanese invasion of Manchuria, on 19 September 1931.[5][6]Others follow the British historianA. J. P. Taylor, who stated that the Sino-Japanese War and war in Europe and its colonies occurred simultaneously, and the two wars became World War II in 1941.[7]Other proposed starting dates for World War II include theItalian invasion of Abyssiniaon 3 October 1935.[8]The British historianAntony Beevorviews the beginning of World WarII as theBattles of Khalkhin Golfought betweenJapanand the forces ofMongoliaand theSoviet Unionfrom May to September 1939.[9]Others view theSpanish Civil Waras the start or prelude to World War II.[10][11]
The exact date of the war's end also is not universally agreed upon. It was generally accepted at the time that the war ended with the armistice of 15 August 1945 (V-J Day), rather than with the formalsurrender of Japanon 2 September 1945, which officiallyended the war in Asia. Apeace treaty between Japan and the Allieswas signed in 1951.[12]A 1990treaty regarding Germany's futureallowed thereunification of East and West Germanyto take place.[13]No formal peace treaty between Japan and the Soviet Union was ever signed,[14]although the state of war between the two countries was terminated by theSoviet–Japanese Joint Declaration of 1956, which also restored full diplomatic relations between them.[15]
World War Ihad radically altered the political European map with the defeat of theCentral Powers—includingAustria-Hungary,Germany,Bulgaria, and theOttoman Empire—and the 1917Bolshevik seizure of powerinRussia, which led to the founding of the Soviet Union. Meanwhile, the victoriousAllies of World War I, such as France, Belgium, Italy, Romania, and Greece, gained territory, and newnation-stateswere created out of the dissolution of the Austro-Hungarian, Ottoman, and Russian Empires.[16]
To prevent a future world war, theLeague of Nationswas established in 1920 by theParis Peace Conference. The organisation's primary goals were to prevent armed conflict through collective security, military, andnaval disarmament, as well as settling international disputes through peaceful negotiations and arbitration.[17]
Despite strong pacifist sentimentafter World WarI,[18]irredentistandrevanchistnationalismhad emerged in several European states. These sentiments were especially marked in Germany because of the significant territorial, colonial, and financial losses imposed by theTreaty of Versailles. Under the treaty, Germany lost around 13 percent of its home territory and allits overseas possessions, while German annexation of other states was prohibited,reparationswere imposed, and limits were placed on the size and capability of the country'sarmed forces.[19]
The German Empire was dissolved in theGerman revolution of 1918–1919, and a democratic government, later known as theWeimar Republic, was created. The interwar period saw strife between supporters of the new republic and hardline opponents on both the political right and left. Italy, as an Entente ally, had made some post-war territorial gains; however, Italian nationalists were angered that thepromises madeby the United Kingdom and France to secure Italian entrance into the war were not fulfilled in the peace settlement. From 1922 to 1925, theFascistmovement led byBenito Mussoliniseized power in Italy with a nationalist,totalitarian, andclass collaborationistagenda that abolished representative democracy, repressed socialist, left-wing, and liberal forces, and pursued an aggressive expansionist foreign policy aimed at making Italy a world power, promising the creation of a "New Roman Empire".[20]
Adolf Hitler, after anunsuccessful attempt to overthrow the German governmentin 1923, eventuallybecame the chancellor of Germanyin 1933 when PresidentPaul von Hindenburgand the Reichstag appointed him. Following Hindenburg's death in 1934, Hitler proclaimed himselfFührerof Germany and abolished democracy, espousing aradical, racially motivated revision of the world order, and soon began a massiverearmament campaign.[21]France, seeking to secure its alliance with Italy,allowed Italy a free hand in Ethiopia, which Italy desired as a colonial possession. The situation was aggravated in early 1935 when theTerritory of the Saar Basinwas legally reunited with Germany, and Hitler repudiated the Treaty of Versailles, accelerated his rearmament programme, and introduced conscription.[22]
The United Kingdom, France and Italy formed theStresa Frontin April 1935 in order to contain Germany, a key step towardsmilitary globalisation; however, that June, the United Kingdom made anindependent naval agreementwith Germany, easing prior restrictions. The Soviet Union, concerned by Germany'sgoals of capturing vast areas of Eastern Europe, drafted a treaty of mutual assistance with France. Before taking effect, though, theFranco-Soviet pactwas required to go through the bureaucracy of the League of Nations, which rendered it essentially toothless.[23]The United States, concerned with events in Europe and Asia, passed theNeutrality Actin August of the same year.[24]
Hitler defied the Versailles andLocarno Treatiesbyremilitarising the Rhinelandin March 1936, encountering little opposition due to the policy ofappeasement.[25]In October 1936, Germany and Italy formed theRome–Berlin Axis. A month later, Germany and Japan signed theAnti-Comintern Pact, which Italy joined the following year.[26]
TheKuomintangparty in China launched aunification campaignagainstregional warlordsand nominally unified China in the mid-1920s, but was soon embroiled ina civil waragainst its formerChinese Communist Party(CCP) allies[27]andnew regional warlords. In 1931, anincreasingly militaristicEmpire of Japan, which had long sought influence in China[28]as the first step of what its government saw as the country'sright to rule Asia, staged theMukden incidentas a pretext toinvade Manchuriaand establish thepuppet stateofManchukuo.[29]
China appealed to theLeague of Nationsto stop the Japanese invasion of Manchuria. Japan withdrew from the League of Nations after beingcondemnedfor its incursion into Manchuria. The two nations then fought several battles, inShanghai,ReheandHebei, until theTanggu Trucewas signed in 1933. Thereafter, Chinese volunteer forces continued the resistance to Japanese aggression inManchuria, andChahar and Suiyuan.[30]After the 1936Xi'an Incident, the Kuomintang and CCP forces agreed on a ceasefire to presenta united frontto oppose Japan.[31]
TheSecond Italo-Ethiopian Warwas a briefcolonial warthat began in October 1935 and ended in May 1936. The war began with the invasion of theEthiopian Empire(also known asAbyssinia) by the armed forces of theKingdom of Italy(Regno d'Italia), which was launched fromItalian SomalilandandEritrea.[32]The war resulted in themilitary occupationof Ethiopia and itsannexationinto the newly created colony ofItalian East Africa(Africa Orientale Italiana); in addition it exposed the weakness of theLeague of Nationsas a force to preserve peace. Both Italy and Ethiopia were member nations,but the League did littlewhen the former clearly violated Article X of the League'sCovenant.[33]The United Kingdom and France supported imposing sanctions on Italy for the invasion, but the sanctions were not fully enforced and failed to end the Italian invasion.[34]Italy subsequently dropped its objections to Germany's goal of absorbingAustria.[35]
When civil war broke out in Spain, Hitler and Mussolini lent military support to theNationalist rebels, led by GeneralFrancisco Franco. Italy supported the Nationalists to a greater extent than the Nazis: Mussolini sent more than 70,000 ground troops, 6,000 aviation personnel, and 720 aircraft to Spain.[36]The Soviet Union supported the existing government of theSpanish Republic. More than 30,000 foreign volunteers, known as theInternational Brigades, also fought against the Nationalists. Both Germany and the Soviet Union used thisproxy waras an opportunity to test in combat their most advanced weapons and tactics. The Nationalists won the civil war in April 1939; Franco, now dictator, remained officially neutral during World WarII butgenerally favoured the Axis.[37]His greatest collaboration with Germany was the sending ofvolunteersto fight on theEastern Front.[38]
In July 1937, Japan captured the former Chinese imperial capital ofPekingafter instigating theMarco Polo Bridge incident, which culminated in the Japanese campaign to invade all of China.[39]The Soviets quickly signed anon-aggression pact with Chinato lendmaterielsupport, effectively ending China's priorcooperation with Germany. From September to November, the Japanese attackedTaiyuan, engaged theKuomintang Armyaround Xinkou,[40]and foughtCommunist forcesin Pingxingguan.[41][42]GeneralissimoChiang Kai-shekdeployed hisbest armytodefend Shanghai, but after three months of fighting, Shanghai fell. The Japanese continued to push Chinese forces back,capturing the capital Nankingin December 1937. After the fall of Nanking, tens or hundreds of thousands of Chinese civilians and disarmed combatants weremurdered by the Japanese.[43][44]
In March 1938, Nationalist Chinese forces won theirfirst major victory at Taierzhuang, but then the city ofXuzhouwas taken by the Japanesein May.[45]In June 1938, Chinese forces stalled the Japanese advance byflooding the Yellow River; this manoeuvre bought time for the Chinese to prepare their defences atWuhan, but thecity was takenby October.[46]Japanese military victories did not bring about the collapse of Chinese resistance that Japan had hoped to achieve; instead, the Chinese government relocated inland toChongqingand continued the war.[47][48]
In the mid-to-late 1930s, Japanese forces inManchukuohad sporadic border clashes with the Soviet Union andMongolia. The Japanese doctrine ofHokushin-ron, which emphasised Japan's expansion northward, was favoured by the Imperial Army during this time. This policy would prove difficult to maintain in light of the Japanese defeat atKhalkin Golin 1939, the ongoing Second Sino-Japanese War[49]and ally Nazi Germany pursuing neutrality with the Soviets. Japan and the Soviet Union eventually signed aNeutrality Pactin April 1941, and Japan adopted the doctrine ofNanshin-ron, promoted by the Navy, which took its focus southward and eventually led to war with the United States and the Western Allies.[50][51]
In Europe, Germany and Italy were becoming more aggressive. In March 1938, Germanyannexed Austria, again provokinglittle responsefrom other European powers.[52]Encouraged, Hitler began pressing German claims on theSudetenland, an area ofCzechoslovakiawith a predominantlyethnic Germanpopulation. Soon the United Kingdom and France followed the appeasement policy of British Prime MinisterNeville Chamberlainand conceded this territory to Germany in theMunich Agreement, which was made against the wishes of the Czechoslovak government, in exchange for a promise of no further territorial demands.[53]Soon afterwards, Germany and Italy forced Czechoslovakia tocede additional territoryto Hungary, and Poland annexed theTrans-Olzaregion of Czechoslovakia.[54]
Although all of Germany's stated demands had been satisfied by the agreement, privately Hitler was furious that British interference had prevented him from seizing all of Czechoslovakia in one operation. In subsequent speeches Hitler attacked British and Jewish "war-mongers" and in January 1939secretly ordered a major build-up of the German navyto challenge British naval supremacy. In March 1939,Germany invaded the remainder of Czechoslovakiaand subsequently split it into the GermanProtectorate of Bohemia and Moraviaand a pro-Germanclient state, theSlovak Republic.[55]Hitler also delivered anultimatum to Lithuaniaon 20 March 1939, forcing the concession of theKlaipėda Region, formerly the GermanMemelland.[56]
Greatly alarmed and with Hitler making further demands on theFree City of Danzig, the United Kingdom and Franceguaranteed their support for Polish independence; whenItaly conquered Albaniain April 1939, the same guarantee was extended to theKingdoms of RomaniaandGreece.[57]Shortly after theFranco-Britishpledge to Poland, Germany and Italy formalised their own alliance with thePact of Steel.[58]Hitler accused the United Kingdom and Poland of trying to "encircle" Germany and renounced theAnglo-German Naval Agreementand theGerman–Polish declaration of non-aggression.[59]
The situation became a crisis in late August as German troops continued to mobilise against the Polish border. On 23 August the Soviet Union signeda non-aggression pactwith Germany,[60]after tripartite negotiations for a military alliance between France, the United Kingdom, and Soviet Union had stalled.[61]This pact had a secret protocol that defined German and Soviet "spheres of influence" (westernPolandand Lithuania for Germany;eastern Poland, Finland,Estonia,LatviaandBessarabiafor the Soviet Union), and raised the question of continuing Polish independence.[62]The pact neutralised the possibility of Soviet opposition to a campaign against Poland and assured that Germany would not have to face the prospect of a two-front war, as it had in World WarI. Immediately afterwards, Hitler ordered the attack to proceed on 26 August, but upon hearing that the United Kingdom had concluded a formal mutual assistance pact with Poland and that Italy would maintain neutrality, he decided to delay it.[63]
In response to British requests for direct negotiations to avoid war, Germany made demands on Poland, which served as a pretext to worsen relations.[64]On 29 August, Hitler demanded that a Polishplenipotentiaryimmediately travel to Berlin to negotiate the handover ofDanzig, and to allow aplebiscitein thePolish Corridorin which the German minority would vote on secession.[64]The Poles refused to comply with the German demands, and on the night of 30–31 August in a confrontational meeting with the British ambassadorNevile Henderson, Ribbentrop declared that Germany considered its claims rejected.[65]
On 1 September 1939, Germanyinvaded Polandafterhaving stagedseveralfalse flag border incidentsas a pretext to initiate the invasion.[67]The first German attack of the war came against thePolish defences at Westerplatte.[68]The United Kingdom responded with an ultimatum for Germany to cease military operations, and on 3 September, after the ultimatum was ignored, Britain and France declared war on Germany.[c]During thePhoney Warperiod, the alliance provided no direct military support to Poland, outside of acautious French probe into the Saarland.[69]The Western Allies also began anaval blockade of Germany, which aimed to damage the country's economy and war effort.[70]Germany responded by orderingU-boat warfareagainst Allied merchant and warships, which would later escalate into theBattle of the Atlantic.[71]On 8 September, German troops reached the suburbs ofWarsaw. The Polishcounter-offensiveto the west halted the German advance for several days, but it was outflanked and encircled by theWehrmacht. Remnants of the Polish army broke through tobesieged Warsaw. On 17 September 1939, two days after signing acease-fire with Japan, theSoviet Union invaded Poland[72]under the supposed pretext that the Polish state had ceased to exist.[73]On 27 September, the Warsaw garrison surrendered to the Germans, andthe last large operational unit of the Polish Armysurrendered on 6October. Despite the military defeat, Poland never surrendered; instead, it formed thePolish government-in-exileand aclandestine state apparatus remainedin occupied Poland.[74]A significant part of Polish military personnelevacuated to Romaniaand Latvia; many of them laterfought against the Axisin other theatres of the war.[75]
Germanyannexed westernPoland andoccupied central Poland; the Soviet Unionannexed eastern Poland; small shares of Polish territory were transferred toLithuaniaandSlovakia. On 6 October, Hitler made a public peace overture to the United Kingdom and France but said that the future of Poland was to be determined exclusively by Germany and the Soviet Union. The proposal was rejected[65]and Hitler ordered an immediate offensive against France,[76]which was postponed until the spring of 1940 due to bad weather.[77][78][79]
After the outbreak of war in Poland, Stalin threatenedEstonia,Latvia, andLithuaniawith military invasion, forcing the threeBaltic countriesto signpactsallowing the creation of Soviet military bases in these countries; in October 1939, significant Soviet military contingents were moved there.[80][81][82]Finlandrefused to sign a similar pact and rejected ceding part of its territory to the Soviet Union.The Soviet Union invaded Finlandin November 1939,[83]and was subsequently expelled from theLeague of Nationsfor this crime of aggression.[84]Despite overwhelming numerical superiority, Soviet military success during theWinter Warwas modest,[85]and the Finno-Soviet war ended in March 1940 withsome Finnish concessions of territory.[86]
In June 1940, the Soviet Unionoccupiedthe entire territories of Estonia, Latvia and Lithuania,[81]as well as the Romanian regions ofBessarabia, Northern Bukovina, and the Hertsa region. In August 1940, Hitler imposed theSecond Vienna Awardon Romania which led to the transfer ofNorthern Transylvaniato Hungary.[87]In September 1940, Bulgaria demandedSouthern Dobrujafrom Romania with German and Italian support, leading to theTreaty of Craiova.[88]The loss of one-third of Romania's 1939 territory caused a coup against King Carol II, turning Romania into a fascist dictatorship under MarshalIon Antonescu, with a course set towards the Axis in the hopes of a German guarantee.[89]Meanwhile, German-Soviet political relations and economic co-operation[90][91]gradually stalled,[92][93]and both states began preparations for war.[94]
In April 1940,Germany invaded Denmark and Norwayto protect shipments ofiron ore from Sweden, which the Allies wereattempting to cut off.[95]Denmark capitulated after six hours, anddespite Allied support, Norway was conquered within two months.[96]British discontent over the Norwegian campaignled to the resignation of Prime MinisterNeville Chamberlain, who was replaced byWinston Churchillon 10May 1940.[97]
On the same day, Germanylaunched an offensive against France. To circumvent the strongMaginot Linefortifications on the Franco-German border, Germany directed its attack at the neutral nations ofBelgium,the Netherlands, andLuxembourg.[98]The Germans carried out a flanking manoeuvre through theArdennesregion,[99]which was mistakenly perceived by the Allies as an impenetrable natural barrier against armoured vehicles.[100][101]By successfully implementing newBlitzkriegtactics, theWehrmachtrapidly advanced to the Channel and cut off the Allied forces in Belgium, trapping the bulk of the Allied armies in a cauldron on the Franco-Belgian border near Lille. The United Kingdom was ableto evacuate a significant number of Allied troopsfrom the continent by early June, although they had to abandon almost all their equipment.[102]
On 10 June,Italy invaded France, declaring war on both France and the United Kingdom.[103]The Germans turned south against the weakened French army, andParisfell to them on 14June. Eight days laterFrance signed an armistice with Germany; it was divided intoGermanandItalian occupation zones,[104]and an unoccupiedrump stateunder theVichy Regime, which, though officially neutral, was generally aligned with Germany. France kept its fleet, whichthe United Kingdom attackedon 3July in an attempt to prevent its seizure by Germany.[105]
The airBattle of Britain[106]began in early July withLuftwaffe attacks on shipping and harbours.[107]TheGerman campaign for air superioritystarted in August but its failure to defeatRAF Fighter Commandforced the indefinite postponement of theproposed German invasion of Britain. The Germanstrategic bombingoffensive intensified with night attacks on London and other cities inthe Blitz, but largely ended in May 1941[108]after failing to significantly disrupt the British war effort.[107]
Using newly captured French ports, the German Navyenjoyed successagainst an over-extendedRoyal Navy, usingU-boatsagainst British shippingin the Atlantic.[109]The BritishHome Fleetscored a significant victory on 27May 1941 bysinking the German battleshipBismarck.[110]
In November 1939, the United States was assisting China and the Western Allies, and had amended theNeutrality Actto allow "cash and carry" purchases by the Allies.[111]In 1940, following the German capture of Paris, the size of theUnited States Navywassignificantly increased. In September the United States further agreed to atrade of American destroyers for British bases.[112]Still, a large majority of the American public continued to oppose any direct military intervention in the conflict well into 1941.[113]In December 1940, Roosevelt accused Hitler of planning world conquest and ruled out any negotiations as useless, calling for the United States to become an "arsenal of democracy" and promotingLend-Leaseprogrammes of military and humanitarian aid to support the British war effort; Lend-Lease was later extended to the other Allies, including the Soviet Union after it wasinvadedby Germany.[114]The United States started strategic planning to prepare for a full-scale offensive against Germany.[115]
At the end of September 1940, theTripartite Pactformally united Japan, Italy, and Germany as theAxis powers. The Tripartite Pact stipulated that any country—with the exception of the Soviet Union—that attacked any Axis Power would be forced to go to war against all three.[116]The Axis expanded in November 1940 whenHungary,Slovakia, andRomaniajoined.[117]RomaniaandHungarylater made major contributions to the Axis war against the Soviet Union, in Romania's case partially to recaptureterritory ceded to the Soviet Union.[118]
In early June 1940, the ItalianRegia Aeronauticaattacked and besieged Malta, a British possession. From late summer to early autumn, Italyconquered British Somalilandand made anincursion into British-held Egypt. In October,Italy attacked Greece, but the attack was repulsed with heavy Italian casualties; the campaign ended within months with minor territorial changes.[119]To assist Italy and prevent Britain from gaining a foothold, Germany prepared to invade the Balkans, which would threaten Romanian oil fields and strike against British dominance of the Mediterranean.[120]
In December 1940, British Empire forces begancounter-offensivesagainst Italian forces in Egypt andItalian East Africa.[121]The offensives were successful; by early February 1941, Italy had lost control of eastern Libya, and large numbers of Italian troops had been taken prisoner. TheItalian Navyalso suffered significant defeats, with the Royal Navy putting three Italian battleships out of commission after acarrier attack at Taranto, and neutralising several more warships at theBattle of Cape Matapan.[122]
Italian defeats prompted Germany todeploy an expeditionary forceto North Africa; at the end of March 1941,Rommel'sAfrika Korpslaunched an offensivewhich drove back Commonwealth forces.[123]In less than a month, Axis forces advanced to western Egypt andbesieged the port of Tobruk.[124]
By late March 1941,BulgariaandYugoslaviasigned theTripartite Pact; however, the Yugoslav government wasoverthrown two days laterby pro-British nationalists. Germany and Italy responded with simultaneous invasions of bothYugoslaviaandGreece, commencing on 6 April 1941; both nations were forced to surrender within the month.[125]The airborneinvasion of the Greek island of Creteat the end of May completed the German conquest of the Balkans.[126]Partisan warfare subsequently broke out against theAxis occupation of Yugoslavia, which continued until the end of the war.[127]
In the Middle East in May, Commonwealth forcesquashed an uprising in Iraqwhich had been supported by German aircraft from bases within Vichy-controlledSyria.[128]Between June and July, British-led forcesinvaded and occupied the French possessions of Syria and Lebanon, assisted by theFree French.[129]
With the situation in Europe and Asia relatively stable, Germany, Japan, and the Soviet Union made preparations for war. With the Soviets wary of mounting tensions with Germany, and the Japanese planning to take advantage of the European War by seizing resource-rich European possessions inSoutheast Asia, the two powers signed theSoviet–Japanese Neutrality Pactin April 1941.[130]By contrast, the Germans were steadily making preparations for an attack on the Soviet Union, massing forces on the Soviet border.[131]
Hitler believed that the United Kingdom's refusal to end the war was based on the hope that the United States and the Soviet Union would enter the war against Germany sooner or later.[132]On 31 July 1940, Hitler decided that the Soviet Union should be eliminated and aimed for the conquest ofUkraine, theBaltic statesandByelorussia.[133]However, other senior German officials like Ribbentrop saw an opportunity to create a Euro-Asian bloc against the British Empire by inviting the Soviet Union into the Tripartite Pact.[134]In November 1940,negotiations took placeto determine if the Soviet Union would join the pact. The Soviets showed some interest but asked for concessions from Finland, Bulgaria, Turkey, and Japan that Germany considered unacceptable. On 18 December 1940, Hitler issued the directive to prepare for an invasion of the Soviet Union.[135]
On 22 June 1941, Germany, supported by Italy and Romania, invaded the Soviet Union inOperation Barbarossa, with Germany accusing the Soviets ofplotting against them; they were joined shortly by Finland and Hungary.[136]The primary targets of this surprise offensive[137]were theBaltic region, Moscow and Ukraine, with theultimate goalof ending the 1941 campaign near theArkhangelsk-Astrakhan line—from theCaspianto theWhite Seas. Hitler's objectives were to eliminate the Soviet Union as a military power, exterminateCommunism, generateLebensraum("living space")[138]bydispossessing the native population,[139]and guarantee access to the strategic resources needed to defeat Germany's remaining rivals.[140]
Although theRed Armywas preparing for strategiccounter-offensivesbefore the war,[141]OperationBarbarossaforced theSoviet supreme commandto adoptstrategic defence. During the summer, the Axis made significant gains into Soviet territory, inflicting immense losses in both personnel and materiel. By mid-August, however, the GermanArmy High Commanddecided tosuspend the offensiveof a considerably depletedArmy Group Centre, and to divert the2nd Panzer Groupto reinforce troops advancing towards central Ukraine and Leningrad.[142]TheKiev offensivewas overwhelmingly successful, resulting in encirclement and elimination of four Soviet armies, and made possible furtheradvance into Crimeaand industrially-developed Eastern Ukraine (theFirst Battle of Kharkov).[143]
The diversion of three-quarters of the Axis troops and the majority of their air forces from France and the central Mediterranean to theEastern Front[144]prompted the United Kingdom to reconsider itsgrand strategy.[145]In July, the UK and the Soviet Union formed amilitary alliance against Germany[146]and in August, the United Kingdom and the United States jointly issued theAtlantic Charter, which outlined British and American goals for the post-war world.[147]In late August the British and Sovietsinvaded neutral Iranto secure thePersian Corridor, Iran'soil fields, and preempt any Axis advances through Iran toward the Baku oil fields or India.[148]
By October, Axis powers had achievedoperational objectivesin Ukraine and the Baltic region, with only the sieges ofLeningrad[149]andSevastopolcontinuing.[150]A majoroffensive against Moscowwas renewed; after two months of fierce battles in increasingly harsh weather, the German army almost reached the outer suburbs of Moscow, where the exhausted troops[151]were forced to suspend the offensive.[152]Large territorial gains were made by Axis forces, but their campaign had failed to achieve its main objectives: two key cities remained in Soviet hands, the Sovietcapability to resistwas not broken, and the Soviet Union retained a considerable part of its military potential. Theblitzkriegphaseof the war in Europe had ended.[153]
By early December, freshly mobilisedreserves[154]allowed the Soviets to achieve numerical parity with Axis troops.[155]This, as well asintelligence datawhich established that a minimal number of Soviet troops in the East would be sufficient to deter any attack by the JapaneseKwantung Army,[156]allowed the Soviets to begin amassive counter-offensivethat started on 5 December all along the front and pushed German troops 100–250 kilometres (62–155 mi) west.[157]
Following the Japanesefalse flagMukden incidentin 1931, the Japanese shelling of the AmericangunboatUSS Panayin 1937, and the 1937–1938Nanjing Massacre,Japanese-American relations deteriorated. In 1939, the United States notified Japan that it would not be extending its trade treaty and American public opinion opposing Japanese expansionism led to a series of economic sanctions—theExport Control Acts—which banned U.S. exports of chemicals, minerals and military parts to Japan, and increased economic pressure on the Japanese regime.[114][158][159]During 1939 Japan launched itsfirst attack against Changsha, but was repulsed by late September.[160]Despiteseveral offensivesby both sides, by 1940 the war between China and Japan was at a stalemate. To increase pressure on China by blocking supply routes, and to better position Japanese forces in the event of a war with the Western powers, Japan invaded andoccupied northern Indochinain September 1940.[161]
Chinese nationalist forces launched a large-scalecounter-offensivein early 1940. In August,Chinese communistslaunched anoffensive in Central China; in retaliation, Japan institutedharsh measuresin occupied areas to reduce human and material resources for the communists.[162]Continued antipathy between Chinese communist and nationalist forcesculminated in armed clashes in January 1941, effectively ending their co-operation.[163]In March, the Japanese 11th army attacked the headquarters of the Chinese 19th army but was repulsed duringBattle of Shanggao.[164]In September, Japan attempted totake the city of Changshaagain and clashed with Chinese nationalist forces.[165]
German successes in Europe prompted Japan to increase pressure on European governments inSoutheast Asia. The Dutch government agreed to provide Japan with oil supplies from theDutch East Indies, but negotiations for additional access to their resources ended in failure in June 1941.[166]In July 1941 Japan sent troops to southern Indochina, thus threatening British and Dutch possessions in the Far East. The United States, the United Kingdom, and other Western governments reacted to this move with a freeze on Japanese assets and a total oilembargo.[167][168]At the same time, Japan wasplanning an invasion of the Soviet Far East, intending to take advantage of the German invasion in the west, but abandoned the operation after the sanctions.[169]
Since early 1941, the United States and Japan had been engaged in negotiations in an attempt to improve their strained relations and end the war in China. During these negotiations, Japan advanced a number of proposals which were dismissed by the Americans as inadequate.[170]At the same time the United States, the United Kingdom, and the Netherlands engaged in secret discussions for the joint defence of their territories, in the event of a Japanese attack against any of them.[171]Roosevelt reinforcedthe Philippines(an American protectorate scheduled for independence in 1946) and warned Japan that the United States would react to Japanese attacks against any "neighboring countries".[171]
Frustrated at the lack of progress and feeling the pinch of the American–British–Dutch sanctions, Japan prepared for war. EmperorHirohito, after initial hesitation about Japan's chances of victory,[172]began to favour Japan's entry into the war.[173]As a result, Prime MinisterFumimaro Konoeresigned.[174][175]Hirohito refused the recommendation to appointPrince Naruhiko Higashikuniin his place, choosing War MinisterHideki Tojoinstead.[176]On 3 November, Nagano explained in detail the plan of theattack on Pearl Harborto the Emperor.[177]On 5 November, Hirohito approved in imperial conference the operations plan for the war.[178]On 20 November, the new government presented an interim proposal as its final offer. It called for the end of American aid to China and for lifting the embargo on the supply of oil and other resources to Japan. In exchange, Japan promised not to launch any attacks in Southeast Asia and to withdraw its forces from southern Indochina.[170]The American counter-proposal of 26 November required that Japan evacuate all of China without conditions and conclude non-aggression pacts with all Pacific powers.[179]That meant Japan was essentially forced to choose between abandoning its ambitions in China, or seizing the natural resources it needed in the Dutch East Indies by force;[180][181]the Japanese military did not consider the former an option, and many officers considered the oil embargo an unspoken declaration of war.[182]
Japan planned to seize European colonies in Asia to create a large defensive perimeter stretching into the Central Pacific. The Japanese would then be free to exploit the resources of Southeast Asia while exhausting the over-stretched Allies by fighting a defensive war.[183][184]To prevent American intervention while securing the perimeter, it was further planned to neutralise theUnited States Pacific Fleetand the American military presence in the Philippines from the outset.[185]On 7 December 1941 (8 December in Asian time zones), Japan attacked British and American holdings with near-simultaneousoffensives against Southeast Asia and the Central Pacific.[186]These included anattack on the American fleets at Pearl Harborandthe Philippines, as well as invasions ofGuam,Wake Island,Malaya,[186]Thailand, andHong Kong.[187]
These attacks led theUnited States,United Kingdom, China, Australia, and several other states to formally declare war on Japan, whereas the Soviet Union, being heavily involved in large-scale hostilities with European Axis countries, maintained its neutrality agreement with Japan.[188]Germany, followed by the other Axis states, declared war on the United States[189]in solidarity with Japan, citing as justification the American attacks on German war vessels that had been ordered by Roosevelt.[136][190]
On 1 January 1942, theAllied Big Four[191]—the Soviet Union, China, the United Kingdom, and the United States—and 22 smaller or exiled governments issued theDeclaration by United Nations, thereby affirming theAtlantic Charter[192]and agreeing not to sign aseparate peacewith the Axis powers.[193]
During 1942, Allied officials debated on the appropriategrand strategyto pursue. All agreed thatdefeating Germanywas the primary objective. The Americans favoured a straightforward,large-scale attackon Germany through France. The Soviets demanded a second front. The British argued that military operations should target peripheral areas to wear out German strength, leading to increasing demoralisation, and bolstering resistance forces; Germany itself would be subject to a heavy bombing campaign. An offensive against Germany would then be launched primarily by Allied armour, without using large-scale armies.[194]Eventually, the British persuaded the Americans that a landing in France was infeasible in 1942 and they should instead focus on driving the Axis out of North Africa.[195]
At theCasablanca Conferencein early 1943, the Allies reiterated the statements issued in the 1942 Declaration and demanded theunconditional surrenderof their enemies. The British and Americans agreed to continue to press the initiative in the Mediterranean by invading Sicily to fully secure the Mediterranean supply routes.[196]Although the British argued for further operations in the Balkans to bring Turkey into the war, in May 1943, the Americans extracted a British commitment to limit Allied operations in the Mediterranean to an invasion of the Italian mainland, and to invade France in 1944.[197]
By the end of April 1942, Japan and its allyThailandhad almost conqueredBurma,Malaya,the Dutch East Indies,Singapore, andRabaul, inflicting severe losses on Allied troops and taking a large number of prisoners.[198]Despite stubbornresistance by Filipino and U.S. forces, thePhilippine Commonwealthwas eventually captured in May 1942, forcing its government into exile.[199]On 16 April, in Burma, 7,000 British soldiers were encircled by the Japanese 33rd Division during theBattle of Yenangyaungand rescued by the Chinese 38th Division.[200]Japanese forces also achieved naval victories in theSouth China Sea,Java Sea, andIndian Ocean,[201]andbombed the Allied naval baseatDarwin, Australia. In January 1942, the only Allied success against Japan was a Chinesevictory at Changsha.[202]These easy victories over the unprepared U.S. and European opponents left Japan overconfident, and overextended.[203]
In early May 1942, Japan initiated operations tocapture Port Moresbybyamphibious assaultand thus sever communications and supply lines between the United States and Australia. The planned invasion was thwarted when an Allied task force, centred on two American fleet carriers, fought Japanese naval forces to a draw in theBattle of the Coral Sea.[204]Japan's next plan, motivated by the earlierDoolittle Raid, was to seizeMidway Atolland lure American carriers into battle to be eliminated; as a diversion, Japan would also send forces tooccupy the Aleutian Islandsin Alaska.[205]In mid-May, Japan started theZhejiang-Jiangxi campaignin China, with the goal of inflicting retribution on the Chinese who aided the surviving American airmen in the Doolittle Raid by destroying Chinese air bases and fighting against the Chinese 23rd and 32nd Army Groups.[206][207]In early June, Japan put its operations into action, but the Americans had brokenJapanese naval codesin late May and were fully aware of the plans and order of battle, and used this knowledge to achieve a decisivevictory at Midwayover theImperial Japanese Navy.[208]
With its capacity for aggressive action greatly diminished as a result of the Midway battle, Japan attempted to capturePort Moresbyby anoverland campaignin theTerritory of Papua.[209]The Americans planned a counterattack against Japanese positions in the southernSolomon Islands, primarilyGuadalcanal, as a first step towards capturingRabaul, the main Japanese base in Southeast Asia.[210]
Both plans started in July, but by mid-September, theBattle for Guadalcanaltook priority for the Japanese, and troops in New Guinea were ordered to withdraw from the Port Moresby area to thenorthern part of the island, where they faced Australian and United States troops in theBattle of Buna–Gona.[211]Guadalcanal soon became a focal point for both sides with heavy commitments of troops and ships in the battle for Guadalcanal. By the start of 1943, the Japanese were defeated on the island andwithdrew their troops.[212]In Burma, Commonwealth forces mounted two operations. The first was a disastrousoffensive into the Arakan regionin late 1942 that forced a retreat back to India by May 1943.[213]The second was theinsertion of irregular forcesbehind Japanese frontlines in February which, by the end of April, had achieved mixed results.[214]
Despite considerable losses, in early 1942 Germany and its allies stopped a major Soviet offensive incentralandsouthern Russia, keeping most territorial gains they had achieved during the previous year.[215]In May, the Germans defeated Soviet offensives in theKerch Peninsulaand atKharkov,[216]and then in June 1942 launched their mainsummer offensiveagainst southern Russia, to seize theoil fields of the Caucasusand occupy theKubansteppe, while maintaining positions on the northern and central areas of the front. The Germans splitArmy Group Southinto two groups:Army Group Aadvanced to the lowerDon Riverand struck south-east to the Caucasus, whileArmy Group Bheaded towards theVolga River. The Soviets decided to make their stand at Stalingrad on the Volga.[217]
By mid-November, the Germans hadnearly taken Stalingradin bitterstreet fighting. The Soviets began their second winter counter-offensive, starting with anencirclement of German forces at Stalingrad,[218]and an assault on theRzhev salient near Moscow, though the latter failed disastrously.[219]By early February 1943, the German Army had taken tremendous losses; German troops at Stalingrad had been defeated,[220]and the front-line had been pushed back beyond its position before the summer offensive. In mid-February, after the Soviet push had tapered off, the Germans launched anotherattack on Kharkov, creating asalientin their front line around the Soviet city ofKursk.[221]
Exploiting poor American naval command decisions,the German navy ravaged Allied shipping off the American Atlantic coast.[222]By November 1941, Commonwealth forces had launched a counter-offensive in North Africa,Operation Crusader, and reclaimed all the gains the Germans and Italians had made.[223]The Germans also launched a North African offensive in January, pushing the British back to positions at theGazala lineby early February,[224]followed by a temporary lull in combat which Germany used to prepare for their upcoming offensives.[225]Concerns that the Japanese might use bases inVichy-held Madagascarcaused the British toinvade the islandin early May 1942.[226]An Axisoffensive in Libyaforced an Allied retreat deep inside Egypt until Axis forces werestopped at El Alamein.[227]On the Continent, raids of Alliedcommandoson strategic targets, culminating in the failedDieppe Raid,[228]demonstrated the Western Allies' inability to launch an invasion of continental Europe without much better preparation, equipment, and operational security.[229]
In August 1942, the Allies succeeded in repelling asecond attack against El Alamein[230]and, at a high cost, managed todeliver desperately needed supplies to the besieged Malta.[231]A few months later, the Alliescommenced an attack of their ownin Egypt, dislodging the Axis forces and beginning a drive west across Libya.[232]This attack was followed up shortly after byAnglo-American landings in French North Africa, which resulted in the region joining the Allies.[233]Hitler responded to the French colony's defection by ordering theoccupation of Vichy France;[233]although Vichy forces did not resist this violation of the armistice, they managed toscuttle their fleetto prevent its capture by German forces.[233][234]Axis forces in Africa withdrew intoTunisia, which wasconquered by the Alliesin May 1943.[233][235]
In June 1943, the British and Americans begana strategic bombing campaignagainst Germany with a goal to disrupt the war economy, reduce morale, and "de-house" the civilian population.[236]Thefirebombing of Hamburgwas among the first attacks in this campaign, inflicting significant casualties and considerable losses on infrastructure of this important industrial centre.[237]
After the Guadalcanal campaign, the Allies initiated several operations against Japan in the Pacific. In May 1943, Canadian and U.S. forces were sent toeliminate Japanese forces from the Aleutians.[238]Soon after, the United States, with support from Australia, New Zealand and Pacific Islander forces, began major ground, sea and air operations toisolate Rabaul by capturing surrounding islands, andbreach the Japanese Central Pacific perimeter at the Gilbert and Marshall Islands.[239]By the end of March 1944, the Allies had completed both of these objectives and had alsoneutralised the major Japanese base at Trukin theCaroline Islands. In April, the Allies launched an operation toretake Western New Guinea.[240]
In the Soviet Union, both the Germans and the Soviets spent the spring and early summer of 1943 preparing for large offensives incentral Russia. On 5 July 1943, Germanyattacked Soviet forces around the Kursk Bulge. Within a week, German forces had exhausted themselves against the Soviets' well-constructed defences,[241]and for the first time in the war, Hitler cancelled an operation before it had achieved tactical or operational success.[242]This decision was partially affected by the Western Allies'invasion of Sicilylaunched on 9 July, which, combined with previous Italian failures, resulted in theousting and arrest of Mussolinilater that month.[243]
On 12 July 1943, the Soviets launched their owncounter-offensives, thereby dispelling any chance of German victory or even stalemate in the east. The Soviet victory at Kursk marked the end of German superiority,[244]giving the Soviet Union the initiative on the Eastern Front.[245][246]The Germans tried to stabilise their eastern front along the hastily fortifiedPanther–Wotan line, but the Soviets broke through it atSmolenskand theLower Dnieper Offensive.[247]
On 3 September 1943, the Western Alliesinvaded the Italian mainland, followingItaly's armistice with the Alliesand the ensuing German occupation of Italy.[248]Germany, with the help of fascists, responded to the armistice bydisarming Italian forcesthat were in many places without superior orders, seizing military control of Italian areas,[249]and creating a series of defensive lines.[250]German special forces thenrescued Mussolini, who then soon established a new client state in German-occupied Italy named theItalian Social Republic,[251]causing anItalian civil war. The Western Allies fought through several lines until reaching themain German defensive linein mid-November.[252]
German operations in the Atlantic also suffered. ByMay 1943, as Allied counter-measures became increasingly effective, the resulting sizeable German submarine losses forced a temporary halt of the German Atlantic naval campaign.[253]In November 1943,Franklin D. Rooseveltand Winston Churchill met withChiang Kai-shekin Cairoand then with Joseph Stalinin Tehran.[254]The former conference determined the post-war return of Japanese territory[255]and the military planning for theBurma campaign,[256]while the latter included agreement that the Western Allies would invade Europe in 1944 and that the Soviet Union would declare war on Japan within three months of Germany's defeat.[257]
From November 1943, during the seven-weekBattle of Changde, the Chinese awaited allied relief as they forced Japan to fight a costly war of attrition.[258][259][260]In January 1944, the Allies launched aseries of attacks in Italy against the line at Monte Cassinoand tried to outflank it withlandings at Anzio.[261]
On 27 January 1944,Soviettroops launcheda major offensivethat expelled German forces from theLeningrad region, thereby ending themost lethal siege in history.[262]Thefollowing Soviet offensivewashalted on the pre-war Estonian borderby the GermanArmy Group Northaided byEstonianshoping tore-establish national independence. This delay slowed subsequent Soviet operations in theBaltic Searegion.[263]By late May 1944, the Soviets hadliberated Crimea,largely expelled Axis forces from Ukraine, and madeincursions into Romania, which were repulsed by the Axis troops.[264]The Allied offensives in Italy had succeeded and, at the expense of allowing several German divisions to retreat, Rome was captured on 4 June.[265]
The Allies had mixed success in mainland Asia. In March 1944, the Japanese launched the first of two invasions,an operation against Allied positions in Assam, India,[266]and soon besieged Commonwealth positions atImphalandKohima.[267]In May 1944, British and Indian forces mounted a counter-offensive that drove Japanese troops back to Burma by July,[267]and Chinese forces that hadinvaded northern Burmain late 1943besieged Japanese troopsinMyitkyina.[268]Thesecond Japanese invasionof China aimed to destroy China's main fighting forces, secure railways between Japanese-held territory and capture Allied airfields.[269]By June, the Japanese had conquered the province ofHenanand begun anew attack on Changsha.[270]
On 6 June 1944 (commonly known asD-Day), after three years of Soviet pressure,[271]the Western Alliesinvaded northern France. After reassigning several Allied divisions from Italy, they alsoattacked southern France.[272]These landings were successful and led to the defeat of theGerman Army units in France.Pariswasliberatedon 25 August by thelocal resistanceassisted by theFree French Forces, both led by GeneralCharles de Gaulle,[273]and the Western Allies continued topush back German forcesin western Europe during the latter part of the year. An attempt to advance into northern Germany spearheaded bya major airborne operationin the Netherlands failed.[274]After that, the Western Allies slowly pushed into Germany, butfailed to cross the Ruhr river. In Italy, the Allied advance slowed due to thelast major German defensive line.[275]
On 22 June, the Soviets launched a strategic offensive in Belarus ("Operation Bagration") that nearly destroyed the GermanArmy Group Centre.[276]Soon after that,another Soviet strategic offensiveforced German troops from Western Ukraine and Eastern Poland. The Soviets formed thePolish Committee of National Liberationto control territory in Poland and combat the PolishArmia Krajowa; the Soviet Red Army remained in thePragadistrict on the other side of theVistulaand watched passively as the Germans quelled theWarsaw Uprisinginitiated by the Armia Krajowa.[277]Thenational uprisinginSlovakiawas also quelled by the Germans.[278]The SovietRed Army'sstrategic offensive in eastern Romaniacut off and destroyed theconsiderable German troops thereand triggereda successful coup d'état in Romaniaandin Bulgaria, followed by those countries' shift to the Allied side.[279]
In September 1944, Soviet troops advanced intoYugoslaviaand forced the rapid withdrawal of German Army GroupsEandFinGreece,Albaniaand Yugoslavia to rescue them from being cut off.[280]By this point, the communist-ledPartisansunder MarshalJosip Broz Tito, who had led anincreasingly successful guerrilla campaignagainst the occupation since 1941, controlled much of the territory of Yugoslavia and engaged in delaying efforts against German forces further south. In northernSerbia, the SovietRed Army, with limited support from Bulgarian forces, assisted the Partisans in a jointliberation of the capital city of Belgradeon 20 October. A few days later, the Soviets launched amassive assaultagainstGerman-occupiedHungary that lasted untilthe fall of Budapestin February 1945.[281]Unlike impressive Soviet victories in the Balkans,bitter Finnish resistanceto theSoviet offensivein theKarelian Isthmusdenied the Soviets occupation of Finland and led to aSoviet-Finnish armisticeon relatively mild conditions,[282]although Finland was forced tofight their former German allies.[283]
By the start of July 1944, Commonwealth forces in Southeast Asia had repelled the Japanese sieges inAssam, pushing the Japanese back to theChindwin River[284]while the Chinese captured Myitkyina. In September 1944, Chinese forcescaptured Mount Songand reopened theBurma Road.[285]In China, the Japanese had more successes, having finallycaptured Changshain mid-June and the city ofHengyangby early August.[286]Soon after, they invaded the province ofGuangxi, winning major engagements against Chinese forces atGuilin and Liuzhouby the end of November[287]and successfully linking up their forces in China and Indochina by mid-December.[288]
In the Pacific, U.S. forces continued to push back the Japanese perimeter. In mid-June 1944, they began theiroffensive against the Mariana and Palau islandsand decisively defeated Japanese forces in theBattle of the Philippine Sea. These defeats led to the resignation of the Japanese Prime Minister,Hideki Tojo, and provided the United States with air bases to launch intensive heavy bomber attacks on the Japanese home islands. In late October, American forcesinvaded the Filipino island of Leyte; soon after, Allied naval forces scored another large victory in theBattle of Leyte Gulf, one of the largest naval battles in history.[289]
On 16 December 1944, Germany made a last attempt to split the Allies on the Western Front by using most of its remaining reserves to launcha massive counter-offensive in the Ardennesandalong the French-German border, hoping to encircle large portions of Western Allied troops and prompt a political settlement after capturing their primary supply port atAntwerp. By 16 January 1945, this offensive had been repulsed with no strategic objectives fulfilled.[290]In Italy, the Western Allies remained stalemated at the German defensive line. In mid-January 1945, the Red Army attacked in Poland,pushing from the Vistula to the Oderriver in Germany, andoverran East Prussia.[291]On 4 February Soviet, British, and U.S. leaders met for theYalta Conference. They agreed on the occupation of post-war Germany, and on when the Soviet Union would join the war against Japan.[292]
In February, the Sovietsentered SilesiaandPomerania, while theWestern Allies entered western Germanyand closed to theRhineriver. By March, the Western Allies crossed the Rhinenorthandsouthof theRuhr,encircling the German Army Group B.[293]In early March, in an attempt to protect its last oil reserves in Hungary and retake Budapest, Germany launchedits last major offensiveagainst Soviet troops nearLake Balaton. Within two weeks, the offensive had been repulsed, the Soviets advanced toVienna, and captured the city. In early April, Soviet troopscaptured Königsberg, while the Western Allies finallypushed forward in Italyand swept across western Germany capturingHamburgandNuremberg.American and Soviet forces met at the Elbe riveron 25 April, leaving unoccupied pockets in southern Germany and around Berlin.
Soviet troopsstormed and captured Berlinin late April.[294]In Italy,German forces surrenderedon 29 April, while theItalian Social Republiccapitulated two days later. On 30 April, theReichstagwas captured, signalling the military defeat of Nazi Germany.[295]
Major changes in leadership occurred on both sides during this period. On 12 April, President Roosevelt died and was succeeded by his vice president,Harry S. Truman.[296]Benito Mussoliniwas killedbyItalian partisanson 28 April.[297]On 30 April,Hitler committed suicidein hisheadquarters, and was succeeded byGrand AdmiralKarl Dönitz(asPresident of the Reich) andJoseph Goebbels(asChancellor of the Reich); Goebbels also committed suicide on the following day and was replaced byLutz Graf Schwerin von Krosigk, in what would later be known as theFlensburg Government.Total and unconditional surrenderin Europe was signedon 7and 8May, to be effective by the end of8 May.[298]German Army Group Centreresisted in Pragueuntil 11 May.[299]On 23 May all remaining members of the German government were arrested by the Allied Forces inFlensburg, while on 5 June all German political and military institutions were transferred under the control of the Allies through theBerlin Declaration.[300]
In the Pacific theatre, American forces accompanied by the forces of thePhilippine Commonwealthadvancedin the Philippines,clearing Leyteby the end of April 1945. Theylanded on Luzonin January 1945 andrecaptured Manilain March. Fighting continued on Luzon,Mindanao, and other islands of the Philippines until theend of the war.[301]Meanwhile, theUnited States Army Air Forceslauncheda massive firebombing campaignof strategic cities in Japan in an effort to destroy Japanese war industry and civilian morale. A devastatingbombing raid on Tokyo of 9–10 Marchwas the deadliest conventional bombing raid in history.[302]
In May 1945, Australian troopslanded in Borneo, overrunning the oilfields there. British, American, and Chinese forces defeated the Japanese in northernBurmain March, and the British pushed on to reachRangoonby 3 May.[303]Chinese forces started a counterattack in theBattle of West Hunanthat occurred between 6 April and 7 June 1945. American naval and amphibious forces also moved towards Japan, takingIwo Jimaby March, andOkinawaby the end of June.[304]At the same time, a naval blockade bysubmarineswas strangling Japan's economy and drastically reducing its ability to supply overseas forces.[305][306]
On 11 July, Allied leadersmet in Potsdam, Germany. Theyconfirmed earlier agreementsabout Germany,[307]and the American, British and Chinese governments reiterated the demand for unconditional surrender of Japan, specifically stating that "the alternative for Japan is prompt and utter destruction".[308]During this conference, the United Kingdomheld its general election, andClement Attleereplaced Churchill as Prime Minister.[309]
The call for unconditional surrender was rejected by the Japanese government, which believed it would be capable of negotiating for more favourable surrender terms.[310]In early August, the United Statesdropped atomic bombson the Japanese cities ofHiroshimaandNagasaki. Between the two bombings, the Soviets, pursuant to the Yalta agreement,declared war on Japan,invaded Japanese-held Manchuriaand quickly defeated theKwantung Army, which was the largest Japanese fighting force.[311]These two events persuaded previously adamant Imperial Army leaders to accept surrender terms.[312]The Red Army also captured thesouthern part of Sakhalin Islandand theKuril Islands. On the night of 9–10 August 1945, Emperor Hirohito announced his decision to accept the terms demanded by the Allies in thePotsdam Declaration.[313]On 15 August, the Emperor communicated this decision to the Japanese people through a speech broadcast on the radio (Gyokuon-hōsō, literally "broadcast in the Emperor's voice").[314]On 15 August 1945,Japan surrendered, with thesurrender documentsfinally signed atTokyo Bayon the deck of the American battleshipUSSMissourion 2 September 1945, ending the war.[315]
The Allies established occupation administrations inAustriaandGermany, both initially divided between western and eastern occupation zones controlled by the Western Allies and the Soviet Union, respectively. However, their paths soon diverged. In Germany, thewesternandeastern occupation zonescontrolled by the Western Allies and the Soviet Union officially ended in 1949, with the respective zones becoming separate countries,West GermanyandEast Germany.[316]In Austria, however, occupation continued until 1955, when a joint settlement between the Western Allies and the Soviet Union permitted the reunification of Austria as a democratic state officially non-aligned with any political bloc (although in practice having better relations with the Western Allies). Adenazificationprogram in Germany led to the prosecution of Nazi war criminals in theNuremberg trialsand the removal of ex-Nazis from power, although this policy moved towards amnesty and re-integration of ex-Nazis into West German society.[317]
Germany lost a quarter of its pre-war (1937) territory. Among the eastern territories,Silesia,Neumarkand most ofPomeraniawere taken over by Poland,[318]andEast Prussiawas divided between Poland and the Soviet Union, followed by theexpulsion to Germanyof the nine million Germans from these provinces,[319][320]as well as three million Germans from theSudetenlandin Czechoslovakia. By the 1950s, one-fifth of West Germans were refugees from the east. The Soviet Union also took over the Polish provinces east of theCurzon Line,[321]from whichtwo million Poles were expelled.[320][322]North-east Romania,[323][324]parts of eastern Finland,[325]and theBaltic stateswereannexed into the Soviet Union.[326][327]Italylost its monarchy,colonial empireand someEuropean territories.[328]
In an effort to maintainworld peace,[329]the Allies formed theUnited Nations,[330]which officially came into existence on 24 October 1945,[331]and adopted theUniversal Declaration of Human Rightsin 1948 as a common standard for allmember nations.[332]Thegreat powersthat were the victors of the war—France, China, the United Kingdom, the Soviet Union and the United States—became thepermanent membersof the UN'sSecurity Council.[333]The five permanent members remain so to the present, although there have been two seat changes,betweentheRepublic of Chinaand thePeople's Republic of Chinain 1971, and between the Soviet Union and itssuccessor state, theRussian Federation, following thedissolution of the Soviet Unionin 1991. The alliance between the Western Allies and the Soviet Union had begun to deteriorate even before the war was over.[334]
Besides Germany, the rest of Europe was also divided into Western and Sovietspheres of influence.[335]Most eastern and central European countries fell intothe Soviet sphere, which led to establishment of Communist-led regimes, with full or partial support of the Soviet occupation authorities. As a result,East Germany,[336]Poland,Hungary,Romania,Bulgaria,Czechoslovakia, andAlbania[337]became Sovietsatellite states. CommunistYugoslaviaconducted a fullyindependent policy, causingtension with the Soviet Union.[338]ACommunist uprising in Greecewas put down with Anglo-American support and the country remained aligned with the West.[339]
Post-war division of the world was formalised by two international military alliances, the United States-ledNATOand the Soviet-ledWarsaw Pact.[340]The long period of political tensions and military competition between them—theCold War—would be accompanied by an unprecedentedarms raceand number ofproxy warsthroughout the world.[341]
In Asia, the United States led theoccupation of Japanandadministered Japan's former islandsin the Western Pacific, while the Soviets annexedSouth Sakhalinand theKuril Islands.[342]Korea, formerlyunder Japanese colonial rule, wasdivided and occupiedby the Soviet Union in theNorthand the United States in theSouthbetween 1945 and 1948. Separate republics emerged on both sides of the 38th parallel in 1948, each claiming to be the legitimate government for all of Korea, which led ultimately to theKorean War.[343]
In China, nationalist and communist forces resumedthe civil warin June 1946. Communist forces were victorious and established the People's Republic of China on the mainland, while nationalist forces retreated toTaiwanin 1949.[344]In the Middle East, the Arab rejection of theUnited Nations Partition Plan for Palestineand thecreation of Israelmarked the escalation of theArab–Israeli conflict. While European powers attempted to retain some or all of theircolonial empires, their losses of prestige and resources during the war rendered this unsuccessful, leading todecolonisation.[345][346]
The global economy suffered heavily from the war, although participating nations were affected differently. The United States emerged much richer than any other nation, leading to ababy boom, and by 1950 its gross domestic product per person was much higher than that of any of the other powers, and it dominated the world economy.[347]The Allied occupational authorities pursued a policy ofindustrial disarmament in Western Germanyfrom 1945 to 1948.[348]Due to international trade interdependencies, this policy led to an economic stagnation in Europe and delayed European recovery from the war for several years.[349][350]
At theBretton Woods Conferencein July 1944, the Allied nations drew up an economic framework for the post-war world. The agreement created theInternational Monetary Fund(IMF) and theInternational Bank for Reconstruction and Development(IBRD), which later became part of theWorld Bank Group. TheBretton Woods systemlasted until 1973.[351]Recovery began with the mid-1948currency reform in Western Germany, and was sped up by the liberalisation of European economic policy that the U.S.Marshall Planeconomic aid (1948–1951) both directly and indirectly caused.[352][353]The post-1948 West German recovery has been called theGerman economic miracle.[354]Italy also experienced aneconomic boom[355]and theFrench economy rebounded.[356]By contrast, the United Kingdom was in a state of economic ruin,[357]and although receiving a quarter of the total Marshall Plan assistance, more than any other European country,[358]it continued in relative economic decline for decades.[359]The Soviet Union, despite enormous human and material losses, also experienced rapid increase in production in the immediate post-war era,[360]having seized and transferred most of Germany's industrial plants and exactedwar reparationsfrom its satellite states.[d][361]Japan recovered much later.[362]China returned to its pre-war industrial production by 1952.[363]
Estimates for the total number of casualties in the war vary, because many deaths went unrecorded.[364]Most suggest that some 60 million people died in the war, including about20 million military personneland 40 million civilians.[365][366][367]
The Soviet Union alone lost around 27 million people during the war,[368]including 8.7 million military and 19 million civilian deaths.[369]A quarter of the total people in the Soviet Union were wounded or killed.[370]Germany sustained 5.3 million military losses, mostly on the Eastern Front and during the final battles in Germany.[371]
An estimated 11[372]to 17 million[373]civilians died as a direct or as an indirect result of Hitler'sracist policies, includingmass killingofaround 6million Jews, along withRoma,homosexuals, at least 1.9 million ethnicPoles[374][375]andmillions of other Slavs(including Russians, Ukrainians and Belarusians), andother ethnic and minority groups.[376][373]Between 1941 and 1945, more than 200,000 ethnicSerbs, along with Roma and Jews, werepersecuted and murderedby the Axis-aligned CroatianUstašeinYugoslavia.[377]Concurrently,MuslimsandCroatswerepersecuted and killedby Serb nationalistChetniks,[378]with an estimated 50,000–68,000 victims (of which 41,000 were civilians).[379]Also, more than 100,000 Poles were massacred by theUkrainian Insurgent Armyin theVolhynia massacres, between 1943 and 1945.[380]At the same time, about 10,000–15,000 Ukrainians were killed by the PolishHome Armyand other Polish units, in reprisal attacks.[381]
In Asia and the Pacific, the number of people killed by Japanese troops remains contested. According to R.J. Rummel, the Japanese killed between 3million and more than 10 million people, with the most probable case of almost 6,000,000 people.[382]According to the British historianM. R. D. Foot, civilian deaths are between 10 million and 20 million, whereas Chinese military casualties (killed and wounded) are estimated to be over five million.[383]Other estimates say that up to 30 million people, most of them civilians, were killed.[384][385]The most infamous Japanese atrocity was theNanjing Massacre, in which fifty to three hundred thousand Chinese civilians were raped and murdered.[386]Mitsuyoshi Himeta reported that 2.7 million casualties occurred during theThree Alls policy. GeneralYasuji Okamuraimplemented the policy inHebeiandShandong.[387]
Axis forces employedbiologicalandchemical weapons. TheImperial Japanese Armyused a variety of such weapons during itsinvasion and occupation of China(seeUnit 731)[388][389]and inearly conflicts against the Soviets.[390]Both the Germans and theJapanese testedsuch weapons against civilians,[391]and sometimes onprisoners of war.[392]
The Soviet Union was responsible for theKatyn massacreof 22,000 Polish officers,[393]and the imprisonment or execution ofhundreds of thousands of political prisonersby theNKVDsecret police, along withmass civilian deportations to Siberia, in theBaltic statesandeastern Polandannexed by the Red Army.[394]Soviet soldiers committed mass rapes in occupied territories, especiallyin Germany.[395][396]The exact number of German women and girls raped by Soviet troops during the war and occupation is uncertain, but historians estimate their numbers are likely in the hundreds of thousands, and possibly as many as two million,[397]while figures for women raped by German soldiers in the Soviet Union go as far as ten million.[398][399]
The mass bombing of cities in Europe and Asia has often been called a war crime, although nopositiveor specificcustomaryinternational humanitarian lawwith respect toaerial warfareexisted before or during World War II.[400]The USAAFbombed a total of 67 Japanese cities, killing 393,000 civilians, including theatomic bombings of Hiroshima and Nagasaki, and destroying 65% of built-up areas.[401]
Nazi Germany, under thedictatorshipof Adolf Hitler, was responsible for murdering about 6million Jews in what is now known asthe Holocaust. They also murdered an additional 4million others who were deemed "unworthy of life" (including thedisabledandmentally ill,Soviet prisoners of war,Romani,homosexuals,Freemasons, andJehovah's Witnesses) as part of a program of deliberate extermination, in effect becoming a "genocidalstate".[402]Soviet POWswere kept in especially unbearable conditions, and 3.6 million Soviet POWs out of 5.7 million died in Nazi camps during the war.[403][404]In addition toconcentration camps,death campswere created in Nazi Germany to exterminate people on an industrial scale. Nazi Germany extensively usedforced labourers; about 12 millionEuropeansfrom German-occupied countries were abducted and used as a slave work force in German industry, agriculture and war economy.[405]
The SovietGulagbecame ade factosystem of deadly camps during 1942–1943, when wartime privation and hunger caused numerous deaths of inmates,[407]including foreign citizens of Poland andother countriesoccupied in 1939–1940 by the Soviet Union, as well as AxisPOWs.[408]By the end of the war, most Soviet POWs liberated from Nazi camps and many repatriated civilians were detained in special filtration camps where they were subjected toNKVDevaluation, and 226,127 were sent to the Gulag as real or perceived Nazi collaborators.[409]
Japaneseprisoner-of-war camps, many of which were used as labour camps, also had high death rates. TheInternational Military Tribunal for the Far Eastfound the death rate of Western prisoners was 27 percent (for American POWs, 37 percent),[410]seven times that of POWs under the Germans and Italians.[411]While 37,583 prisoners from the UK, 28,500 from the Netherlands, and 14,473 from the United States were released after thesurrender of Japan, the number of Chinese released was only 56.[412]
At least five million Chinese civilians from northern China and Manchukuo were enslaved between 1935 and 1941 by theEast Asia Development Board, orKōain, for work in mines and war industries. After 1942, the number reached 10 million.[413]InJava, between 4and 10 millionrōmusha(Japanese: "manual labourers"), were forced to work by the Japanese military. About 270,000 of these Javanese labourers were sent to other Japanese-held areas in Southeast Asia, and only 52,000 were repatriated to Java.[414]
In Europe, occupation came under two forms. In Western, Northern, and Central Europe (France, Norway, Denmark, the Low Countries, and theannexed portions of Czechoslovakia) Germany established economic policies through which it collected roughly 69.5 billion reichsmarks (27.8 billion U.S. dollars) by the end of the war; this figure does not include theplunderof industrial products, military equipment, raw materials and other goods.[415]Thus, the income from occupied nations was over 40 percent of the income Germany collected from taxation, a figure which increased to nearly 40 percent of total German income as the war went on.[416]
In the East, the intended gains ofLebensraumwere never attained as fluctuating front-lines and Sovietscorched earthpolicies denied resources to the German invaders.[417]Unlike in the West, theNazi racial policyencouraged extreme brutality against what it considered to be the "inferior people" of Slavic descent; most German advances were thus followed bymass atrocities and war crimes.[418]The Naziskilled an estimated 2.77 million ethnic Polesduring the war in addition to Polish-Jewish victims of the Holocaust.[419][better source needed]Althoughresistance groupsformed in most occupied territories, they did not significantly hamper German operations in either the East[420]or the West[421]until late 1943.
In Asia, Japan termed nations under its occupation as being part of theGreater East Asia Co-Prosperity Sphere, essentially a Japanesehegemonywhich it claimed was for purposes of liberating colonised peoples.[422]Although Japanese forces were sometimes welcomed as liberators from European domination,Japanese war crimesfrequently turned local public opinion against them.[423]During Japan's initial conquest, it captured 4,000,000 barrels (640,000 m3) of oil (~550,000 tonnes) left behind by retreating Allied forces; and by 1943, was able to get production in the Dutch East Indies up to 50 million barrels (7,900,000 m3) of oil (~6.8 million tonnes), 76 percent of its 1940 output rate.[423]
In the 1930s Britain and the United States together controlled almost 75% of world mineral output—essential for projecting military power.[424]
In Europe, before the outbreak of the war, the Allies had significant advantages in both population and economics. In 1938, the Western Allies (United Kingdom, France, Poland and the British Dominions) had a 30 percent larger population and a 30 percent higher gross domestic product than the European Axis powers (Germany and Italy); including colonies, the Allies had more than a 5:1 advantage in population and a nearly 2:1 advantage in GDP.[425]In Asia at the same time, China had roughly six times the population of Japan but only an 89 percent higher GDP; this reduces to three times the population and only a 38 percent higher GDP if Japanese colonies are included.[425]
The United States produced about two-thirds of all munitions used by the Allies in World War II, including warships, transports, warplanes, artillery, tanks, trucks, and ammunition.[426]Although the Allies' economic and population advantages were largely mitigated during the initial rapid blitzkrieg attacks of Germany and Japan, they became the decisive factor by 1942, after the United States and Soviet Union joined the Allies and the war evolved into one ofattrition.[427]While the Allies' ability to out-produce the Axis was partly due to more access to natural resources, other factors, such as Germany and Japan's reluctance to employ women in thelabour force,[428]Alliedstrategic bombing,[429]and Germany's late shift to awar economy[430]contributed significantly. Additionally, neither Germany nor Japan planned to fight a protracted war, and had not equipped themselves to do so.[431]To improve their production, Germany and Japan used millions ofslave labourers;[432]Germany enslavedabout 12 million people, mostly from Eastern Europe,[405]whileJapan usedmore than 18 million people in Far East Asia.[413][414]
Aircraft were used forreconnaissance, asfighters,bombers, andground-support, and each role developed considerably. Innovations includedairlift(the capability to quickly move limited high-priority supplies, equipment, and personnel);[433]andstrategic bombing(the bombing of enemy industrial and population centres to destroy the enemy's ability to wage war).[434]Anti-aircraft weaponryalso advanced, including defences such asradarand surface-to-air artillery, in particular the introduction of theproximity fuze. The use of thejet aircraftwas pioneered and led to jets becoming standard in air forces worldwide.[435]
Advances were made in nearly every aspect ofnaval warfare, most notably withaircraft carriersandsubmarines. Althoughaeronauticalwarfare had relatively little success at the start of the war,actions at Taranto,Pearl Harbor, and theCoral Seaestablished the carrier as the dominant capital ship (in place of the battleship).[436][437][438]In the Atlantic,escort carriersbecame a vital part of Allied convoys, increasing the effective protection radius and helping to close theMid-Atlantic gap.[439]Carriers were also more economical thanbattleshipsdue to the relatively low cost of aircraft[440]and because they are not required to be as heavily armoured.[441]Submarines, which had proved to be an effective weapon during theFirst World War,[442]were expected by all combatants to be important in the second. The British focused development onanti-submarineweaponryand tactics, such assonarand convoys, while Germany focused on improving its offensive capability, with designs such as theType VII submarineandwolfpacktactics.[443][better source needed]Gradually, improving Allied technologies such as theLeigh Light,Hedgehog,Squid, andhoming torpedoesproved effective against German submarines.[444]
Land warfarechanged from the static frontlines oftrench warfareof World War I, which had relied on improvedartillerythat outmatched the speed of bothinfantryandcavalry, to increased mobility andcombined arms. Thetank, which had been used predominantly for infantry support in the First World War, had evolved into the primary weapon.[445]In the late 1930s, tank design was considerably more advanced than it had been during World WarI,[446]andadvances continued throughout the warwith increases in speed, armour and firepower.[447][448]At the start of the war, most commanders thought enemy tanks should be met by tanks with superior specifications.[449]This idea was challenged by the poor performance of the relatively light early tank guns against armour, and German doctrine of avoiding tank-versus-tank combat. This, along with Germany's use of combined arms, were among the key elements of their highly successful blitzkrieg tactics across Poland and France.[445]Many means ofdestroying tanks, includingindirect artillery,anti-tank guns(both towed andself-propelled),mines, short-ranged infantry antitank weapons, and other tanks were used.[449]Even with large-scale mechanisation, infantry remained the backbone of all forces,[450]and throughout the war, most infantry were equipped similarly to World War I.[451]The portable machine gun spread, a notable example being the GermanMG 34, and varioussubmachine gunswhich were suited toclose combatin urban and jungle settings.[451]Theassault rifle, a late war development incorporating many features of the rifle and submachine gun, became the standard post-war infantry weapon for most armed forces.[452]
Most major belligerents attempted to solve the problems of complexity and security involved in using largecodebooksforcryptographyby designingcipheringmachines, the most well-known being the GermanEnigma machine.[453]Development ofSIGINT(signalsintelligence) andcryptanalysisenabled the countering process of decryption. Notable examples were the Allied decryption ofJapanese naval codes[454]and BritishUltra, apioneering methodfor decoding Enigma that benefited from information given to the United Kingdom by thePolish Cipher Bureau, which had been decoding early versions of Enigma before the war.[455]Another component ofmilitary intelligencewasdeception, which the Allies used to great effect in operations such asMincemeatandBodyguard.[454][456]
Other technological and engineering feats achieved during, or as a result of, the war include the world's first programmablecomputers(Z3,Colossus, andENIAC),guided missilesandmodern rockets, theManhattan Project's development ofnuclear weapons,operations research, the development ofartificial harbours, andoil pipelines under the English Channel.[457]Penicillinwas firstdeveloped, mass-produced, and usedduring the war.[458]
Calculusis themathematicalstudy of continuous change, in the same way thatgeometryis the study of shape, andalgebrais the study of generalizations ofarithmetic operations.
Originally calledinfinitesimal calculusor "the calculus ofinfinitesimals", it has two major branches,differential calculusandintegral calculus. The former concerns instantaneousrates of change, and theslopesofcurves, while the latter concerns accumulation of quantities, andareasunder or between curves. These two branches are related to each other by thefundamental theorem of calculus. They make use of the fundamental notions ofconvergenceofinfinite sequencesandinfinite seriesto a well-definedlimit.[1]It is the "mathematical backbone" for dealing with problems where variables change with time or another reference variable.[2]
Infinitesimal calculus was formulated separately in the late 17th century byIsaac NewtonandGottfried Wilhelm Leibniz.[3][4]Later work, includingcodifying the idea of limits, put these developments on a more solid conceptual footing. Today, calculus is widely used inscience,engineering,biology, and even has applications insocial scienceand other branches of math.[5][6]
Inmathematics education,calculusis an abbreviation of bothinfinitesimal calculusandintegral calculus, which denotes courses of elementarymathematical analysis.
InLatin, the wordcalculusmeans “small pebble”, (thediminutiveofcalx,meaning "stone"), a meaning which stillpersists in medicine. Because such pebbles were used for counting out distances,[7]tallying votes, and doingabacusarithmetic, the word came to be the Latin word forcalculation. In this sense, it was used in English at least as early as 1672, several years before the publications of Leibniz and Newton, who wrote their mathematical texts in Latin.[8]
In addition to differential calculus and integral calculus, the term is also used for naming specific methods of computation or theories that imply some sort of computation. Examples of this usage includepropositional calculus,Ricci calculus,calculus of variations,lambda calculus,sequent calculus, andprocess calculus. Furthermore, the term "calculus" has variously been applied in ethics and philosophy, for such systems asBentham'sfelicific calculus, and theethical calculus.
Modern calculus was developed in 17th-century Europe byIsaac NewtonandGottfried Wilhelm Leibniz(independently of each other, first publishing around the same time) but elements of it first appeared in ancient Egypt and later Greece, then in China and the Middle East, and still later again in medieval Europe and India.
Calculations ofvolumeandarea, one goal of integral calculus, can be found in theEgyptianMoscow papyrus(c.1820BC), but the formulae are simple instructions, with no indication as to how they were obtained.[9][10]
Laying the foundations for integral calculus and foreshadowing the concept of the limit, ancient Greek mathematicianEudoxus of Cnidus(c.390–337BC) developed  themethod of exhaustionto prove the formulas for cone and pyramid volumes.
During theHellenistic period, this method was further developed byArchimedes(c.287– c.212BC), who combined it with a concept of theindivisibles—a precursor toinfinitesimals—allowing him to solve several problems now treated by integral calculus. InThe Method of Mechanical Theoremshe describes, for example, calculating thecenter of gravityof a solidhemisphere, the center of gravity of afrustumof a circularparaboloid, and the area of a region bounded by aparabolaand one of itssecant lines.[11]
The method of exhaustion was later discovered independently inChinabyLiu Huiin the 3rd century AD to find the area of a circle.[12][13]In the 5th century AD,Zu Gengzhi, son ofZu Chongzhi, established a method[14][15]that would later be calledCavalieri's principleto find the volume of asphere.
In the Middle East,Hasan Ibn al-Haytham, Latinized as Alhazen (c.965– c.1040AD) derived a formula for the sum offourth powers. He determined the equations to calculate the area enclosed by the curve represented byy=xk{\displaystyle y=x^{k}}(which translates to the integral∫xkdx{\displaystyle \int x^{k}\,dx}in contemporary notation), for any given non-negative integer value ofk{\displaystyle k}.[16]He used the results to carry out what would now be called anintegrationof this function, where the formulae for the sums of integral squares and fourth powers allowed him to calculate the volume of aparaboloid.[17]
Bhāskara II(c.1114–1185) was acquainted with some ideas of differential calculus and suggested that the "differential coefficient" vanishes at an extremum value of the function.[18]In his astronomical work, he gave a procedure that looked like a precursor to infinitesimal methods. Namely, ifx≈y{\displaystyle x\approx y}thensin⁡(y)−sin⁡(x)≈(y−x)cos⁡(y).{\displaystyle \sin(y)-\sin(x)\approx (y-x)\cos(y).}This can be interpreted as the discovery thatcosineis the derivative ofsine.[19]In the 14th century, Indian mathematicians gave a non-rigorous method, resembling differentiation, applicable to some trigonometric functions.Madhava of Sangamagramaand theKerala School of Astronomy and Mathematicsstated components of calculus. They studied series equivalent to the Maclaurin expansions of⁠sin⁡(x){\displaystyle \sin(x)}⁠,⁠cos⁡(x){\displaystyle \cos(x)}⁠, and⁠arctan⁡(x){\displaystyle \arctan(x)}⁠more than two hundred years before their introduction in Europe.[20]According toVictor J. Katzthey were not able to "combine many differing ideas under the two unifying themes of thederivativeand theintegral, show the connection between the two, and turn calculus into the great problem-solving tool we have today".[17]
Johannes Kepler's workStereometria Doliorum(1615) formed the basis of integral calculus.[21]Kepler developed a method to calculate the area of an ellipse by adding up the lengths of many radii drawn from a focus of the ellipse.[22]
Significant work was a treatise, the origin being Kepler's methods,[22]written byBonaventura Cavalieri, who argued that volumes and areas should be computed as the sums of the volumes and areas of infinitesimally thin cross-sections. The ideas were similar to Archimedes' inThe Method, but this treatise is believed to have been lost in the 13th century and was only rediscovered in the early 20th century, and so would have been unknown to Cavalieri. Cavalieri's work was not well respected since his methods could lead to erroneous results, and the infinitesimal quantities he introduced were disreputable at first.
The formal study of calculus brought together Cavalieri's infinitesimals with thecalculus of finite differencesdeveloped in Europe at around the same time.Pierre de Fermat, claiming that he borrowed fromDiophantus, introduced the concept ofadequality, which represented equality up to an infinitesimal error term.[23]The combination was achieved byJohn Wallis,Isaac Barrow, andJames Gregory, the latter two proving predecessors to thesecond fundamental theorem of calculusaround 1670.[24][25]
Theproduct ruleandchain rule,[26]the notions ofhigher derivativesandTaylor series,[27]and ofanalytic functions[28]were used byIsaac Newtonin an idiosyncratic notation which he applied to solve problems ofmathematical physics. In his works, Newton rephrased his ideas to suit the mathematical idiom of the time, replacing calculations with infinitesimals by equivalent geometrical arguments which were considered beyond reproach. He used the methods of calculus to solve the problem of planetary motion, the shape of the surface of a rotating fluid, the oblateness of the earth, the motion of a weight sliding on acycloid, and many other problems discussed in hisPrincipia Mathematica(1687). In other work, he developed series expansions for functions, including fractional and irrational powers, and it was clear that he understood the principles of theTaylor series. He did not publish all these discoveries, and at this time infinitesimal methods were still considered disreputable.[29]
These ideas were arranged into a true calculus of infinitesimals byGottfried Wilhelm Leibniz, who was originally accused ofplagiarismby Newton.[30]He is now regarded as anindependent inventorof and contributor to calculus. His contribution was to provide a clear set of rules for working with infinitesimal quantities, allowing the computation of second and higher derivatives, and providing theproduct ruleandchain rule, in their differential and integral forms. Unlike Newton, Leibniz put painstaking effort into his choices of notation.[31]
Today, Leibniz and Newton are usually both given credit for independently inventing and developing calculus. Newton was the first to apply calculus to generalphysics. Leibniz developed much of the notation used in calculus today.[32]: 51–52The basic insights that both Newton and Leibniz provided were the laws of differentiation and integration, emphasizing that differentiation and integration are inverse processes, second and higher derivatives, and the notion of an approximating polynomial series.
When Newton and Leibniz first published their results, there wasgreat controversyover which mathematician (and therefore which country) deserved credit. Newton derived his results first (later to be published in hisMethod of Fluxions), but Leibniz published his "Nova Methodus pro Maximis et Minimis" first. Newton claimed Leibniz stole ideas from his unpublished notes, which Newton had shared with a few members of theRoyal Society. This controversy divided English-speaking mathematicians from continental European mathematicians for many years, to the detriment of English mathematics.[33]A careful examination of the papers of Leibniz and Newton shows that they arrived at their results independently, with Leibniz starting first with integration and Newton with differentiation. It is Leibniz, however, who gave the new discipline its name. Newton called his calculus "the science of fluxions", a term that endured in English schools into the 19th century.[34]: 100The first complete treatise on calculus to be written in English and use the Leibniz notation was not published until 1815.[35]
Since the time of Leibniz and Newton, many mathematicians have contributed to the continuing development of calculus. One of the first and most complete works on both infinitesimal andintegral calculuswas written in 1748 byMaria Gaetana Agnesi.[36][37]
In calculus,foundationsrefers to therigorousdevelopment of the subject fromaxiomsand definitions.  In early calculus, the use ofinfinitesimalquantities was thought unrigorous and was fiercely criticized by several authors, most notablyMichel RolleandBishop Berkeley. Berkeley famously described infinitesimals as theghosts of departed quantitiesin his bookThe Analystin 1734.  Working out a rigorous foundation for calculus occupied mathematicians for much of the century following Newton and Leibniz, and is still to some extent an active area of research today.[38]
Several mathematicians, includingMaclaurin, tried to prove the soundness of using infinitesimals, but it would not be until 150 years later when, due to the work ofCauchyandWeierstrass, a way was finally found to avoid mere "notions" of infinitely small quantities.[39]The foundations of differential and integral calculus had been laid. In Cauchy'sCours d'Analyse, we find a broad range of foundational approaches, including a definition ofcontinuityin terms of infinitesimals, and a (somewhat imprecise) prototype of an(ε, δ)-definition of limitin the definition of differentiation.[40]In his work, Weierstrass formalized the concept oflimitand eliminated infinitesimals (although his definition can validatenilsquareinfinitesimals). Following the work of Weierstrass, it eventually became common to base calculus on limits instead of infinitesimal quantities, though the subject is still occasionally called "infinitesimal calculus".Bernhard Riemannused these ideas to give a precise definition of the integral.[41]It was also during this period that the ideas of calculus were generalized to thecomplex planewith the development ofcomplex analysis.[42]
In modern mathematics, the foundations of calculus are included in the field ofreal analysis, which contains full definitions andproofsof the theorems of calculus. The reach of calculus has also been greatly extended.Henri Lebesgueinventedmeasure theory, based on earlier developments byÉmile Borel, and used it to define integrals of all but the mostpathologicalfunctions.[43]Laurent Schwartzintroduceddistributions, which can be used to take the derivative of any function whatsoever.[44]
Limits are not the only rigorous approach to the foundation of calculus. Another way is to useAbraham Robinson'snon-standard analysis. Robinson's approach, developed in the 1960s, uses technical machinery frommathematical logicto augment the real number system withinfinitesimalandinfinitenumbers, as in the original Newton-Leibniz conception. The resulting numbers are calledhyperreal numbers, and they can be used to give a Leibniz-like development of the usual rules of calculus.[45]There is alsosmooth infinitesimal analysis, which differs from non-standard analysis in that it mandates neglecting higher-power infinitesimals during derivations.[38]Based on the ideas ofF. W. Lawvereand employing the methods ofcategory theory, smooth infinitesimal analysis views all functions as beingcontinuousand incapable of being expressed in terms ofdiscreteentities. One aspect of this formulation is that thelaw of excluded middledoes not hold.[38]The law of excluded middle is also rejected inconstructive mathematics, a branch of mathematics that insists that proofs of the existence of a number, function, or other mathematical object should give a construction of the object. Reformulations of calculus in a constructive framework are generally part of the subject ofconstructive analysis.[38]
While many of the ideas of calculus had been developed earlier inGreece,China,India,Iraq, Persia, andJapan, the use of calculus began in Europe, during the 17th century, when Newton and Leibniz built on the work of earlier mathematicians to introduce its basic principles.[13][29][46]The Hungarian polymathJohn von Neumannwrote of this work,
The calculus was the first achievement of modern mathematics and it is difficult to overestimate its importance. I think it defines more unequivocally than anything else the inception of modern mathematics, and the system of mathematical analysis, which is its logical development, still constitutes the greatest technical advance in exact thinking.[47]
Applications of differential calculus include computations involvingvelocityandacceleration, theslopeof a curve, andoptimization.[48]: 341–453Applications of integral calculus include computations involving area,volume,arc length,center of mass,work, andpressure.[48]: 685–700More advanced applications includepower seriesandFourier series.
Calculus is also used to gain a more precise understanding of the nature of space, time, and motion. For centuries, mathematicians and philosophers wrestled with paradoxes involvingdivision by zeroor sums of infinitely many numbers. These questions arise in the study ofmotionand area. Theancient GreekphilosopherZeno of Eleagave several famous examples of suchparadoxes. Calculus provides tools, especially thelimitand theinfinite series, that resolve the paradoxes.[49]
Calculus is usually developed by working with very small quantities. Historically, the first method of doing so was byinfinitesimals. These are objects which can be treated like real numbers but which are, in some sense, "infinitely small".  For example, an infinitesimal number could be greater than 0, but less than any number in the sequence 1, 1/2, 1/3, ... and thus less than any positivereal number. From this point of view, calculus is a collection of techniques for manipulating infinitesimals. The symbolsdx{\displaystyle dx}anddy{\displaystyle dy}were taken to be infinitesimal, and the derivativedy/dx{\displaystyle dy/dx}was their ratio.[38]
The infinitesimal approach fell out of favor in the 19th century because it was difficult to make the notion of an infinitesimal precise. In the late 19th century, infinitesimals were replaced within academia by theepsilon, deltaapproach tolimits. Limits describe the behavior of afunctionat a certain input in terms of its values at nearby inputs. They capture small-scale behavior using the intrinsic structure of thereal number system(as ametric spacewith theleast-upper-bound property). In this treatment, calculus is a collection of techniques for manipulating certain limits. Infinitesimals get replaced by sequences of smaller and smaller numbers, and the infinitely small behavior of a function is found by taking the limiting behavior for these sequences. Limits were thought to provide a more rigorous foundation for calculus, and for this reason, they became the standard approach during the 20th century. However, the infinitesimal concept was revived in the 20th century with the introduction ofnon-standard analysisandsmooth infinitesimal analysis, which provided solid foundations for the manipulation of infinitesimals.[38]
Differential calculus is the study of the definition, properties, and applications of thederivativeof a function. The process of finding the derivative is calleddifferentiation. Given a function and a point in the domain, the derivative at that point is a way of encoding the small-scale behavior of the function near that point. By finding the derivative of a function at every point in its domain, it is possible to produce a new function, called thederivative functionor just thederivativeof the original function. In formal terms, the derivative is alinear operatorwhich takes a function as its input and produces a second function as its output. This is more abstract than many of the processes studied in elementary algebra, where functions usually input a number and output another number. For example, if the doubling function is given the input three, then it outputs six, and if the squaring function is given the input three, then it outputs nine. The derivative, however, can take the squaring function as an input. This means that the derivative takes all the information of the squaring function—such as that two is sent to four, three is sent to nine, four is sent to sixteen, and so on—and uses this information to produce another function. The function produced by differentiating the squaring function turns out to be the doubling function.[32]: 32
In more explicit terms the "doubling function" may be denoted byg(x) = 2xand the "squaring function" byf(x) =x2. The "derivative" now takes the functionf(x), defined by the expression "x2", as an input, that is all the information—such as that two is sent to four, three is sent to nine, four is sent to sixteen, and so on—and uses this information to output another function, the functiong(x) = 2x, as will turn out.
InLagrange's notation, the symbol for a derivative is anapostrophe-like mark called aprime. Thus, the derivative of a function calledfis denoted byf′, pronounced "f prime" or "f dash". For instance, iff(x) =x2is the squaring function, thenf′(x) = 2xis its derivative (the doubling functiongfrom above).
If the input of the function represents time, then the derivative represents change concerning time. For example, iffis a function that takes time as input and gives the position of a ball at that time as output, then the derivative offis how the position is changing in time, that is, it is thevelocityof the ball.[32]: 18–20
If a function islinear(that is if thegraphof the function is a straight line), then the function can be written asy=mx+b, wherexis the independent variable,yis the dependent variable,bis they-intercept, and:
This gives an exact value for the slope of a straight line.[50]: 6If the graph of the function is not a straight line, however, then the change inydivided by the change inxvaries. Derivatives give an exact meaning to the notion of change in output concerning change in input. To be concrete, letfbe a function, and fix a pointain the domain off.(a,f(a))is a point on the graph of the function. Ifhis a number close to zero, thena+his a number close toa. Therefore,(a+h,f(a+h))is close to(a,f(a)). The slope between these two points is
This expression is called adifference quotient. A line through two points on a curve is called asecant line, somis the slope of the secant line between(a,f(a))and(a+h,f(a+h)). The second line is only an approximation to the behavior of the function at the pointabecause it does not account for what happens betweenaanda+h. It is not possible to discover the behavior ataby settinghto zero because this would requiredividing by zero, which is undefined. The derivative is defined by taking thelimitashtends to zero, meaning that it considers the behavior offfor all small values ofhand extracts a consistent value for the case whenhequals zero:
Geometrically, the derivative is the slope of thetangent lineto the graph offata. The tangent line is a limit of secant lines just as the derivative is a limit of difference quotients. For this reason, the derivative is sometimes called the slope of the functionf.[50]: 61–63
Here is a particular example, the derivative of the squaring function at the input 3. Letf(x) =x2be the squaring function.
The slope of the tangent line to the squaring function at the point (3, 9) is 6, that is to say, it is going up six times as fast as it is going to the right. The limit process just described can be performed for any point in the domain of the squaring function. This defines thederivative functionof the squaring function or just thederivativeof the squaring function for short. A computation similar to the one above shows that the derivative of the squaring function is the doubling function.[50]: 63
A common notation, introduced by Leibniz, for the derivative in the example above is
In an approach based on limits, the symbol⁠dy/dx⁠is to be interpreted not as the quotient of two numbers but as a shorthand for the limit computed above.[50]: 74Leibniz, however, did intend it to represent the quotient of two infinitesimally small numbers,dybeing the infinitesimally small change inycaused by an infinitesimally small changedxapplied tox. We can also think of⁠d/dx⁠as a differentiation operator, which takes a function as an input and gives another function, the derivative, as the output. For example:
In this usage, thedxin the denominator is read as "with respect tox".[50]: 79Another example of correct notation could be:
Even when calculus is developed using limits rather than infinitesimals, it is common to manipulate symbols likedxanddyas if they were real numbers; although it is possible to avoid such manipulations, they are sometimes notationally convenient in expressing operations such as thetotal derivative.
Integral calculusis the study of the definitions, properties, and applications of two related concepts, theindefinite integraland thedefinite integral. The process of finding the value of an integral is calledintegration.[48]: 508The indefinite integral, also known as theantiderivative, is the inverse operation to the derivative.[50]: 163–165Fis an indefinite integral offwhenfis a derivative ofF.  (This use of lower- and upper-case letters for a function and its indefinite integral is common in calculus.) The definite integral inputs a function and outputs a number, which gives the algebraic sum of areas between the graph of the input and thex-axis. The technical definition of the definite integral involves thelimitof a sum of areas of rectangles, called aRiemann sum.[51]: 282
A motivating example is the distance traveled in a given time.[50]: 153If the speed is constant, only multiplication is needed:
But if the speed changes, a more powerful method of finding the distance is necessary. One such method is to approximate the distance traveled by breaking up the time into many short intervals of time, then multiplying the time elapsed in each interval by one of the speeds in that interval, and then taking the sum (aRiemann sum) of the approximate distance traveled in each interval. The basic idea is that if only a short time elapses, then the speed will stay more or less the same. However, a Riemann sum only gives an approximation of the distance traveled. We must take the limit of all such Riemann sums to find the exact distance traveled.
When velocity is constant, the total distance traveled over the given time interval can be computed by multiplying velocity and time.  For example, traveling a steady 50 mph for 3 hours results in a total distance of 150 miles.  Plotting the velocity as a function of time yields a rectangle with a height equal to the velocity and a width equal to the time elapsed.  Therefore, the product of velocity and time also calculates the rectangular area under the (constant) velocity curve.[48]: 535This connection between the area under a curve and the distance traveled can be extended toanyirregularly shaped region exhibiting a fluctuating velocity over a given period. Iff(x)represents speed as it varies over time, the distance traveled between the times represented byaandbis the area of the region betweenf(x)and thex-axis, betweenx=aandx=b.
To approximate that area, an intuitive method would be to divide up the distance betweenaandbinto several equal segments, the length of each segment represented by the symbolΔx. For each small segment, we can choose one value of the functionf(x). Call that valueh. Then the area of the rectangle with baseΔxand heighthgives the distance (timeΔxmultiplied by speedh) traveled in that segment.   Associated with each segment is the average value of the function above it,f(x) =h. The sum of all such rectangles gives an approximation of the area between the axis and the curve, which is an approximation of the total distance traveled. A smaller value forΔxwill give more rectangles and in most cases a better approximation, but for an exact answer, we need to take a limit asΔxapproaches zero.[48]: 512–522
The symbol of integration is∫{\displaystyle \int }, anelongatedSchosen to suggest summation.[48]: 529The definite integral is written as:
and is read "the integral fromatoboff-of-xwith respect tox." The Leibniz notationdxis intended to suggest dividing the area under the curve into an infinite number of rectangles so that their widthΔxbecomes the infinitesimally smalldx.[32]: 44
The indefinite integral, or antiderivative, is written:
Functions differing by only a constant have the same derivative, and it can be shown that the antiderivative of a given function is a family of functions differing only by a constant.[51]: 326Since the derivative of the functiony=x2+C, whereCis any constant, isy′= 2x, the antiderivative of the latter is given by:
The unspecified constantCpresent in the indefinite integral or antiderivative is known as theconstant of integration.[52]: 135
Thefundamental theorem of calculusstates that differentiation and integration are inverse operations.[51]: 290More precisely, it relates the values of antiderivatives to definite integrals. Because it is usually easier to compute an antiderivative than to apply the definition of a definite integral, the fundamental theorem of calculus provides a practical way of computing definite integrals. It can also be interpreted as a precise statement of the fact that differentiation is the inverse of integration.
The fundamental theorem of calculus states: If a functionfiscontinuouson the interval[a,b]and ifFis a function whose derivative isfon the interval(a,b), then
This realization, made by bothNewtonandLeibniz, was key to the proliferation of analytic results after their work became known. (The extent to which Newton and Leibniz were influenced by immediate predecessors, and particularly what Leibniz may have learned from the work ofIsaac Barrow, is difficult to determine because of the priority dispute between them.[53]) The fundamental theorem provides an algebraic method of computing many definite integrals—without performing limit processes—by finding formulae forantiderivatives. It is also a prototype solution of adifferential equation. Differential equations relate an unknown function to its derivatives and are ubiquitous in the sciences.[54]: 351–352
Calculus is used in every branch of the physical sciences,[55]: 1actuarial science,computer science,statistics,engineering,economics,business,medicine,demography, and in other fields wherever a problem can bemathematically modeledand anoptimalsolution is desired.[56]It allows one to go from (non-constant) rates of change to the total change or vice versa, and many times in studying a problem we know one and are trying to find the other.[57]Calculus can be used in conjunction with other mathematical disciplines. For example, it can be used withlinear algebrato find the "best fit" linear approximation for a set of points in a domain. Or, it can be used inprobability theoryto determine theexpectation valueof a continuous random variable given aprobability density function.[58]: 37Inanalytic geometry, the study of graphs of functions, calculus is used to find high points and low points (maxima and minima), slope,concavityandinflection points. Calculus is also used to find approximate solutions to equations; in practice, it is the standard way to solve differential equations and do root finding in most applications. Examples are methods such asNewton's method,fixed point iteration, andlinear approximation. For instance, spacecraft use a variation of theEuler methodto approximate curved courses within zero-gravity environments.
Physicsmakes particular use of calculus; all concepts inclassical mechanicsandelectromagnetismare related through calculus. Themassof an object of knowndensity, themoment of inertiaof objects, and thepotential energiesdue to gravitational and electromagnetic forces can all be found by the use of calculus. An example of the use of calculus in mechanics isNewton's second law of motion, which states that the derivative of an object'smomentumconcerning time equals the netforceupon it. Alternatively, Newton's second law can be expressed by saying that the net force equals the object's mass times itsacceleration, which is the time derivative of velocity and thus the second time derivative of spatial position. Starting from knowing how an object is accelerating, we use calculus to derive its path.[59]
Maxwell's theory ofelectromagnetismandEinstein's theory ofgeneral relativityare also expressed in the language of differential calculus.[60][61]: 52–55Chemistry also uses calculus in determining reaction rates[62]: 599and in studying radioactive decay.[62]: 814In biology, population dynamics starts with reproduction and death rates to model population changes.[63][64]: 631
Green's theorem, which gives the relationship between a line integral around a simple closed curve C and a double integral over the plane region D bounded by C, is applied in an instrument known as aplanimeter, which is used to calculate the area of a flat surface on a drawing.[65]For example, it can be used to calculate the amount of area taken up by an irregularly shaped flower bed or swimming pool when designing the layout of a piece of property.
In the realm of medicine, calculus can be used to find the optimal branching angle of ablood vesselto maximize flow.[66]Calculus can be applied to understand how quickly a drug is eliminated from a body or how quickly acanceroustumor grows.[67]
In economics, calculus allows for the determination of maximal profit by providing a way to easily calculate bothmarginal costandmarginal revenue.[68]: 387
Ablack holeis a massive, compactastronomical objectso dense that its gravity prevents anything from escaping, even light.Albert Einstein's theory ofgeneral relativitypredicts that a sufficiently compactmasswill form a black hole.[3]Theboundaryof no escape is called theevent horizon. A black hole has a great effect on the fate and circumstances of an object crossing it, but has no locally detectable features according to general relativity.[4]In many ways, a black hole acts like an idealblack body, as it reflects no light.[5][6]Quantum field theory in curved spacetimepredicts that event horizons emitHawking radiation, withthe same spectrumas a black body of atemperatureinversely proportional to its mass. This temperature is of theorder of billionthsof akelvinforstellar black holes, making it essentially impossible to observe directly.
Objects whosegravitational fieldsare too strong for light to escape were first considered in the 18th century byJohn MichellandPierre-Simon Laplace. In 1916,Karl Schwarzschildfound the first modern solution of general relativity that would characterise a black hole. Due to his influential research, theSchwarzschild metricis named after him.David Finkelstein, in 1958, first published the interpretation of "black hole" as a region of space from which nothing can escape. Black holes were long considered a mathematical curiosity; it was not until the 1960s that theoretical work showed they were a generic prediction of general relativity. The discovery ofneutron starsbyJocelyn Bell Burnellin 1967 sparked interest ingravitationally collapsedcompact objects as a possible astrophysical reality. The first black hole known wasCygnus X-1, identified by several researchers independently in 1971.[7][8]
Black holes typically form whenmassive stars collapseat the end of theirlife cycle. After a black hole has formed, it can grow by absorbing mass from its surroundings. Supermassive black holes of millions of solar masses (M☉) may form by absorbing other stars and merging with other black holes, or viadirect collapseofgas clouds. There is consensus thatsupermassive black holesexist in the centres of mostgalaxies.
The presence of a black hole can be inferred through its interaction with othermatterand with electromagnetic radiation such as visible light. Matter falling toward a black hole can form anaccretion diskof infalling plasma, heated byfrictionand emitting light. In extreme cases, this creates aquasar, some of the brightest objects in the universe. Stars passing too close to a supermassive black hole can be shredded into streamers that shine very brightly before being "swallowed."[9]If other stars are orbiting a black hole, their orbits can be used to determine the black hole's mass and location. Such observations can be used to exclude possible alternatives such asneutron stars. In this way, astronomers have identified numerous stellar black hole candidates inbinary systemsand established that the radio source known asSagittarius A*, at the core of theMilky Waygalaxy, contains a supermassive black hole of about 4.3 millionsolarmasses.
The idea of a body so big that even light could not escape was briefly proposed by English astronomical pioneer and clergymanJohn Michelland independently by French scientistPierre-Simon Laplace. Both scholars proposed very large stars rather than the modern model of stars with extraordinary density.[10]
Michell's idea, in a short part of a letter published in 1784, calculated that a star with the same density but 500 times the radius of the sun would not let any emitted light escape; the surfaceescape velocitywould exceed the speed of light. Michell correctly noted that such supermassive but non-radiating bodies might be detectable through their gravitational effects on nearby visible bodies.[10][11][12]
In 1796, Laplace mentioned that a star could be invisible if it were sufficiently large while speculating on the origin of the Solar System in his bookExposition du Système du Monde.Franz Xaver von Zachasked Laplace for a mathematical analysis, which Laplace provided and published in journal edited by von Zach.[10]
Scholars of the time were initially excited by the proposal that giant but invisible 'dark stars' might be hiding in plain view, but enthusiasm dampened when the wavelike nature of light became apparent in the early nineteenth century,[13]since light was understood as a wave rather than a particle, it was unclear what, if any, influence gravity would have on escaping light waves.[10][12]
In 1915,Albert Einsteindeveloped his theory ofgeneral relativity, having earlier shown that gravity does influence light's motion. Only a few months later,Karl Schwarzschildfound asolutionto theEinstein field equationsthat describes thegravitational fieldof apoint massand a spherical mass.[14][15]A few months after Schwarzschild,Johannes Droste, a student ofHendrik Lorentz, independently gave the same solution for the point mass and wrote more extensively about its properties.[16][17]This solution had a peculiar behaviour at what is now called theSchwarzschild radius, where it becamesingular, meaning that some of the terms in the Einstein equations became infinite. The nature of this surface was not quite understood at the time.
In 1924,Arthur Eddingtonshowed that the singularity disappeared after a change of coordinates. In 1933,Georges Lemaîtrerealised that this meant the singularity at the Schwarzschild radius was a non-physicalcoordinate singularity.[18]Arthur Eddington commented on the possibility of a star with mass compressed to the Schwarzschild radius in a 1926 book, noting that Einstein's theory allows us to rule out overly large densities for visible stars likeBetelgeusebecause "a star of 250 million km radius could not possibly have so high a density as the Sun. Firstly, the force of gravitation would be so great that light would be unable to escape from it, the rays falling back to the star like a stone to the earth. Secondly, the red shift of the spectral lines would be so great that the spectrum would be shifted out of existence. Thirdly, the mass would produce so much curvature of the spacetime metric that space would close up around the star, leaving us outside (i.e., nowhere)."[19][20]
In 1931,Subrahmanyan Chandrasekharcalculated, using special relativity, that a non-rotating body ofelectron-degenerate matterabove a certain limiting mass (now called theChandrasekhar limitat 1.4M☉) has no stable solutions.[21]His arguments were opposed by many of his contemporaries like Eddington andLev Landau, who argued that some yet unknown mechanism would stop the collapse.[22]They were partly correct: awhite dwarfslightly more massive than the Chandrasekhar limit will collapse into aneutron star,[23]which is itself stable.
In 1939,Robert Oppenheimerand others predicted that neutron stars above another limit, theTolman–Oppenheimer–Volkoff limit, would collapse further for the reasons presented by Chandrasekhar, and concluded that no law of physics was likely to intervene and stop at least some stars from collapsing to black holes.[24]Their original calculations, based on thePauli exclusion principle, gave it as 0.7M☉. Subsequent consideration of neutron-neutron repulsion mediated by the strong force raised the estimate to approximately 1.5M☉to 3.0M☉.[25]Observations of the neutron star mergerGW170817, which is thought to have generated a black hole shortly afterward, have refined the TOV limit estimate to ~2.17M☉.[26][27][28][29][30]
Oppenheimer and his co-authors interpreted the singularity at the boundary of the Schwarzschild radius as indicating that this was the boundary of a bubble in which time stopped.  This is a valid point of view for external observers, but not for infalling observers.  The hypothetical collapsed stars were called "frozen stars", because an outside observer would see the surface of the star frozen in time at the instant where its collapse takes it to the Schwarzschild radius.[31]
Also in 1939, Einstein attempted to prove that black holes were impossible in his publication "On a Stationary System with Spherical Symmetry Consisting of Many Gravitating Masses", using his theory of general relativity to defend his argument.[32]Months later, Oppenheimer and his studentHartland Snyderprovided theOppenheimer–Snyder modelin their paper "On Continued Gravitational Contraction",[33]which predicted the existence of black holes. In the paper, which made no reference to Einstein's recent publication, Oppenheimer and Snyder used Einstein's own theory of general relativity to show the conditions on how a black hole could develop, for the first time in contemporary physics.[32]
In 1958,David Finkelsteinidentified the Schwarzschild surface as an event horizon, "a perfect unidirectional membrane: causal influences can cross it in only one direction".[34]This did not strictly contradict Oppenheimer's results, but extended them to include the point of view of infalling observers. Finkelstein's solution extended the Schwarzschild solution for the future of observers falling into a black hole. Acomplete extensionhad already been found byMartin Kruskal, who was urged to publish it.[35]
These results came at the beginning of thegolden age of general relativity, which was marked by general relativity and black holes becoming mainstream subjects of research. This process was helped by the discovery ofpulsarsbyJocelyn Bell Burnellin 1967,[36][37]which, by 1969, were shown to be rapidly rotating neutron stars.[38]Until that time, neutron stars, like black holes, were regarded as just theoretical curiosities; but the discovery of pulsars showed their physical relevance and spurred a further interest in all types of compact objects that might be formed by gravitational collapse.[39]
In this period more general black hole solutions were found. In 1963,Roy Kerrfoundthe exact solutionfor arotating black hole. Two years later,Ezra Newmanfound theaxisymmetricsolution for a black hole that is both rotating andelectrically charged.[40]Through the work ofWerner Israel,[41]Brandon Carter,[42][43]and David Robinson[44]theno-hair theorememerged, stating that a stationary black hole solution is completely described by the three parameters of theKerr–Newman metric:mass,angular momentum, and electric charge.[45]
At first, it was suspected that the strange features of the black hole solutions were pathological artefacts from the symmetry conditions imposed, and that the singularities would not appear in generic situations. This view was held in particular byVladimir Belinsky,Isaak Khalatnikov, andEvgeny Lifshitz, who tried to prove that no singularities appear in generic solutions. However, in the late 1960sRoger Penrose[46]andStephen Hawkingused global techniques to prove that singularities appear generically.[47]For this work, Penrose received half of the 2020Nobel Prize in Physics, Hawking having died in 2018.[48]Based on observations inGreenwichandTorontoin the early 1970s,Cygnus X-1, a galacticX-raysource discovered in 1964, became the first astronomical object commonly accepted to be a black hole.[49][50]
Work byJames Bardeen,Jacob Bekenstein, Carter, and Hawking in the early 1970s led to the formulation ofblack hole thermodynamics.[51]These laws describe the behaviour of a black hole in close analogy to thelaws of thermodynamicsby relating mass to energy, area toentropy, andsurface gravitytotemperature. The analogy was completed when Hawking, in 1974, showed thatquantum field theoryimplies that black holes should radiate like ablack bodywith a temperature proportional to the surface gravity of the black hole, predicting the effect now known asHawking radiation.[52]
On 11 February 2016, theLIGO Scientific Collaborationand theVirgo collaborationannounced the first direct detectionofgravitational waves, representing the first observation of a black hole merger.[53]On 10 April 2019, the first direct image of a black hole and its vicinity was published, following observations made by theEvent Horizon Telescope(EHT) in 2017 of the supermassive black hole in Messier 87'sgalactic centre.[54][55][56]Gaia missionobservations have found evidence of a Sun-like star orbiting a black hole namedGaia BH1around 1,560light-years(480parsecs) away;[57]evidence suggests abrown dwarf starorbitsGaia BH2.[58]Though only a couple dozen black holes have been found so far in the Milky Way, there are thought to be hundreds of millions, most of which are solitary and do not cause emission of radiation.[59]Therefore, they would only be detectable bygravitational lensing.
Science writer Marcia Bartusiak traces the term "black hole" to physicistRobert H. Dicke, who in the early 1960s reportedly compared the phenomenon to theBlack Hole of Calcutta, notorious as a prison where people entered but never left alive.
The term "black hole" was used in print byLifeandScience Newsmagazines in 1963, and by science journalist Ann Ewing in her article"'Black Holes' in Space", dated 18 January 1964, which was a report on a meeting of theAmerican Association for the Advancement of Scienceheld in Cleveland, Ohio.[60]
In December 1967, a student reportedly suggested the phrase "black hole" at a lecture byJohn Wheeler;[60]Wheeler adopted the term for its brevity and "advertising value", and it quickly caught on,[61]leading some to credit Wheeler with coining the phrase.[62]
Theescape velocityfrom a black hole exceeds thespeed of light. The formula for escape velocity isV=2MG/R{\displaystyle V={\sqrt {2MG/R}}}for an object at radiusRfrom a spherical massM, withGbeing the gravitational constant. When the velocity is the speed of light,c, the radius,Rs=2MG/c2,{\displaystyle R_{s}=2MG/c^{2},}is called theSchwarzschild radius.[63]: 27A technical definition of a black hole is any object whose mass is contained in a radius is smaller than itsSchwarzschild radius, a limit derived from one solution to the equations of general relativity.[64]: 410
Theno-hair theorempostulates that, once it achieves a stable condition after formation, a black hole has only three independent physical properties: mass, electric charge, and angular momentum; the black hole is otherwise featureless. If the conjecture is true, any two black holes that share the same values for these properties, or parameters, are indistinguishable from one another. The degree to which the conjecture is true for real black holes under the laws of modern physics is currently an unsolved problem.[45]
These properties are special because they are visible from outside a black hole. For example, a charged black hole repels other like charges just like any other charged object. Similarly, the total mass inside a sphere containing a black hole can be found by using the gravitational analogue ofGauss's law(through theADM mass), far away from the black hole.[65]Likewise, the angular momentum (or spin) can be measured from far away usingframe draggingby thegravitomagnetic field, through for example theLense–Thirring effect.[66]
When an object falls into a black hole, any information about the shape of the object or distribution of charge on it is evenly distributed along the horizon of the black hole, and is lost to outside observers. The behaviour of the horizon in this situation is adissipative systemthat is closely analogous to that of a conductive stretchy membrane with friction andelectrical resistance—themembrane paradigm.[67]This is different from otherfield theoriessuch as electromagnetism, which do not have any friction or resistivity at the microscopic level, because they aretime-reversible.[68][69]
Because a black hole eventually achieves a stable state with only three parameters, there is no way to avoid losing information about the initial conditions: the gravitational and electric fields of a black hole give very little information about what went in. The information that is lost includes every quantity that cannot be measured far away from the black hole horizon, includingapproximately conservedquantum numberssuch as the totalbaryon numberandlepton number. This behaviour is so puzzling that it has been called theblack hole information loss paradox.[68][69]
The simplest static black holes have mass but neither electric charge nor angular momentum. These black holes are often referred to as Schwarzschild black holes afterKarl Schwarzschildwho discovered thissolutionin 1916.[15]According toBirkhoff's theorem, it is the onlyvacuum solutionthat isspherically symmetric.[70]This means there is no observable difference at a distance between the gravitational field of such a black hole and that of any other spherical object of the same mass. The popular notion of a black hole "sucking in everything" in its surroundings is therefore correct only near a black hole's horizon; far away, the external gravitational field is identical to that of any other body of the same mass.[71]
Solutions describing more general black holes also exist. Non-rotatingcharged black holesare described by theReissner–Nordström metric, while the Kerr metric describes a non-charged rotating black hole. The most generalstationaryblack hole solution known is the Kerr–Newman metric, which describes a black hole with both charge and angular momentum.[72]
While the mass of a black hole can take any positive value, the charge and angular momentum are constrained by the mass. The total electric chargeQand the total angular momentumJare expected to satisfy the inequalityQ24πϵ0+c2J2GM2≤GM2{\displaystyle {\frac {Q^{2}}{4\pi \epsilon _{0}}}+{\frac {c^{2}J^{2}}{GM^{2}}}\leq GM^{2}}for a black hole of massM. Black holes with the minimum possible mass satisfying this inequality are calledextremal. Solutions of Einstein's equations that violate this inequality exist, but they do not possess an event horizon. These solutions have so-callednaked singularitiesthat can be observed from the outside, and hence are deemedunphysical. Thecosmic censorship hypothesisrules out the formation of such singularities, when they are created through the gravitational collapse ofrealistic matter.[73]This is supported by numerical simulations.[74]
Due to the relatively large strength of theelectromagnetic force, black holes forming from the collapse of stars are expected to retain the nearly neutral charge of the star. Rotation, however, is expected to be a universal feature of compact astrophysical objects. The black-hole candidate binary X-ray sourceGRS 1915+105[75]appears to have an angular momentum near the maximum allowed value. That uncharged limit is[76]J≤GM2c,{\displaystyle J\leq {\frac {GM^{2}}{c}},}allowing definition of adimensionlessspin parameter such that[76]0≤cJGM2≤1.{\displaystyle 0\leq {\frac {cJ}{GM^{2}}}\leq 1.}[76][Note 1]
Black holes are commonly classified according to their mass, independent of angular momentum,J. The size of a black hole, as determined by the radius of the event horizon, or Schwarzschild radius, is proportional to the mass,M, throughrs=2GMc2≈2.95MM⊙km,{\displaystyle r_{\mathrm {s} }={\frac {2GM}{c^{2}}}\approx 2.95\,{\frac {M}{M_{\odot }}}~\mathrm {km,} }wherersis the Schwarzschild radius andM☉is themass of the Sun.[78]For a black hole with nonzero spin or electric charge, the radius is smaller,[Note 2]until an extremal black hole could have an event horizon close to[79]r+=GMc2.{\displaystyle r_{\mathrm {+} }={\frac {GM}{c^{2}}}.}
The defining feature of a black hole is the appearance of an event horizon—a boundary inspacetimethrough which matter and light can pass only inward towards the mass of the black hole. Nothing, not even light, can escape from inside the event horizon.[81][82]The event horizon is referred to as such because if an event occurs within the boundary, information from that event cannot reach an outside observer, making it impossible to determine whether such an event occurred.[83]
As predicted by general relativity, the presence of a mass deforms spacetime in such a way that the paths taken by particles bend towards the mass.[84]At the event horizon of a black hole, this deformation becomes so strong that there are no paths that lead away from the black hole.[85]
In a thought experiment, a distant observer can imagine clocks near a black hole which would appear to tick more slowly than those farther away from the black hole.[86]This effect, known asgravitational time dilation, would also cause an object falling into a black hole to appear to slow as it approaches the event horizon, taking an infinite amount of time to reach it.[87]All processes on this object would appear to slow down, from the viewpoint of a fixed outside observer, and any light emitted by the object to appear redder and dimmer, an effect known asgravitational redshift.[88]Eventually, the falling object fades away until it can no longer be seen. Typically this process happens very rapidly with an object disappearing from view within less than a second.[89]
On the other hand, imaginary, indestructible observers falling into a black hole would not notice any of these effects as they cross the event horizon. Their own clocks appear to them to tick normally, they cross the event horizon after a finite time without noting any singular behaviour.
Ingeneral relativity, it is impossible to determine the location of the event horizon from local observations, due to Einstein'sequivalence principle.[90][91]
Thetopologyof the event horizon of a black hole at equilibrium is always spherical.[Note 4][94]For non-rotating (static) black holes the geometry of the event horizon is precisely spherical, while for rotating black holes the event horizon is oblate.[95][96][97]
At the centre of a black hole, as described by general relativity, may lie agravitational singularity, a region where the spacetime curvature becomes infinite.[98]For a non-rotating black hole, this region takes the shape of a single point; for a rotating black hole it is smeared out to form aring singularitythat lies in the plane of rotation.[99]In both cases, the singular region has zero volume. It can also be shown that the singular region contains all the mass of the black hole solution.[100]The singular region can thus be thought of as having infinitedensity.[101]
Observers falling into a Schwarzschild black hole (i.e., non-rotating and not charged) cannot avoid being carried into the singularity once they cross the event horizon. They can prolong the experience by accelerating away to slow their descent, but only up to a limit.[102]When they reach the singularity, they are crushed to infinite density and their mass is added to the total of the black hole. Before that happens, they will have been torn apart by the growingtidal forcesin a process sometimes referred to asspaghettificationor the "noodle effect".[103]
In the case of a charged (Reissner–Nordström) or rotating (Kerr) black hole, it is possible to avoid the singularity. Extending these solutions as far as possible reveals the hypothetical possibility of exiting the black hole into a different spacetime with the black hole acting as awormhole.[104]The possibility of travelling to another universe is, however, only theoretical since any perturbation would destroy this possibility.[105]It also appears to be possible to followclosed timelike curves(returning to one's own past) around the Kerr singularity, which leads to problems withcausalitylike thegrandfather paradox.[106]It is expected that none of these peculiar effects would survive in a proper quantum treatment of rotating and charged black holes.[107]
The appearance of singularities in general relativity is commonly perceived as signalling the breakdown of the theory.[108]This breakdown, however, is expected; it occurs in a situation wherequantum effectsshould describe these actions, due to the extremely high density and therefore particle interactions. To date, it has not been possible to combine quantum and gravitational effects into a single theory, although there exist attempts to formulate such a theory ofquantum gravity. It is generally expected that such a theory will not feature any singularities.[109][110]
The photon sphere is a spherical boundary where photons that move on tangents to that sphere would be trapped in a non-stable but circular orbit around the black hole.[111]For non-rotating black holes, the photon sphere has a radius 1.5 times the Schwarzschild radius. Their orbits would bedynamically unstable, hence any small perturbation, such as a particle of infalling matter, would cause an instability that would grow over time, either setting the photon on an outward trajectory causing it to escape the black hole, or on an inward spiral where it would eventually cross the event horizon.[112]
While light can still escape from the photon sphere, any light that crosses the photon sphere on an inbound trajectory will be captured by the black hole. Hence any light that reaches an outside observer from the photon sphere must have been emitted by objects between the photon sphere and the event horizon.[112]For a Kerr black hole the radius of the photon sphere depends on the spin parameter and on the details of the photon orbit, which can be prograde (the photon rotates in the same sense of the black hole spin) or retrograde.[113][114]
Rotating black holes are surrounded by a region of spacetime in which it is impossible to stand still, called the ergosphere. This is the result of a process known asframe-dragging; general relativity predicts that any rotating mass will tend to slightly "drag" along the spacetime immediately surrounding it. Any object near the rotating mass will tend to start moving in the direction of rotation. For a rotating black hole, this effect is so strong near the event horizon that an object would have to move faster than thespeed of lightin the opposite direction to just stand still.[116]
The ergosphere of a black hole is a volume bounded by the black hole's event horizon and theergosurface, which coincides with the event horizon at the poles but is at a much greater distance around the equator.[115]
Objects and radiation can escape normally from the ergosphere. Through thePenrose process, objects can emerge from the ergosphere with more energy than they entered with. The extra energy is taken from the rotational energy of the black hole. Thereby the rotation of the black hole slows down.[117]A variation of the Penrose process in the presence of strong magnetic fields, theBlandford–Znajek processis considered a likely mechanism for the enormous luminosity and relativistic jets ofquasarsand otheractive galactic nuclei.
InNewtonian gravity,test particlescan stably orbit at arbitrary distances from a central object. In general relativity, however, there exists an innermost stable circular orbit (often called the ISCO), for which any infinitesimal inward perturbations to a circular orbit will lead to spiraling into the black hole, and any outward perturbations will, depending on the energy, result in spiraling in, stably orbiting between apastron and periastron, or escaping to infinity.[118]The location of the ISCO depends on the spin of the black hole, in the case of a Schwarzschild black hole (spin zero) is:rISCO=3rs=6GMc2,{\displaystyle r_{\rm {ISCO}}=3\,r_{s}={\frac {6\,GM}{c^{2}}},}and decreases with increasing black hole spin for particles orbiting in the same direction as the spin.[113]
The final observable region of spacetime around a black hole is called the plunging region. In this area it is no longer possible for matter to follow circular orbits or to stop a final descent into the black hole. Instead it will rapidly plunge toward the black hole close to the speed of light.[119][120]
Given the bizarre character of black holes, it was long questioned whether such objects could actually exist in nature or whether they were merely pathological solutions to Einstein's equations. Einstein himself wrongly thought black holes would not form, because he held that the angular momentum of collapsing particles would stabilise their motion at some radius.[121]This led the general relativity community to dismiss all results to the contrary for many years. However, a minority of relativists continued to contend that black holes were physical objects,[122]and by the end of the 1960s, they had persuaded the majority of researchers in the field that there is no obstacle to the formation of an event horizon.[123]
Penrose demonstrated that once an event horizon forms, general relativity without quantum mechanics requires that a singularity will form within.[46]Shortly afterwards, Hawking showed that many cosmological solutions that describe theBig Banghave singularities withoutscalar fieldsor otherexotic matter.[clarification needed]TheKerr solution, the no-hair theorem, and the laws of black hole thermodynamics showed that the physical properties of black holes were simple and comprehensible, making them respectable subjects for research.[124]Conventional black holes are formed bygravitational collapseof heavy objects such as stars, but they can also in theory be formed by other processes.[125][126]
Gravitational collapse occurs when an object's internalpressureis insufficient to resist the object's own gravity. For stars this usually occurs either because a star has too little "fuel" left to maintain its temperature throughstellar nucleosynthesis, or because a star that would have been stable receives extra matter in a way that does not raise its core temperature. In either case the star's temperature is no longer high enough to prevent it from collapsing under its own weight.[128]
The collapse may be stopped by thedegeneracy pressureof the star's constituents, allowing the condensation of matter into an exoticdenser state. The result is one of the various types ofcompact star. Which type forms depends on the mass of the remnant of the original star left if the outer layers have been blown away (for example, in aType II supernova). The mass of the remnant, the collapsed object that survives the explosion, can be substantially less than that of the original star. Remnants exceeding 5M☉are produced by stars that were over 20M☉before the collapse.[128]
If the mass of the remnant exceeds about 3–4M☉(the Tolman–Oppenheimer–Volkoff limit[24]), either because the original star was very heavy or because the remnant collected additional mass through accretion of matter, even the degeneracy pressure ofneutronsis insufficient to stop the collapse. No known mechanism (except possibly quark degeneracy pressure) is powerful enough to stop the implosion and the object will inevitably collapse to form a black hole.[128]
The gravitational collapse of heavy stars is assumed to be responsible for the formation ofstellar mass black holes.Star formationin the early universe may have resulted in very massive stars, which upon their collapse would have produced black holes of up to 103M☉. These black holes could be the seeds of the supermassive black holes found in the centres of most galaxies.[129]It has further been suggested that massive black holes with typical masses of ~105M☉could have formed from the direct collapse of gas clouds in the young universe.[125]These massive objects have been proposed as the seeds that eventually formed the earliest quasars observed already at redshiftz∼7{\displaystyle z\sim 7}.[130]Some candidates for such objects have been found in observations of the young universe.[125]
While most of the energy released during gravitational collapse is emitted very quickly, an outside observer does not actually see the end of this process. Even though the collapse takes a finite amount of time from thereference frameof infalling matter, a distant observer would see the infalling material slow and halt just above the event horizon, due to gravitational time dilation. Light from the collapsing material takes longer and longer to reach the observer, with the light emitted just before the event horizon forms delayed an infinite amount of time. Thus the external observer never sees the formation of the event horizon; instead, the collapsing material seems to become dimmer and increasingly red-shifted, eventually fading away.[131]
Gravitational collapse requires great density. In the current epoch of the universe these high densities are found only in stars, but in the early universe shortly after the Big Bang densities were much greater, possibly allowing for the creation of black holes. High density alone is not enough to allow black hole formation since a uniform mass distribution will not allow the mass to bunch up. In order forprimordial black holesto have formed in such a dense medium, there must have been initial density perturbations that could then grow under their own gravity. Different models for the early universe vary widely in their predictions of the scale of these fluctuations. Various models predict the creation of primordial black holes ranging in size from aPlanck mass(mP=ℏc/G{\displaystyle m_{P}={\sqrt {\hbar c/G}}}≈1.2×1019GeV/c2≈2.2×10−8kg) to hundreds of thousands of solar masses.[126]
Despite the early universe being extremelydense, it did not re-collapse into a black hole during the Big Bang, since the expansion rate was greater than the attraction. Followinginflation theorythere was a net repulsive gravitation in the beginning until the end of inflation. Since then theHubble flowwas slowed by the energy density of the universe.
Models for the gravitational collapse of objects of relatively constant size, such asstars, do not necessarily apply in the same way to rapidly expanding space such as the Big Bang.[132]
Gravitational collapse is not the only process that could create black holes. In principle, black holes could be formed inhigh-energycollisions that achieve sufficient density. As of 2002, no such events have been detected, either directly or indirectly as a deficiency of the mass balance inparticle acceleratorexperiments.[133]This suggests that there must be a lower limit for the mass of black holes. Theoretically, this boundary is expected to lie around the Planck mass, where quantum effects are expected to invalidate the predictions of general relativity.[134]
This would put the creation of black holes firmly out of reach of any high-energy process occurring on or near the Earth. However, certain developments in quantum gravity suggest that the minimum black hole mass could be much lower: somebraneworldscenarios for example put the boundary as low as1 TeV/c2.[135]This would make it conceivable formicro black holesto be created in the high-energy collisions that occur whencosmic rayshit the Earth's atmosphere, or possibly in theLarge Hadron CollideratCERN. These theories are very speculative, and the creation of black holes in these processes is deemed unlikely by many specialists.[136]Even if micro black holes could be formed, it is expected that they wouldevaporatein about 10−25seconds, posing no threat to the Earth.[137]
Once a black hole has formed, it can continue to grow by absorbing additionalmatter. Any black hole will continually absorb gas andinterstellar dustfrom its surroundings. This growth process is one possible way through which some supermassive black holes may have been formed, although theformation of supermassive black holesis still an open field of research.[129]A similar process has been suggested for the formation ofintermediate-mass black holesfound inglobular clusters.[138]Black holes can also merge with other objects such as stars or even other black holes. This is thought to have been important, especially in the early growth of supermassive black holes, which could have formed from the aggregation of many smaller objects.[129]The process has also been proposed as the origin of some intermediate-mass black holes.[139][140]
In 1974, Hawking predicted that black holes are not entirely black but emit small amounts of thermal radiation at a temperatureħc3/(8πGMkB);[52]this effect has become known as Hawking radiation. By applying quantum field theory to a static black hole background, he determined that a black hole should emit particles that display a perfectblack body spectrum. Since Hawking's publication, many others have verified the result through various approaches.[141]If Hawking's theory of black hole radiation is correct, then black holes are expected to shrink and evaporate over time as they lose mass by the emission of photons and other particles.[52]The temperature of this thermal spectrum (Hawking temperature) is proportional to the surface gravity of the black hole, which, for a Schwarzschild black hole, is inversely proportional to the mass. Hence, large black holes emit less radiation than small black holes.[142]
A stellar black hole of 1M☉has a Hawking temperature of 62nanokelvins.[143]This is far less than the 2.7 K temperature of thecosmic microwave backgroundradiation. Stellar-mass or larger black holes receive more mass from the cosmic microwave background than they emit through Hawking radiation and thus will grow instead of shrinking.[144]To have a Hawking temperature larger than 2.7 K (and be able to evaporate), a black hole would need a mass less than theMoon. Such a black hole would have a diameter of less than a tenth of a millimetre.[145]
If a black hole is very small, the radiation effects are expected to become very strong. A black hole with the mass of a car would have a diameter of about 10−24m and take a nanosecond to evaporate, during which time it would briefly have a luminosity of more than 200 times that of the Sun. Lower-mass black holes are expected to evaporate even faster; for example, a black hole of mass 1 TeV/c2would take less than 10−88seconds to evaporate completely. For such a small black hole, quantum gravity effects are expected to play an important role and could hypothetically make such a small black hole stable, although current developments in quantum gravity do not indicate this is the case.[146][147]
The Hawking radiation for an astrophysical black hole is predicted to be very weak and would thus be exceedingly difficult to detect from Earth. A possible exception, however, is the burst of gamma rays emitted in the last stage of the evaporation of primordial black holes. Searches for such flashes have proven unsuccessful and provide stringent limits on the possibility of existence of low mass primordial black holes.[148]NASA'sFermi Gamma-ray Space Telescopelaunched in 2008 will continue the search for these flashes.[149]
If black holes evaporate via Hawking radiation, a solar mass black hole will evaporate (beginning once the temperature of the cosmic microwave background drops below that of the black hole) over a period of 1064years.[150]A supermassive black hole with a mass of 1011M☉will evaporate in around 2×10100years.[151]During the collapse of a supercluster of galaxies, supermassive black holes are predicted to grow to perhaps 1014M☉. Even these would evaporate over atimescaleof up to 10106years.[150]
By nature, black holes do not themselves emit any electromagnetic radiation other than the hypothetical Hawking radiation, so astrophysicists searching for black holes must generally rely on indirect observations. For example, a black hole's existence can sometimes be inferred by observing its gravitational influence on its surroundings.[152]
TheEvent Horizon Telescope(EHT) is an active program that directly observes the immediate environment of black holes' event horizons, such as the black hole at the centre of the Milky Way. In April 2017, EHT began observing the black hole at the centre ofMessier 87.[153][154]"In all, eight radio observatories on six mountains and four continents observed the galaxy in Virgo on and off for 10 days in April 2017" to provide the data yielding the image in April 2019.[155]
After two years of data processing, EHT released its first  image of a black hole, at the center of the Messier 87 galaxy.[156][157]What is visible is not the black hole—which shows as black because of the loss of all light within this dark region. Instead, it is the gases at the edge of the event horizon, displayed as orange or red, that define the black hole.[158]
On 12 May 2022, the EHT released the first image ofSagittarius A*, the supermassive black hole at the centre of the Milky Way galaxy. The published image displayed the same ring-like structure and "shadow" seen in the M87* black hole. The boundary of the shadow or area of less brightness matches the predicted gravitationally lensed photon orbits.[159]The image was created using the same techniques as for the M87 black hole. The imaging process for Sagittarius A*, which is more than a thousand times smaller and less massive than M87*, was significantly more complex because of the instability of its surroundings.[160]The image of Sagittarius A* was partially blurred by turbulentplasmaon the way to the galactic centre, an effect which prevents resolution of the image at longer wavelengths.[161]
The brightening of this material in the 'bottom' half of the processed EHT image is thought to be caused byDoppler beaming, whereby material approaching the viewer at relativistic speeds is perceived as brighter than material moving away. In the case of a black hole, this phenomenon implies that the visible material is rotating at relativistic speeds (>1,000 km/s [2,200,000 mph]), the only speeds at which it is possible to centrifugally balance the immense gravitational attraction of the singularity, and thereby remain in orbit above the event horizon. This configuration of bright material implies that the EHT observedM87*from a perspective catching the black hole's accretion disc nearly edge-on, as the whole system rotated clockwise.[162][158]
The extreme gravitational lensing associated with black holes produces the illusion of a perspective that sees the accretion disc from above. In reality, most of the ring in the EHT image was created when the light emitted by the far side of the accretion disc bent around the black hole's gravity well and escaped, meaning that most of the possible perspectives on M87* can see the entire disc, even that directly behind the "shadow".
In 2015, the EHT detected magnetic fields just outside the event horizon of Sagittarius A* and even discerned some of their properties. The field lines that pass through the accretion disc were a complex mixture of ordered and tangled. Theoretical studies of black holes had predicted the existence of magnetic fields.[163][164]
In April 2023, an image of the shadow of the Messier 87 black hole and the related high-energy jet, viewed together for the first time, was presented.[165][166]
On 14 September 2015, theLIGOgravitational wave observatory made the first-ever successfuldirect observation of gravitational waves.[53][167]The signal was consistent with theoretical predictions for the gravitational waves produced by the merger of two black holes: one with about 36 solar masses, and the other around 29 solar masses.[53][168]This observation provides the most concrete evidence for the existence of black holes to date. For instance, the gravitational wave signal suggests that the separation of the two objects before the merger was just 350 km, or roughly four times the Schwarzschild radius corresponding to the inferred masses. The objects must therefore have been extremely compact, leaving black holes as the most plausible interpretation.[53]
More importantly, the signal observed by LIGO also included the start of the post-mergerringdown, the signal produced as the newly formed compact object settles down to a stationary state. Arguably, the ringdown is the most direct way of observing a black hole.[169]From the LIGO signal, it is possible to extract the frequency and damping time of the dominant mode of the ringdown. From these, it is possible to infer the mass and angular momentum of the final object, which match independent predictions from numerical simulations of the merger.[170]The frequency and decay time of the dominant mode are determined by the geometry of the photon sphere. Hence, observation of this mode confirms the presence of a photon sphere; however, it cannot exclude possible exotic alternatives to black holes that are compact enough to have a photon sphere.[169][171]
The observation also provides the first observational evidence for the existence of stellar-mass black hole binaries. Furthermore, it is the first observational evidence of stellar-mass black holes weighing 25 solar masses or more.[172]
Since then, many moregravitational wave eventshave been observed.[173]
Theproper motionsof stars near the centre of our own Milky Way provide strong observational evidence that these stars are orbiting a supermassive black hole.[174]Since 1995, astronomers have tracked the motions of 90 stars orbiting an invisible object coincident with the radio source Sagittarius A*. By fitting their motions toKeplerian orbits, the astronomers were able to infer, in 1998, that a2.6×106M☉object must be contained in a volume with a radius of 0.02light-yearsto cause the motions of those stars.[175]
Since then, one of the stars—calledS2—has completed a full orbit. From the orbital data, astronomers were able to refine the calculations of the mass to4.3×106M☉and a radius of less than 0.002 light-years for the object causing the orbital motion of those stars.[174]The upper limit on the object's size is still too large to test whether it is smaller than its Schwarzschild radius. Nevertheless, these observations strongly suggest that the central object is a supermassive black hole as there are no other plausible scenarios for confining so much invisible mass into such a small volume.[175]Additionally, there is some observational evidence that this object might possess an event horizon, a feature unique to black holes.[176]
Due toconservation of angular momentum,[178]gas falling into thegravitational wellcreated by a massive object will typically form a disk-like structure around the object. Artists' impressions such as the accompanying representation of a black hole with corona commonly depict the black hole as if it were a flat-space body hiding the part of the disk just behind it, but in reality gravitational lensing would greatly distort the image of the accretion disk.[179]
Within such a disk, friction would cause angular momentum to be transported outward, allowing matter to fall farther inward, thus releasing potential energy and increasing the temperature of the gas.[180]
When the accreting object is a neutron star or a black hole, the gas in the inner accretion disk orbits at very high speeds because of its proximity to thecompact object. The resulting friction is so significant that it heats the inner disk to temperatures at which it emits vast amounts of electromagnetic radiation (mainlyX-rays). These bright X-ray sources may be detected by telescopes. This process of accretion is one of the most efficient energy-producing processes known. Up to 40% of the rest mass of the accreted material can be emitted as radiation.[180]In nuclear fusion only about 0.7% of the rest mass will be emitted as energy. In many cases, accretion disks are accompanied byrelativistic jetsthat are emitted along the poles, which carry away much of the energy. The mechanism for the creation of these jets is currently not well understood, in part due to insufficient data.[181]
As such, many of the universe's more energetic phenomena have been attributed to the accretion of matter on black holes. In particular, active galactic nuclei andquasarsare believed to be the accretion disks of supermassive black holes.[182]Similarly, X-ray binaries are generally accepted to bebinary starsystems in which one of the two stars is a compact object accreting matter from its companion.[182]It has also been suggested that someultraluminous X-ray sourcesmay be the accretion disks of intermediate-mass black holes.[183]
Stars have been observed to get torn apart by tidal forces in the immediate vicinity of supermassive black holes in galaxy nuclei, in what is known as atidal disruption event (TDE). Some of the material from the disrupted star forms an accretion disk around the black hole, which emits observable electromagnetic radiation.
In November 2011 the first direct observation of a quasar accretion disk around a supermassive black hole was reported.[184][185]
X-ray binariesare binary star systems that emit a majority of their radiation in theX-raypart of the spectrum. These X-ray emissions are generally thought to result when one of the stars (compact object) accretes matter from another (regular) star. The presence of an ordinary star in such a system provides an opportunity for studying the central object and to determine if it might be a black hole.[182]
If such a system emits signals that can be directly traced back to the compact object, it cannot be a black hole. The absence of such a signal does, however, not exclude the possibility that the compact object is a neutron star. By studying the companion star it is often possible to obtain the orbital parameters of the system and to obtain an estimate for the mass of the compact object. If this is much larger than the Tolman–Oppenheimer–Volkoff limit (the maximum mass a star can have without collapsing) then the object cannot be a neutron star and is generally expected to be a black hole.[182]
The first strong candidate for a black hole,Cygnus X-1, was discovered in this way byCharles Thomas Bolton,[8]Louise Webster, andPaul Murdin[7]in 1972.[186][50]Some doubt remained, due to the uncertainties that result from the companion star being much heavier than the candidate black hole. Currently, better candidates for black holes are found in a class of X-ray binaries called soft X-ray transients. In this class of system, the companion star is of relatively low mass allowing for more accurate estimates of the black hole mass. These systems actively emit X-rays for only several months once every 10–50 years. During the period of low X-ray emission, called quiescence, the accretion disk is extremely faint, allowing detailed observation of the companion star during this period. One of the best such candidates isV404 Cygni.[182]
The X-ray emissions from accretion disks sometimes flicker at certain frequencies. These signals are calledquasi-periodic oscillationsand are thought to be caused by material moving along the inner edge of the accretion disk (the innermost stable circular orbit). As such their frequency is linked to the mass of the compact object. They can thus be used as an alternative way to determine the mass of candidate black holes.[187]
Astronomers use the term "active galaxy" to describe galaxies with unusual characteristics, such as unusualspectral lineemission and very strong radio emission. Theoretical and observational studies have shown that the activity in these active galactic nuclei (AGN) may be explained by the presence of supermassive black holes, which can be millions of times more massive than stellar ones. The models of these AGN consist of a central black hole that may be millions or billions of times more massive than theSun; a disk ofinterstellar gasand dust called an accretion disk; and twojetsperpendicular to the accretion disk.[189][190]
Although supermassive black holes are expected to be found in most AGN, only some galaxies' nuclei have been more carefully studied in attempts to both identify and measure the actual masses of the central supermassive black hole candidates. Some of the most notable galaxies with supermassive black hole candidates include theAndromeda Galaxy,M32,M87,NGC 3115,NGC 3377,NGC 4258,NGC 4889,NGC 1277,OJ 287,APM 08279+5255and theSombrero Galaxy.[191]
It is now widely accepted that the centre of nearly every galaxy, not just active ones, contains a supermassive black hole.[192]The close observational correlation between the mass of this hole and the velocity dispersion of the host galaxy'sbulge, known as theM–sigma relation, strongly suggests a connection between the formation of the black hole and that of the galaxy itself.[193]
Another way the black hole nature of an object may be tested is through observation of effects caused by a strong gravitational field in their vicinity. One such effect is gravitational lensing: The deformation of spacetime around a massive object causes light rays to be deflected, such as light passing through an opticlens. Observations have been made of weak gravitational lensing, in which light rays are deflected by only a fewarcseconds.Microlensingoccurs when the sources are unresolved and the observer sees a small brightening. The turn of the millennium saw the first 3 candidate detections of black holes in this way,[194][195]and in January 2022, astronomers reported the first confirmed detection of a microlensing event from an isolated black hole.[196]
Another possibility for observing gravitational lensing by a black hole would be to observe stars orbiting the black hole. There are several candidates for such an observation in orbit aroundSagittarius A*.[197]
The evidence for stellar black holes strongly relies on the existence of an upper limit for the mass of a neutron star. The size of this limit heavily depends on the assumptions made about the properties of dense matter. New exoticphases of mattercould push up this bound.[182]A phase of freequarksat high density might allow the existence of dense quark stars,[198]and somesupersymmetricmodels predict the existence ofQ stars.[199]Some extensions of thestandard modelposit the existence ofpreonsas fundamental building blocks of quarks andleptons, which could hypothetically formpreon stars.[200]These hypothetical models could potentially explain a number of observations of stellar black hole candidates. However, it can be shown from arguments in general relativity that any such object will have a maximum mass.[182]
Since the average density of a black hole inside its Schwarzschild radius is inversely proportional to the square of its mass, supermassive black holes are much less dense than stellar black holes. The average density of a 108M☉black hole is comparable to that of water.[182]Consequently, the physics of matter forming a supermassive black hole is much better understood and the possible alternative explanations for supermassive black hole observations are much more mundane. For example, a supermassive black hole could be modelled by a large cluster of very dark objects. However, such alternatives are typically not stable enough to explain the supermassive black hole candidates.[182]
The evidence for the existence of stellar and supermassive black holes implies that in order for black holes not to form, general relativity must fail as a theory of gravity, perhaps due to the onset ofquantum mechanicalcorrections. A much anticipated feature of a theory of quantum gravity is that it will not feature singularities or event horizons and thus black holes would not be real artefacts.[201]For example, in thefuzzballmodel[202]based onstring theory, the individual states of a black hole solution do not generally have an event horizon or singularity, but for a classical/semiclassical observer the statistical average of such states appears just as an ordinary black hole as deduced from general relativity.[203]
A few theoretical objects have been conjectured to match observations of astronomical black hole candidates identically or near-identically,[171]but which function via a different mechanism. These include thegravastar,[204]theblack star,[205]relatednestar[206]and thedark-energy star.[207]
In 1971, Hawking showed under general conditions[Note 5]that the total area of the event horizons of any collection of classical black holes can never decrease, even if they collide and merge.[208]This result, now known as thesecond law of black hole mechanics, is remarkably similar to thesecond law of thermodynamics, which states that the total entropy of an isolated system can never decrease. As with classical objects atabsolute zerotemperature, it was assumed that black holes had zero entropy. If this were the case, the second law of thermodynamics would be violated by entropy-laden matter entering a black hole, resulting in a decrease in the total entropy of the universe. Therefore, Bekenstein proposed that a black hole should have an entropy, and that it should be proportional to its horizon area.[209]
The link with the laws of thermodynamics was further strengthened by Hawking's discovery in 1974 that quantum field theory predicts that a black hole radiatesblackbody radiationat a constant temperature. This seemingly causes a violation of the second law of black hole mechanics, since the radiation will carry away energy from the black hole causing it to shrink. The radiation also carries away entropy, and it can be proven under general assumptions that the sum of the entropy of the matter surrounding a black hole and one quarter of the area of the horizon as measured in Planck units is in fact always increasing. This allows the formulation of thefirst law of black hole mechanicsas an analogue of thefirst law of thermodynamics, with the mass acting as energy, the surface gravity as temperature and the area as entropy.[209]
One puzzling feature is that the entropy of a black hole scales with its area rather than with its volume, since entropy is normally anextensive quantitythat scales linearly with the volume of the system. This odd property ledGerard 't HooftandLeonard Susskindto propose theholographic principle, which suggests that anything that happens in a volume of spacetime can be described by data on the boundary of that volume.[210]
Although general relativity can be used to perform a semiclassical calculation of black hole entropy, this situation is theoretically unsatisfying. Instatistical mechanics, entropy is understood as counting the number of microscopic configurations of a system that have the same macroscopic qualities, such as mass, charge, pressure, etc. Without a satisfactory theory of quantum gravity, one cannot perform such a computation for black holes. Some progress has been made in various approaches to quantum gravity. In 1995,Andrew StromingerandCumrun Vafashowed that counting the microstates of a specific supersymmetric black hole in string theory reproduced the Bekenstein–Hawking entropy.[211]Since then, similar results have been reported for different black holes both in string theory and in other approaches to quantum gravity likeloop quantum gravity.[212]
Because a black hole has only a few internal parameters, most of the information about the matter that went into forming the black hole is lost. Regardless of the type of matter which goes into a black hole, it appears that only information concerning the total mass, charge, and angular momentum are conserved. As long as black holes were thought to persist forever this information loss is not that problematic, as the information can be thought of as existing inside the black hole, inaccessible from the outside, but represented on the event horizon in accordance with the holographic principle. However, black holes slowly evaporate by emitting Hawking radiation. This radiation does not appear to carry any additional information about the matter that formed the black hole, meaning that this information appears to be gone forever.[213]
The question whether information is truly lost in black holes (theblack hole information paradox) has divided the theoretical physics community. In quantum mechanics, loss of information corresponds to the violation of a property calledunitarity, and it has been argued that loss of unitarity would also imply violation of conservation of energy,[214]though this has also been disputed.[215]Over recent years evidence has been building that indeed information and unitarity are preserved in a full quantum gravitational treatment of the problem.[216]
One attempt to resolve the black hole information paradox is known asblack hole complementarity. In 2012, the "firewall paradox" was introduced with the goal of demonstrating that black hole complementarity fails to solve the information paradox. According toquantum field theory in curved spacetime, asingle emissionof Hawking radiation involves two mutuallyentangledparticles. The outgoing particle escapes and is emitted as a quantum of Hawking radiation; the infalling particle is swallowed by the black hole. Assume a black hole formed a finite time in the past and will fully evaporate away in some finite time in the future. Then, it will emit only a finite amount of information encoded within its Hawking radiation. According to research by physicists likeDon Page[217][218]and Leonard Susskind, there will eventually be a time by which an outgoing particle must be entangled with all the Hawking radiation the black hole has previously emitted.
This seemingly creates a paradox: a principle called "monogamy of entanglement" requires that, like any quantum system, the outgoing particle cannot be fully entangled with two other systems at the same time; yet here the outgoing particle appears to be entangled both with the infalling particle and, independently, with past Hawking radiation.[219]In order to resolve this contradiction, physicists may eventually be forced to give up one of three time-tested principles: Einstein's equivalence principle, unitarity, or local quantum field theory. One possible solution, which violates the equivalence principle, is that a "firewall" destroys incoming particles at the event horizon.[220]In general, which—if any—of these assumptions should be abandoned remains a topic of debate.[215]
Christopher Nolan's2014 science fiction epicInterstellarfeatures a black hole known as Gargantua, which is the central object of a planetary system in a distant galaxy. Humanity accessed this system via awormholein the outersolar system, nearSaturn.
Present-dayclimate changeincludes bothglobal warming—the ongoing increase inglobal average temperature—and its wider effects on Earth’sclimate system.Climate change in a broader sensealso includes previous long-term changes to Earth'sclimate. The current rise in global temperatures isdriven by human activities, especiallyfossil fuelburning since theIndustrial Revolution.[3][4]Fossil fuel use,deforestation, and someagriculturalandindustrialpractices releasegreenhouse gases.[5]These gasesabsorb some of the heatthat the Earthradiatesafter it warms fromsunlight, warming the lower atmosphere.Carbon dioxide, the primary gas driving global warming,has increased in concentration by about 50%since the pre-industrial era to levels not seen for millions of years.[6]
Climate change has an increasingly largeimpact on the environment.Deserts are expanding, whileheat wavesandwildfiresare becoming more common.[7]Amplified warming in the Arctichas contributed to thawingpermafrost,retreat of glaciersandsea ice decline.[8]Higher temperatures are also causingmore intense storms, droughts, and otherweather extremes.[9]Rapid environmental change inmountains,coral reefs, andthe Arcticis forcing many species to relocate orbecome extinct.[10]Even if efforts to minimize future warming are successful, some effects will continue for centuries. These includeocean heating,ocean acidificationandsea level rise.[11]
Climate changethreatens peoplewith increasedflooding, extreme heat, increasedfoodandwaterscarcity, more disease, andeconomic loss.[12]Human migrationand conflict can also be a result.[13]TheWorld Health Organizationcalls climate change one of the biggest threats toglobal healthin the 21st century.[14]Societies and ecosystems will experience more severe risks withoutaction to limit warming.[15]Adapting to climate changethrough efforts likeflood controlmeasures ordrought-resistant cropspartially reduces climate change risks, although some limits toadaptationhave already been reached.[16]Poorer communities are responsible fora small share of global emissions, yet have the least ability to adapt and are mostvulnerable to climate change.[17][18]
Many climate change impacts have been observed in the first decades of the 21st century, with 2024 the warmest on record at +1.60 °C (2.88 °F) since regular tracking began in 1850.[20][21]Additional warming will increase these impacts and can triggertipping points, such as melting all of theGreenland ice sheet.[22]Under the 2015Paris Agreement, nations collectively agreed to keep warming "well under 2 °C". However, with pledges made under the Agreement, global warming would still reach about 2.8 °C (5.0 °F) by the end of the century.[23]Limiting warming to 1.5 °C would require halving emissions by 2030 and achievingnet-zeroemissions by 2050.[24][25]
There is widespread support for climate action worldwide.[26][27]Fossil fuel use can be phased outbyconserving energyand switching to energy sources that do not produce significant carbon pollution. These energy sources includewind,solar,hydro, andnuclear power.[28]Cleanly generated electricity can replace fossil fuels forpowering transportation,heating buildings, and running industrial processes.[29]Carbon can also beremoved from the atmosphere, for instance byincreasing forest coverand farming with methods thatcapture carbon in soil.[30]
Before the 1980s it was unclear whether the warming effect ofincreased greenhouse gaseswas stronger than thecooling effect of airborne particulatesinair pollution. Scientists used the terminadvertent climate modificationto refer to human impacts on the climate at this time.[31]In the 1980s, the termsglobal warmingandclimate changebecame more common, often being used interchangeably.[32][33][34]Scientifically,global warmingrefers only to increased surface warming, whileclimate changedescribes both global warming and its effects on Earth'sclimate system, such as precipitation changes.[31]
Climate changecan also be used more broadly to includechanges to the climatethat have happened throughout Earth's history.[35]Global warming—used as early as 1975[36]—became the more popular term afterNASAclimate scientistJames Hansenused it in his 1988 testimony in theU.S. Senate.[37]Since the 2000s,climate changehas increased usage.[38]Various scientists, politicians and media may use the termsclimate crisisorclimate emergencyto talk about climate change, and may use the termglobal heatinginstead ofglobal warming.[39][40]
Over the last few million years the climate cycled throughice ages. One of the hotter periods was theLast Interglacial, around 125,000 years ago, where temperatures were between 0.5 °C and 1.5 °C warmer than before the start of global warming.[43]This period saw sea levels 5 to 10 metres higher than today. The mostrecent glacial maximum20,000 years ago was some 5–7 °C colder. This period has sea levels that were over 125 metres (410 ft) lower than today.[44]
Temperatures stabilized in the current interglacial period beginning11,700 years ago.[45]This period also saw the start of agriculture.[46]Historical patterns of warming and cooling, like theMedieval Warm Periodand theLittle Ice Age, did not occur at the same time across different regions. Temperatures may have reached as high as those of the late 20th century in a limited set of regions.[47][48]Climate information for that period comes fromclimate proxies, such as trees andice cores.[49][50]
Around 1850thermometerrecords began to provide global coverage.[53]Between the 18th century and 1970 there was little net warming, as the warming impact of greenhouse gas emissions was offset by cooling fromsulfur dioxideemissions. Sulfur dioxide causesacid rain, but it also producessulfateaerosols in the atmosphere, which reflect sunlight and causeglobal dimming. After 1970, the increasing accumulation of greenhouse gases and controls on sulfur pollution led to a marked increase in temperature.[54][55][56]
Ongoing changes in climate have had no precedent for several thousand years.[57]Multiple independent datasets all show worldwide increases in surface temperature,[58]at a rate of around 0.2 °C per decade.[59]The 2014–2023 decade warmed to an average 1.19 °C [1.06–1.30 °C] compared to the pre-industrial baseline (1850–1900).[60]Not every single year was warmer than the last: internalclimate variabilityprocesses can make any year 0.2 °C warmer or colder than the average.[61]From 1998 to 2013, negative phases of two such processes,Pacific Decadal Oscillation (PDO)[62]andAtlantic Multidecadal Oscillation (AMO)[63]caused a short slower period of warming called the "global warming hiatus".[64]After the "hiatus", the opposite occurred, with 2024 well above the recent average at more than +1.5 °C.[65]This is why the temperature change is defined in terms of a 20-year average, which reduces the noise of hot and cold years and decadal climate patterns, and detects the long-term signal.[66]: 5[67]
A wide range of other observations reinforce the evidence of warming.[68][69]The upper atmosphere is cooling, becausegreenhouse gasesare trapping heat near the Earth's surface, and so less heat is radiating into space.[70]Warming reduces average snow cover andforces the retreat of glaciers. At the same time, warming also causesgreater evaporation from the oceans, leading to moreatmospheric humidity, more and heavierprecipitation.[71][72]Plants arefloweringearlier in spring, and thousands of animal species have been permanently moving to cooler areas.[73]
Different regions of the worldwarm at different rates. The pattern is independent of where greenhouse gases are emitted, because the gases persist long enough to diffuse across the planet. Since the pre-industrial period, the average surface temperature over land regions has increased almost twice as fast as the global average surface temperature.[74]This is because oceans lose more heat byevaporationandoceans can store a lot of heat.[75]The thermal energy in the global climate system has grown with only brief pauses since at least 1970, and over 90% of this extra energy has beenstored in the ocean.[76][77]The rest has heated theatmosphere, melted ice, and warmed the continents.[78]
TheNorthern Hemisphereand theNorth Polehave warmed much faster than theSouth PoleandSouthern Hemisphere. The Northern Hemisphere not only has much more land, but also more seasonal snow cover andsea ice. As these surfaces flip from reflecting a lot of light to being dark after the ice has melted, they startabsorbing more heat.[79]Localblack carbondeposits on snow and ice also contribute to Arctic warming.[80]Arctic surface temperatures are increasingbetween three and four times fasterthan in the rest of the world.[81][82][83]Melting ofice sheetsnear the poles weakens both theAtlanticand theAntarcticlimb ofthermohaline circulation, which further changes the distribution of heat andprecipitationaround the globe.[84][85][86][87]
TheWorld Meteorological Organizationestimates there is almost a 50% chance of the five-year average global temperature exceeding +1.5 °C between 2024 and 2028.[90]The IPCC expects the 20-year average to exceed +1.5 °C in the early 2030s.[91]
TheIPCC Sixth Assessment Report(2021) included projections that by 2100 global warming is very likely to reach 1.0–1.8 °C under ascenario with very low emissions of greenhouse gases, 2.1–3.5 °C under anintermediate emissions scenario,
or 3.3–5.7 °C undera very high emissions scenario.[92]The warming will continue past 2100 in the intermediate and high emission scenarios,[93][94]with future projections of global surface temperatures by year 2300 being similar to millions of years ago.[95]
The remainingcarbon budgetfor staying beneath certain temperature increases is determined by modelling the carbon cycle andclimate sensitivityto greenhouse gases.[96]According toUNEP, global warming can be kept below 1.5 °C with a 50% chance if emissions after 2023 do not exceed 200 gigatonnes of CO2. This corresponds to around 4 years of current emissions. To stay under 2.0 °C, the carbon budget is 900 gigatonnes of CO2, or 16 years of current emissions.[97]
The climate system experiences various cycles on its own which can last for years, decades or even centuries. For example,El Niñoevents cause short-term spikes in surface temperature whileLa Niñaevents cause short term cooling.[98]Their relative frequency can affect global temperature trends on a decadal timescale.[99]Other changes are caused by animbalance of energyfromexternal forcings.[100]Examples of these include changes in the concentrations ofgreenhouse gases,solar luminosity,volcaniceruptions, andvariations in the Earth's orbitaround the Sun.[101]
To determine the human contribution to climate change, unique "fingerprints" for all potential causes are developed and compared with both observed patterns and known internalclimate variability.[102]For example, solar forcing—whose fingerprint involves warming the entire atmosphere—is ruled out because only the lower atmosphere has warmed.[103]Atmospheric aerosols produce a smaller, cooling effect. Other drivers, such as changes inalbedo, are less impactful.[104]
Greenhouse gases are transparent tosunlight, and thus allow it to pass through the atmosphere to heat the Earth's surface. The Earthradiates it as heat, and greenhouse gases absorb a portion of it. This absorption slows the rate at which heat escapes into space, trapping heat near the Earth's surface and warming it over time.[105]
Whilewater vapour(≈50%) and clouds (≈25%) are the biggest contributors to the greenhouse effect, they primarily change as a function of temperature and are therefore mostly considered to befeedbacksthat changeclimate sensitivity. On the other hand, concentrations of gases such as CO2(≈20%),tropospheric ozone,[106]CFCsandnitrous oxideare added or removed independently from temperature, and are therefore considered to beexternal forcingsthat change global temperatures.[107]
Before theIndustrial Revolution, naturally-occurring amounts of greenhouse gases caused the air near the surface to be about 33 °C warmer than it would have been in their absence.[108][109]Human activity since the Industrial Revolution, mainly extracting and burning fossil fuels (coal,oil, andnatural gas),[110]has increased the amount of greenhouse gases in the atmosphere. In 2022, theconcentrations of CO2and methane had increased by about 50% and 164%, respectively, since 1750.[111]These CO2levels are higher than they have been at any time during the last 14 million years.[112]Concentrations of methaneare far higher than they were over the last 800,000 years.[113]
Global human-caused greenhouse gas emissions in 2019 wereequivalent to59 billion tonnes of CO2. Of these emissions, 75% was CO2, 18% wasmethane, 4% was nitrous oxide, and 2% wasfluorinated gases.[114]CO2emissions primarily come from burning fossil fuels to provide energy fortransport, manufacturing,heating, and electricity.[5]Additional CO2emissions come fromdeforestationandindustrial processes, which include the CO2released by the chemical reactions formaking cement,steel,aluminum, andfertilizer.[115][116][117][118]Methane emissionscome from livestock, manure,rice cultivation, landfills, wastewater, andcoal mining, as well asoil and gas extraction.[119][120]Nitrous oxide emissions largely come from the microbial decomposition offertilizer.[121][122]
While methane only lasts in the atmosphere for an average of 12 years,[123]CO2lasts much longer. The Earth's surface absorbs CO2as part of thecarbon cycle. While plants on land and in the ocean absorb most excess emissions of CO2every year, that CO2is returned to the atmosphere when biological matter is digested, burns, or decays.[124]Land-surfacecarbon sinkprocesses, such ascarbon fixationin the soil and photosynthesis, remove about 29% of annual global CO2emissions.[125]The ocean has absorbed 20 to 30% of emitted CO2over the last two decades.[126]CO2is only removed from the atmosphere for the long term when it is stored in the Earth's crust, which is a process that can take millions of years to complete.[124]
Around 30% of Earth's land area is largely unusable for humans (glaciers,deserts, etc.), 26% isforests, 10% isshrublandand 34% isagricultural land.[128]Deforestationis the mainland use changecontributor to global warming,[129]as the destroyed trees release CO2, and are not replaced by new trees, removing thatcarbon sink.[130]Between 2001 and 2018, 27% of deforestation was from permanent clearing to enableagricultural expansionfor crops and livestock. Another 24% has been lost to temporary clearing under theshifting cultivationagricultural systems. 26% was due tologgingfor wood and derived products, andwildfireshave accounted for the remaining 23%.[131]Some forests have not been fully cleared, but were already degraded by these impacts. Restoring these forests also recovers their potential as a carbon sink.[132]
Local vegetation cover impacts how much of the sunlight gets reflected back into space (albedo), and how muchheat is lost by evaporation. For instance, the change from a darkforestto grassland makes the surface lighter, causing it to reflect more sunlight. Deforestation can also modify the release of chemical compounds that influence clouds, and by changing wind patterns.[133]In tropic and temperate areas the net effect is to produce significant warming, and forest restoration can make local temperatures cooler.[132]At latitudes closer to the poles, there is a cooling effect as forest is replaced by snow-covered (and more reflective) plains.[133]Globally, these increases in surface albedo have been the dominant direct influence on temperature from land use change. Thus, land use change to date is estimated to have a slight cooling effect.[134]
Air pollution, in the form ofaerosols, affects the climateon a large scale.[135]Aerosols scatter and absorb solar radiation. From 1961 to 1990, a gradual reduction in the amount ofsunlight reaching the Earth's surfacewas observed. This phenomenon is popularly known asglobal dimming,[136]and is primarily attributed tosulfateaerosols produced by the combustion of fossil fuels with heavy sulfur concentrations likecoalandbunker fuel.[56]Smaller contributions come fromblack carbon(from combustion of fossil fuels and biomass), and from dust.[137][138][139]Globally, aerosols have been declining since 1990 due to pollution controls, meaning that they no longer mask greenhouse gas warming as much.[140][56]
Aerosols also have indirect effects on theEarth's energy budget. Sulfate aerosols act ascloud condensation nucleiand lead to clouds that have more and smaller cloud droplets. These clouds reflect solar radiation more efficiently than clouds with fewer and larger droplets.[141]They also reduce thegrowth of raindrops, which makes clouds more reflective to incoming sunlight.[142]Indirect effects of aerosols are the largest uncertainty inradiative forcing.[143]
While aerosols typically limit global warming by reflecting sunlight,black carboninsootthat falls on snow or ice can contribute to global warming. Not only does this increase the absorption of sunlight, it also increases melting and sea-level rise.[144]Limiting new black carbon deposits in the Arctic could reduce global warming by 0.2 °C by 2050.[145]The effect of decreasing sulfur content of fuel oil for ships since 2020[146]is estimated to cause an additional 0.05 °C increase in global mean temperature by 2050.[147]
As the Sun is the Earth's primary energy source, changes in incoming sunlight directly affect theclimate system.[143]Solar irradiancehas been measured directly bysatellites,[150]and indirect measurements are available from the early 1600s onwards.[143]Since 1880, there has been no upward trend in the amount of the Sun's energy reaching the Earth, in contrast to the warming of the lower atmosphere (thetroposphere).[151]The upper atmosphere (thestratosphere) would also be warming if the Sun was sending more energy to Earth, but instead, it has been cooling.[103]This is consistent with greenhouse gases preventing heat from leaving the Earth's atmosphere.[152]
Explosive volcanic eruptionscan release gases, dust and ash that partially block sunlight and reduce temperatures, or they can send water vapour into the atmosphere, which adds to greenhouse gases and increases temperatures.[153]These impacts on temperature only last for several years, because both water vapour and volcanic material have low persistence in the atmosphere.[154]volcanic CO2emissionsare more persistent, but they are equivalent to less than 1% of current human-caused CO2emissions.[155]Volcanic activity still represents the single largest natural impact (forcing) on temperature in the industrial era. Yet, like the other natural forcings, it has had negligible impacts on global temperature trends since the Industrial Revolution.[154]
The climate system's response to an initial forcing is shaped by feedbacks, which either amplify or dampen the change.Self-reinforcingorpositivefeedbacks increase the response, whilebalancingornegativefeedbacks reduce it.[157]The main reinforcing feedbacks are thewater-vapour feedback, theice–albedo feedback, and the net effect of clouds.[158][159]The primary balancing mechanism isradiative cooling, as Earth's surface gives off moreheatto space in response to rising temperature.[160]In addition to temperature feedbacks, there are feedbacks in the carbon cycle, such as the fertilizing effect of CO2on plant growth.[161]Feedbacks are expected to trend in a positive direction as greenhouse gas emissions continue, raising climate sensitivity.[162]
These feedback processes alter the pace of global warming. For instance, warmer aircan hold more moisturein the form ofwater vapour, which is itself a potent greenhouse gas.[158]Warmer air can also make clouds higher and thinner, and therefore more insulating, increasing climate warming.[163]The reduction of snow cover and sea ice in the Arctic is another major feedback, this reduces the reflectivity of the Earth's surface in the region andaccelerates Arctic warming.[164][165]This additional warming also contributes topermafrostthawing, which releases methane and CO2into the atmosphere.[166]
Around half of human-caused CO2emissions have been absorbed by land plants and by the oceans.[167]This fraction is not static and if future CO2emissions decrease, the Earth will be able to absorb up to around 70%. If they increase substantially, it'll still absorb more carbon than now, but the overall fraction will decrease to below 40%.[168]This is because climate change increases droughts and heat waves that eventually inhibit plant growth on land, and soils will release more carbon from dead plantswhen they are warmer.[169][170]The rate at which oceans absorb atmospheric carbon will be lowered as they become more acidic and experience changes inthermohaline circulationandphytoplanktondistribution.[171][172][85]Uncertainty over feedbacks, particularly cloud cover,[173]is the major reason why different climate models project different magnitudes of warming for a given amount of emissions.[174]
Aclimate modelis a representation of the physical, chemical and biological processes that affect the climate system.[175]Models include natural processes like changes in the Earth's orbit, historical changes in the Sun's activity, and volcanic forcing.[176]Models are used to estimate the degree of warming future emissions will cause when accounting for thestrength of climate feedbacks.[177][178]Models also predict the circulation of the oceans, the annual cycle of the seasons, and the flows of carbon between the land surface and the atmosphere.[179]
The physical realism of models is tested by examining their ability to simulate current or past climates.[180]Past models have underestimated the rate ofArctic shrinkage[181]and underestimated the rate of precipitation increase.[182]Sea level rise since 1990 was underestimated in older models, but more recent models agree well with observations.[183]The 2017 United States-publishedNational Climate Assessmentnotes that "climate models may still be underestimating or missing relevant feedback processes".[184]Additionally, climate models may be unable to adequately predict short-term regional climatic shifts.[185]
Asubset of climate modelsadd societal factors to a physical climate model. These models simulate how population,economic growth, and energy use affect—and interact with—the physical climate. With this information, these models can produce scenarios of future greenhouse gas emissions. This is then used as input for physical climate models and carbon cycle models to predict how atmospheric concentrations of greenhouse gases might change.[186][187]Depending on thesocioeconomic scenarioand the mitigation scenario, models produce atmospheric CO2concentrations that range widely between 380 and 1400 ppm.[188]
The environmental effects of climate change are broad and far-reaching,affecting oceans, ice, and weather. Changes may occur gradually or rapidly. Evidence for these effects comes from studying climate change in the past, from modelling, and from modern observations.[189]Since the 1950s,droughtsand heat waves have appeared simultaneously with increasing frequency.[190]Extremely wet or dry events within themonsoonperiod have increased in India and East Asia.[191]Monsoonal precipitation over the Northern Hemisphere has increased since 1980.[192]The rainfall rate and intensity ofhurricanes and typhoons is likely increasing,[193]and the geographic range likely expanding poleward in response to climate warming.[194]Frequency of tropical cyclones has not increased as a result of climate change.[195]
Global sea level is rising as a consequence ofthermal expansionandthe melting of glaciersandice sheets. Sea level rise has increased over time, reaching 4.8 cm per decade between 2014 and 2023.[197]Over the 21st century, the IPCC projects 32–62 cm of sea level rise under a low emission scenario, 44–76 cm under an intermediate one and 65–101 cm under a very high emission scenario.[198]Marine ice sheet instabilityprocesses in Antarctica may add substantially to these values,[199]including the possibility of a 2-meter sea level rise by 2100 under high emissions.[200]
Climate change has led to decades ofshrinking and thinning of the Arctic sea ice.[201]While ice-free summers are expected to be rare at 1.5 °C degrees of warming, they are set to occur once every three to ten years at a warming level of 2 °C.[202]Higher atmospheric CO2concentrations cause more CO2to dissolve in the oceans, which ismaking them more acidic.[203]Because oxygen is less soluble in warmer water,[204]its concentrations in the oceanare decreasing, anddead zonesare expanding.[205]
Greater degrees of global warming increase the risk of passing through 'tipping points'—thresholds beyond which certain major impacts can no longer be avoided even if temperatures return to their previous state.[208][209]For instance, theGreenland ice sheetis already melting, but if global warming reaches levels between 1.7 °C and 2.3 °C, its melting will continue until it fully disappears. If the warming is later reduced to 1.5 °C or less, it will still lose a lot more ice than if the warming was never allowed to reach the threshold in the first place.[210]While the ice sheets would melt over millennia, other tipping points would occur faster and give societies less time to respond. The collapse of majorocean currentslike theAtlantic meridional overturning circulation(AMOC), and irreversible damage to key ecosystems like theAmazon rainforestandcoral reefscan unfold in a matter of decades.[207]Thecollapse of the AMOCwould be a severe climate catastrophe, resulting in a cooling of the Northern Hemisphere.[211]
The long-termeffects of climate change on oceansinclude further ice melt,ocean warming, sea level rise, ocean acidification and ocean deoxygenation.[212]The timescale of long-term impacts are centuries to millennia due to CO2's long atmospheric lifetime.[213]The result is an estimated total sea level rise of 2.3 metres per degree Celsius (4.2 ft/°F) after 2000 years.[214]Oceanic CO2uptake is slow enough that ocean acidification will also continue for hundreds to thousands of years.[215]Deep oceans (below 2,000 metres (6,600 ft)) are also already committed to losing over 10% of their dissolved oxygen by the warming which occurred to date.[216]Further, theWest Antarctic ice sheetappears committed to practically irreversible melting, which would increase the sea levels by at least 3.3 m (10 ft 10 in) over approximately 2000 years.[207][217][218]
Recent warming has driven many terrestrial and freshwater species poleward and towards higheraltitudes.[219]For instance, the range of hundreds of North Americanbirdshas shifted northward at an average rate of 1.5 km/year over the past 55 years.[220]Higher atmospheric CO2levels and an extended growing season have resulted in global greening. However, heatwaves and drought have reducedecosystemproductivity in some regions. The future balance of these opposing effects is unclear.[221]A related phenomenon driven by climate change iswoody plant encroachment, affecting up to 500 million hectares globally.[222]Climate change has contributed to the expansion of drier climate zones, such as theexpansion of desertsin thesubtropics.[223]The size and speed of global warming is makingabrupt changes in ecosystemsmore likely.[224]Overall, it is expected that climate change will result in theextinctionof many species.[225]
The oceans have heated more slowly than the land, but plants and animals in the ocean have migrated towards the colder poles faster than species on land.[226]Just as on land,heat waves in the oceanoccur more frequently due to climate change, harming a wide range of organisms such as corals,kelp, andseabirds.[227]Ocean acidification makes it harder formarine calcifying organismssuch asmussels,barnaclesand corals toproduce shells and skeletons; and heatwaves havebleached coral reefs.[228]Harmful algal bloomsenhanced by climate change andeutrophicationlower oxygen levels, disruptfood websand cause great loss of marine life.[229]Coastal ecosystems are under particular stress. Almost half of global wetlands have disappeared due to climate change and other human impacts.[230]Plants have come under increased stress from damage by insects.[231]
The effects of climate change are impacting humans everywhere in the world.[237]Impacts can be observed on all continents and ocean regions,[238]with low-latitude,less developed areasfacing the greatest risk.[239]Continued warming has potentially "severe, pervasive and irreversible impacts" for people and ecosystems.[240]The risks are unevenly distributed, but are generally greater for disadvantaged people in developing and developed countries.[241]
TheWorld Health Organizationcalls climate change one of the biggest threats to global health in the 21st century.[14]Scientists have warned about the irreversible harms it poses.[242]Extreme weatherevents affect public health, andfoodandwater security.[243][244][245]Temperature extremeslead to increased illness and death.[243][244]Climate change increases the intensity and frequency of extreme weather events.[244][245]It can affect transmission ofinfectious diseases, such asdengue feverandmalaria.[242][243]According to theWorld Economic Forum, 14.5 million more deaths are expected due to climate change by 2050.[246]30% of the global population currently live in areas where extreme heat and humidity are already associated with excess deaths.[247][248]By 2100, 50% to 75% of the global population would live in such areas.[247][249]
While totalcrop yieldshave been increasing in the past 50 years due to agricultural improvements,climate change has already decreased the rate of yield growth.[245]Fisheries have been negatively affectedin multiple regions.[245]Whileagricultural productivityhas been positively affected in some highlatitudeareas, mid- and low-latitude areas have been negatively affected.[245]According to the World Economic Forum, an increase indroughtin certain regions could cause 3.2 million deaths frommalnutritionby 2050 andstuntingin children.[250]With 2 °C warming, globallivestockheadcounts could decline by 7–10% by 2050, as less animal feed will be available.[251]If the emissions continue to increase for the rest of century, then over 9 million climate-related deaths would occur annually by 2100.[252]
Economic damages due to climate change may be severe and there is a chance of disastrous consequences.[253]Severe impacts are expected in South-East Asia andsub-Saharan Africa, where most of the local inhabitants are dependent upon natural and agricultural resources.[254][255]Heat stresscan prevent outdoor labourers from working. If warming reaches 4 °C then labour capacity in those regions could be reduced by 30 to 50%.[256]TheWorld Bankestimates that between 2016 and 2030, climate change could drive over 120 million people into extreme poverty without adaptation.[257]
Inequalities based on wealth and social status have worsened due to climate change.[258]Major difficulties in mitigating, adapting to, and recovering from climate shocks are faced by marginalized people who have less control over resources.[259][254]Indigenous people, who are subsistent on their land and ecosystems, will face endangerment to their wellness and lifestyles due to climate change.[260]An expert elicitation concluded that the role of climate change inarmed conflicthas been small compared to factors such as socio-economic inequality and state capabilities.[261]
While women are not inherently more at risk from climate change and shocks, limits on women's resources and discriminatory gender norms constrain their adaptive capacity and resilience.[262]For example, women's work burdens, including hours worked in agriculture, tend to decline less than men's during climate shocks such as heat stress.[262]
Low-lying islands and coastal communities are threatened by sea level rise, which makesurban floodingmore common. Sometimes, land is permanently lost to the sea.[263]This could lead tostatelessnessfor people in island nations, such as theMaldivesandTuvalu.[264]In some regions, the rise in temperature and humidity may be too severe for humans to adapt to.[265]With worst-case climate change, models project that almost one-third of humanity might live in Sahara-like uninhabitable and extremely hot climates.[266]
These factors can driveclimateorenvironmental migration, within and between countries.[267]More people are expected to be displaced because of sea level rise, extreme weather and conflict from increased competition over natural resources. Climate change may also increase vulnerability, leading to "trapped populations" who are not able to move due to a lack of resources.[268]
Climate change can be mitigated by reducing the rate at which greenhouse gases are emitted into the atmosphere, and by increasing the rate at which carbon dioxide is removed from the atmosphere.[274]To limit global warming to less than 1.5 °C global greenhouse gas emissions needs to benet-zeroby 2050, or by 2070 with a 2 °C target.[275]This requires far-reaching, systemic changes on an unprecedented scale in energy, land, cities, transport, buildings, and industry.[276]
TheUnited Nations Environment Programmeestimates that countries need to triple theirpledges under the Paris Agreementwithin the next decade to limit global warming to 2 °C. An even greater level of reduction is required to meet the 1.5 °C goal.[277]With pledges made under the Paris Agreement as of 2024, there would be a 66% chance that global warming is kept under 2.8 °C by the end of the century (range: 1.9–3.7 °C, depending on exact implementation and technological progress). When only considering current policies, this raises to 3.1 °C.[278]Globally, limiting warming to 2 °C may result in higher economic benefits than economic costs.[279]
Although there is no single pathway to limit global warming to 1.5 or 2 °C,[280]most scenarios and strategies see a major increase in the use of renewable energy in combination with increased energy efficiency measures to generate the needed greenhouse gas reductions.[281]To reduce pressures on ecosystems and enhance their carbon sequestration capabilities, changes would also be necessary in agriculture and forestry,[282]such as preventingdeforestationand restoring natural ecosystems byreforestation.[283]
Other approaches to mitigating climate change have a higher level of risk. Scenarios that limit global warming to 1.5 °C typically project the large-scale use ofcarbon dioxide removal methodsover the 21st century.[284]There are concerns, though, about over-reliance on these technologies, and environmental impacts.[285]
Solar radiation modification(SRM) is a proposal for reducing global warming by reflecting some sunlight away from Earth and back into space. Because it does not reduce greenhouse gas concentrations, it would not address ocean acidification[286]and is not considered mitigation.[287]SRM should be considered only as a supplement to mitigation, not a replacement for it,[288]due to risks such as rapid warming if it were abruptly stopped and not restarted.[289]The most-studied approach isstratospheric aerosol injection.[290]SRM could reduce global warming and some of its impacts, though imperfectly.[291]It poses environmental risks, such as changes to rainfall patterns,[292]as well as political challenges, such as who would decide whether to use it.[293]
Renewable energy is key to limiting climate change.[295]For decades, fossil fuels have accounted for roughly 80% of the world's energy use.[296]The remaining share has been split between nuclear power and renewables (includinghydropower,bioenergy, wind and solar power andgeothermal energy).[297]Fossil fuel use is expected to peak in absolute terms prior to 2030 and then to decline, with coal use experiencing the sharpest reductions.[298]Renewables represented 86% of all new electricity generation installed in 2023.[299]Other forms of clean energy, such as nuclear and hydropower, currently have a larger share of the energy supply. However, their future growth forecasts appear limited in comparison.[300]
Whilesolar panelsand onshore wind are now among the cheapest forms of adding new power generation capacity in many locations,[301]green energy policies are needed to achieve a rapid transition from fossil fuels to renewables.[302]To achieve carbon neutrality by 2050, renewable energy would become the dominant form of electricity generation, rising to 85% or more by 2050 in some scenarios. Investment in coal would be eliminated and coal use nearly phased out by 2050.[303][304]
Electricity generated from renewable sources would also need to become the main energy source for heating and transport.[305]Transport can switch away frominternal combustion enginevehicles and towardselectric vehicles, public transit, andactive transport(cycling and walking).[306][307]For shipping and flying, low-carbon fuels would reduce emissions.[306]Heating could be increasingly decarbonized with technologies likeheat pumps.[308]
There are obstacles to the continued rapid growth of clean energy, including renewables.[309]Wind and solar produce energyintermittently and with seasonal variability. Traditionally,hydro dams with reservoirsand fossil fuel power plants have been used when variable energy production is low. Going forward,battery storagecan be expanded,energy demand and supplycan be matched, and long-distancetransmissioncan smooth variability of renewable outputs.[295]Bioenergy is often not carbon-neutral and may have negative consequences for food security.[310]The growth of nuclear power is constrained by controversy aroundradioactive waste,nuclear weapon proliferation, andaccidents.[311][312]Hydropower growth is limited by the fact that the best sites have been developed, and new projects are confronting increased social and environmental concerns.[313]
Low-carbon energyimproves human health by minimizing climate change as well as reducing air pollution deaths,[314]which were estimated at 7 million annually in 2016.[315]Meeting the Paris Agreement goals that limit warming to a 2 °C increase could save about a million of those lives per year by 2050, whereas limiting global warming to 1.5 °C could save millions and simultaneously increaseenergy securityand reduce poverty.[316]Improving air quality also has economic benefits which may be larger than mitigation costs.[317]
Reducing energy demand is another major aspect of reducing emissions.[318]If less energy is needed, there is more flexibility for clean energy development. It also makes it easier to manage the electricity grid, and minimizescarbon-intensiveinfrastructure development.[319]Major increases in energy efficiency investment will be required to achieve climate goals, comparable to the level of investment in renewable energy.[320]SeveralCOVID-19related changes in energy use patterns, energy efficiency investments, and funding have made forecasts for this decade more difficult and uncertain.[321]
Strategies to reduce energy demand vary by sector. In the transport sector, passengers and freight can switch to more efficient travel modes, such as buses and trains, or use electric vehicles.[322]Industrial strategies to reduce energy demand include improving heating systems and motors, designing less energy-intensive products, and increasing product lifetimes.[323]In the building sector the focus is on better design of new buildings, and higher levels of energy efficiency in retrofitting.[324]The use of technologies like heat pumps can also increase building energy efficiency.[325]
Agriculture and forestry face a triple challenge of limiting greenhouse gas emissions, preventing the further conversion of forests to agricultural land, and meeting increases in world food demand.[326]A set of actions could reduce agriculture and forestry-based emissions by two-thirds from 2010 levels. These include reducing growth in demand for food and other agricultural products, increasing land productivity, protecting and restoring forests, and reducing greenhouse gas emissions from agricultural production.[327]
On the demand side, a key component of reducing emissions is shifting people towardsplant-based diets.[328]Eliminating the production of livestock formeat and dairywould eliminate about 3/4ths of all emissions from agriculture and other land use.[329]Livestock also occupy 37% of ice-free land area on Earth and consume feed from the 12% of land area used for crops, driving deforestation and land degradation.[330]
Steel and cement production are responsible for about 13% of industrial CO2emissions. In these industries, carbon-intensive materials such as coke and lime play an integral role in the production, so that reducing CO2emissions requires research into alternative chemistries.[331]Where energy production or CO2-intensiveheavy industriescontinue to produce waste CO2, technology can sometimes be used to capture and store most of the gas instead of releasing it to the atmosphere.[332]This technology,carbon capture and storage(CCS), could have a critical but limited role in reducing emissions.[332]It is relatively expensive[333]and has been deployed only to an extent that removes around 0.1% of annual greenhouse gas emissions.[332]
Natural carbon sinks can be enhanced to sequester significantly larger amounts of CO2beyond naturally occurring levels.[334]Reforestation andafforestation(planting forests where there were none before) are among the most mature sequestration techniques, although the latter raises food security concerns.[335]Farmers can promote sequestration ofcarbon in soilsthrough practices such as use of wintercover crops, reducing the intensity and frequency oftillage, and using compost and manure as soil amendments.[336]Forest and landscape restoration yields many benefits for the climate, including greenhouse gas emissions sequestration and reduction.[132]Restoration/recreation of coastal wetlands,prairie plotsandseagrass meadowsincreases the uptake of carbon into organic matter.[337][338]When carbon is sequestered in soils and in organic matter such as trees, there is a risk of the carbon being re-released into the atmosphere later through changes in land use, fire, or other changes in ecosystems.[339]
The use of bioenergy in conjunction with carbon capture and storage (BECCS) can result in net negative emissions as CO2is drawn from the atmosphere.[340]It remains highly uncertain whether carbon dioxide removal techniques will be able to play a large role in limiting warming to 1.5 °C. Policy decisions that rely on carbon dioxide removal increase the risk of global warming rising beyond international goals.[341]
Adaptation is "the process of adjustment to current or expected changes in climate and its effects".[342]: 5Without additional mitigation, adaptation cannot avert the risk of "severe, widespread and irreversible" impacts.[343]More severe climate change requires more transformative adaptation, which can be prohibitively expensive.[344]Thecapacity and potential for humans to adaptis unevenly distributed across different regions and populations, and developing countries generally have less.[345]The first two decades of the 21st century saw an increase in adaptive capacity in most low- and middle-income countries with improved access to basicsanitationand electricity, but progress is slow. Many countries have implemented adaptation policies. However, there is a considerable gap between necessary and available finance.[346]
Adaptation to sea level rise consists of avoiding at-risk areas, learning to live with increased flooding, and buildingflood controls. If that fails,managed retreatmay be needed.[347]There are economic barriers for tackling dangerous heat impact. Avoiding strenuous work or havingair conditioningis not possible for everybody.[348]In agriculture, adaptation options include a switch to more sustainable diets, diversification, erosion control, and genetic improvements for increased tolerance to a changing climate.[349]Insurance allows for risk-sharing, but is often difficult to get for people on lower incomes.[350]Education, migration andearly warning systemscan reduce climate vulnerability.[351]Planting mangroves or encouraging other coastal vegetation can buffer storms.[352][353]
Ecosystems adapt to climate change, a process that can be supported by human intervention. By increasing connectivity between ecosystems, species can migrate to more favourable climate conditions. Species can also beintroduced to areas acquiring a favourable climate. Protection and restoration of natural and semi-natural areas helps build resilience, making it easier for ecosystems to adapt. Many of the actions that promote adaptation in ecosystems, also help humans adapt viaecosystem-based adaptation. For instance, restoration ofnatural fire regimesmakes catastrophic fires less likely, and reduces human exposure. Giving rivers more space allows for more water storage in the natural system, reducing flood risk. Restored forest acts as a carbon sink, but planting trees in unsuitable regions can exacerbate climate impacts.[354]
There aresynergiesbut also trade-offs between adaptation and mitigation.[355]An example for synergy is increased food productivity, which has large benefits for both adaptation and mitigation.[356]An example of a trade-off is that increased use of air conditioning allows people to better cope with heat, but increases energy demand. Another trade-off example is that more compacturban developmentmay reduce emissions from transport and construction, but may also increase theurban heat islandeffect, exposing people to heat-related health risks.[357]
Countries that are mostvulnerable to climate changehave typically been responsible for a small share of global emissions. This raises questions about justice and fairness.[358]Limiting global warming makes it much easier to achieve the UN'sSustainable Development Goals, such as eradicating poverty and reducing inequalities. The connection is recognized inSustainable Development Goal 13which is to "take urgent action to combat climate change and its impacts".[359]The goals on food, clean water and ecosystem protection have synergies with climate mitigation.[360]
Thegeopoliticsof climate change is complex. It has often been framed as afree-rider problem, in which all countries benefit from mitigation done by other countries, but individual countries would lose from switching to alow-carbon economythemselves. Sometimes mitigation also has localized benefits though. For instance, the benefits of acoal phase-outto public health and local environments exceed the costs in almost all regions.[361]Furthermore, net importers of fossil fuels win economically from switching to clean energy, causing net exporters to facestranded assets: fossil fuels they cannot sell.[362]
A wide range ofpolicies,regulations, and laws are being used to reduce emissions. As of 2019,carbon pricingcovers about 20% of global greenhouse gas emissions.[363]Carbon can be priced withcarbon taxesandemissions trading systems.[364]Direct globalfossil fuel subsidiesreached $319 billion in 2017, and $5.2 trillion when indirect costs such as air pollution are priced in.[365]Ending these can cause a 28% reduction in global carbon emissions and a 46% reduction in air pollution deaths.[366]Money saved on fossil subsidies could be used to support thetransition to clean energyinstead.[367]More direct methods to reduce greenhouse gases include vehicle efficiency standards, renewable fuel standards, and air pollution regulations on heavy industry.[368]Several countriesrequire utilities to increase the share of renewables in power production.[369]
Policy designed through the lens ofclimate justicetries to addresshuman rightsissues and social inequality. According to proponents of climate justice, the costs of climate adaptation should be paid by those most responsible for climate change, while the beneficiaries of payments should be those suffering impacts. One way this can be addressed in practice is to have wealthy nations pay poorer countries to adapt.[370]
Oxfam found that in 2023 the wealthiest 10% of people were responsible for 50% of global emissions, while the bottom 50% were responsible for just 8%.[371]Production of emissions is another way to look at responsibility: under that approach, the top 21 fossil fuel companies would owe cumulativeclimate reparationsof $5.4 trillion over the period 2025–2050.[372]To achieve ajust transition, people working in the fossil fuel sector would also need other jobs, and their communities would need investments.[373]
Nearly all countries in the world are parties to the 1994United Nations Framework Convention on Climate Change(UNFCCC).[375]The goal of the UNFCCC is to prevent dangerous human interference with the climate system.[376]As stated in the convention, this requires that greenhouse gas concentrations are stabilized in the atmosphere at a level where ecosystems can adapt naturally to climate change, food production is not threatened, andeconomic developmentcan be sustained.[377]The UNFCCC does not itself restrict emissions but rather provides a framework for protocols that do. Global emissions have risen since the UNFCCC was signed.[378]Its yearly conferencesare the stage of global negotiations.[379]
The 1997Kyoto Protocolextended the UNFCCC and included legally binding commitments for most developed countries to limit their emissions.[380]During the negotiations, theG77(representingdeveloping countries) pushed for a mandate requiringdeveloped countriesto "[take] the lead" in reducing their emissions,[381]since developed countries contributed most to theaccumulation of greenhouse gasesin the atmosphere. Per-capita emissions were also still relatively low in developing countries and developing countries would need to emit more to meet their development needs.[382]
The 2009Copenhagen Accordhas been widely portrayed as disappointing because of its low goals, and was rejected by poorer nations including the G77.[383]Associated parties aimed to limit the global temperature rise to below 2 °C.[384]The Accord set the goal of sending $100 billion per year to developing countries for mitigation and adaptation by 2020, and proposed the founding of theGreen Climate Fund.[385]As of 2020[update], only 83.3 billion were delivered. Only in 2023 the target is expected to be achieved.[386]
In 2015 all UN countries negotiated theParis Agreement, which aims to keep global warming well below 2.0 °C and contains an aspirational goal of keeping warming under1.5 °C.[387]The agreement replaced the Kyoto Protocol. Unlike Kyoto, no binding emission targets were set in the Paris Agreement. Instead, a set of procedures was made binding. Countries have to regularly set ever more ambitious goals and reevaluate these goals every five years.[388]The Paris Agreement restated that developing countries must be financially supported.[389]As of March 2025[update], 194 states and theEuropean Unionhave acceded to orratifiedthe agreement.[390]
The 1987Montreal Protocol, an international agreement to phase out production of ozone-depleting gases, has had benefits for climate change mitigation.[391]Several ozone-depleting gases likechlorofluorocarbonsare powerful greenhouse gases, so banning their production and usage may have avoided a temperature rise of 0.5 °C–1.0 °C,[392]as well as additional warming by preventing damage to vegetation fromultravioletradiation.[393]It is estimated that the agreement has been more effective at curbing greenhouse gas emissions than the Kyoto Protocol specifically designed to do so.[394]The most recent amendment to the Montreal Protocol, the 2016Kigali Amendment, committed to reducing the emissions ofhydrofluorocarbons, which served as a replacement for banned ozone-depleting gases and are also potent greenhouse gases.[395]Should countries comply with the amendment, a warming of 0.3 °C–0.5 °C is estimated to be avoided.[396]
In 2019, theUnited Kingdom parliamentbecame the first national government to declare a climate emergency.[398]Other countries andjurisdictionsfollowed suit.[399]That same year, theEuropean Parliamentdeclared a "climate and environmental emergency".[400]TheEuropean Commissionpresented itsEuropean Green Dealwith the goal of making the EU carbon-neutral by 2050.[401]In 2021, the European Commission released its "Fit for 55" legislation package, which contains guidelines for thecar industry; all new cars on the European market must bezero-emission vehiclesfrom 2035.[402]
Major countries in Asia have made similar pledges: South Korea and Japan have committed to become carbon-neutral by 2050, and China by 2060.[403]While India has strong incentives for renewables, it also plans a significant expansion of coal in the country.[404]Vietnam is among very few coal-dependent, fast-developing countries that pledged to phase out unabated coal power by the 2040s or as soon as possible thereafter.[405]
As of 2021, based on information from 48national climate plans, which represent 40% of the parties to the Paris Agreement, estimated total greenhouse gas emissions will be 0.5% lower compared to 2010 levels, below the 45% or 25% reduction goals to limit global warming to 1.5 °C or 2 °C, respectively.[406]
Public debate about climate change has been strongly affected by climate change denial andmisinformation, which originated in the United States and has since spread to other countries, particularly Canada and Australia. Climate change denial has originated from fossil fuel companies, industry groups,conservativethink tanks, andcontrarianscientists.[408]Like the tobacco industry, the main strategy of these groups has been to manufacture doubt about climate-change related scientific data and results.[409]People who hold unwarranted doubt about climate change are called climate change "skeptics", although "contrarians" or "deniers" are more appropriate terms.[410]
There are different variants of climate denial: some deny that warming takes place at all, some acknowledge warming but attribute it to natural influences, and some minimize the negative impacts of climate change.[411]Manufacturing uncertainty about the science later developed into amanufactured controversy: creating the belief that there is significant uncertainty about climate change within the scientific community to delay policy changes.[412]Strategies to promote these ideas include criticism of scientific institutions,[413]and questioning the motives of individual scientists.[411]Anecho chamberof climate-denyingblogsand media has further fomented misunderstanding of climate change.[414]
Climate change came to international public attention in the late 1980s.[418]Due to media coverage in the early 1990s, people often confused climate change with other environmental issues like ozone depletion.[419]In popular culture, theclimate fictionmovieThe Day After Tomorrow(2004) and theAl GoredocumentaryAn Inconvenient Truth(2006) focused on climate change.[418]
Significant regional, gender, age and political differences exist in both public concern for, and understanding of, climate change. More highly educated people, and in some countries, women and younger people, were more likely to see climate change as a serious threat.[420]College biology textbooks from the 2010s featured less content on climate change compared to those from the preceding decade, with decreasing emphasis on solutions.[421]Partisan gaps also exist in many countries,[422]and countries with highCO2emissionstend to be less concerned.[423]Views on causes of climate change vary widely between countries.[424]Media coverage linked to protests has had impacts on public sentiment as well as on which aspects of climate change are focused upon.[425]Higher levels of worry are associated with stronger public support for policies that address climate change.[426]Concern has increased over time,[427]and in 2021 a majority of citizens in 30 countries expressed a high level of worry about climate change, or view it as a global emergency.[428]A 2024 survey across 125 countries found that 89% of the global population demanded intensified political action, but systematicallyunderestimated other peoples'willingness to act.[26][27]
Climate protests demand that political leaders take action to prevent climate change. They can take the form of public demonstrations,fossil fuel divestment, lawsuits and other activities.[429][430]Prominent demonstrations include theSchool Strike for Climate. In this initiative, young people across the globe have been protesting since 2018 by skipping school on Fridays, inspired by Swedish activist and then-teenagerGreta Thunberg.[431]Masscivil disobedienceactions by groups likeExtinction Rebellionhave protested by disrupting roads and public transport.[432]
Litigationis increasingly used as a tool to strengthen climate action from public institutions and companies. Activists also initiate lawsuits which target governments and demand that they take ambitious action or enforce existing laws on climate change.[433]Lawsuits against fossil-fuel companies generally seek compensation forloss and damage.[434]
Scientists in the 19th century such asAlexander von Humboldtbegan to foresee the effects of climate change.[436][437][438][439]In the 1820s,Joseph Fourierproposed the greenhouse effect to explain why Earth's temperature was higher than the Sun's energy alone could explain. Earth's atmosphere is transparent to sunlight, so sunlight reaches the surface where it is converted to heat. However, the atmosphere is not transparent to heat radiating from the surface, and captures some of that heat, which in turn warms the planet.[440]
In 1856Eunice Newton Footedemonstrated that the warming effect of the Sun is greater for air with water vapour than for dry air, and that the effect is even greater with carbon dioxide (CO2). She concluded that "An atmosphere of that gas would give to our earth a high temperature..."[441][442]
Starting in 1859,[443]John Tyndallestablished that nitrogen and oxygen—together totalling 99% of dry air—are transparent to radiated heat. However, water vapour and gases such as methane and carbon dioxide absorb radiated heat and re-radiate that heat into the atmosphere. Tyndall proposed that changes in the concentrations of these gases may have caused climatic changes in the past, includingice ages.[444]
Svante Arrheniusnoted that water vapour in air continuously varied, but the CO2concentration in air was influenced by long-term geological processes. Warming from increased CO2levels would increase the amount of water vapour, amplifying warming in a positive feedback loop. In 1896, he published the firstclimate modelof its kind, projecting that halving CO2levels could have produced a drop in temperature initiating an ice age. Arrhenius calculated the temperature increase expected from doubling CO2to be around 5–6 °C.[445]Other scientists were initially sceptical and believed that the greenhouse effect was saturated so that adding more CO2would make no difference, and that the climate would be self-regulating.[446]Beginning in 1938,Guy Stewart Callendarpublished evidence that climate was warming and CO2levels were rising,[447]but his calculations met the same objections.[446]
In the 1950s,Gilbert Plasscreated a detailed computer model that included different atmospheric layers and the infrared spectrum. This model predicted that increasing CO2levels would cause warming. Around the same time,Hans Suessfound evidence that CO2levels had been rising, andRoger Revelleshowed that the oceans would not absorb the increase. The two scientists subsequently helpedCharles Keelingto begin a record of continued increase—the "Keeling Curve"[446]—which was part of continued scientific investigation through the 1960s into possible human causation of global warming.[452]Studies such as theNational Research Council's 1979Charney Reportsupported the accuracy of climate models that forecast significant warming.[453]Human causation of observed global warming and dangers of unmitigated warming were publicly presented inJames Hansen's 1988 testimony before aUS Senatecommittee.[454][37]TheIntergovernmental Panel on Climate Change(IPCC), set up in 1988 to provide formal advice to the world's governments, spurredinterdisciplinary research.[455]As part of theIPCC reports, scientists assess the scientific discussion that takes place inpeer-reviewedjournalarticles.[456]
There is a near-complete scientific consensus that the climate is warming and that this is caused by human activities. As of 2019, agreement in recent literature reached over 99%.[449][450]No scientific body of national or international standingdisagrees with this view.[457]Consensus has further developed that some form of action should be taken to protect people against the impacts of climate change. National science academies have called on world leaders to cut global emissions.[458]The 2021 IPCC Assessment Report stated that it is "unequivocal" that climate change is caused by humans.[450]
This article incorporates text from afree contentwork. Licensed under CC BY-SA 3.0. Text taken fromThe status of women in agrifood systems – Overview​, FAO, FAO.
Special Report: The Ocean and Cryosphere in a Changing Climate
TheIndustrial Revolution, sometimes divided into theFirst Industrial RevolutionandSecond Industrial Revolution, was a transitional period of theglobal economytoward more widespread, efficient and stable manufacturing processes, succeeding theSecond Agricultural Revolution. Beginning inGreat Britainaround 1760, the Industrial Revolution had spread tocontinental Europeand theUnited Statesby about 1840.[1]This transition included going fromhand production methodstomachines; newchemical manufacturingandiron productionprocesses; the increasing use ofwater powerandsteam power; the development ofmachine tools; and the rise of themechanisedfactory system. Output greatly increased, and the result was an unprecedented rise in population and the rate ofpopulation growth. Thetextile industrywas the first to use modern production methods,[2]: 40andtextilesbecame the dominant industry in terms of employment, value of output, andcapitalinvested.
Many of thetechnologicalandarchitectural innovationswere of British origin.[3][4]By the mid-18th century, Britain was the world's leading commercial nation,[5]controlling a global trading empire withcoloniesin North America and the Caribbean. Britain had major military and political hegemony on theIndian subcontinent; particularly with theproto-industrialisedMughal Bengal, which underwent thede-industrialisation of Indiathrough the activities of theEast India Company.[6][7][8][9]The development of trade and the rise of business were among the major causes of the Industrial Revolution.[2]: 15Developments inlawalso facilitated the revolution, such as courts ruling in favour ofproperty rights. An entrepreneurial spirit and consumer revolution helped driveindustrialisationin Britain, which after 1800, was emulated in Belgium, the United States, and France.[10]
The Industrial Revolution marked a major turning point in history, comparable only to humanity'sadoption of agriculturewith respect to material advancement.[11]The Industrial Revolution influenced in some way almost every aspect of daily life. In particular, average income and population began to exhibit unprecedented sustained growth. Some economists have said the most important effect of the Industrial Revolution was that thestandard of livingfor the general population in theWestern worldbegan to increase consistently for the first time in history, although others have said that it did not begin to improve meaningfully until the late 19th and 20th centuries.[12][13][14]GDP per capitawas broadly stable before the Industrial Revolution and the emergence of the moderncapitalisteconomy,[15]while the Industrial Revolution began an era of per-capitaeconomic growthin capitalist economies.[16]Economic historians agree that the onset of the Industrial Revolution is the most important event inhuman historysince thedomesticationof animals and plants.[17]
The precise start and end of the Industrial Revolution is still debated among historians, as is the pace of economic andsocial changes.[18][19][20][21]According to Cambridge historian Leigh Shaw-Taylor, Britain was already industrialising in the 17th century, and "Our database shows that a groundswell of enterprise and productivity transformed the economy in the 17th century, laying the foundations for the world's first industrial economy. Britain was already a nation of makers by the year 1700" and "the history of Britain needs to be rewritten".[22][23]Eric Hobsbawmheld that the Industrial Revolution began in Britain in the 1780s and was not fully felt until the 1830s or 1840s,[18]whileT. S. Ashtonheld that it occurred roughly between 1760 and 1830.[19]Rapid adoption of mechanized textiles spinning occurred in Britain in the 1780s,[24]and high rates of growth insteampower and iron production occurred after 1800. Mechanised textile production spread from Great Britain to continental Europe and the United States in the early 19th century, with important centres of textiles, iron andcoalemerging in Belgium and the United States and later textiles in France.[2]
An economic recession occurred from the late 1830s to the early 1840s when the adoption of the Industrial Revolution's early innovations, such as mechanised spinning and weaving, slowed as their markets matured; and despite the increasing adoption of locomotives, steamboats and steamships, andhot blastironsmelting. New technologies such as theelectrical telegraph, widely introduced in the 1840s and 1850sin the United Kingdomand the United States, were not powerful enough to drive high rates of economic growth.
Rapid economic growth began to reoccur after 1870, springing from a new group of innovations in what has been called theSecond Industrial Revolution. These included newsteel-making processes,mass production,assembly lines,electrical gridsystems, the large-scale manufacture of machine tools, and the use of increasingly advanced machinery in steam-powered factories.[2][25][26][27]
The earliest recorded use of the term "Industrial Revolution" was in July 1799 by French envoyLouis-Guillaume Otto, announcing that France had entered the race to industrialise.[28]In his 1976 bookKeywords: A Vocabulary of Culture and Society,Raymond Williamsstates in the entry for "Industry": "The idea of a new social order based on major industrial change was clear inSoutheyandOwen, between 1811 and 1818, and was implicit as early asBlakein the early 1790s andWordsworthat the turn of the [19th] century." The termIndustrial Revolutionapplied to technological change was becoming more common by the late 1830s, as inJérôme-Adolphe Blanqui's description in 1837 ofla révolution industrielle.[29]
Friedrich EngelsinThe Condition of the Working Class in Englandin 1844 spoke of "an industrial revolution, a revolution which at the same time changed the whole of civil society". Although Engels wrote his book in the 1840s, it was not translated into English until the late 19th century, and his expression did not enter everyday language until then. Credit for popularising the term may be given toArnold Toynbee, whose 1881 lectures gave a detailed account of the term.[30]
Economic historians and authors such as Mendels,Pomeranz, and Kridte argue that proto-industrialisation in parts of Europe, theMuslim world,Mughal India, andChinacreated the social and economic conditions that led to the Industrial Revolution, thus causing theGreat Divergence.[31][32][33]Some historians, such asJohn ClaphamandNicholas Crafts, have argued that the economic and social changes occurred gradually and that the termrevolutionis a misnomer. This is still a subject of debate among some historians.[34]
Six factors facilitated industrialisation: high levels of agricultural productivity, such as that reflected in theBritish Agricultural Revolution, to provide excess manpower and food; a pool of managerial and entrepreneurial skills; available ports, rivers, canals, and roads to cheaply move raw materials and outputs; natural resources such as coal, iron, and waterfalls; political stability and a legal system that supported business; and financial capital available to invest. Once industrialisation began in Great Britain, new factors can be added: the eagerness of British entrepreneurs to export industrial expertise and the willingness to import the process. Britain met the criteria and industrialized starting in the 18th century, and then it exported the process to western Europe (especially Belgium, France, and the German states) in the early 19th century. The United States copied the British model in the early 19th century, and Japan copied the Western European models in the late 19th century.[35][36]
The commencement of the Industrial Revolution is closely linked to a small number of innovations,[37]beginning in the second half of the 18th century. By the 1830s, the following gains had been made in important technologies:
In 1750, Britain imported 2.5 million pounds of raw cotton, most of which was spun and woven by the cottage industry inLancashire. The work was done by hand in workers' homes or occasionally in master weavers' shops. Wages in Lancashire were about six times those in India in 1770 when overall productivity in Britain was about three times higher than in India.[44]In 1787, raw cotton consumption was 22 million pounds, most of which was cleaned, carded, and spun on machines.[2]: 41–42The British textile industry used 52 million pounds of cotton in 1800, which increased to 588 million pounds in 1850.[45]
The share of value added by the cotton textile industry in Britain was 2.6% in 1760, 17% in 1801, and 22.4% in 1831. Value added by the British woollen industry was 14.1% in 1801. Cotton factories in Britain numbered approximately 900 in 1797. In 1760, approximately one-third of cotton cloth manufactured in Britain was exported, rising to two-thirds by 1800. In 1781, cotton spun amounted to 5.1 million pounds, which increased to 56 million pounds by 1800. In 1800, less than 0.1% of world cotton cloth was produced on machinery invented in Britain. In 1788, there were 50,000 spindles in Britain, rising to 7 million over the next 30 years.[44]
The earliest European attempts at mechanised spinning were with wool; however, wool spinning proved more difficult to mechanise than cotton. Productivity improvement in wool spinning during the Industrial Revolution was significant but far less than that of cotton.[2][9]
Arguably the first highly mechanised factory wasJohn Lombe'swater-powered silk millatDerby, operational by 1721. Lombe learned silk thread manufacturing by taking a job in Italy and acting as an industrial spy; however, because the Italian silk industry guarded its secrets closely, the state of the industry at that time is unknown. Although Lombe's factory was technically successful, the supply of raw silk from Italy was cut off to eliminate competition. In order to promote manufacturing, the Crown paid for models of Lombe's machinery which were exhibited in theTower of London.[46][47]
Parts of India, China, Central America, South America, and the Middle East have a long history of hand manufacturing cotton textiles, which became a major industry sometime after 1000 AD. In tropical and subtropical regions where it was grown, most was grown by small farmers alongside their food crops and was spun and woven in households, largely for domestic consumption. In the 15th century, China began to require households to pay part of their taxes in cotton cloth. By the 17th century, almost all Chinese wore cotton clothing. Almost everywhere cotton cloth could be used as amedium of exchange. In India, a significant amount of cotton textiles were manufactured for distant markets, often produced by professional weavers. Some merchants also owned small weaving workshops. India produced a variety of cotton cloth, some of exceptionally fine quality.[44]
Cotton was a difficultraw materialfor Europe to obtain before it was grown oncolonial plantationsin the Americas.[44]The early Spanish explorers foundNative Americansgrowing unknown species of excellent quality cotton: sea island cotton (Gossypium barbadense) and upland green seeded cottonGossypium hirsutum. Sea island cotton grew in tropical areas and onbarrier islandsof Georgia and South Carolina but did poorly inland. Sea island cotton began being exported from Barbados in the 1650s. Upland green seeded cotton grew well on inland areas of the southern U.S. but was not economical because of the difficulty of removing seed, a problem solved by thecotton gin.[26]: 157A strain of cotton seed brought from Mexico toNatchez, Mississippi, in 1806 became the parent genetic material for over 90% of world cotton production today; it produced bolls that were three to four times faster to pick.[44]
TheAge of Discoverywas followed by a period ofcolonialismbeginning around the 16th century. Following the discovery of atrade routeto India around southern Africa by the Portuguese, the British founded theEast India Company, along with smaller companies of different nationalities which established trading posts and employed agents to engage in trade throughout the Indian Ocean region.[44]
One of the largest segments of this trade was in cotton textiles, which were purchased in India and sold inSoutheast Asia, including theIndonesian archipelagowhere spices were purchased for sale to Southeast Asia and Europe. By the mid-1760s, cloth was over three-quarters of the East India Company's exports. Indian textiles were in demand in the North Atlantic region of Europe where previously only wool and linen were available; however, the number of cotton goods consumed in Western Europe was minor until the early 19th century.[44]
By 1600,Flemishrefugees began weaving cotton cloth in English towns where cottage spinning and weaving of wool and linen was well established. They were left alone by theguildswho did not consider cotton a threat. Earlier European attempts at cotton spinning and weaving were in 12th-century Italy and 15th-century southern Germany, but these industries eventually ended when the supply of cotton was cut off. TheMoorsin Spain grew, spun, and wove cotton beginning around the 10th century.[44]
British cloth could not compete with Indian cloth because India's labour cost was approximately one-fifth to one-sixth that of Britain's.[24]In 1700 and 1721, the British government passedCalico Actstoprotectthe domestic woollen and linen industries from the increasing amounts of cotton fabric imported from India.[2][48]
The demand for heavier fabric was met by a domestic industry based around Lancashire that producedfustian, a cloth with flaxwarpand cottonweft. Flax was used for the warp because wheel-spun cotton did not have sufficient strength, but the resulting blend was not as soft as 100% cotton and was more difficult to sew.[48]
On the eve of the Industrial Revolution, spinning and weaving were done in households, for domestic consumption, and as a cottage industry under theputting-out system. Occasionally, the work was done in the workshop of a master weaver. Under the putting-out system, home-based workers produced under contract to merchant sellers, who often supplied the raw materials. In the off-season, the women, typically farmers' wives, did the spinning and the men did the weaving. Using thespinning wheel, it took anywhere from four to eight spinners to supply one handloom weaver.[2][48][49]: 823
Theflying shuttle, patented in 1733 byJohn Kay—with a number of subsequent improvements including an important one in 1747—doubled the output of a weaver, worsening the imbalance between spinning and weaving. It became widely used around Lancashire after 1760 when John's son,Robert, invented the dropbox, which facilitated changing thread colors.[49]: 821–822
Lewis Paulpatented the rollerspinning frameand the flyer-and-bobbinsystem for drawing wool to a more even thickness. The technology was developed with the help of John Wyatt ofBirmingham. Paul and Wyatt opened a mill in Birmingham which used their rolling machine powered by a donkey. In 1743, a factory opened inNorthamptonwith 50 spindles on each of five of Paul and Wyatt's machines. This operated until about 1764. A similar mill was built byDaniel BourninLeominster, but this burnt down. Both Lewis Paul and Daniel Bourn patentedcardingmachines in 1748. Based on two sets of rollers that travelled at different speeds, it was later used in the firstcotton spinning mill.
In 1764, in the village of Stanhill, Lancashire,James Hargreavesinvented thespinning jenny, which he patented in 1770. It was the first practical spinning frame with multiple spindles.[50]The jenny worked in a similar manner to the spinning wheel, by first clamping down on the fibres, then by drawing them out, followed by twisting.[51]It was a simple, wooden framed machine that only cost about £6 for a 40-spindle model in 1792[52]and was used mainly by home spinners. The jenny produced a lightly twisted yarn only suitable for weft, not warp.[49]: 825–827
The spinning frame or water frame was developed byRichard Arkwrightwho, along with two partners, patented it in 1769. The design was partly based on a spinning machine built by Kay, who was hired by Arkwright.[49]: 827–830For each spindle the water frame used a series of four pairs of rollers, each operating at a successively higher rotating speed, to draw out the fibre which was then twisted by the spindle. The roller spacing was slightly longer than the fibre length. Too close a spacing caused the fibres to break while too distant a spacing caused uneven thread. The top rollers were leather-covered and loading on the rollers was applied by a weight. The weights kept the twist from backing up before the rollers. The bottom rollers were wood and metal, with fluting along the length. The water frame was able to produce a hard, medium-count thread suitable for warp, finally allowing 100% cotton cloth to be made in Britain. Arkwright and his partners used water power at a factory inCromford,Derbyshirein 1771, giving the invention its name.
Samuel Cromptoninvented thespinning mulein 1779, so called because it is a hybrid of Arkwright'swater frameandJames Hargreaves'sspinning jennyin the same way that amuleis the product of crossbreeding afemale horsewith amale donkey. Crompton's mule was able to produce finer thread than hand spinning and at a lower cost. Mule-spun thread was of suitable strength to be used as a warp and finally allowed Britain to produce highly competitive yarn in large quantities.[49]: 832
Realising that the expiration of the Arkwright patent would greatly increase the supply of spun cotton and lead to a shortage of weavers,Edmund Cartwrightdeveloped a verticalpower loomwhich he patented in 1785. In 1776, he patented a two-man operated loom.[49]: 834Cartwright's loom design had several flaws, the most serious being thread breakage. Samuel Horrocks patented a fairly successful loom in 1813. Horock's loom was improved byRichard Robertsin 1822, and these were produced in large numbers by Roberts, Hill & Co. Roberts was additionally a maker of high-quality machine tools and a pioneer in the use of jigs and gauges for precision workshop measurement.[53]
The demand for cotton presented an opportunity toplantersin the Southern United States, who thought upland cotton would be a profitable crop if a better way could be found to remove the seed.Eli Whitneyresponded to the challenge by inventing the inexpensivecotton gin. A man using a cotton gin could remove seed from as much upland cotton in one day as would previously have taken two months to process, working at the rate of one pound of cotton per day.[26][54]
These advances were capitalised on byentrepreneurs, of whom the best known is Arkwright. He is credited with a list of inventions, but these were actually developed by such people as Kay andThomas Highs; Arkwright nurtured the inventors, patented the ideas, financed the initiatives, and protected the machines. He created the cotton mill which brought the production processes together in a factory, and he developed the use of power—first horsepower and then water power—which made cotton manufacture a mechanised industry. Other inventors increased the efficiency of the individual steps of spinning (carding, twisting and spinning, and rolling) so that the supply ofyarnincreased greatly. Steam power was then applied to drive textile machinery.Manchesteracquired the nicknameCottonopolisduring the early 19th century owing to its sprawl of textile factories.[55]
Although mechanisation dramatically decreased the cost of cotton cloth, by the mid-19th century machine-woven cloth still could not equal the quality of hand-woven Indian cloth, in part because of the fineness of thread made possible by the type of cotton used in India, which allowed high thread counts. However, the high productivity of British textile manufacturing allowed coarser grades of British cloth to undersell hand-spun and woven fabric in low-wage India, eventually destroying the Indian industry.[44]
Bar iron was the commodity form of iron used as the raw material for making hardware goods such as nails, wire, hinges, horseshoes, wagon tires, chains, etc., as well as structural shapes. A small amount of bar iron was converted into steel. Cast iron was used for pots, stoves, and other items where its brittleness was tolerable. Most cast iron was refined and converted to bar iron, with substantial losses. Bar iron was made by thebloomeryprocess, which was the predominant iron smelting process until the late 18th century.
In the UK in 1720, there were 20,500 tons of cast iron produced with charcoal and 400 tons with coke. In 1750charcoal ironproduction was 24,500 and coke iron was 2,500 tons. In 1788, the production of charcoal cast iron was 14,000 tons while coke iron production was 54,000 tons. In 1806, charcoal cast iron production was 7,800 tons and coke cast iron was 250,000 tons.[41]: 125
In 1750, the UK imported 31,200 tons of bar iron and either refined from cast iron or directly produced 18,800 tons of bar iron using charcoal and 100 tons using coke. In 1796, the UK was making 125,000 tons of bar iron with coke and 6,400 tons with charcoal; imports were 38,000 tons and exports were 24,600 tons. In 1806 the UK did not import bar iron but exported 31,500 tons.[41]: 125
A major change in the iron industries during the Industrial Revolution was the replacement of wood and other bio-fuels withcoal; for a given amount of heat,mining coalrequired much less labour than cutting wood and converting it tocharcoal,[57]and coal was much more abundant than wood, supplies of which were becoming scarce before the enormous increase in iron production that took place in the late 18th century.[2][41]: 122
In 1709,Abraham Darbymade progress using coke to fuel his blast furnaces atCoalbrookdale.[58]However, the coke pig iron he made was not suitable for making wrought iron and was used mostly for the production of cast iron goods, such as pots and kettles. He had the advantage over his rivals in that his pots, cast by his patented process, were thinner and cheaper than theirs.
In 1750,cokehad generally replaced charcoal in the smelting of copper and lead and was in widespread use in glass production. In the smelting and refining of iron, coal and coke produced inferior iron to that made with charcoal because of the coal's sulfur content. Low sulfur coals were known, but they still contained harmful amounts. Conversion of coal to coke only slightly reduces the sulfur content.[41]: 122–125A minority of coals are coking. Another factor limiting the iron industry before the Industrial Revolution was the scarcity of water power to power blast bellows. This limitation was overcome by the steam engine.[41]
Use of coal in iron smelting started somewhat before the Industrial Revolution, based on innovations byClement Clerkeand others from 1678, using coalreverberatory furnacesknown as cupolas. These were operated by the flames playing on the ore and charcoal or coke mixture,reducingtheoxideto metal. This has the advantage that impurities (such as sulphur ash) in the coal do not migrate into the metal. This technology was applied to lead from 1678 and to copper from 1687. It was also applied to iron foundry work in the 1690s, but in this case the reverberatory furnace was known as an air furnace. (Thefoundry cupolais a different, and later, innovation.)[59]
Coke pig iron was hardly used to produce wrought iron until 1755–56, when Darby's sonAbraham Darby IIbuilt furnaces atHorsehayandKetleywhere low sulfur coal was available (and not far from Coalbrookdale). These furnaces were equipped with water-powered bellows, the water being pumped byNewcomen steam engines. The Newcomen engines were not attached directly to the blowing cylinders because the engines alone could not produce a steady air blast.Abraham Darby IIIinstalled similar steam-pumped, water-powered blowing cylinders at the Dale Company when he took control in 1768. The Dale Company used several Newcomen engines to drain its mines and made parts for engines which it sold throughout the country.[41]: 123–125
Steam engines made the use of higher-pressure and volume blast practical; however, the leather used in bellows was expensive to replace. In 1757, ironmasterJohn Wilkinsonpatented a hydraulic poweredblowing enginefor blast furnaces.[60]The blowing cylinder for blast furnaces was introduced in 1760 and the first blowing cylinder made of cast iron is believed to be the one used at Carrington in 1768 that was designed byJohn Smeaton.[41]: 124, 135
Cast iron cylinders for use with a piston were difficult to manufacture; the cylinders had to be free of holes and had to be machined smooth and straight to remove any warping.James Watthad great difficulty trying to have a cylinder made for his first steam engine. In 1774 Wilkinson invented a precision boring machine for boring cylinders. After Wilkinson bored the first successful cylinder for aBoulton and Wattsteam engine in 1776, he was given an exclusive contract for providing cylinders.[26][61]After Watt developed a rotary steam engine in 1782, they were widely applied to blowing, hammering, rolling and slitting.[41]: 124
The solutions to the sulfur problem were the addition of sufficient limestone to the furnace to force sulfur into theslagas well as the use of low sulfur coal. The use of lime or limestone required higher furnace temperatures to form a free-flowing slag. The increased furnace temperature made possible by improved blowing also increased the capacity of blast furnaces and allowed for increased furnace height.[41]: 123–125
In addition to lower cost and greater availability, coke had other important advantages over charcoal in that it was harder and made the column of materials (iron ore, fuel, slag) flowing down the blast furnace more porous and did not crush in the much taller furnaces of the late 19th century.[62][63]
As cast iron became cheaper and widely available, it began being a structural material for bridges and buildings. A famous early example is theIron Bridgebuilt in 1778 with cast iron produced by Abraham Darby III.[56]However, most cast iron was converted to wrought iron. Conversion of cast iron had long been done in afinery forge. An improved refining process known aspotting and stampingwas developed, but this was superseded byHenry Cort'spuddlingprocess. Cort developed two significant iron manufacturing processes:rollingin 1783 and puddling in 1784.[2]: 91Puddling produced a structural grade iron at a relatively low cost.
Puddling was a means of decarburizing molten pig iron by slow oxidation in a reverberatory furnace by manually stirring it with a long rod. The decarburized iron, having a higher melting point than cast iron, was raked into globs by the puddler. When the glob was large enough, the puddler would remove it. Puddling was backbreaking and extremely hot work. Few puddlers lived to be 40.[2]: 218Because puddling was done in a reverberatory furnace, coal or coke could be used as fuel. The puddling process continued to be used until the late 19th century when iron was being displaced by mild steel. Because puddling required human skill in sensing the iron globs, it was never successfully mechanised. Rolling was an important part of the puddling process because the grooved rollers expelled most of the molten slag and consolidated the mass of hot wrought iron. Rolling was 15 times faster at this than atrip hammer. A different use of rolling, which was done at lower temperatures than that for expelling slag, was in the production of iron sheets, and later structural shapes such as beams, angles, and rails.
The puddling process was improved in 1818 by Baldwyn Rogers, who replaced some of the sand lining on the reverberatory furnace bottom withiron oxide.[64]In 1838John Hallpatented the use of roasted tap cinder (iron silicate) for the furnace bottom, greatly reducing the loss of iron through increased slag caused by a sand lined bottom. The tap cinder also tied up some phosphorus, but this was not understood at the time.[41]: 166Hall's process also used iron scale orrustwhich reacted with carbon in the molten iron. Hall's process, calledwet puddling, reduced losses of iron with the slag from almost 50% to around 8%.[2]: 93
Puddling became widely used after 1800. Up to that time, British iron manufacturers had used considerable amounts of iron imported from Sweden and Russia to supplement domestic supplies. Because of the increased British production, imports began to decline in 1785, and by the 1790s Britain eliminated imports and became a net exporter of bar iron.
Hot blast, patented by the Scottish inventorJames Beaumont Neilsonin 1828, was the most important development of the 19th century for saving energy in making pig iron. By using preheated combustion air, the amount of fuel to make a unit of pig iron was reduced at first by between one-third using coke or two-thirds using coal;[65]the efficiency gains continued as the technology improved.[66]Hot blast also raised the operating temperature of furnaces, increasing their capacity. Using less coal or coke meant introducing fewer impurities into the pig iron. This meant that lower quality coal could be used in areas wherecoking coalwas unavailable or too expensive;[67]however, by the end of the 19th century transportation costs fell considerably.
Shortly before the Industrial Revolution, an improvement was made in the production ofsteel, which was an expensive commodity and used only where iron would not do, such as for cutting edge tools and for springs.Benjamin Huntsmandeveloped hiscrucible steeltechnique in the 1740s. The raw material for this was blister steel, made by thecementation process.[68]The supply of cheaper iron and steel aided a number of industries, such as those making nails, hinges, wire, and other hardware items. The development of machine tools allowed better working of iron, causing it to be increasingly used in the rapidly growing machinery and engine industries.[69]
The development of thestationary steam enginewas an important element of the Industrial Revolution; however, during the early period of the Industrial Revolution, most industrial power was supplied by water and wind. In Britain, by 1800 an estimated 10,000 horsepower was being supplied by steam. By 1815 steam power had grown to 210,000 hp.[70]
The first commercially successful industrial use of steam power was patented byThomas Saveryin 1698. He constructed in London a low-lift combined vacuum and pressure water pump that generated about onehorsepower(hp) and was used in numerous waterworks and in a few mines (hence its "brand name",The Miner's Friend). Savery's pump was economical in small horsepower ranges but was prone to boiler explosions in larger sizes. Savery pumps continued to be produced until the late 18th century.[71]
The first successful piston steam engine was introduced byThomas Newcomenbefore 1712. Newcomen engines were installed for draining hitherto unworkable deep mines, with the engine on the surface; these were large machines, requiring a significant amount of capital to build, and produced upwards of 3.5 kW (5 hp). They were also used to power municipal water supply pumps. They were extremely inefficient by modern standards, but when located where coal was cheap at pit heads, they opened up a great expansion in coal mining by allowing mines to go deeper.[72]Despite their disadvantages, Newcomen engines were reliable and easy to maintain and continued to be used in the coalfields until the early decades of the 19th century.
By 1729, when Newcomen died, his engines had spread to Hungary in 1722, and then to Germany, Austria, and Sweden. A total of 110 are known to have been built by 1733 when the joint patent expired, of which 14 were abroad. In the 1770s the engineerJohn Smeatonbuilt some very large examples and introduced a number of improvements. A total of 1,454 engines had been built by 1800.[72]
A fundamental change in working principles was brought about byScotsmanJames Watt. With financial support from his business partnerEnglishmanMatthew Boulton, he had succeeded by 1778 in perfectinghis steam engine, which incorporated a series of radical improvements, notably the closing off of the upper part of the cylinder thereby making the low-pressure steam drive the top of the piston instead of the atmosphere; use of a steam jacket; and the celebrated separate steam condenser chamber. The separate condenser did away with the cooling water that had been injected directly into the cylinder which cooled the cylinder and wasted steam. Likewise, the steam jacket kept steam from condensing in the cylinder, also improving efficiency. These improvements increased engine efficiency so that Boulton and Watt's engines used only 20–25% as much coal per horsepower-hour as Newcomen's. Boulton and Watt opened theSoho Foundryfor the manufacture of such engines in 1795.
In 1783, the Watt steam engine had been fully developed into adouble-actingrotative type, which meant that it could be used to directly drive the rotary machinery of a factory or mill. Both of Watt's basic engine types were commercially very successful, and by 1800 the firmBoulton & Watthad constructed 496 engines, with 164 driving reciprocating pumps, 24 serving blast furnaces, and 308 powering mill machinery; most of the engines generated from 3.5 to 7.5 kW (5 to 10 hp).
Until about 1800, the most common pattern of steam engine was thebeam engine, built as an integral part of a stone or brick engine-house, but soon various patterns of self-contained rotative engines (readily removable but not on wheels) were developed, such as thetable engine. Around the start of the 19th century, at which time the Boulton and Watt patent expired, the Cornish engineerRichard Trevithickand the AmericanOliver Evansbegan to construct higher-pressure non-condensing steam engines, exhausting against the atmosphere. High pressure yielded an engine and boiler compact enough to be used on mobile road and raillocomotivesandsteamboats.[73]
Small industrial power requirements continued to be provided by animal and human muscle until widespreadelectrificationin the early 20th century. These includedcrank-powered,treadle-powered and horse-powered workshop, and light industrial machinery.[74]
Pre-industrial machinery was built by various craftsmen—millwrightsbuiltwatermillsandwindmills; carpenters made wooden framing; and smiths and turners made metal parts. Wooden components had the disadvantage of changing dimensions with temperature and humidity, and the various joints tended to rack (work loose) over time. As the Industrial Revolution progressed, machines with metal parts and frames became more common. Other important uses of metal parts were in firearms and threadedfasteners, such as machine screws, bolts, and nuts. There was also the need for precision in making parts. Precision would allow better working machinery,interchangeability of parts, and standardization of threaded fasteners.
The demand for metal parts led to the development of severalmachine tools. They have their origins in the tools developed in the 18th century by makers of clocks and watches and scientific instrument makers to enable them to batch-produce small mechanisms. Before the advent of machine tools, metal was worked manually using the basic hand tools of hammers, files, scrapers, saws, and chisels. Consequently, the use of metal machine parts was kept to a minimum. Hand methods of production were laborious and costly, and precision was difficult to achieve.[43][26]
The first large precision machine tool was the cylinderboring machineinvented by John Wilkinson in 1774. It was designed to bore the large cylinders on early steam engines. Wilkinson's machine was the first to use the principle of line-boring, where the tool is supported on both ends, unlike earlier designs used for boring cannon that relied on a less stablecantileveredboring bar.[26]
Theplaning machine, themilling machineand theshaping machinewere developed in the early decades of the 19th century. Although the milling machine was invented at this time, it was not developed as a serious workshop tool until somewhat later in the 19th century.[43][26]James FoxofDerbyandMatthew MurrayofLeedswere manufacturers of machine tools who found success in exporting from England and are also notable for having developed the planer around the same time asRichard RobertsofManchester.
Henry Maudslay, who trained a school of machine tool makers early in the 19th century, was a mechanic with superior ability who had been employed at theRoyal Arsenal,Woolwich. He worked as an apprentice at the Royal Arsenal underJan Verbruggen. In 1774 Verbruggen had installed ahorizontal boring machinewhich was the first industrial size lathe in the UK. Maudslay was hired away byJoseph Bramahfor the production of high-security metal locks that required precision craftsmanship. Bramah patented a lathe that had similarities to the slide rest lathe.[26][49]: 392–395Maudslay perfected the slide rest lathe, which could cut machine screws of different thread pitches by using changeable gears between the spindle and the lead screw. Before its invention, screws could not be cut to any precision using various earlier lathe designs, some of which copied from a template.[26][49]: 392–395The slide rest lathe was called one of history's most important inventions. Although it was not entirely Maudslay's idea, he was the first person to build a functional lathe using a combination of known innovations of the lead screw, slide rest, and change gears.[26]: 31, 36
Maudslay left Bramah's employment and set up his own shop. He was engaged to build the machinery for making ships' pulley blocks for theRoyal Navyin thePortsmouth Block Mills. These machines were all-metal and were the first machines for mass production and making components with a degree of interchangeability. The lessons Maudslay learned about the need for stability and precision he adapted to the development of machine tools, and in his workshops, he trained a generation of men to build on his work, such asRichard Roberts,Joseph ClementandJoseph Whitworth.[26]
The techniques to make mass-produced metal parts of sufficient precision to be interchangeable is largely attributed to a program of theU.S. Department of Warwhich perfectedinterchangeable partsfor firearms in the early 19th century.[43]In the half-century following the invention of the fundamental machine tools, the machine industry became the largest industrial sector of the U.S. economy, by value added.[75]
The large-scale production of chemicals was an important development during the Industrial Revolution. The first of these was the production ofsulphuric acidby thelead chamber processinvented by the EnglishmanJohn Roebuck(James Watt's first partner) in 1746. He was able to greatly increase the scale of the manufacture by replacing the relatively expensive glass vessels formerly used with larger, less expensive chambers made ofrivetedsheets of lead. Instead of making a small amount each time, he was able to make around 50 kilograms (100 pounds) in each of the chambers, at least a tenfold increase.
The production of analkalion a large scale became an important goal as well, andNicolas Leblancsucceeded in 1791 in introducing a method for the production ofsodium carbonate(soda ash). TheLeblanc processwas a reaction ofsulfuric acidwithsodium chlorideto givesodium sulfateandhydrochloric acid. The sodium sulfate was heated withcalcium carbonateand coal to give a mixture of sodium carbonate andcalcium sulfide. Adding water separated the soluble sodium carbonate from the calcium sulfide. The process produced a large amount of pollution (the hydrochloric acid was initially vented to the atmosphere, and calcium sulfide was awasteproduct). Nonetheless, this synthetic soda ash proved economical compared to that produced from burning specific plants (barillaorkelp), which were the previously dominant sources of soda ash,[76]and also topotash(potassium carbonate) produced from hardwood ashes. These two chemicals were very important because they enabled the introduction of a host of other inventions, replacing many small-scale operations with more cost-effective and controllable processes. Sodium carbonate had many uses in the glass, textile, soap, and paper industries. Early uses for sulfuric acid includedpickling(removing rust from) iron and steel, and forbleaching cloth.
The development of bleaching powder (calcium hypochlorite) by Scottish chemistCharles Tennantin about 1800, based on the discoveries of French chemistClaude Louis Berthollet, revolutionised the bleaching processes in the textile industry by dramatically reducing the time required (from months to days) for the traditional process then in use, which required repeated exposure to the sun in bleach fields after soaking the textiles with alkali or sour milk. Tennant'sfactory at St Rollox,Glasgow, became the largest chemical plant in the world.
After 1860 the focus on chemical innovation was indyestuffs, and Germany took world leadership, building a strong chemical industry.[77]Aspiring chemists flocked to German universities in the 1860–1914 era to learn the latest techniques. British scientists by contrast, lacked research universities and did not train advanced students; instead, the practice was to hire German-trained chemists.[78]
In 1824Joseph Aspdin, a Britishbricklayerturned builder, patented a chemical process for makingportland cementwhich was an important advance in the building trades. This process involvessinteringa mixture ofclayandlimestoneto about 1,400 °C (2,552 °F), thengrindingit into a fine powder which is then mixed with water, sand andgravelto produceconcrete. Portland cement concrete was used by the English engineerMarc Isambard Brunelseveral years later when constructing theThames Tunnel.[79]Concrete was used on a large scale in the construction of theLondon sewer systema generation later.
Though others made a similar innovation elsewhere, the large-scale introduction ofgas lightingwas the work ofWilliam Murdoch, an employee of Boulton & Watt. The process consisted of the large-scale gasification of coal in furnaces, the purification of the gas (removal of sulphur, ammonia, and heavy hydrocarbons), and its storage and distribution. The first gas lighting utilities were established in London between 1812 and 1820. They soon became one of the major consumers of coal in the UK. Gas lighting affected social and industrial organisation because it allowed factories and stores to remain open longer than with tallow candles oroil lamps. Its introduction allowed nightlife to flourish in cities and towns as interiors and streets could be lighted on a larger scale than before.[80]
Glass was made in ancient Greece and Rome.[81]A new method ofglass production, known as thecylinder process, was developed in Europe during the early 19th century. In 1832 this process was used by theChance Brothersto createsheet glass. They became the leading producers of window and plate glass. This advancement allowed for larger panes of glass to be created without interruption, thus freeing up the space planning in interiors as well as the fenestration of buildings.The Crystal Palaceis the supreme example of the use of sheet glass in a new and innovative structure.[82]
A machine for making a continuous sheet of paper on a loop of wire fabric was patented in 1798 byLouis-Nicolas Robertin France. Thepaper machineis known as a Fourdrinier after the financiers, brothers Sealy andHenry Fourdrinier, who werestationersin London. Although greatly improved and with many variations, the Fourdrinier machine is the predominant means of paper production today. The method ofcontinuous productiondemonstrated by the paper machine influenced the development of continuous rolling of iron and later steel and other continuous production processes.[83]
TheBritish Agricultural Revolutionis considered one of the causes of the Industrial Revolution because improvedagricultural productivityfreed up workers to work in other sectors of the economy.[84]In contrast, per-capita food supply in Europe was stagnant or declining and did not improve in some parts of Europe until the late 18th century.[85]
The English lawyerJethro Tullinvented an improvedseed drillin 1701. It was a mechanical seeder that distributed seeds evenly across a plot of land and planted them at the correct depth. This was important because theyieldof seeds harvested to seeds planted at that time was around four or five. Tull's seed drill was very expensive and not very reliable and therefore did not have much of an effect. Good quality seed drills were not produced until the mid 18th century.[60]: 26
Joseph Foljambe'sRotherhamploughof 1730 was the first commercially successful iron plough.[84]: 122[86][60]: 18, 21[87]Thethreshing machine, invented by the Scottish engineerAndrew Meiklein 1784, displaced handthreshingwith aflail, a laborious job that took about one-quarter of agricultural labour.[88]: 286Lower labor requirements subsequently result in lowered wages and numbers of farm labourers, who faced near starvation, leading to the 1830 agricultural rebellion of theSwing Riots.
Machine tools and metalworking techniques developed during the Industrial Revolution eventually resulted in precision manufacturing techniques in the late 19th century for mass-producing agricultural equipment, such as reapers, binders, and combine harvesters.[43]
Coal miningin Britain, particularly inSouth Wales, started early. Before the steam engine,pitswere often shallowbell pitsfollowing a seam of coal along the surface, which were abandoned as the coal was extracted. In other cases, if the geology was favourable the coal was mined by means of anaditordrift minedriven into the side of a hill.Shaft miningwas done in some areas, but the limiting factor was the problem of removing water. It could be done by hauling buckets of water up the shaft or to asough(a tunnel driven into a hill to drain a mine). In either case, the water had to be discharged into a stream or ditch at a level where it could flow away by gravity.[89]
The introduction of the steam pump by Thomas Savery in 1698 and the Newcomen steam engine in 1712 greatly facilitated the removal of water and enabled shafts to be made deeper, enabling more coal to be extracted. These were developments that had begun before the Industrial Revolution, but the adoption of John Smeaton's improvements to the Newcomen engine followed by James Watt's more efficient steam engines from the 1770s reduced the fuel costs of engines, making mines more profitable. TheCornish engine, developed in the 1810s, was much more efficient than the Watt steam engine.[89]
Coal mining was very dangerous owing to the presence offiredampin many coal seams. Some degree of safety was provided by thesafety lampwhich was invented in 1816 by SirHumphry Davyand independently byGeorge Stephenson. However, the lamps proved a false dawn because they became unsafe very quickly and provided a weak light. Firedamp explosions continued, often setting offcoal dustexplosions, so casualties grew during the entire 19th century. Conditions of work were very poor, with a high casualty rate from rock falls.
At the beginning of the Industrial Revolution, inland transport was by navigable rivers and roads, with coastal vessels employed to move heavy goods by sea.Wagonwayswere used for conveying coal to rivers for further shipment, butcanalshad not yet been widely constructed. Animals supplied all of the motive power on land, with sails providing the motive power on the sea. The first horse railways were introduced toward the end of the 18th century, withsteam locomotivesbeing introduced in the early decades of the 19th century. Improving sailing technologies boosted average sailing speed by 50% between 1750 and 1830.[90]
The Industrial Revolution improved Britain's transport infrastructure with a turnpike road network, a canal and waterway network, and a railway network. Raw materials and finished products could be moved more quickly and cheaply than before. Improved transportation also allowed new ideas to spread quickly.
Before and during the Industrial Revolution navigation on several British rivers was improved by removing obstructions, straightening curves, widening and deepening, and building navigationlocks. Britain had over 1,600 kilometres (1,000 mi) of navigable rivers and streams by 1750.[2]: 46Canals and waterways allowedbulk materialsto be economically transported long distances inland. This was because a horse could pull a barge with a load dozens of times larger than the load that could be drawn in a cart.[49][91]
Canals began to be built in the UK in the late 18th century to link the major manufacturing centres across the country. Known for its huge commercial success, theBridgewater CanalinNorth West England, which opened in 1761 and was mostly funded byThe 3rd Duke of Bridgewater. FromWorsleyto the rapidly growing town ofManchesterits construction cost £168,000 (£22,589,130 as of 2013[update]),[92][93]but its advantages over land and river transport meant that within a year of its opening in 1761, the price of coal in Manchester fell by about half.[94]This success helped inspire a period of intense canal building, known asCanal Mania.[95]Canals were hastily built with the aim of replicating the commercial success of the Bridgewater Canal, the most notable being theLeeds and Liverpool Canaland theThames and Severn Canalwhich opened in 1774 and 1789 respectively.
By the 1820s a national network was in existence. Canal construction served as a model for the organisation and methods later used to construct the railways. They were eventually largely superseded as profitable commercial enterprises by the spread of the railways from the 1840s on. The last major canal to be built in the United Kingdom was theManchester Ship Canal, which upon opening in 1894 was the largestship canalin the world,[96]and opened Manchester as aport. However, it never achieved the commercial success its sponsors had hoped for and signalled canals as a dying mode of transport in an age dominated by railways, which were quicker and often cheaper.
Britain's canal network, together with its surviving mill buildings, is one of the most enduring features of the early Industrial Revolution to be seen in Britain.[97]
France was known for having an excellent system of roads at the time of the Industrial Revolution; however, most of the roads on the European continent and in the UK were in bad condition and dangerously rutted.[91][27]Much of the original British road system was poorly maintained by thousands of local parishes, but from the 1720s (and occasionally earlier)turnpike trustswere set up to charge tolls and maintain some roads. Increasing numbers of main roads were turnpiked from the 1750s to the extent that almost every main road in England and Wales was the responsibility of a turnpike trust. New engineered roads were built byJohn Metcalf,Thomas Telfordand most notablyJohn McAdam, with the first 'macadam' stretch of road being Marsh Road atAshton Gate,Bristolin 1816.[99]The first macadam road in the U.S. was the "Boonsborough Turnpike Road" betweenHagerstownandBoonsboro, Marylandin 1823.[98]
The major turnpikes radiated from London and were the means by which theRoyal Mailwas able to reach the rest of the country. Heavy goods transport on these roads was by means of slow, broad-wheeled carts hauled by teams of horses. Lighter goods were conveyed by smaller carts or by teams ofpackhorse.Stagecoachescarried the rich, and the less wealthy could pay to ride oncarriers carts. Productivity of road transport increased greatly during the Industrial Revolution, and the cost of travel fell dramatically. Between 1690 and 1840 productivity almost tripled for long-distance carrying and increased four-fold in stage coaching.[100]
Railways were made practical by the widespread introduction of inexpensive puddled iron after 1800, the rolling mill for making rails, and the development of the high-pressure steam engine also around 1800. Reducing friction was one of the major reasons for the success of railroads compared to wagons. This was demonstrated on an iron plate-covered wooden tramway in 1805 at Croydon, England.
A good horse on an ordinary turnpike road can draw two thousand pounds, or one ton. A party of gentlemen were invited to witness the experiment, that the superiority of the new road might be established by ocular demonstration. Twelve wagons were loaded with stones, till each wagon weighed three tons, and the wagons were fastened together. A horse was then attached, which drew the wagons with ease, six miles [10 km] in two hours, having stopped four times, in order to show he had the power of starting, as well as drawing his great load.[101]
Wagonways for moving coal in the mining areas had started in the 17th century and were often associated with canal or river systems for the further movement of coal. These were all horse-drawn or relied on gravity, with a stationary steam engine to haul the wagons back to the top of the incline. The first applications of the steam locomotive were on wagon or plate ways (as they were then often called from the cast-iron plates used). Horse-drawn public railways begin in the early 19th century when improvements to pig and wrought iron production were lowering costs.
Steam locomotives began being built after the introduction of high-pressure steam engines after the expiration of the Boulton and Watt patent in 1800. High-pressure engines exhausted used steam to the atmosphere, doing away with the condenser and cooling water. They were also much lighter weight and smaller in size for a given horsepower than the stationary condensing engines. A few of these early locomotives were used in mines. Steam-hauled public railways began with theStockton and Darlington Railwayin 1825.[102]
The rapid introduction of railways followed the 1829Rainhill trials, which demonstratedRobert Stephenson's successful locomotive design and the 1828 development ofhot blast, which dramatically reduced the fuel consumption of making iron and increased the capacity of the blast furnace. On 15 September 1830, theLiverpool and Manchester Railway, the first inter-city railway in the world, wasopenedand was attended by Prime MinisterArthur Wellesley.[103]The railway was engineered byJoseph LockeandGeorge Stephenson, linked the rapidly expanding industrial town of Manchester with the port town of Liverpool. The opening was marred by problems caused by the primitive nature of the technology being employed; however, problems were gradually solved, and the railway became highly successful, transporting passengers and freight.
The success of the inter-city railway, particularly in the transport of freight and commodities, led toRailway Mania. Construction of major railways connecting the larger cities and towns began in the 1830s but only gained momentum at the very end of the first Industrial Revolution. After many of the workers had completed the railways, they did not return to their rural lifestyles but instead remained in the cities, providing additional workers for the factories.
On a structural level the Industrial Revolution asked society the so-calledsocial question, demanding new ideas for managing large groups of individuals. Visible poverty on one hand and growing population and materialistic wealth on the other caused tensions between the very rich and the poorest people within society.[104]These tensions were sometimes violently released[105]and led to philosophical ideas such associalism,communismandanarchism.
Prior to the Industrial Revolution, most of the workforce was employed in agriculture, either as self-employed farmers as landowners or tenants or aslandlessagricultural labourers. It was common for families in various parts of the world to spin yarn, weave cloth and make their own clothing. Households also spun and wove for market production. At the beginning of the Industrial Revolution, India, China, and regions of Iraq and elsewhere in Asia and the Middle East produced most of the world's cotton cloth while Europeans produced wool and linen goods.
InGreat Britainin the 16th century, theputting-out systemwas practised, by which farmers and townspeople produced goods for a market in their homes, often described ascottage industry. Typical putting-out system goods included spinning and weaving. Merchant capitalists typically provided the raw materials, paid workersby the piece, and were responsible for the sale of the goods. Embezzlement of supplies by workers and poor quality were common problems. The logistical effort in procuring and distributing raw materials and picking up finished goods were also limitations of the putting-out system.[2]: 57–59
Some early spinning and weaving machinery, such as a 40 spindle jenny for about six pounds in 1792, was affordable for cottagers.[2]: 59Later machinery such as spinning frames, spinning mules and power looms were expensive (especially if water-powered), giving rise to capitalist ownership of factories.
The majority of textile factory workers during the Industrial Revolution were unmarried women and children, including many orphans. They typically worked for 12 to 14 hours per day with only Sundays off. It was common for women to take factory jobs seasonally during slack periods of farm work. Lack of adequate transportation, long hours, and poor pay made it difficult to recruit and maintain workers.[44]
The change in the social relationship of the factory worker compared to farmers and cottagers was viewed unfavourably byKarl Marx; however, he recognized the increase in productivity made possible by technology.[106]
Some economists, such asRobert Lucas Jr., say that the real effect of the Industrial Revolution was that "for the first time in history, the living standards of the masses of ordinary people have begun to undergo sustained growth ... Nothing remotely like this economic behaviour is mentioned by the classical economists, even as a theoretical possibility."[12]Others argue that while the growth of the economy's overall productive powers was unprecedented during the Industrial Revolution,living standardsfor the majority of the population did not grow meaningfully until the late 19th and 20th centuries and that in many ways workers' living standards declined under early capitalism: some studies have estimated that real wages in Britain only increased 15% between the 1780s and 1850s and thatlife expectancyin Britain did not begin to dramatically increase until the 1870s.[13][14]
The average height of the population declined during the Industrial Revolution, implying that their nutritional status was also decreasing.[107][108]
During the Industrial Revolution, the life expectancy of children increased dramatically. The percentage of the children born in London who died before the age of five decreased from 74.5% in 1730–1749 to 31.8% in 1810–1829.[109]The effects on living conditions have been controversial and were hotly debated by economic and social historians from the 1950s to the 1980s.[110]Over the course of the period from 1813 to 1913, there was a significant increase in worker wages.[111][112]
Chronic hunger and malnutrition were the norms for the majority of the population of the world including Britain and France until the late 19th century. Until about 1750, malnutrition limited life expectancy in France to about 35 years and about 40 years in Britain. The United States population of the time was adequately fed, much taller on average, and had a life expectancy of 45–50 years, although U.S. life expectancy declined by a few years by the mid 19th century. Food consumption per capita also declined during an episode known as theAntebellum Puzzle.[113]
Food supply in Great Britain was adversely affected by theCorn Laws(1815–1846) which imposed tariffs on imported grain. The laws were enacted to keep prices high in order to benefit domestic producers. The Corn Laws were repealed in the early years of theGreat Irish Famine.
The initial technologies of the Industrial Revolution, such as mechanized textiles, iron and coal, did little, if anything, to lowerfood prices.[85]In Britain and the Netherlands, food supply increased before the Industrial Revolution with better agricultural practices; however, population grew as well.[2][88][114][115]
The rapid population growth in the 19th century included the new industrial and manufacturing cities, as well as service centers such asEdinburghand London.[116]The critical factor was financing, which was handled by building societies that dealt directly with large contracting firms.[117][118]Private renting from housing landlords was the dominant tenure. P. Kemp says this was usually of advantage to tenants.[119]People moved in so rapidly there was not enough capital to build adequate housing for everyone, so low-income newcomers squeezed into increasingly overcrowdedslums.Clean water,sanitation, and public health facilities were inadequate; the death rate was high, especially infant mortality, andtuberculosisamong young adults.Cholerafrom polluted water andtyphoidwere endemic. Unlike rural areas, there were no famines such as the one that devastated Ireland in the 1840s.[120][121][122]
A large exposé literature grew up condemning the unhealthy conditions. By far the most famous publication was by one of the founders of the socialist movement,The Condition of the Working Class in Englandin 1844Friedrich Engelsdescribes backstreet sections of Manchester and other mill towns, where people lived in crude shanties and shacks, some not completely enclosed, some with dirt floors. Theseshanty townshad narrow walkways between irregularly shaped lots and dwellings. There were no sanitary facilities. The population density was extremely high.[123]However, not everyone lived in such poor conditions. The Industrial Revolution also created a middle class of businessmen, clerks, foremen, and engineers who lived in much better conditions.
Conditions improved over the course of the 19th century with new public health acts regulating things such as sewage, hygiene, and home construction. In the introduction of his 1892 edition, Engels notes that most of the conditions he wrote about in 1844 had been greatly improved. For example, thePublic Health Act 1875(38 & 39 Vict.c. 55) led to the more sanitarybyelaw terraced house.
Pre-industrial water supply relied on gravity systems, and pumping of water was done by water wheels. Pipes were typically made of wood. Steam-powered pumps and iron pipes allowed the widespread piping of water to horse watering troughs and households.[27]
Engels' book describes how untreated sewage created awful odours and turned the rivers green in industrial cities. In 1854John Snowtraced a cholera outbreak inSohoin London to fecal contamination of a public water well by a homecesspit. Snow's findings that cholera could be spread by contaminated water took some years to be accepted, but his work led to fundamental changes in the design of public water and waste systems.
In the 18th century, there were relatively high levels of literacy among farmers in England and Scotland. This permitted the recruitment of literate craftsmen, skilled workers, foremen, and managers who supervised the emerging textile factories and coal mines. Much of the labour was unskilled, and especially in textile mills children as young as eight proved useful in handling chores and adding to the family income. Indeed, children were taken out of school to work alongside their parents in the factories. However, by the mid-19th century, unskilled labor forces were common in Western Europe, and British industry moved upscale, needing many more engineers and skilled workers who could handle technical instructions and handle complex situations. Literacy was essential to be hired.[124][125]A senior government official told Parliament in 1870:
The invention of the paper machine and the application of steam power to the industrial processes ofprintingsupported a massive expansion of newspaper and pamphlet publishing, which contributed to rising literacy and demands for mass political participation.[127]
Consumers benefited from falling prices for clothing and household articles such as cast iron cooking utensils, and in the following decades, stoves for cooking and space heating. Coffee, tea, sugar, tobacco, and chocolate became affordable to many in Europe. Theconsumer revolutionin England from the early 17th century to the mid-18th century had seen a marked increase in the consumption and variety of luxury goods and products by individuals from different economic and social backgrounds.[128]With improvements in transport and manufacturing technology, opportunities for buying and selling became faster and more efficient than previous. The expanding textile trade in the north of England meant the three-piece suit became affordable to the masses.[129]Founded by potter and retail entrepreneurJosiah Wedgwoodin 1759,Wedgwoodfine china and porcelaintablewarewas starting to become a common feature on dining tables.[130]Rising prosperity and social mobility in the 18th century increased the number of people with disposable income for consumption, and the marketing of goods (of which Wedgwood was a pioneer) for individuals, as opposed to items for the household, started to appear, and the new status of goods as status symbols related to changes in fashion and desired for aesthetic appeal.[130]
With the rapid growth of towns and cities, shopping became an important part of everyday life. Window shopping and the purchase of goods became a cultural activity in its own right, and many exclusive shops were opened in elegant urban districts: in the Strand and Piccadilly in London, for example, and in spa towns such as Bath and Harrogate. Prosperity and expansion in manufacturing industries such as pottery and metalware increased consumer choice dramatically. Where once labourers ate from metal platters with wooden implements, ordinary workers now dined on Wedgwood porcelain. Consumers came to demand an array of new household goods and furnishings: metal knives and forks, for example, as well as rugs, carpets, mirrors, cooking ranges, pots, pans, watches, clocks, and a dizzying array of furniture. The age ofmass consumptionhad arrived.
New businesses in various industries appeared in towns and cities throughout Britain. Confectionery was one such industry that saw rapid expansion. According to food historianPolly Russell: "chocolate andbiscuitsbecame products for the masses, thanks to the Industrial Revolution and the consumers it created. By the mid-19th century, sweet biscuits were an affordable indulgence and business was booming. Manufacturers such asHuntley & Palmersin Reading,Carr'sof Carlisle andMcVitie'sin Edinburgh transformed from small family-run businesses into state-of-the-art operations".[131]In 1847Fry'sof Bristol produced the firstchocolate bar.[132]Their competitorCadburyof Birmingham was the first to commercialize the association between confectionery and romance when they produced a heart-shaped box of chocolates forValentine's Dayin 1868.[133]Thedepartment storebecame a common feature in majorHigh Streetsacross Britain; one of the first was opened in 1796 byHarding, Howell & Co.onPall Mallin London.[134]In the 1860s,fish and chipshops emerged across the country in order to satisfy the needs of the growing industrial population.[135]
In addition to goods being sold in the growing number of stores,street sellerswere common in an increasinglyurbanizedcountry. Matthew White: "Crowds swarmed in everythoroughfare. Scores of street sellers 'cried' merchandise from place to place, advertising the wealth of goods and services on offer. Milkmaids, orange sellers, fishwives and piemen, for example, all walked the streets offering their various wares for sale, while knife grinders and the menders of broken chairs and furniture could be found on street corners".[136]An earlysoft drinkscompany,R. White's Lemonade, began in 1845 by selling drinks in London in a wheelbarrow.[137]
Increased literacy rates, industrialisation, and the invention of the railway created a new market for cheap popular literature for the masses and the ability for it to be circulated on a large scale.Penny dreadfulswere created in the 1830s to meet this demand.[138]The Guardiandescribed penny dreadfuls as "Britain's first taste of mass-produced popular culture for the young", and "the Victorian equivalent of video games".[139]By the 1860s and 1870s more than one million boys' periodicals were sold per week.[139]Labelled an "authorpreneur" byThe Paris Review,Charles Dickensused innovations from the revolution to sell his books, such as the new printing presses, enhanced advertising revenues, and the expansion of railroads.[140]His first novel,The Pickwick Papers(1836), became a publishing phenomenon with its unprecedented success sparking numerous spin-offs and merchandise ranging fromPickwickcigars, playing cards, china figurines,Sam Wellerpuzzles, Weller boot polish and joke books.[140]Nicholas Dames inThe Atlanticwrites, "Literature" is not a big enough category forPickwick. It defined its own, a new one that we have learned to call "entertainment".[141]Rapid industrialisation and urbanisation of previously rural populations led to the development of themusic hallin the 1850s, with the newly created urban communities, cut off from their cultural roots, requiring new and readily accessible forms of entertainment.[142]
In 1861, Welsh entrepreneurPryce Pryce-Jonesformed the firstmail orderbusiness, an idea which would change the nature ofretail.[143]Selling Welshflannel, he createdmail ordercatalogues, with customers able to order by mail for the first time—this following theUniform Penny Postin 1840 and the invention of the postage stamp (Penny Black) where there was a charge of one penny for carriage and delivery between any two places in the United Kingdom irrespective of distance—and the goods were delivered throughout the UK via the newly created railway system.[144]As the railway network expanded overseas, so did his business.[144]
The Industrial Revolution was the first period in history during which there was a simultaneous increase in both population and per capita income.[145]According toRobert HughesinThe Fatal Shore, thepopulation of Englandand Wales, which had remained steady at six million from 1700 to 1740, rose dramatically after 1740. The population of England had more than doubled from 8.3 million in 1801 to 16.8 million in 1850 and, by 1901, had nearly doubled again to 30.5 million.[146]Improved conditions led to the population of Britain increasing from 10 million to 30 million in the 19th century.[147][148]Europe's population increased from about 100 million in 1700 to 400 million by 1900.[149]
The growth of the modern industry since the late 18th century led to massiveurbanisationand the rise of new great cities, first in Europe and then in other regions, as new opportunities brought huge numbers of migrants from rural communities into urban areas. In 1800, only 3% of the world's population lived in cities,[150]compared to nearly 50% by the beginning of the 21st century.[151]Manchesterhad a population of 10,000 in 1717, but by 1911 it had burgeoned to 2.3 million.[152]
Women's historians have debated the effect of the Industrial Revolution and capitalism generally on the status of women.[153][154]Taking a pessimistic side,Alice Clarkargues that when capitalism arrived in 17th-century England, it lowered the status of women as they lost much of their economic importance. Clark argues that in 16th-century England, women were engaged in many aspects of industry and agriculture. The home was a central unit of production, and women played a vital role in running farms and in some trades and landed estates. Their useful economic roles gave them a sort of equality with their husbands. However, Clark argues, as capitalism expanded in the 17th century, there was more division of labour with the husband taking paid labour jobs outside the home, and the wife was reduced to unpaid household work. Middle- and upper-class women were confined to an idle domestic existence, supervising servants; lower-class women were forced to take poorly paid jobs. Capitalism, therefore, had a negative effect on powerful women.[155]
In a more positive interpretation,Ivy Pinchbeckargues that capitalism created the conditions for women's emancipation.[156]Tilly and Scott have emphasised the continuity in the status of women, finding three stages in English history. In the pre-industrial era, production was mostly for home use, and women produced much of the needs of the households. The second stage was the "family wage economy" of early industrialisation; the entire family depended on the collective wages of its members, including husband, wife, and older children. The third or modern stage is the "family consumer economy", in which the family is the site of consumption, and women are employed in large numbers in retail and clerical jobs to support rising standards of consumption.[157]
Ideas of thrift and hard work characterised middle-class families as the Industrial Revolution swept Europe. These values were displayed inSamuel Smiles' bookSelf-Help, in which he states that the misery of the poorer classes was "voluntary and self-imposed—the results of idleness, thriftlessness, intemperance, and misconduct."[158]
In terms of social structure, the Industrial Revolution witnessed the triumph of amiddle classof industrialists and businessmen over a landed class of nobility and gentry. Ordinary working people found increased opportunities for employment in mills and factories, but these were often under strict working conditions with long hours of labour dominated by a pace set by machines. As late as 1900, most industrial workers in the United States worked a 10-hour day (12 hours in the steel industry), yet earned 20–40% less than the minimum deemed necessary for a decent life;[159]however, most workers in textiles, which was by far the leading industry in terms of employment, were women and children.[44]For workers of the labouring classes, industrial life "was a stony desert, which they had to make habitable by their own efforts."[160]
Harsh working conditions were prevalent long before the Industrial Revolution took place.Pre-industrial societywas very static and often cruel—child labour, dirty living conditions, and long working hours were just as prevalent before the Industrial Revolution.[161]
Industrialisation led to the creation of thefactory. Thefactory systemcontributed to the growth of urban areas as large numbers of workers migrated into the cities in search of work in the factories. Nowhere was this better illustrated than the mills and associated industries of Manchester, nicknamed "Cottonopolis", and the world's first industrial city.[162]Manchester experienced a six-times increase in its population between 1771 and 1831. Bradford grew by 50% every ten years between 1811 and 1851, and by 1851 only 50% of the population of Bradford were actually born there.[163]
In addition, between 1815 and 1939, 20% of Europe's population left home, pushed by poverty, a rapidly growing population, and the displacement of peasant farming and artisan manufacturing. They were pulled abroad by the enormous demand for labour overseas, the ready availability of land, and cheap transportation. Still, many did not find a satisfactory life in their new homes, leading 7 million of them to return to Europe.[164]This mass migration had large demographic effects: in 1800, less than 1% of the world population consisted of overseas Europeans and their descendants; by 1930, they represented 11%.[165]The Americas felt the brunt of this huge emigration, largely concentrated in the United States.
For much of the 19th century, production was done in small mills which were typicallywater-poweredand built to serve local needs. Later, each factory would have its own steam engine and a chimney to give an efficient draft through its boiler.
In other industries, the transition to factory production was not so divisive. Some industrialists tried to improve factory and living conditions for their workers. One of the earliest such reformers wasRobert Owen, known for his pioneering efforts in improving conditions for workers at theNew Lanark millsand often regarded as one of the key thinkers of theearly socialist movement.
By 1746 an integratedbrass millwas working atWarmleynearBristol. Raw material went in at one end, was smelted into brass and was turned into pans, pins, wire, and other goods. Housing was provided for workers on site.Josiah WedgwoodandMatthew Boulton(whoseSoho Manufactorywas completed in 1766) were other prominent early industrialists who employed the factory system.
The Industrial Revolution led to a population increase, but the chances of surviving childhood did not improve throughout the Industrial Revolution, althoughinfantmortality rates were reduced markedly.[109][167]There was still limited opportunity for education, and children were expected to work. Employers could pay a child less than an adult even though their productivity was comparable; there was no need for strength to operate an industrial machine, and since the industrial system was new, there were no experienced adult labourers. This made child labour the labour of choice for manufacturing in the early phases of the Industrial Revolution between the 18th and 19th centuries. In England and Scotland in 1788, two-thirds of the workers in 143 water-powered cotton mills were described as children.[168]
Child labour existed before the Industrial Revolution, but with the increase in population and education it became more visible. Many children were forced to work in relatively bad conditions for much lower pay than their elders,[169]10–20% of an adult male's wage.[170]
Reports were written detailing some of the abuses, particularly in the coal mines[171]and textile factories,[172]and these helped to popularise the children's plight. The public outcry, especially among the upper and middle classes, helped stir change in the young workers' welfare.
Politicians and the government tried to limit child labour by law, but factory owners resisted; some felt that they were aiding the poor by giving their children money to buy food to avoid starvation, and others simply welcomed the cheap labour. In 1833 and 1844, the first general laws against child labour, theFactory Acts, were passed in Britain: children younger than nine were not allowed to work, children were not permitted to work at night, and the workday of youth under age 18 was limited to twelve hours. Factory inspectors supervised the execution of the law; however, their scarcity made enforcement difficult.[173]About ten years later, the employment of children and women in mining was forbidden. Although laws such as these decreased the number of child labourers, child labour remained significantly present in Europe and the United States until the 20th century.[174]
The Industrial Revolution concentrated labour into mills, factories, and mines, thus facilitating the organisation ofcombinationsortrade unionsto help advance the interests of working people. The power of a union could demand better terms by withdrawing all labour and causing a consequent cessation of production. Employers had to decide between giving in to the union demands at a cost to themselves or suffering the cost of the lost production. Skilled workers were difficult to replace, and these were the first groups to successfully advance their conditions through this kind of bargaining.
The main method the unions used to effect change wasstrike action. Many strikes were painful events for both sides, the unions and the management. In Britain, theCombination Act 1799forbade workers to form any kind of trade union until its repeal in 1824. Even after this, unions were still severely restricted. One British newspaper in 1834 described unions as "the most dangerous institutions that were ever permitted to take root, under shelter of law, in any country..."[175]
In 1832, theReform Actextended the vote in Britain but did not grantuniversal suffrage. That year six men fromTolpuddlein Dorset founded the Friendly Society of Agricultural Labourers to protest against the gradual lowering of wages in the 1830s. They refused to work for less than ten shillings per week, although by this time wages had been reduced to seven shillings per week and were due to be further reduced to six. In 1834 James Frampton, a local landowner, wrote to Prime MinisterLord Melbourneto complain about the union, invoking an obscure law from 1797 prohibiting people from swearing oaths to each other, which the members of the Friendly Society had done. Six men were arrested, found guilty, andtransported to Australia. They became known as theTolpuddle Martyrs. In the 1830s and 1840s, thechartistmovement was the first large-scale organised working-class political movement that campaigned for political equality and social justice. ItsCharterof reforms received over three million signatures but was rejected by Parliament without consideration.
Working people also formedfriendly societiesandcooperativesocieties as mutual support groups against times of economic hardship. Enlightened industrialists, such as Robert Owen supported these organisations to improve the conditions of the working class. Unions slowly overcame the legal restrictions on the right to strike. In 1842, ageneral strikeinvolving cotton workers and colliers was organised through the chartist movement which stopped production across Great Britain.[176]Eventually, effective political organisation for working people was achieved through the trades unions who, after the extensions of the franchise in 1867 and 1885, began to support socialist political parties that later merged to become the BritishLabour Party.
The rapid industrialisation of the English economy cost many craft workers their jobs. The movement started first withlaceandhosieryworkers near Nottingham and spread to other areas of the textile industry. Many weavers also found themselves suddenly unemployed since they could no longer compete with machines which only required relatively limited (and unskilled) labour to produce more cloth than a single weaver. Many such unemployed workers, weavers, and others turned their animosity towards the machines that had taken their jobs and began destroying factories and machinery. These attackers became known as Luddites, supposedly followers ofNed Ludd, a folklore figure.[177]The first attacks of the Luddite movement began in 1811. The Luddites rapidly gained popularity, and the British government took drastic measures using the militia or army to protect industry. Those rioters who were caught were tried and hanged, ortransportedfor life.[178]
Unrest continued in other sectors as they industrialised, such as with agricultural labourers in the 1830s when large parts of southern Britain were affected by theCaptain Swingdisturbances. Threshing machines were a particular target, andhayrickburning was a popular activity. However, the riots led to the first formation of trade unions and further pressure for reform.
The traditional centres of hand textile production such as India, parts of the Middle East, and later China could not withstand the competition from machine-made textiles, which over a period of decades destroyed the hand-made textile industries and left millions of people without work, many of whom starved.[44]
The Industrial Revolution generated an enormous and unprecedented economic division in the world, as measured by the share of manufacturing output.
Cheap cotton textiles increased the demand for raw cotton; previously, it had primarily been consumed in subtropical regions where it was grown, with little raw cotton available for export. Consequently, prices of raw cotton rose. British production grew from 2 million pounds in 1700 to 5 million pounds in 1781 to 56 million in 1800.[180]The invention of the cotton gin by American Eli Whitney in 1792 was the decisive event. It allowed green-seeded cotton to become profitable, leading to the widespread growth of the large slaveplantationin the United States, Brazil, and the West Indies. In 1791 American cotton production was about 2 million pounds, soaring to 35 million by 1800, half of which was exported. America'scotton plantationswere highly efficient and profitable and were able to keep up with demand.[181]The U.S. Civil War created a "cotton famine" that led to increased production in other areas of the world, includingEuropean colonies in Africa.[182]
The origins of theenvironmental movementlay in the response to increasing levels of smoke pollution in the atmosphere during the Industrial Revolution. The emergence of great factories and the concomitant immense growth incoal consumptiongave rise to an unprecedented level ofair pollutionin industrial centres; after 1900 the large volume of industrial chemical discharges added to the growing load ofuntreated human waste.[183]The first large-scale, modern environmental laws came in the form of Britain'sAlkali Acts, passed in 1863, to regulate the deleterious air pollution (gaseoushydrochloric acid) given off by the Leblanc process used to produce soda ash. An alkali inspector and four sub-inspectors were appointed to curb this pollution. The responsibilities of the inspectorate were gradually expanded, culminating in the Alkali Order 1958 which placed all major heavy industries that emitted smoke, grit, dust, and fumes under supervision.
The manufactured gas industry began in British cities in 1812–1820. The technique used produced highly toxic effluent that was dumped into sewers and rivers. The gas companies were repeatedly sued in nuisance lawsuits. They usually lost and modified the worst practices. The City of London repeatedly indicted gas companies in the 1820s for polluting the Thames and poisoning its fish. Finally, Parliament wrote company charters to regulate toxicity.[184]The industry reached the U.S. around 1850 causing pollution and lawsuits.[185]
In industrial cities local experts and reformers, especially after 1890, took the lead in identifying environmental degradation and pollution, and initiating grass-roots movements to demand and achieve reforms.[186]Typically the highest priority went to water and air pollution. TheCoal Smoke Abatement Societywas formed in Britain in 1898 making it one of the oldest environmentalnon-governmental organisations. It was founded by artistWilliam Blake Richmond, frustrated with the pall cast by coal smoke. Although there were earlier pieces of legislation, thePublic Health Act 1875required all furnaces and fireplaces to consume their own smoke. It also provided for sanctions against factories that emitted large amounts of black smoke. The provisions of this law were extended in 1926 with the Smoke Abatement Act to include other emissions, such as soot, ash, and gritty particles, and to empower local authorities to impose their own regulations.[187]
The Industrial Revolution in continental Europe came later than in Great Britain. It started in Belgium and France, then spread to the German states by the middle of the 19th century. In many industries, this involved the application of technology developed in Britain in new places. Typically, the technology was purchased from Britain or British engineers and entrepreneurs moved abroad in search of new opportunities. By 1809, part of theRuhr Valleyin Westphalia was called 'Miniature England' because of its similarities to the industrial areas of Britain. Most European governments provided state funding to the new industries. In some cases (such asiron), the different availability of resources locally meant that only some aspects of the British technology were adopted.[188][189]
The Habsburg realms which becameAustria-Hungaryin 1867 included 23 million inhabitants in 1800, growing to 36 million by 1870. Nationally, the per capita rate of industrial growth averaged about 3% between 1818 and 1870. However, there were strong regional differences. The railway system was built in the 1850–1873 period. Before they arrived transportation was very slow and expensive. In the Alpine and Bohemian (modern-dayCzech Republic) regions, proto-industrialisation began by 1750 and became the center of the first phases of the Industrial Revolution after 1800. The textile industry was the main factor, utilising mechanisation, steam engines, and the factory system. In theCzech lands, the "first mechanical loom followed inVarnsdorfin 1801",[190]with the first steam engines appearing inBohemiaandMoraviajust a few years later. The textile production flourished particularly inPrague[191]andBrno(German: Brünn), which was considered the 'Moravian Manchester'.[192]TheCzech lands, especially Bohemia, became the centre of industrialisation due to its natural and human resources. The iron industry had developed in the Alpine regions after 1750, with smaller centers in Bohemia and Moravia. Hungary—the eastern half of the Dual Monarchy, was heavily rural with little industry before 1870.[193]
In 1791,Pragueorganised the firstWorld's Fair/List of world's fairs,Bohemia(modern-dayCzech Republic). The first industrial exhibition was on the occasion of the coronation ofLeopold IIas a king of Bohemia, which took place inClementinum, and therefore celebrated the considerable sophistication of manufacturing methods in theCzech landsduring that time period.[194]
Technological change accelerated industrialisation and urbanisation. The GNP per capita grew roughly 1.76% per year from 1870 to 1913. That level of growth compared very favourably to that of other European nations such as Britain (1%), France (1.06%), and Germany (1.51%).[195]However, in a comparison with Germany and Britain: the Austro-Hungarian economy as a whole still lagged considerably, as sustained modernisation had begun much later.[196]
Belgiumwas the second country in which the Industrial Revolution took place and the first in continental Europe:Wallonia(French-speaking southern Belgium) took the lead. Starting in the middle of the 1820s, and especially after Belgium became an independent nation in 1830, numerous works comprising coke blast furnaces as well as puddling and rolling mills were built in the coal mining areas aroundLiègeandCharleroi. The leader wasJohn Cockerill, a transplanted Englishman . His factories atSeraingintegrated all stages of production, from engineering to the supply of raw materials, as early as 1825.[197][198]
Wallonia exemplified the radical evolution of industrial expansion. Thanks to coal (the French word "houille" was coined in Wallonia),[199]the region geared up to become the 2nd industrial power in the world after Britain. But it is also pointed out by many researchers, with itsSillon industriel, "Especially in theHaine,SambreandMeusevalleys, between theBorinageandLiège...there was a huge industrial development based on coal-mining and iron-making...".[200]Philippe Raxhon wrote about the period after 1830: "It was not propaganda but a reality the Walloon regions were becoming the second industrial power all over the world after Britain."[201]"The sole industrial centre outside the collieries and blast furnaces of Walloon was the old cloth-making town ofGhent."[202]Professor Michel De Coster stated: "The historians and the economists say that Belgium was the second industrial power of the world, in proportion to its population and its territory [...] But this rank is the one of Wallonia where the coal-mines, the blast furnaces, the iron and zinc factories, the wool industry, the glass industry, the weapons industry... were concentrated."[203]Many of the 19th-century coal mines in Wallonia are now protected asWorld Heritage Sites.[204]
Wallonia was also the birthplace of a strong socialist party and strong trade unions in a particular sociological landscape. At the left, theSillon industriel, which runs fromMonsin the west, toVerviersin the east (except part of North Flanders, in another period of the Industrial Revolution, after 1920). Even if Belgium is the second industrial country after Britain, the effect of the Industrial Revolution there was very different. In 'Breaking stereotypes', Muriel Neven and Isabelle Devious say:
The Industrial Revolution changed a mainly rural society into an urban one, but with a strong contrast between northern and southern Belgium. During the Middle Ages and the early modern period, Flanders was characterised by the presence of large urban centres [...] at the beginning of the nineteenth century this region (Flanders), with an urbanisation degree of more than 30 percent, remained one of the most urbanised in the world. By comparison, this proportion reached only 17 percent in Wallonia, barely 10 percent in most West European countries, 16 percent in France, and 25 percent in Britain. Nineteenth-century industrialisation did not affect the traditional urban infrastructure, except in Ghent... Also, in Wallonia, the traditional urban network was largely unaffected by the industrialisation process, even though the proportion of city-dwellers rose from 17 to 45 percent between 1831 and 1910. Especially in theHaine,SambreandMeusevalleys, between theBorinageandLiège, where there was a huge industrial development based on coal-mining and iron-making, urbanisation was fast. During these eighty years, the number of municipalities with more than 5,000 inhabitants increased from only 21 to more than one hundred, concentrating nearly half of the Walloon population in this region. Nevertheless, industrialisation remained quite traditional in the sense that it did not lead to the growth of modern and large urban centres, but to a conurbation of industrial villages and towns developed around a coal mine or a factory. Communication routes between these small centres only became populated later and created a much less dense urban morphology than, for instance, the area around Liège where the old town was there to direct migratory flows.[205]
The Industrial Revolution in France followed a particular course as it did not correspond to the main model followed by other countries. Notably, mostFrench historiansargue France did not go through a cleartake-off.[206]Instead, France's economic growth and industrialisation process was slow and steady through the 18th and 19th centuries. However, some stages were identified by Maurice Lévy-Leboyer:
Based on its leadership in chemical research in the universities and industrial laboratories,Germany, which was unified in 1871, became dominant in the world's chemical industry in the late 19th century. At first the production of dyes based onanilinewas critical.[207]
Germany's political disunity—with three dozen states—and a pervasive conservatism made it difficult to build railways in the 1830s. However, by the 1840s, trunk lines linked the major cities; each German state was responsible for the lines within its own borders. Lacking a technological base at first, the Germans imported their engineering and hardware from Britain, but quickly learned the skills needed to operate and expand the railways. In many cities, the new railway shops were the centres of technological awareness and training, so that by 1850, Germany was self-sufficient in meeting the demands of railroad construction, and the railways were a major impetus for the growth of the new steel industry. Observers found that even as late as 1890, their engineering was inferior to Britain's. However, German unification in 1871 stimulated consolidation, nationalisation into state-owned companies, and further rapid growth. Unlike the situation in France, the goal was the support of industrialisation, and so heavy lines crisscrossed the Ruhr and other industrial districts and provided good connections to the major ports of Hamburg and Bremen. By 1880, Germany had 9,400 locomotives pulling 43,000 passengers and 30,000 tons of freight, and pulled ahead of France.[208]
During the period 1790–1815, Sweden experienced two parallel economic movements: anagricultural revolutionwith larger agricultural estates, new crops, and farming tools and commercialisation of farming, and aproto industrialisation, with small industries being established in the countryside and with workers switching between agricultural work in summer and industrial production in winter. This led to economic growth benefiting large sections of the population and leading up to a consumption revolution starting in the 1820s. Between 1815 and 1850, the protoindustries developed into more specialised and larger industries. This period witnessed increasing regional specialisation with mining inBergslagen, textile mills in Sjuhäradsbygden, and forestry inNorrland. Several important institutional changes took place in this period, such as free and mandatory schooling introduced in 1842 (as the first country in the world), the abolition of the national monopoly on trade in handicrafts in 1846, and a stock company law in 1848.[209]
From 1850 to 1890, Sweden experienced its "first" Industrial Revolution with a veritable explosion in export, dominated by crops, wood, and steel. Sweden abolished most tariffs and other barriers to free trade in the 1850s and joined the gold standard in 1873. Large infrastructural investments were made during this period, mainly in the expanding railroad network, which was financed in part by the government and in part by private enterprises.[210]From 1890 to 1930, new industries developed with their focus on the domestic market: mechanical engineering, power utilities,papermakingand textile.
The Industrial Revolution began about 1870 asMeiji periodleaders decided to catch up with the West. The government built railroads, improved roads, and inaugurated a land reform program to prepare the country for further development. It inaugurated a new Western-based education system for all young people, sent thousands of students to the United States and Europe, and hired more than 3,000 Westerners to teach modern science, mathematics, technology, and foreign languages in Japan (Foreign government advisors in Meiji Japan).
In 1871, a group of Japanese politicians known as theIwakura Missiontoured Europe and the United States to learn Western ways. The result was a deliberate state-led industrialisation policy to enable Japan to quickly catch up. TheBank of Japan, founded in 1882,[211]used taxes to fund model steel and textile factories. Education was expanded and Japanese students were sent to study in the West.
Modern industry first appeared in textiles, including cotton and especially silk, which was based in home workshops in rural areas.[212]
During the late 18th and early 19th centuries when the UK and parts of Western Europe began to industrialise, the US was primarily an agricultural and natural resource producing and processing economy.[213]The building of roads and canals, the introduction of steamboats and the building of railroads were important for handling agricultural and natural resource products in the large and sparsely populated country of the period.[214][215]
Important American technological contributions during the period of the Industrial Revolution were thecotton ginand the development of a system for makinginterchangeable parts, which was aided by the development of themilling machinein the United States. The development of machine tools and the system of interchangeable parts was the basis for the rise of the US as the world's leading industrial nation in the late 19th century.
Oliver Evansinvented an automated flour mill in the mid-1780s that usedcontrol mechanismsand conveyors so that no labour was needed from the time grain was loaded into the elevator buckets until the flour was discharged into a wagon. This is considered to be the first modernmaterials handling system, an important advance in the progress towardmass production.[43]
The United States originally used horse-powered machinery for small-scale applications such as grain milling, but eventually switched to water power after textile factories began being built in the 1790s. As a result, industrialisation was concentrated inNew Englandand theNortheastern United States, which has fast-moving rivers. The newer water-powered production lines proved more economical than horse-drawn production. In the late 19th century steam-powered manufacturing overtook water-powered manufacturing, allowing the industry to spread to the Midwest.
Thomas Somersand theCabot Brothersfounded theBeverly Cotton Manufactoryin 1787, the first cotton mill in America, the largest cotton mill of its era,[216]and a significant milestone in the research and development of cotton mills in the future. This mill was designed to use horsepower, but the operators quickly learned that the horse-drawn platform was economically unstable, and had economic losses for years. Despite the losses, the Manufactory served as a playground of innovation, both in turning a large amount of cotton, but also developing the water-powered milling structure used in Slater's Mill.[217]
In 1793,Samuel Slater(1768–1835) founded theSlater MillatPawtucket, Rhode Island. He had learned of the new textile technologies as a boy apprentice inDerbyshire, England, and defied laws against the emigration of skilled workers by leaving for New York in 1789, hoping to make money with his knowledge. After founding Slater's Mill, he went on to own 13 textile mills.[218]Daniel Dayestablished a wool carding mill in theBlackstone ValleyatUxbridge, Massachusettsin 1809, the third woollen mill established in the US (The first was inHartford, Connecticut, and the second atWatertown, Massachusetts.). TheJohn H. ChafeeBlackstone River Valley National Heritage Corridorretraces the history of "America's Hardest-Working River', the Blackstone. TheBlackstone Riverand its tributaries, which cover more than 70 kilometres (45 mi) fromWorcester, MassachusettstoProvidence, Rhode Island, was the birthplace of America's Industrial Revolution. At its peak over 1,100 mills operated in this valley, including Slater's Mill, and with it the earliest beginnings of America's industrial and technological development.
Merchant Francis Cabot LowellfromNewburyport, Massachusetts, memorised the design of textile machines on his tour of British factories in 1810. Realising that theWar of 1812had ruined his import business but that demand for domestic finished cloth was emerging in America, on his return to the United States, he set up theBoston Manufacturing Company. Lowell and his partners built America's second cotton-to-cloth textile mill atWaltham, Massachusetts, second to theBeverly Cotton Manufactory. After his death in 1817, his associates built America's first planned factory town, which they named after him. This enterprise was capitalised in apublic stock offering, one of the first uses of it in the United States.Lowell, Massachusetts, usingnine kilometres (5+1⁄2miles) of canals and 7,500 kilowatts (10,000 horsepower) delivered by theMerrimack River, is considered by some as a major contributor to the success of the American Industrial Revolution. The short-lived utopia-likeWaltham-Lowell systemwas formed, as a direct response to the poor working conditions in Britain. However, by 1850, especially following theGreat Famine of Ireland, the system had been replaced by poor immigrant labour.
A major U.S. contribution to industrialisation was the development of techniques to makeinterchangeable partsfrom metal. Precision metal machining techniques were developed by the U.S. Department of War to make interchangeable parts for small firearms. The development work took place at the Federal Arsenals at Springfield Armory and Harpers Ferry Armory. Techniques for precision machining using machine tools included using fixtures to hold the parts in the proper position, jigs to guide the cutting tools and precision blocks and gauges to measure the accuracy. Themilling machine, a fundamental machine tool, is believed to have been invented byEli Whitney, who was a government contractor who built firearms as part of this program. Another important invention was the Blanchard lathe, invented byThomas Blanchard. The Blanchard lathe, or pattern tracing lathe, was actually a shaper that could produce copies of wooden gun stocks. The use of machinery and the techniques for producing standardised and interchangeable parts became known as theAmerican system of manufacturing.[43]
Precision manufacturing techniques made it possible to build machines that mechanised the shoe industry[219]and the watch industry. The industrialisation of the watch industry started in 1854 also in Waltham, Massachusetts, at theWaltham Watch Company, with the development of machine tools, gauges and assembling methods adapted to the micro precision required for watches.
Steelis often cited as the first of several new areas for industrial mass-production, which are said to characterise a "Second Industrial Revolution", beginning around 1850, although a method for mass manufacture ofsteelwas not invented until the 1860s, whenSir Henry Bessemerinvented a new furnace which could convert moltenpig ironinto steel in large quantities. However, it only became widely available in the 1870s after the process was modified to produce more uniform quality.[49][220]Bessemer steel was being displaced by theopen hearth furnacenear the end of the 19th century.
This Second Industrial Revolution gradually grew to include chemicals, mainly thechemical industries,petroleum(refining and distribution), and, in the 20th century, theautomotive industry, and was marked by a transition of technological leadership from Britain to the United States and Germany.
The increasing availability of economical petroleum products also reduced the importance of coal and further widened the potential for industrialisation.
A new revolution began with electricity andelectrificationin theelectrical industries. The introduction ofhydroelectric powergeneration in theAlpsenabled the rapid industrialisation of coal-deprived northern Italy, beginning in the 1890s.
By the 1890s, industrialisation in these areas had created the first giant industrial corporations with burgeoning global interests, as companies likeU.S. Steel,General Electric,Standard OilandBayer AGjoined the railroad and ship companies on the world'sstock markets.
The New Industrialist movement advocates for increasing domestic manufacturing while reducing emphasis on a financial-based economy that relies on real estate and trading speculative assets. New Industrialism has been described as "supply-side progressivism" or embracing the idea of "Building More Stuff".[221]New Industrialism developed after theChina Shockthat resulted in lost manufacturing jobs in the U.S. after China joined theWorld Trade Organizationin 2001. The movement strengthened after the reduction of manufacturing jobs during theGreat Recessionand when the U.S. was not able to manufacture enough tests or facemasks during theCOVID-19 pandemic.[222]New Industrialism calls for building enough housing to satisfy demand in order to reduce the profit inland speculation, to invest ininfrastructure, and to develop advanced technology to manufacturegreen energyfor the world.[222]New Industrialists believe that the United States is not building enough productive capital and should invest more into economic growth.[223]
The causes of the Industrial Revolution were complicated and remain a topic for debate. Geographic factors include Britain's vast mineral resources. In addition to metal ores, Britain had the highest qualitycoalreserves known at the time, as well as abundant water power, highly productive agriculture, and numerous seaports and navigable waterways.[63]
Some historians believe the Industrial Revolution was an outgrowth of social and institutional changes brought by the end offeudalisminBritainafter theEnglish Civil Warin the 17th century, although feudalism began to break down after theBlack Deathof the mid 14th century, followed by other epidemics, until the population reached a low in the 14th century. This created labour shortages and led to falling food prices and a peak in real wages around 1500, after which population growth began reducing wages. After 1540, increasing precious metals supply from the Americas caused coinage debasement (inflation), which caused land rents (often long-term leases that transferred to heirs on death) to fall in real terms.[224]
TheEnclosuremovement and theBritish Agricultural Revolutionmade food production more efficient and less labour-intensive, forcing the farmers who could no longer be self-sufficient in agriculture intocottage industry, for exampleweaving, and in the longer term into the cities and the newly developedfactories.[225]Thecolonial expansionof the 17th century with the accompanying development of international trade, creation offinancial marketsand accumulation ofcapitalare also cited as factors, as is thescientific revolutionof the 17th century.[226]A change in marrying patterns to getting married later made people able to accumulate more human capital during their youth, thereby encouraging economic development.[227]
Until the 1980s, it was universally believed by academic historians that technological innovation was the heart of the Industrial Revolution and the key enabling technology was the invention and improvement of the steam engine.[228]Marketing professor Ronald Fullerton suggested that innovative marketing techniques, business practices, and competition also influenced changes in the manufacturing industry.[229]
Lewis Mumfordhas proposed that the Industrial Revolution had its origins in theEarly Middle Ages, much earlier than most estimates.[230]He explains that the model for standardisedmass productionwas theprinting pressand that "the archetypal model for the industrial era was the clock". He also cites themonasticemphasis on order and time-keeping, as well as the fact thatmedievalcities had at their centre a church with bell ringing at regular intervals as being necessary precursors to a greater synchronisation necessary for later, more physical, manifestations such as the steam engine.
The presence of a large domestic market should also be considered an important driver of the Industrial Revolution, particularly explaining why it occurred in Britain. In other nations, such as France, markets were split up by local regions, which often imposed tolls andtariffson goods traded among them.[231]Internal tariffs were abolished byHenry VIII of England, they survived in Russia until 1753, 1789 in France and 1839 in Spain.
Governments' grant of limitedmonopoliesto inventors under a developingpatentsystem (theStatute of Monopoliesin 1623) is considered an influential factor. The effects of patents, both good and ill, on the development of industrialisation are clearly illustrated in the history of thesteam engine, the key enabling technology. In return for publicly revealing the workings of an invention the patent system rewarded inventors such asJames Wattby allowing them to monopolise the production of the first steam engines, thereby rewarding inventors and increasing the pace of technological development. However, monopolies bring with them their own inefficiencies which may counterbalance, or even overbalance, the beneficial effects of publicising ingenuity and rewarding inventors.[232]Watt's monopoly prevented other inventors, such asRichard Trevithick,William Murdoch, orJonathan Hornblower, whom Boulton and Watt sued, from introducing improved steam engines, thereby retarding the spread ofsteam power.[233][234]
One question of active interest to historians is why the Industrial Revolution occurred in Europe and not in other parts of the world in the 18th century, particularly China,India, and the Middle East (which pioneered in shipbuilding, textile production, water mills, and much more in the period between 750 and 1100[235]), or at other times like inClassical Antiquity[236]or theMiddle Ages.[237]A recent account argued that Europeans have been characterized for thousands of years by a freedom-loving culture originating from the aristocratic societies of early Indo-European invaders.[238]Many historians, however, have challenged this explanation as being not only Eurocentric, but also ignoring historical context. In fact, before the Industrial Revolution, "there existed something of a global economic parity between the most advanced regions in the world economy."[239]These historians have suggested a number of other factors, including education, technological changes[240](seeScientific Revolutionin Europe), "modern" government, "modern" work attitudes, ecology, and culture.[241]
Chinawas the world's most technologically advanced country for many centuries; however, China stagnated economically and technologically and was surpassed by Western Europe before theAge of Discovery, by which time China banned imports and denied entry to foreigners. China was also a totalitarian society. It also taxed transported goods heavily.[242][243]Modern estimates of per capita income in Western Europe in the late 18th century are of roughly 1,500 dollars inpurchasing power parity(and Britain had aper capita incomeof nearly 2,000 dollars[244]) whereas China, by comparison, had only 450 dollars. India was essentially feudal, politically fragmented and not as economically advanced as Western Europe.[245]
Historians such asDavid Landesand sociologistsMax WeberandRodney Starkcredit the different belief systems in Asia and Europe with dictating where the revolution occurred.[246][247]The religion and beliefs of Europe were largely products ofJudaeo-ChristianityandGreekthought. Conversely, Chinese society was founded on men likeConfucius,Mencius,Han Feizi(Legalism),Lao Tzu(Taoism), andBuddha(Buddhism), resulting in very different worldviews.[248]Other factors include the considerable distance of China's coal deposits, though large, from its cities as well as the then unnavigableYellow Riverthat connects these deposits to the sea.[249]
Economic historianJoel Mokyrargued thatpolitical fragmentation, the presence of a large number of European states, made it possible for heterodox ideas to thrive, as entrepreneurs, innovators, ideologues and heretics could easily flee to a neighboring state in the event that the one state would try to suppress their ideas and activities. This is what set Europe apart from the technologically advanced, large unitary empires such as China and India[contradictory]by providing "an insurance against economic and technological stagnation".[250]China had both a printing press and movable type, and India had similar levels of scientific and technological achievement as Europe in 1700, yet the Industrial Revolution would occur in Europe, not China or India. In Europe, political fragmentation was coupled with an "integrated market for ideas" where Europe's intellectuals used thelingua francaof Latin, had a shared intellectual basis in Europe's classical heritage and the pan-European institution of theRepublic of Letters.[251]Political institutions[252]could contribute to the relation betweendemocratization and economic growthduring Great Divergence.[253]
In addition, Europe's monarchs desperately needed revenue, pushing them into alliances with their merchant classes. Small groups of merchants were granted monopolies and tax-collecting responsibilities in exchange for payments to the state. Located in a region "at the hub of the largest and most varied network of exchange in history",[254]Europe advanced as the leader of the Industrial Revolution. In the Americas, Europeans found a windfall of silver, timber, fish, and maize, leading historian Peter Stearns to conclude that "Europe's Industrial Revolution stemmed in great part from Europe's ability to draw disproportionately on world resources."[255]
Modern capitalism originated in theItalian city-statesaround the end of the first millennium. The city-states were prosperous cities that were independent from feudal lords. They were largely republics whose governments were typically composed of merchants, manufacturers, members of guilds, bankers and financiers. The Italian city-states built a network of branch banks in leading western European cities and introduceddouble entry bookkeeping. Italian commerce was supported by schools that taught numeracy in financial calculations throughabacusschools.[247]
Great Britain provided the legal and cultural foundations that enabled entrepreneurs to pioneer the Industrial Revolution.[256]Key factors fostering this environment were:
"An unprecedented explosion of new ideas, and new technological inventions, transformed our use of energy, creating an increasingly industrial and urbanised country. Roads, railways and canals were built. Great cities appeared. Scores of factories and mills sprang up. Our landscape would never be the same again. It was a revolution that transformed not only the country, but the world itself."
There were two main values that drove the Industrial Revolution in Britain. These values were self-interest and anentrepreneurialspirit. Because of these interests, many industrial advances were made that resulted in a huge increase in personal wealth and aconsumerrevolution.[130]These advancements also greatly benefitted British society as a whole. Countries around the world started to recognise the changes and advancements in Britain and use them as an example to begin their own Industrial Revolutions.[10]
A debate sparked by Trinidadian politician and historianEric Williamsin his workCapitalism and Slavery(1944) concerned the role ofslaveryin financing the Industrial Revolution. Williams argued that European capital amassed from slavery was vital in the early years of the revolution, contending that the rise of industrial capitalism was the driving force behindabolitionisminstead ofhumanitarianmotivations. These arguments led to significanthistoriographicaldebates among historians, with American historianSeymour Dreschercritiquing Williams' arguments inEconocide(1977).[257]
Instead, the greaterliberalisationof trade from a large merchant base may have allowed Britain to produce and use emerging scientific and technological developments more effectively than countries with stronger monarchies, particularly China and Russia. Britain emerged from theNapoleonic Warsas the only European nation not ravaged by financial plunder and economic collapse, and having the only merchant fleet of any useful size (European merchant fleets were destroyed during the war by theRoyal Navy[a]). Britain's extensive exporting cottage industries also ensured markets were already available for many early forms of manufactured goods. The conflict resulted in most British warfare being conducted overseas, reducing the devastating effects of territorial conquest that affected much of Europe. This was further aided by Britain's geographical position—an island separated from the rest of mainland Europe.
Another theory is that Britain was able to succeed in the Industrial Revolution due to the availability of key resources it possessed. It had a dense population for its small geographical size.Enclosureof common land and the related agricultural revolution made a supply of this labour readily available. There was also a local coincidence of natural resources in theNorth of England, theEnglish Midlands,South Walesand theScottish Lowlands. Local supplies of coal, iron, lead, copper, tin, limestone and water power resulted in excellent conditions for the development and expansion of industry. Also, the damp, mild weather conditions of the North West of England provided ideal conditions for the spinning of cotton, providing a natural starting point for the birth of the textiles industry.
The stable political situation in Britain from around 1689 following theGlorious Revolution, and British society's greater receptiveness to change (compared with other European countries) can also be said to be factors favouring the Industrial Revolution. Peasant resistance to industrialisation was largely eliminated by the Enclosure movement, and the landed upper classes developed commercial interests that made them pioneers in removing obstacles to the growth of capitalism.[259](This point is also made inHilaire Belloc'sThe Servile State.)
The French philosopherVoltairewrote about capitalism and religious tolerance in his book on English society,Letters on the English(1733), noting why England at that time was more prosperous in comparison to the country's less religiously tolerant European neighbours. "Take a view of theRoyal Exchange in London, a place more venerable than many courts of justice, where the representatives of all nations meet for the benefit of mankind. There the Jew, the Mahometan [Muslim], and the Christian transact together, as though they all professed the same religion, and give the name of infidel to none but bankrupts. There the Presbyterian confides in the Anabaptist, and the Churchman depends on the Quaker's word. If one religion only were allowed in England, the Government would very possibly become arbitrary; if there were but two, the people would cut one another's throats; but as there are such a multitude, they all live happy and in peace."[260]
Britain's population grew 280% from 1550 to 1820, while the rest of Western Europe grew 50–80%. Seventy percent of European urbanisation happened in Britain from 1750 to 1800. By 1800, only the Netherlands was more urbanised than Britain. This was only possible because coal, coke, imported cotton, brick and slate had replaced wood, charcoal, flax, peat and thatch. The latter compete with land grown to feed people while mined materials do not. Yet more land would be freed when chemical fertilisers replaced manure and horse's work was mechanised. A workhorse needs 1.2 to 2.0 ha (3 to 5 acres) for fodder while even early steam engines produced four times more mechanical energy.
In 1700, five-sixths of the coal mined worldwide was in Britain, while the Netherlands had none; so despite having Europe's best transport, lowest taxes, and most urbanised, well-paid, and literate population, it failed to industrialise. In the 18th century, it was the only European country whose cities and population shrank. Without coal, Britain would have run out of suitable river sites for mills by the 1830s.[261]Based on science and experimentation from the continent, the steam engine was developed specifically for pumping water out of mines, many of which in Britain had been mined to below the water table. Although extremely inefficient they were economical because they used unsaleable coal.[262]Iron rails were developed to transport coal, which was a major economic sector in Britain.
Economic historianRobert Allenhas argued that high wages, cheap capital and very cheap energy in Britain made it the ideal place for the industrial revolution to occur.[263]These factors made it vastly more profitable to invest in research and development, and to put technology to use in Britain than other societies.[263]However, two 2018 studies inThe Economic History Reviewshowed that wages were not particularly high in the Britishspinningsector or the construction sector, casting doubt on Allen's explanation.[264][265]A 2022 study in theJournal of Political Economyby Morgan Kelly,Joel Mokyr, andCormac O Gradafound that industrialization happened in areas with low wages and high mechanical skills, whereas literacy, banks and proximity to coal had little explanatory power.[266]
Knowledge of innovation was spread by several means. Workers who were trained in the technique might move to another employer or might be poached. A common method was for someone to make a study tour, gathering information where he could. During the whole of the Industrial Revolution and for the century before, all European countries and America engaged in study-touring; some nations, like Sweden and France, even trained civil servants or technicians to undertake it as a matter of state policy. In other countries, notably Britain and America, this practice was carried out by individual manufacturers eager to improve their own methods. Study tours were common then, as now, as was the keeping of travel diaries. Records made by industrialists and technicians of the period are an incomparable source of information about their methods.
Another means for the spread of innovation was by the network of informal philosophical societies, like theLunar Society of Birmingham, in which members met to discuss natural philosophy and often its application to manufacturing. The Lunar Society flourished from 1765 to 1809, and it has been said of them, "They were, if you like, the revolutionary committee of that most far reaching of all the eighteenth-century revolutions, the Industrial Revolution".[267]Other such societies published volumes of proceedings and transactions. For example, the London-basedRoyal Society of Artspublished an illustrated volume of new inventions, as well as papers about them in its annualTransactions.
There were publications describing technology.Encyclopaediassuch asHarris'sLexicon Technicum(1704) andAbraham Rees'sCyclopaedia(1802–1819) contain much of value.Cyclopaediacontains an enormous amount of information about the science and technology of the first half of the Industrial Revolution, very well illustrated by fine engravings. Foreign printed sources such as theDescriptions des Arts et Métiersand Diderot'sEncyclopédieexplained foreign methods with fine engraved plates.
Periodical publications about manufacturing and technology began to appear in the last decade of the 18th century, and many regularly included notice of the latest patents. Foreign periodicals, such as theAnnales des Mines, published accounts of travels made by French engineers who observed British methods on study tours.
Another theory is that the British advance was due to the presence of anentrepreneurialclass which believed in progress, technology and hard work.[268]The existence of this class is often linked to theProtestant work ethic(seeMax Weber) and the particular status of theBaptistsand the dissenting Protestant sects, such as theQuakersandPresbyteriansthat had flourished with theEnglish Civil War. Reinforcement of confidence in the rule of law, which followed establishment of the prototype of constitutional monarchy in Britain in theGlorious Revolutionof 1688, and the emergence of a stable financial market there based on the management of thenational debtby theBank of England, contributed to the capacity for, and interest in, private financial investment in industrial ventures.[269]
Dissentersfound themselves barred or discouraged from almost all public offices, as well as education at England'sonly two universitiesat the time (although dissenters were still free to study at Scotland'sfour universities). When the restoration of the monarchy took place and membership in the officialAnglican Churchbecame mandatory due to theTest Act, they thereupon became active in banking, manufacturing and education. TheUnitarians, in particular, were very involved in education, by running Dissenting Academies, where, in contrast to the universities of Oxford and Cambridge and schools such as Eton and Harrow, much attention was given to mathematics and the sciences – areas of scholarship vital to the development of manufacturing technologies.
Historians sometimes consider this social factor to be extremely important, along with the nature of the national economies involved. While members of these sects were excluded from certain circles of the government, they were considered fellow Protestants, to a limited extent, by many in themiddle class, such as traditional financiers or other businessmen. Given this relative tolerance and the supply of capital, the natural outlet for the more enterprising members of these sects would be to seek new opportunities in the technologies created in the wake of the scientific revolution of the 17th century.
The industrial revolution has been criticised for causingecosystem collapse, mental illness, pollution and detrimental social systems.[270][271]It has also been criticised for valuingprofitsand corporate growth over life andwellbeing. Multiple movements have arisen which reject aspects of the industrial revolution, such as theAmishorprimitivists.[272]
Humanists and individualists criticise the Industrial revolution for mistreating women and children and turning men into work machines that lackedautonomy.[273]Critics of the Industrial revolution promoted a more interventionist state and formed new organisations to promote human rights.[274]
Primitivismargues that the Industrial Revolution has created an unnatural frame of society and the world in which humans need to adapt to an unnatural urban landscape in which humans are perpetual cogs without personal autonomy.[275]
Certain primitivists argue for a return to pre-industrial society,[276]while others argue that technology such asmodern medicine, andagriculture[277]are all positive for humanity assuming they are controlled by and serve humanity and have no effect on the natural environment.
The Industrial Revolution has been criticised for leading to immense ecological and habitat destruction. It has led to immense decrease in thebiodiversityof life on Earth. The Industrial revolution has been said to be inherently unsustainable and will lead to eventualcollapse of society, mass hunger, starvation, andresource scarcity.[278]
TheAnthropoceneis a proposedepochor mass extinction coming from humanity (anthropo-is the Greek root forhumanity). Since the start of the Industrial revolution humanity has permanently changed the Earth, such as immense decrease in biodiversity, andmass extinctioncaused by the Industrial revolution. The effects include permanent changes to the Earth'satmosphereand soil,biosphere, andclimate. Most organisms are unable to adapt leading tomass extinctionwith the remaining undergoingevolutionary rescue, as a result of the Industrial revolution.
Permanent changes in the distribution of organisms from human influence will become identifiable in thegeologic record. Researchers have documented the movement of many species into regions formerly too cold for them, often at rates faster than initially expected.[279]This has occurred in part as a result of changing climate, but also in response to farming and fishing, and to the accidental introduction of non-native species to new areas through global travel.[280]The ecosystem of the entireBlack Seamay have changed during the last 2000 years as a result of nutrient and silica input from eroding deforested lands along theDanube River.[281]
During the Industrial Revolution, an intellectual and artistic hostility towards the new industrialisation developed, associated with the Romantic movement. Romanticism revered the traditionalism of rural life and recoiled against the upheavals caused by industrialisation, urbanisation and the wretchedness of the working classes.[282]Its major exponents in English included the artist and poetWilliam Blakeand poetsWilliam Wordsworth,Samuel Taylor Coleridge,John Keats,Lord ByronandPercy Bysshe Shelley.
The movement stressed the importance of "nature" in art and language, in contrast to "monstrous" machines and factories; the "Dark satanic mills" of Blake's poem "And did those feet in ancient time".[283]Mary Shelley'sFrankensteinreflected concerns that scientific progress might be two-edged. French Romanticism likewise was highly critical of industry.[284]
William Shakespeare[a](c.23 April 1564[b]– 23 April 1616)[c]was an English playwright, poet and actor. He is widely regarded as the greatest writer in the English language and the world's pre-eminent dramatist. He is often called England'snational poetand the "BardofAvon" or simply "the Bard". His extant works, includingcollaborations, consist of some39 plays,154 sonnets, three longnarrative poemsand a few other verses, some of uncertain authorship. His playshave been translatedinto every majorliving languageand are performed more often than those of any other playwright. Shakespeare remains arguably the most influential writer in the English language, and his works continue to be studied and reinterpreted.
Shakespeare was born and raised inStratford-upon-Avon, Warwickshire. At the age of 18, he marriedAnne Hathaway, with whom he had three children:Susanna, and twinsHamnetandJudith. Sometime between 1585 and 1592, he began a successful career in London as an actor, writer, andpart-owner("sharer") of aplaying companycalled theLord Chamberlain's Men, later known as theKing's Menafter the ascension ofKing James VI of Scotlandto the English throne. At age 49 (around 1613), he appears to have retired to Stratford, where he died three years later. Few records of Shakespeare's private life survive; this has stimulated considerable speculation about such matters ashis physical appearance,his sexuality,his religious beliefsand even certainfringe theoriesas to whether the works attributed to him werewritten by others.
Shakespeare produced most of his known works between 1589 and 1613. His early plays were primarilycomediesandhistoriesand are regarded as some of the best works produced in these genres. He then wrote mainlytragediesuntil 1608, among themHamlet,Othello,King LearandMacbeth, all considered to be among the finest works in English. In the last phase of his life, he wrotetragicomedies(also known asromances) such asThe Winter's TaleandThe Tempest, and collaborated with other playwrights.
Many of Shakespeare's plays were published in editions of varying quality and accuracy during his lifetime. However, in 1623,John HemingesandHenry Condell, two fellow actors and friends of Shakespeare's, published a more definitive text known as theFirst Folio, a posthumous collected edition of Shakespeare's dramatic works that includes 36 of his plays. Its Preface includes a prescient poem byBen Jonson, a former rival of Shakespeare, who hailed Shakespeare with the now famous epithet: "not of an age, but for all time".
Shakespeare was the son ofJohn Shakespeare, analdermanand a successful glover (glove-maker) originally fromSnitterfieldinWarwickshire, andMary Arden, the daughter of anaffluent landowning family.[3]He was born inStratford-upon-Avon, where he wasbaptisedon 26 April 1564. His date of birth is unknown but is traditionally observed on 23 April,Saint George's Day.[1]This date, which can be traced toWilliam OldysandGeorge Steevens, has proved appealing to biographers because Shakespeare died on the same date in 1616.[4][5]He was the third of eight children, and the eldest surviving son.[6]
Although no attendance records for the period survive, most biographers agree that Shakespeare was probably educated at theKing's New Schoolin Stratford,[7][8][9]a free school chartered in 1553,[10]about a quarter-mile (400 m) from his home.Grammar schoolsvaried in quality during theElizabethan era, but grammar school curricula were largely similar: the basicLatintext was standardised by royal decree,[11][12]and the school would have provided an intensive education in grammar based upon Latinclassicalauthors.[13]
At the age of 18, Shakespeare married 26-year-oldAnne Hathaway. Theconsistory courtof theDiocese of Worcesterissued a marriage licence on 27 November 1582. The next day, two of Hathaway's neighbours posted bonds guaranteeing that no lawful claims impeded the marriage.[14]The ceremony may have been arranged in some haste since the Worcesterchancellorallowed themarriage bannsto be read once instead of the usual three times,[15][16]and six months after the marriage Anne gave birth to a daughter,Susanna, baptised 26 May 1583.[17]Twins, sonHamnetand daughterJudith, followed almost two years later and were baptised 2 February 1585.[18]Hamnet died of unknown causes at the age of 11 and was buried 11 August 1596.[19]
After the birth of the twins, Shakespeare left few historical traces until he is mentioned as part of the London theatre scene in 1592. The exception is the appearance of his name in the "complaints bill" of a law case before the Queen's Bench court at Westminster datedMichaelmas Term1588 and 9 October 1589.[20]Scholars refer to the years between 1585 and 1592 as Shakespeare's "lost years".[21]Biographers attempting to account for this period have reported manyapocryphalstories.Nicholas Rowe, Shakespeare's first biographer, recounted a Stratford legend that Shakespeare fled the town for London to escape prosecution for deerpoachingin the estate of local squireThomas Lucy. Shakespeare is also supposed to have taken his revenge on Lucy by writing a scurrilous ballad about him.[22][23]Another 18th-century story has Shakespeare starting his theatrical career minding the horses of theatre patrons in London.[24]John Aubreyreported that Shakespeare had been a country schoolmaster.[25]Some 20th-century scholars suggested that Shakespeare may have been employed as a schoolmaster by Alexander Hoghton ofLancashire, a Catholic landowner who named a certain "William Shakeshafte" in his will.[26][27]Little evidence substantiates such stories other thanhearsaycollected after his death, and Shakeshafte was a common name in the Lancashire area.[28][29]
It is not known definitively when Shakespeare began writing, but contemporary allusions and records of performances show that several of his plays were on the London stage by 1592.[30]By then, he was sufficiently known in London to be attacked in print by the playwrightRobert Greenein hisGroats-Worth of Witfrom that year:
... there is an upstart Crow, beautified with our feathers, that with hisTiger's heart wrapped in a Player's hide, supposes he is as well able to bombast out a blank verse as the best of you: and being an absoluteJohannes factotum, is in his own conceit the only Shake-scene in a country.[31]
Scholars differ on the exact meaning of Greene's words,[31][32]but most agree that Greene was accusing Shakespeare of reaching above his rank in trying to match such university-educated writers asChristopher Marlowe,Thomas Nashe, and Greene himself (the so-called "University Wits").[33]The italicised phrase parodying the line "Oh, tiger's heart wrapped in a woman's hide" from Shakespeare'sHenry VI, Part 3, along with the pun "Shake-scene", clearly identify Shakespeare as Greene's target. As used here,Johannes Factotum("Jack of all trades") refers to a second-rate tinkerer with the work of others, rather than the more common "universal genius".[31][34]
Greene's attack is the earliest surviving mention of Shakespeare's work in the theatre. Biographers suggest that his career may have begun any time from the mid-1580s to just before Greene's remarks.[35][36][37]After 1594, Shakespeare's plays were performed atThe Theatre, inShoreditch, only by theLord Chamberlain's Men, a company owned by a group of players, including Shakespeare, that soon became the leadingplaying companyin London.[38]After the death ofQueen Elizabethin 1603, the company was awarded a royal patent by the newKing James I, and changed its name to theKing's Men.[39]
All the world's a stage,and all the men and women merely players:they have their exits and their entrances;and one man in his time plays many parts ...
In 1599, a partnership of members of the company built their own theatre on the south bank of theRiver Thames, which they named theGlobe. In 1608, the partnership also took over theBlackfriars indoor theatre. Extant records of Shakespeare's property purchases and investments indicate that his association with the company made him a wealthy man,[41]and in 1597, he bought the second-largest house in Stratford,New Place, and in 1605, invested in a share of the parishtithesin Stratford.[42]
Some of Shakespeare's plays were published inquartoeditions, beginning in 1594, and by 1598, his name had become a selling point and began to appear on thetitle pages.[43][44][45]Shakespeare continued to act in his own and other plays after his success as a playwright. The 1616 edition ofBen Jonson'sWorksnames him on the cast lists forEvery Man in His Humour(1598) andSejanus His Fall(1603).[46]The absence of his name from the 1605 cast list for Jonson'sVolponeis taken by some scholars as a sign that his acting career was nearing its end.[35]TheFirst Folioof 1623, however, lists Shakespeare as one of "the Principal Actors in all these Plays", some of which were first staged afterVolpone, although one cannot know for certain which roles he played.[47]In 1610,John Davies of Herefordwrote that "good Will" played "kingly" roles.[48]In 1709, Rowe passed down a tradition that Shakespeare played the ghost of Hamlet's father.[49]Later traditions maintain that he also played Adam inAs You Like It, and the Chorus inHenry V,[50][51]though scholars doubt the sources of that information.[52]
Throughout his career, Shakespeare divided his time between London and Stratford. In 1596, the year before he bought New Place as his family home in Stratford, Shakespeare was living in the parish ofSt Helen's,Bishopsgate, north of the River Thames.[53][54]He moved across the river toSouthwarkby 1599, the same year his company constructed the Globe Theatre there.[53][55]By 1604, he had moved north of the river again, to an area north ofSt Paul's Cathedralwith many fine houses. There, he rented rooms from a FrenchHuguenotnamed Christopher Mountjoy, a maker of women's wigs and other headgear.[56][57]
Nicholas Rowewas the first biographer to record the tradition, repeated bySamuel Johnson, that Shakespeare retired to Stratford "some years before his death".[58][59]He was still working as an actor in London in 1608; in an answer to the sharers' petition in 1635,Cuthbert Burbagestated that after purchasing the lease of theBlackfriars Theatrein 1608 fromHenry Evans, the King's Men "placed men players" there, "which wereHeminges,Condell, Shakespeare, etc.".[60]However, it is perhaps relevant that thebubonic plagueraged in London throughout 1609.[61][62]The London public playhouses were repeatedly closed during extended outbreaks of the plague (a total of over 60 months closure between May 1603 and February 1610),[63]which meant there was often no acting work. Retirement from all work was uncommon at that time.[64]Shakespeare continued to visit London during the years 1611–1614.[58]In 1612, he was called as a witness inBellott v Mountjoy, a court case concerning the marriage settlement of Mountjoy's daughter, Mary.[65][66]In March 1613, he bought agatehousein the formerBlackfriarspriory;[67]and from November 1614, he was in London for several weeks with his son-in-law,John Hall.[68]After 1610, Shakespeare wrote fewer plays, and none are attributed to him after 1613.[69]His last three plays were collaborations, probably withJohn Fletcher,[70]who succeeded him as the house playwright of the King's Men. He retired in 1613, before theGlobe Theatreburned down during the performance ofHenry VIIIon 29 June.[69]
Shakespeare died on 23 April 1616, at the age of 52.[e]He died within a month of signing his will, a document which he begins by describing himself as being in "perfect health". No extant contemporary source explains how or why he died. Half a century later,John Ward, the vicar of Stratford, wrote in his notebook: "Shakespeare, Drayton, and Ben Jonson had a merry meeting and, it seems, drank too hard, for Shakespeare died of a fever there contracted",[72][73]not an impossible scenario since Shakespeare knew Jonson andDrayton. Of thetributesfrom fellow authors, one refers to his relatively sudden death: "We wondered, Shakespeare, that thou went'st so soon / From the world's stage to the grave's tiring room."[74][f]
He was survived by his wife and two daughters. Susanna had married a physician, John Hall, in 1607,[75]and Judith had marriedThomas Quiney, avintner, two months before Shakespeare's death.[76]Shakespeare signed his last will and testament on 25 March 1616; the following day, Thomas Quiney, his new son-in-law, was found guilty of fathering an illegitimate son by Margaret Wheeler, both of whom had died during childbirth. Thomas was ordered by the church court to do public penance, which would have caused much shame and embarrassment for the Shakespeare family.[76]
Shakespeare bequeathed the bulk of his large estate to his elder daughter Susanna[77]under stipulations that she pass it down intact to "the first son of her body".[78]The Quineys had three children, all of whom died without marrying.[79][80]The Halls had one child, Elizabeth, who married twice but died without children in 1670, ending Shakespeare's direct line.[81][82]Shakespeare's will scarcely mentions his wife, Anne, who was probably entitled to one-third of his estate automatically.[g]He did make a point, however, of leaving her "my second best bed", a bequest that has led to much speculation.[84][85][86]Some scholars see the bequest as an insult to Anne, whereas others believe that the second-best bed would have been the matrimonial bed and therefore rich in significance.[87]
Shakespeare was buried in thechancelof theHoly Trinity Churchtwo days after his death.[88][89]The epitaph carved into the stone slab covering his grave includes a curse against moving his bones, which was carefully avoided during restoration of the church in 2008:[90]
Good frend for Iesvs sake forbeare,To digg the dvst encloased heare.Bleste be yͤ man yͭ spares thes stones,And cvrst be he yͭ moves my bones.[91][h]
Good friend, for Jesus' sake forbear,To dig the dust enclosed here.Blessed be the man that spares these stones,And cursed be he that moves my bones.
Some time before 1623, afunerary monumentwas erected in his memory on the north wall, with a half-effigy of him in the act of writing. Its plaque compares him toNestor,Socrates, andVirgil.[92]In 1623, in conjunction with the publication of theFirst Folio, theDroeshout engravingwas published.[93]Shakespeare has been commemorated in manystatues and memorialsaround the world, including funeral monuments inSouthwark CathedralandPoets' CornerinWestminster Abbey.[94][95]
Most playwrights of the period typically collaborated with others at some point, as critics agree Shakespeare did, mostly early and late in his career.[96]
The first recorded works of Shakespeare areRichard IIIand the three parts ofHenry VI, written in the early 1590s during a vogue forhistorical drama. Shakespeare's plays are difficult to date precisely, however,[97][98]and studies of the texts suggest thatTitus Andronicus,The Comedy of Errors,The Taming of the Shrew,andThe Two Gentlemen of Veronamay also belong to Shakespeare's earliest period.[99][97]His firsthistories, which draw heavily on the 1587 edition of Raphael Holinshed'sChronicles of England, Scotland, and Ireland,[100]dramatise the destructive results of weak or corrupt rule and have been interpreted as a justification for the origins of theTudor dynasty.[101]The early plays were influenced by the works of other Elizabethan dramatists, especiallyThomas KydandChristopher Marlowe, by the traditions of medieval drama, and by the plays ofSeneca.[102][103][104]The Comedy of Errorswas also based on classical models, but no source forThe Taming of the Shrewhas been found, though it has an identical plot but different wording as another play with a similar name.[105][106]LikeThe Two Gentlemen of Verona, in which two friends appear to approve of rape,[107][108][109]theShrew's story of the taming of a woman's independent spirit by a man sometimes troubles modern critics, directors, and audiences.[110]
Shakespeare's early classical and Italianate comedies, containing tight double plots and precise comic sequences, give way in the mid-1590s to the romantic atmosphere of his most acclaimed comedies.[111]A Midsummer Night's Dreamis a witty mixture of romance, fairy magic, and comic lowlife scenes.[112]Shakespeare's next comedy, the equally romanticThe Merchant of Venice, contains a portrayal of the vengeful Jewish moneylenderShylock, which reflects dominant Elizabethan views but may appear derogatory to modern audiences.[113][114]The wit and wordplay ofMuch Ado About Nothing,[115]the charming rural setting ofAs You Like It, and the lively merrymaking ofTwelfth Nightcomplete Shakespeare's sequence of great comedies.[116]After the lyricalRichard II, written almost entirely in verse, Shakespeare introduced prose comedy into the histories of the late 1590s,Henry IV, Part 1and2, andHenry V.Henry IVfeaturesFalstaff, rogue, wit and friend of Prince Hal. His characters become more complex and tender as he switches deftly between comic and serious scenes, prose and poetry, and achieves the narrative variety of his mature work.[117][118][119]This period begins and ends with two tragedies:Romeo and Juliet, the famous romantic tragedy of sexually charged adolescence, love, and death;[120][121]andJulius Caesar—based on SirThomas North's 1579 translation ofPlutarch'sParallel Lives—which introduced a new kind of drama.[122][123]According to Shakespearean scholarJames Shapiro, inJulius Caesar, "the various strands of politics, character, inwardness, contemporary events, even Shakespeare's own reflections on the act of writing, began to infuse each other".[124]
In the early 17th century, Shakespeare wrote the so-called "problem plays"Measure for Measure,Troilus and Cressida, andAll's Well That Ends Welland a number of his best knowntragedies.[125][126]Many critics believe that Shakespeare's tragedies represent the peak of his art.Hamlethas probably been analysed more than any other Shakespearean character, especially for his famoussoliloquywhich begins "To be or not to be; that is the question".[127]Unlike the introverted Hamlet, whose fatal flaw is hesitation,Othelloand Lear are undone by hasty errors of judgement.[128]The plots of Shakespeare's tragedies often hinge on such fatal errors or flaws, which overturn order and destroy the hero and those he loves.[129]InOthello,Iagostokes Othello's sexual jealousy to the point where he murders the innocent wife who loves him.[130][131]InKing Lear, the old king commits the tragic error of giving up his powers, initiating the events which lead to the torture and blinding of the Earl of Gloucester and the murder of Lear's youngest daughter,Cordelia. According to the criticFrank Kermode, "the play...offers neither its good characters nor its audience any relief from its cruelty".[132][133][134]InMacbeth, the shortest and most compressed of Shakespeare's tragedies,[135]uncontrollable ambition incites Macbeth and his wife,Lady Macbeth, to murder the rightful king and usurp the throne until their own guilt destroys them in turn.[136]In this play, Shakespeare adds a supernatural element to the tragic structure. His last major tragedies,Antony and CleopatraandCoriolanus, contain some of Shakespeare's finest poetry and were considered his most successful tragedies by the poet and criticT. S. Eliot.[137][138][139]Eliot wrote, "Shakespeare acquired more essential history from Plutarch than most men could from the wholeBritish Museum."[140]
In his final period, Shakespeare turned toromanceortragicomedyand completed three more major plays:Cymbeline,The Winter's Tale,andThe Tempest, as well as the collaboration,Pericles, Prince of Tyre. Less bleak than the tragedies, these four plays are graver in tone than the comedies of the 1590s, but they end with reconciliation and the forgiveness of potentially tragic errors.[141]Some commentators have seen this change in mood as evidence of a more serene view of life on Shakespeare's part, but it may merely reflect the theatrical fashion of the day.[142][143][144]Shakespeare collaborated on two further surviving plays,Henry VIIIandThe Two Noble Kinsmen, probably withJohn Fletcher.[145]
Shakespeare's works include the 36 plays printed in theFirst Folioof 1623, listed according to their folio classification ascomedies,histories, andtragedies.[146]Two plays not included in the First Folio,[147]The Two Noble KinsmenandPericles, Prince of Tyre, are now accepted as part of the canon, with today's scholars agreeing that Shakespeare made major contributions to the writing of both.[148][149]No Shakespearean poems were included in the First Folio, partly because the collection was compiled by men of theatre.[150]
In the late 19th century,Edward Dowdenclassified four of the late comedies asromances, and though many scholars prefer to call themtragicomedies, Dowden's term is often used.[151][152]In 1896,Frederick S. Boascoined the term "problem plays" to describe four plays:All's Well That Ends Well,Measure for Measure,Troilus and Cressida,andHamlet.[153]"Dramas as singular in theme and temper cannot be strictly called comedies or tragedies", he wrote. "We may, therefore, borrow a convenient phrase from the theatre of today and class them together as Shakespeare's problem plays."[154]The term, much debated and sometimes applied to other plays, remains in use, thoughHamletis definitively classed as a tragedy.[155][156][157]
It is not clear for which companies Shakespeare wrote his early plays. The title page of the 1594 edition ofTitus Andronicusreveals that the play had been acted by three different troupes.[158]After theplaguesof 1592–93, Shakespeare's plays were performed by his own company atThe Theatreand theCurtaininShoreditch, north of the Thames.[159]Londoners flocked there to see the first part ofHenry IV,Leonard Diggesrecording, "Let but Falstaff come, Hal, Poins, the rest ... and you scarce shall have a room".[160]When the company found themselves in dispute with their landlord, they pulled The Theatre down and used the timbers to construct theGlobe Theatre, the first playhouse built by actors for actors, on the south bank of the Thames atSouthwark.[161][162]The Globe opened in autumn 1599, withJulius Caesarone of the first plays staged. Most of Shakespeare's greatest post-1599 plays were written for the Globe, includingHamlet,Othello,andKing Lear.[161][163][164]
After the Lord Chamberlain's Men were renamed theKing's Menin 1603, they entered a special relationship with the newKing James. Although the performance records are patchy, the King's Men performed seven of Shakespeare's plays at court between 1 November 1604, and 31 October 1605, including two performances ofThe Merchant of Venice.[51]After 1608, they performed at the indoorBlackfriars Theatreduring the winter and the Globe during the summer.[165]The indoor setting, combined with theJacobeanfashion for lavishly stagedmasques, allowed Shakespeare to introduce more elaborate stage devices. InCymbeline, for example,Jupiterdescends "in thunder and lightning, sitting upon an eagle: he throws a thunderbolt. The ghosts fall on their knees."[166][167]
The actors in Shakespeare's company included the famousRichard Burbage,William Kempe,Henry CondellandJohn Heminges. Burbage played the leading role in the first performances of many of Shakespeare's plays, includingRichard III,Hamlet,Othello, andKing Lear.[168]The popular comic actor Will Kempe played the servant Peter inRomeo and JulietandDogberryinMuch Ado About Nothing, among other characters.[169][170]He was replaced around 1600 byRobert Armin, who played roles such asTouchstoneinAs You Like Itand the fool inKing Lear.[171]In 1613, SirHenry Wottonrecorded thatHenry VIII"was set forth with many extraordinary circumstances of pomp and ceremony".[172]On 29 June, however, a cannon set fire to the thatch of the Globe and burned the theatre to the ground, an event which pinpoints the date of a Shakespeare play with rare precision.[172]
In 1623,John HemingesandHenry Condell, two of Shakespeare's friends from the King's Men, published theFirst Folio, a collected edition of Shakespeare's plays. It contained 36 texts, including 18 printed for the first time.[173]Most of the others had already appeared inquartoversions—flimsy books made from sheets of paper folded twice to make four leaves.[174][175]No evidence suggests that Shakespeare approved these editions, which the First Folio describes as "stol'n and surreptitious copies".[176]
Alfred Pollardtermed some of the pre-1623 versions as "bad quartos" because of their adapted, paraphrased or garbled texts, which may in places have been reconstructed from memory.[174][176][177]Where several versions of a play survive, eachdiffers from the others. The differences may stem from copying orprintingerrors, from notes by actors or audience members, or from Shakespeare's ownpapers.[178][179]In some cases, for example,Hamlet,Troilus and Cressida,andOthello, Shakespeare could have revised the texts between the quarto and folio editions. In the case ofKing Lear, however, while most modern editions do conflate them, the 1623 folio version is so different from the 1608 quarto that theOxford Shakespeareprints them both, arguing that they cannot be conflated without confusion.[180]
In 1593 and 1594, when the theatres were closed because ofplague, Shakespeare published two narrative poems on sexual themes,Venus and AdonisandThe Rape of Lucrece. He dedicated them toHenry Wriothesley, Earl of Southampton. InVenus and Adonis, an innocentAdonisrejects the sexual advances ofVenus; while inThe Rape of Lucrece, the virtuous wifeLucreceis raped by the lustfulTarquin.[181]Influenced byOvid'sMetamorphoses,[182]the poems show the guilt and moral confusion that result from uncontrolled lust.[183]Both proved popular and were often reprinted during Shakespeare's lifetime. A third narrative poem,A Lover's Complaint, in which a young woman laments her seduction by a persuasive suitor, was printed in the first edition of theSonnetsin 1609. Most scholars now accept that Shakespeare wroteA Lover's Complaint. Critics consider that its fine qualities are marred by leaden effects.[184][185][186]The Phoenix and the Turtle, printed in Robert Chester's 1601Love's Martyr, mourns the deaths of the legendaryphoenixand his lover, the faithfulturtle dove. In 1599, two early drafts of sonnets 138 and 144 appeared inThe Passionate Pilgrim, published under Shakespeare's name but without his permission.[184][186][187]
Published in 1609, theSonnetswere the last of Shakespeare's non-dramatic works to be printed. Scholars are not certain when each of the 154 sonnets was composed, but evidence suggests that Shakespeare wrote sonnets throughout his career for a private readership.[188][189]Even before the two unauthorised sonnets appeared inThe Passionate Pilgrimin 1599,Francis Mereshad referred in 1598 to Shakespeare's "sugred Sonnets among his private friends".[190]Few analysts believe that the published collection follows Shakespeare's intended sequence.[191]He seems to have planned two contrasting series: one about uncontrollable lust for a married woman of dark complexion (the "dark lady"), and one about conflicted love for a fair young man (the "fair youth"). It remains unclear if these figures represent real individuals, or if the authorial "I" who addresses them represents Shakespeare himself, thoughWordsworthbelieved that with the sonnets "Shakespeare unlocked his heart".[190][189]
Shall I compare thee to a summer's day?Thou art more lovely and more temperate ...
The 1609 edition was dedicated to a "Mr. W.H.", credited as "the only begetter" of the poems. It is not known whether this was written by Shakespeare himself or by the publisher,Thomas Thorpe, whose initials appear at the foot of the dedication page; nor is it known who Mr. W.H. was, despite numerous theories, or whether Shakespeare even authorised the publication.[193]Critics praise theSonnetsas a profound meditation on the nature of love, sexual passion, procreation, death, and time.[194]
Shakespeare's first plays were written in the conventional style of the day. He wrote them in a stylised language that does not always spring naturally from the needs of the characters or the drama.[195]The poetry depends on extended, sometimes elaborate metaphors and conceits, and the language is often rhetorical—written for actors to declaim rather than speak. The grand speeches inTitus Andronicus, in the view of some critics, often hold up the action, for example; and the verse inThe Two Gentlemen of Veronahas been described as stilted.[196][197]
"And pity, like a naked new-born babe,Striding the blast, or heaven's cherubim, hors'dUpon the sightless couriers of the air."[198]
However, Shakespeare soon began to adapt the traditional styles to his own purposes. The openingsoliloquyofRichard IIIhas its roots in the self-declaration ofVicein medieval drama. At the same time, Richard's vivid self-awareness looks forward to the soliloquies of Shakespeare's mature plays.[199][200]No single play marks a change from the traditional to the freer style. Shakespeare combined the two throughout his career, withRomeo and Julietperhaps the best example of the mixing of the styles.[201]By the time ofRomeo and Juliet,Richard II, andA Midsummer Night's Dreamin the mid-1590s, Shakespeare had begun to write a more natural poetry. He increasingly tuned his metaphors and images to the needs of the drama itself.
Shakespeare's standard poetic form wasblank verse, composed iniambic pentameter. In practice, this meant that his verse was usually unrhymed and consisted of ten syllables to a line, spoken with a stress on every second syllable. The blank verse of his early plays is quite different from that of his later ones. It is often beautiful, but its sentences tend to start, pause, and finish at theend of lines, with the risk of monotony.[202]Once Shakespeare mastered traditional blank verse, he began to interrupt and vary its flow. This technique releases the new power and flexibility of the poetry in plays such asJulius CaesarandHamlet. Shakespeare uses it, for example, to convey the turmoil in Hamlet's mind:[203]
Sir, in my heart there was a kind of fightingThat would not let me sleep. Methought I layWorse than the mutines in the bilboes. Rashly—And prais'd be rashness for it—let us knowOur indiscretion sometimes serves us well ...
AfterHamlet, Shakespeare varied his poetic style further, particularly in the more emotional passages of the late tragedies. The literary criticA. C. Bradleydescribed this style as "more concentrated, rapid, varied, and, in construction, less regular, not seldom twisted or elliptical".[204]In the last phase of his career, Shakespeare adopted many techniques to achieve these effects. These includedrun-on lines, irregular pauses and stops, and extreme variations in sentence structure and length.[205]InMacbeth, for example, the language darts from one unrelated metaphor or simile to another: "was the hope drunk/ Wherein you dressed yourself?" (1.7.35–38); "... pity, like a naked new-born babe/ Striding the blast, or heaven's cherubim, hors'd/ Upon the sightless couriers of the air ..." (1.7.21–25). The listener is challenged to complete the sense.[205]The late romances, with their shifts in time and surprising turns of plot, inspired a last poetic style in which long and short sentences are set against one another, clauses are piled up, subject and object are reversed, and words are omitted, creating an effect of spontaneity.[206]
Shakespeare combined poetic genius with a practical sense of the theatre.[207]Like all playwrights of the time, he dramatised stories from sources such asPlutarchandHolinshed.[208]He reshaped each plot to create several centres of interest and to show as many sides of a narrative to the audience as possible. This strength of design ensures that a Shakespeare play can survive translation, cutting, and wide interpretation without loss to its core drama.[209]As Shakespeare's mastery grew, he gave his characters clearer and more varied motivations and distinctive patterns of speech. He preserved aspects of his earlier style in the later plays, however. InShakespeare's late romances, he deliberately returned to a more artificial style, which emphasised the illusion of theatre.[210][211]
Shakespeare's work has made a significant and lasting impression on later theatre and literature. In particular, he expanded the dramatic potential ofcharacterisation, plot,language, and genre.[212]UntilRomeo and Juliet, for example, romance had not been viewed as a worthy topic for tragedy.[213]Soliloquieshad been used mainly to convey information about characters or events, but Shakespeare used them to explore characters' minds.[214]His work heavily influenced later poetry. TheRomantic poetsattempted to revive Shakespearean verse drama, though with little success. CriticGeorge Steinerdescribed all English verse dramas fromColeridgetoTennysonas "feeble variations on Shakespearean themes".[215]John Milton, considered by many to be the most important English poet after Shakespeare, wrote in tribute: "Thou in our wonder and astonishment/ Hast built thyself a live-long monument."[216]
Shakespeare influenced novelists such asThomas Hardy,William Faulkner, andCharles Dickens. The American novelistHerman Melville's soliloquies owe much to Shakespeare; hisCaptain AhabinMoby-Dickis a classictragic hero, inspired byKing Lear.[217]Scholars have identified 20,000 pieces of music linked to Shakespeare's works, includingFelix Mendelssohn'soverture and incidental music forA Midsummer Night's DreamandSergei Prokofiev's balletRomeo and Juliet. His work has inspired several operas, among themGiuseppe Verdi'sMacbeth,OtelloandFalstaff, whose critical standing compares with that of the source plays.[218]Shakespeare has also inspired many painters, including the Romantics and thePre-Raphaelites, whileWilliam Hogarth's 1745 painting of actorDavid Garrickplaying Richard III was decisive in establishing the genre of theatrical portraiture in Britain.[219]The Swiss Romantic artistHenry Fuseli, a friend ofWilliam Blake, even translatedMacbethinto German.[220]The psychoanalystSigmund Freuddrew on Shakespearean psychology, in particular, that of Hamlet, for his theories of human nature.[221]Shakespeare has been a rich source for filmmakers;Akira KurosawaadaptedMacbethandKing LearasThrone of BloodandRan, respectively. Other examples of Shakespeare on film includeMax Reinhardt'sA Midsummer Night's Dream,Laurence Olivier'sHamletandAl Pacino's documentaryLooking For Richard.[222]Orson Welles, a lifelong lover of Shakespeare, directed and starred inMacbeth,OthelloandChimes at Midnight, in which he playsJohn Falstaff, which Welles himself called his best work.[223]
In Shakespeare's day, English grammar, spelling, and pronunciation were less standardised than they are now,[224]and his use of language helped shape modern English.[225]Samuel Johnsonquoted him more often than any other author in hisA Dictionary of the English Language, the first serious work of its type.[226]Expressions such as "with bated breath" (Merchant of Venice) and "a foregone conclusion" (Othello) have found their way into everyday English speech.[227][228]
Shakespeare's influence extends far beyond his native England and the English language. His reception in Germany was particularly significant; as early as the 18th century Shakespeare was widely translated and popularised in Germany, and gradually became a "classic of theGerman Weimar era;"Christoph Martin Wielandwas the first to produce complete translations of Shakespeare's plays in any language.[229][230]Actor and theatre directorSimon Callowwrites, "this master, this titan, this genius, so profoundly British and so effortlessly universal, each different culture – German, Italian, Russian – was obliged to respond to the Shakespearean example; for the most part, they embraced it, and him, with joyous abandon, as the possibilities of language and character in action that he celebrated liberated writers across the continent. Some of the most deeply affecting productions of Shakespeare have been non-English, and non-European. He is that unique writer: he has something for everyone."[231]
According toGuinness World Records, Shakespeare remains the world's best-selling playwright, with sales of his plays and poetry believed to have achieved in excess of four billion copies in the almost 400 years since his death. He is also the thirdmost translated author in history.[232]
Shakespeare was not revered in his lifetime, but he received a large amount of praise.[234][235]In 1598, the cleric and authorFrancis Meressingled him out from a group of English playwrights as "the most excellent" in both comedy and tragedy.[236][237]The authors oftheParnassusplaysatSt John's College, Cambridge, numbered him withChaucer,Gower, andSpenser.[238]In theFirst Folio,Ben Jonsoncalled Shakespeare the "Soul of the age, the applause, delight, the wonder of our stage", although he had remarked elsewhere that "Shakespeare wanted art" (lacked skill).[233]
Betweenthe Restorationof the monarchy in 1660 and the end of the 17th century, classical ideas were in vogue. As a result, critics of the time mostly rated Shakespeare belowJohn Fletcherand Ben Jonson.[239]Thomas Rymer, for example, condemned Shakespeare for mixing the comic with the tragic. Nevertheless, poet and criticJohn Drydenrated Shakespeare highly, saying of Jonson, "I admire him, but I love Shakespeare".[240]He also famously remarked that Shakespeare "was naturally learned; he needed not the spectacles of books to read nature; he looked inwards, and found her there."[241]For several decades, Rymer's view held sway. But during the 18th century, critics began to respond to Shakespeare on his own terms and, like Dryden, to acclaim what they termed his natural genius. A series of scholarly editions of his work, notably those ofSamuel Johnsonin 1765 andEdmond Malonein 1790, added to his growing reputation.[242][243]By 1800, he was firmly enshrined as the national poet,[244]and described as the "Bardof Avon" (or simply "the Bard").[245][i]In the 18th and 19th centuries, his reputation also spread abroad. Among those who championed him were the writersVoltaire,Goethe,Stendhal, andVictor Hugo.[247][j]
During theRomantic era, Shakespeare was praised by the poet and literary philosopherSamuel Taylor Coleridge, and the criticAugust Wilhelm Schlegeltranslated his plays in the spirit ofGerman Romanticism.[249]In the 19th century, critical admiration for Shakespeare's genius often bordered on adulation.[250]"This King Shakespeare," the essayistThomas Carlylewrote in 1840, "does not he shine, in crowned sovereignty, over us all, as the noblest, gentlest, yet strongest of rallying signs; indestructible".[251]TheVictoriansproduced his plays as lavish spectacles on a grand scale.[252]The playwright and criticGeorge Bernard Shawmocked the cult of Shakespeare worship as "bardolatry", claiming that the newnaturalismofIbsen's plays had made Shakespeare obsolete.[253]
The modernist revolution in the arts during the early 20th century, far from discarding Shakespeare, eagerly enlisted his work in the service of theavant-garde. TheExpressionists in Germanyand theFuturistsin Moscow mounted productions of his plays. Marxist playwright and directorBertolt Brechtdevised anepic theatreunder the influence of Shakespeare. The poet and criticT. S. Eliotargued against Shaw that Shakespeare's "primitiveness" in fact made him truly modern.[254]Eliot, along withG. Wilson Knightand the school ofNew Criticism, led a movement towards a closer reading of Shakespeare's imagery. In the 1950s, a wave of new critical approaches replaced modernism and paved the way forpost-modernstudies of Shakespeare.[255]Comparing Shakespeare's accomplishments to those of leading figures in philosophy and theology,Harold Bloomwrote, "Shakespeare was larger thanPlatoand thanSt. Augustine. Heenclosesus because weseewith his fundamental perceptions."[256]
Around 230 years after Shakespeare's death, doubts began to be expressed about the authorship of the works attributed to him.[257]Proposed alternative candidates includeFrancis Bacon,Christopher Marlowe, andEdward de Vere, 17th Earl of Oxford.[258]Several "group theories" have also been proposed.[259]All but a few Shakespeare scholars and literary historians consider it afringe theory, with only a small minority of academics who believe that there is reason to question the traditional attribution,[260]but interest in the subject, particularly theOxfordian theory of Shakespeare authorship, continues into the 21st century.[261][262][263]
Shakespeare conformed to the official state religion,[k]but his private views on religion have been the subject of debate.Shakespeare's willuses a Protestant formula, and he was a confirmed member of theChurch of England, where he was married, his children were baptised, and where he is buried.
Some scholars are of the view that members of Shakespeare's family were Catholics, at a time when practising Catholicism in England was against the law.[265]Shakespeare's mother,Mary Arden, certainly came from a pious Catholic family. The strongest evidence might be a Catholic statement of faith signed by his father,John Shakespeare, found in 1757 in the rafters of his former house in Henley Street. However, the document is now lost and scholars differ as to its authenticity.[266][267]In 1591, the authorities reported that John Shakespeare had missed church "for fear of process for debt", a common Catholic excuse.[268][269][270]In 1606, the name of William's daughter Susanna appears on a list of those who failed to attend Eastercommunionin Stratford.[268][269][270]
Other authors argue that there is a lack of evidence about Shakespeare's religious beliefs. Scholars find evidence both for and against Shakespeare's Catholicism, Protestantism, or lack of belief in his plays, but the truth may be impossible to prove.[271][272]
Few details of Shakespeare's sexuality are known. At 18, he married 26-year-oldAnne Hathaway, who was pregnant. Susanna, the first of their three children, was born six months later on 26 May 1583. Over the centuries, some readers have posited that Shakespeare's sonnets are autobiographical,[273]and point to them as evidence of his love for a young man. Others read the same passages as the expression of intense friendship rather than romantic love.[274][275][276]The 26 so-called"Dark Lady"sonnets, addressed to a married woman, are taken as evidence of heterosexual liaisons.[277]
No written contemporary description of Shakespeare's physical appearance survives, and no evidence suggests that he ever commissioned a portrait. From the 18th century, the desire for authentic Shakespeare portraits fuelled claims that various surviving pictures depicted Shakespeare.[278]That demand also led to the production of several fake portraits, as well as misattributions, re-paintings, and relabelling of portraits of other people.[279][280]
Some scholars suggest that theDroeshout portrait, whichBen Jonsonapproved of as a good likeness,[281]and hisStratford monumentprovide perhaps the best evidence of his appearance.[282]Of the claimed paintings, art historianTarnya Cooperconcluded that theChandos portraithad "the strongest claim of any of the known contenders to be a true portrait of Shakespeare". After a three-year study supported by theNational Portrait Gallery, London, the portrait's owners, Cooper contended that its composition date, contemporary with Shakespeare, its subsequent provenance, and the sitter's attire, all supported the attribution.[283]
Ancient Egypt(Egyptian:km.t) was acradle of civilizationconcentrated along the lower reaches of theNile RiverinNortheast Africa. It emerged fromprehistoric Egyptaround 3150BC (according toconventional Egyptian chronology),[1]whenUpper and Lower Egyptwere amalgamated byMenes, who is believed by the majority ofEgyptologiststo have been the same person asNarmer.[2]Thehistory of ancient Egyptunfolded as a series of stable kingdoms interspersed by the "Intermediate Periods" of relative instability. These stable kingdoms existed in one of three periods: theOld Kingdomof theEarly Bronze Age; theMiddle Kingdomof theMiddle Bronze Age; or theNew Kingdomof theLate Bronze Age.
The pinnacle of ancient Egyptian power was achieved during the New Kingdom, which extended its rule to much ofNubiaand a considerable portion of theLevant. After this period, Egypt entered an era of slow decline. Over the course of its history, it was invaded or conquered by a number of foreign civilizations, including theHyksos, theKushites, theAssyrians, thePersians, and, most notably, theGreeksand then theRomans. The end of ancient Egypt is variously defined as occurring with the end of theLate Periodduring theWars of Alexander the Greatin 332 BC or with the end of the Greek-ruledPtolemaic Kingdomduring theRoman conquest of Egyptin 30 BC.[3]In AD 642, theArab conquest of Egyptbrought an end to the region's millennium-longGreco-Roman period.
The success of ancient Egyptian civilization came partly from its ability to adapt to the Nile's conditions foragriculture. The predictableflooding of the Nileand controlled irrigation of its fertile valley produced surplus crops, which supported a more dense population, and thereby substantial social and cultural development. With resources to spare, the administration sponsored the mineral exploitation of the valley and its surrounding desert regions, the early development ofan independent writing system, the organization of collective construction and agricultural projects, trade with other civilizations, anda militaryto assert Egyptian dominance throughout theNear East. Motivating and organizing these activities was a bureaucracy of elite scribes, religious leaders, and administrators under the control of the reigningpharaoh, who ensured the cooperation and unity of theEgyptian peoplein the context ofan elaborate system of religious beliefs.[4]
Among the many achievements of ancient Egypt are: thequarrying, surveying, and construction techniques that supported the building of monumentalpyramids,temples, andobelisks; asystem of mathematics; a practical and effectivesystem of medicine; irrigation systems and agricultural production techniques; the first known planked boats;[5]Egyptian faienceandglass technology; new forms ofliterature; and theearliest known peace treaty, which was ratified with theAnatolia-basedHittite Empire.[6]Itsartandarchitecturewere widely copied and itsantiquitieswere carried off to be studied, admired, or coveted in the far corners of the world. Likewise, its monumental ruinsinspired the imaginationsof travelers and writers for millennia. A newfound European and Egyptian respect for antiquities and excavations that began in earnest in theearly modern periodhas led to much scientific investigation of ancient Egypt and its society, as well as a greater appreciation of its cultural legacy.[7]
TheNilehas been the lifeline of its region for much of human history. The fertile floodplain of the Nile gave humans the opportunity to develop a settledagricultural economyand a more sophisticated, centralized society that became a cornerstone in the history of human civilization.[8]
In Predynastic andEarly Dynastictimes, theEgyptian climate was much less arid than it is today. Large regions of Egypt weresavannaand traversed by herds of grazingungulates. Foliage and fauna were far more prolific in all environs, and the Nile region supported large populations ofwaterfowl. Hunting would have been common for Egyptians, and this is also the period when many animals were firstdomesticated.[9]
By about5500 BC, small tribes living in the Nile valley had developed into a series of cultures demonstrating firm control of agriculture andanimal husbandry, and identifiable by theirpotteryand personal items, such as combs, bracelets, and beads. The largest of these early cultures in upper (Southern) Egypt was theBadarian culture, which probably originated in theWestern Desert; it was known for its high-quality ceramics,stone tools, and its use of copper.[10]
The Badari was followed by theNaqada culture: the Naqada I (Amratian), the Naqada II (Gerzeh), and Naqada III (Semainean).[11]These brought a number of technological improvements. As early as the Naqada I Period, predynasticEgyptiansimportedobsidianfromEthiopia, used to shape blades and other objects fromflakes.[12][13]Mutual trade with theLevantwas established during Naqada II (c.3600–3350 BC); this period was also the beginning oftrade with Mesopotamia, which continued into the early dynastic period and beyond.[14]Over a period of about 1,000 years, the Naqada culture developed from a few small farming communities into a powerful civilization whose leaders were in complete control of the people and resources of the Nile valley.[15]Establishing a power center atNekhen, and later atAbydos,Naqada IIIleaders expanded their control of Egypt northwards along theNile.[16]They also traded withNubiato the south, the oases of thewestern desertto the west, and the cultures of theeastern MediterraneanandNear Eastto the east.[17]
The Naqada culture manufactured a diverse selection of material goods, reflective of the increasing power and wealth of the elite, as well as societal personal-use items, which included combs, small statuary, painted pottery, high qualitydecorative stone vases,cosmetic palettes, and jewelry made of gold,lapis, andivory. They also developed aceramic glazeknown asfaience, which was used well into theRoman Periodto decorate cups, amulets, and figurines.[18][19]During the last predynastic phase, the Naqada culture began using written symbols that eventually were developed into a full system ofhieroglyphsfor writing the ancient Egyptian language.[20]
The Early Dynastic Period was approximately contemporary to the earlySumerian-Akkadiancivilization ofMesopotamiaand of ancientElam. The third-centuryBC Egyptian priestManethogrouped the long line of kings fromMenesto his own time into 30 dynasties, a system still used today. He began his official history with the king named "Meni" (or Menes in Greek), who was believed to have united the two kingdoms ofUpperandLower Egypt.[21]
The transition to a unified state happened more gradually than ancient Egyptian writers represented, and there is no contemporary record of Menes. Some scholars now believe, however, that the mythical Menes may have been the kingNarmer, who is depicted wearingroyal regaliaon the ceremonialNarmer Palette, in a symbolic act of unification.[23]In the Early Dynastic Period, which began about 3000BC, the first of the Dynastic kings solidified control over Lower Egypt by establishing a capital atMemphis, from which he could control thelabor forceand agriculture of the fertiledelta region, as well as the lucrative and criticaltrade routesto theLevant. The increasing power and wealth of the kings during the early dynastic period was reflected in their elaboratemastabatombs andmortuary cultstructures at Abydos, which were used to celebrate thedeified kingafter his death.[24]The strong institution of kingship developed by the kings served to legitimize state control over the land, labor, and resources that were essential to the survival and growth of ancient Egyptian civilization.[25]
Major advances in architecture, art, and technology were made during theOld Kingdom, fueled by the increasedagricultural productivityand resulting population growth, made possible by a well-developed central administration.[26]Some of ancient Egypt's crowning achievements, theGiza pyramidsandGreat Sphinx, were constructed during the Old Kingdom. Under the direction of thevizier, state officials collected taxes, coordinated irrigation projects to improvecrop yield, and drafted peasants to work on construction projects.[27]
With the rising importance of central administration in Egypt, a new class of educated scribes and officials arose who were granted estates by the king in payment for their services. Kings also made land grants to their mortuary cults and localtemples, to ensure that these institutions had the resources to worship the king after his death. Scholars believe that five centuries of these practices slowly eroded the economic vitality of Egypt, and that the economy could no longer afford to support a large centralized administration.[28]As the power of the kings diminished, regional governors callednomarchsbegan to challenge the supremacy of the office of king. This, coupled withsevere droughtsbetween 2200 and 2150BC,[29]is believed to have caused the country to enter the 140-year period of famine and strife known as the First Intermediate Period.[30]
After Egypt'scentral governmentcollapsed at the end of the Old Kingdom, the administration could no longer support or stabilize the country's economy. The ensuing food shortages and political disputes escalated into famines and small-scale civil wars. Yet despite difficult problems, local leaders, owing no tribute to the king, used their new-found independence to establish a thriving culture in the provinces. Once in control of their own resources, the provinces became economically richer—which was demonstrated by larger and better burials among all social classes.[31]
Free from their loyalties to the king, local rulers began competing with each other for territorial control andpolitical power. By 2160BC, rulers inHerakleopoliscontrolled Lower Egypt in the north, while a rival clan based inThebes, theIntef family, took control of Upper Egypt in the south. As the Intefs grew in power and expanded their control northward, a clash between the two rival dynasties became inevitable. Around 2055BC the northern Theban forces underNebhepetre Mentuhotep IIfinally defeated the Herakleopolitan rulers, reuniting the Two Lands. They inaugurated a period of economic and cultural renaissance known as theMiddle Kingdom.[32]
The kings of the Middle Kingdom restored the country's stability, which saw a resurgence of art and monumental building projects, and a new flourishing ofliterature.[35]Mentuhotep II and hisEleventh Dynastysuccessors ruled from Thebes, but the vizierAmenemhat I, upon assuming the kingship at the beginning of theTwelfth Dynastyaround 1985BC, shifted the kingdom's capital to the city ofItjtawy, located inFaiyum.[36]From Itjtawy, the kings of the Twelfth Dynasty undertook a far-sightedland reclamationand irrigation scheme to increase agricultural output in the region. Moreover, the military reconquered territory inNubiathat was rich in quarries and gold mines, while laborers built a defensive structure in the Eastern Delta, called the "Walls of the Ruler", to defend against foreign attack.[37]
With the kings having secured the country militarily and politically and with vast agricultural and mineral wealth at their disposal, the nation's population, arts, and religion flourished. The Middle Kingdom displayed an increase in expressions of personal piety toward the gods. Middle Kingdom literature featured sophisticated themes and characters written in a confident, eloquent style.[38]Thereliefand portrait sculpture of the period captured subtle, individual details that reached new heights of technical sophistication.[39]
Around 1785BC, as the power of the Middle Kingdom kings weakened, aWestern Asianpeople called theHyksos, who had already settled in the Delta, seized control of Egypt and established their capital atAvaris, forcing the former central government to retreat toThebes. The king was treated as a vassal and expected to pay tribute.[40]The Hyksos ('foreign rulers') retained Egyptian models of government and identified as kings, thereby integrating Egyptian elements into their culture.[41]
After retreating south, the native Theban kings found themselves trapped between the Canaanite Hyksos ruling the north and the Hyksos'Nubianallies, theKushites, to the south. After years of vassalage, Thebes gathered enough strength to challenge the Hyksos in a conflict that lasted more than 30 years, until 1555BC.[40]Ahmose Iwaged a series of campaigns that permanently eradicated the Hyksos' presence in Egypt. He is considered the founder of theEighteenth Dynasty, and the military became a central priority for his successors, who sought to expand Egypt's borders and attempted to gain mastery of theNear East.[42]
The New Kingdompharaohsestablished a period of unprecedented prosperity by securing their borders and strengthening diplomatic ties with their neighbours, including theMitanniEmpire,Assyria, andCanaan. Military campaigns waged underTuthmosis Iand his grandsonTuthmosis IIIextended the influence of the pharaohs to the largest empire Egypt had ever seen.
Between their reigns,Hatshepsut, a queen who established herself as pharaoh, launched many building projects, including the restoration of temples damaged by the Hyksos, and sent trading expeditions toPuntand the Sinai.[43]When Tuthmosis III died in 1425BC, Egypt had an empire extending fromNiyain north westSyriato theFourth Cataractof the Nile inNubia, cementing loyalties and opening access to critical imports such asbronzeandwood.[44]
The New Kingdom pharaohs began a large-scale building campaign to promote the godAmun, whose growing cult was based inKarnak. They also constructed monuments to glorify their own achievements, both real and imagined. The Karnak temple is the largest Egyptian temple ever built.[45]
Around 1350BC, the stability of the New Kingdom was threatened when Amenhotep IV ascended the throne and instituted a series of radical and chaotic reforms. Changing his name toAkhenaten, he touted the previously obscuresun deityAtenas thesupreme deity, suppressed the worship of most other deities, and moved the capital to the new city of Akhetaten (modern-dayAmarna).[46]He was devoted to his newreligionandartistic style. After his death, the cult of the Aten was quickly abandoned and the traditional religious order restored. The subsequent pharaohs,Tutankhamun,Ay, andHoremheb, worked to erase all mention of Akhenaten's heresy, now known as theAmarna Period.[47]
Around 1279BC,Ramesses II, also known as Ramesses the Great, ascended the throne, and went on to build more temples, erect more statues and obelisks, and sire more children than any other pharaoh in history.[c]A bold military leader, Ramesses II led his army against the Hittites in theBattle of Kadesh(in modernSyria) and, after fighting to a stalemate, finally agreed to the first recordedpeace treaty, around 1258BC.[48]
Egypt's wealth, however, made it a tempting target for invasion, particularly by theLibyanBerbersto the west, and theSea Peoples, a conjectured confederation of seafarers from theAegean Sea.[d]Initially, the military was able torepelthese invasions, but Egypt eventually lost control of its remaining territories in southernCanaan, much of it falling to the Assyrians. The effects of external threats were exacerbated by internal problems such as corruption, tomb robbery, andcivil unrest. After regaining their power, the high priests at thetemple of Amunin Thebes accumulated vast tracts of land and wealth, and their expanded power splintered the country during the Third Intermediate Period.[49]
Following the death ofRamesses XIin 1078BC,Smendesassumed authority over the northern part of Egypt, ruling from the city ofTanis. The south was effectively controlled by theHigh Priests of Amun at Thebes, who recognized Smendes in name only.[50]During this time, Libyans had been settling in the western delta, and chieftains of these settlers began increasing their autonomy. Libyan princes took control of the delta underShoshenq Iin 945BC, founding the so-called Libyan or Bubastite dynasty that would rule for some 200 years. Shoshenq also gained control of southern Egypt by placing his family members in important priestly positions. Libyan control began to erode as a rival dynasty in the delta arose inLeontopolis, andKushitesthreatened from the south.
Around 727BC the Kushite kingPiyeinvaded northward, seizing control of Thebes and eventually the Delta, which established the25th Dynasty.[52]During the 25th Dynasty, PharaohTaharqacreated an empire nearly as large as theNew Kingdom's. Twenty-fifth Dynasty pharaohs built, or restored, temples and monuments throughout the Nile valley, including at Memphis, Karnak, Kawa, and Jebel Barkal.[53]During this period, the Nile valley saw the first widespread construction ofpyramids (many in modern Sudan)since the Middle Kingdom.[54][55][56]
Egypt's far-reaching prestige declined considerably toward the end of the Third Intermediate Period. Its foreign allies had fallen into theAssyriansphere of influence, and by 700BC war between the two states became inevitable. Between 671 and 667BC the Assyrians began theAssyrian conquest of Egypt. The reigns of bothTaharqaand his successor,Tanutamun, were filled with frequent conflict with the Assyrians. Ultimately, the Assyrians pushed the Kushites back into Nubia, occupied Memphis, andsacked the temples of Thebes.[57]
The Assyrians left control of Egypt to a series of vassals who became known as the Saite kings of theTwenty-Sixth Dynasty. By 653BC, the Saite kingPsamtik Iwas able to oust the Assyrians with the help of Greek mercenaries, who were recruited to form Egypt's firstnavy.Greek influenceexpanded greatly as thecity-stateofNaucratisbecame the home of Greeks in the Nile Delta. The Saite kings based in the new capital ofSaiswitnessed a brief but spirited resurgence in the economy and culture, but in 525BC, the Persian Empire, led byCambyses II, began its conquest of Egypt, eventually defeating the pharaohPsamtik IIIat theBattle of Pelusium. Cambyses II then assumed the formal title of pharaoh, but ruled Egypt from Iran, leaving Egypt under the control of asatrap. A few revolts against the Persians marked the 5th centuryBC, but Egypt was never able to overthrow the Persians until the end of the century.[58]
Following its annexation by Persia, Egypt was joined withCyprusandPhoeniciain the sixth satrapy of theAchaemenid Persian Empire. This first period of Persian rule over Egypt, also known as theTwenty-Seventh Dynasty, ended in 402BC, when Egypt regained independence under a series of native dynasties. The last of these dynasties, theThirtieth, proved to be the last native royal house of ancient Egypt, ending with the kingship ofNectanebo II. A brief restoration of Persian rule, sometimes known as theThirty-First Dynasty, began in 343BC, but shortly after, in 332BC, the Persian ruler Mazaces handed Egypt over toAlexander the Greatwithout a fight.[59]
In 332BC,Alexander the Greatconquered Egypt with little resistance from thePersiansand was welcomed by the Egyptians as a deliverer. The administration established by Alexander's successors, theMacedonianPtolemaic Kingdom, was based on an Egyptian model and based in the newcapital cityofAlexandria. The city showcased the power and prestige of Hellenistic rule, and became a centre of learning and culture that included the famousLibrary of Alexandriaand theMouseion.[60]TheLighthouse of Alexandrialit the way for the many ships that kept trade flowing through the city—as thePtolemiesmade commerce and revenue-generating enterprises, such as papyrus manufacturing, their top priority.[61]
Hellenistic culturedid not supplant native Egyptian culture, as the Ptolemies supported time-honored traditions in an effort to secure the loyalty of the populace. They built new temples in Egyptian style, supported traditional cults, and portrayed themselves as pharaohs. Some traditions merged, as Greek andEgyptian godsweresyncretizedinto composite deities, such asSerapis, andclassical Greekforms of sculpture influenced traditional Egyptian motifs. Despite their efforts to appease the Egyptians, the Ptolemies were challenged by native rebellion, bitter family rivalries, and frequent mob violence in Alexandria.[62]In addition, asRomerelied more heavily on imports of grain from Egypt, theRomanstook great interest in the political situation in the country. Continued Egyptian revolts, ambitious politicians, and powerful opponents from the Near East made this situation unstable, leading Rome to send forces to secure the country as a province of its empire.[63]
Egypt became a province of theRoman Empirein 30BC, following the defeat ofMark AntonyandPtolemaicQueenCleopatra VIIbyOctavian(laterEmperorAugustus) in theBattle of Actium. The Romans relied heavily on grain shipments from Egypt, and theRoman army, under the control of a prefect appointed by the emperor, quelled rebellions, strictly enforced the collection of heavy taxes, and prevented attacks by bandits, which had become a notorious problem during the period.[64]Alexandria became an increasingly important center on the trade route with the orient, as exotic luxuries were in high demand in Rome.[65]
Although the Romans had a more hostile attitude than the Greeks towards the Egyptians, some traditions such as mummification and worship of the traditional gods continued.[66]The art of mummy portraiture flourished, and some Roman emperors had themselves depicted as pharaohs, though not to the extent that the Ptolemies had. The former lived outside Egypt and did not perform the ceremonial functions of Egyptian kingship. Local administration became Roman in style and closed to native Egyptians.[66]
From the mid-first century AD,Christianitytook root in Egypt and it was originally seen as another cult that could be accepted. However, it was an uncompromising religion that sought to win converts from the paganEgyptianandGreco-Romanreligions and threatened popular religious traditions. This led to the persecution of converts to Christianity, culminating in the great purges ofDiocletianstarting in 303, but eventually Christianity won out.[67]In 391, the Christian emperorTheodosiusintroduced legislation that banned pagan rites and closed temples.[68]Alexandria became the scene of great anti-pagan riots with public and private religious imagery destroyed.[69]As a consequence, Egypt's native religious culture was continually in decline. While the native population continued to speaktheir language, the ability to readhieroglyphic writingslowly disappeared as the role of the Egyptian temple priests and priestesses diminished. The temples themselves were sometimes converted tochurchesor abandoned to the desert.[70]
The pharaoh was the absolute monarch of the country and, at least in theory, wielded complete control of the land and its resources. The king was the suprememilitary commanderand head of the government, who relied on a bureaucracy of officials to manage his affairs. In charge of the administration was his second in command, thevizier, who acted as the king's representative and coordinated land surveys, the treasury, building projects, the legal system, and thearchives.[71]At a regional level, the country was divided into as many as 42 administrative regions callednomeseach governed by anomarch, who was accountable to the vizier for his jurisdiction. The temples formed the backbone of the economy. Not only were theyplaces of worship, but were also responsible for collecting and storing the kingdom's wealth in a system ofgranariesand treasuries administered byoverseers, who redistributed grain and goods.[72]
Much of the economy was centrally organized and strictly controlled. Although the ancient Egyptians did not usecoinageuntil theLate period,[73]they did use a type of money-barter system,[74]with standard sacks of grain and thedeben, a weight of roughly 91 grams (3 oz) of copper or silver, forming a common denominator.[75]Workers were paid in grain; a simple laborer might earn5+1⁄2sacks (200 kg or 400 lb) of grain per month, while a foreman might earn7+1⁄2sacks (250 kg or 550 lb). Prices were fixed across the country and recorded in lists to facilitate trading; for example a shirt cost five copper deben, while a cow cost 140deben.[75]Grain could be traded for other goods, according to the fixed price list.[75]During the fifth centuryBC coined money was introduced into Egypt from abroad. At first the coins were used as standardized pieces ofprecious metalrather than true money, but in the following centuries international traders came to rely on coinage.[76]
Egyptian society was highly stratified, andsocial statuswas expressly displayed. Farmers made up the bulk of the population, but agricultural produce was owned directly by the state, temple, ornoble familythat owned the land.[77]Farmers were also subject to a labor tax and were required to work on irrigation or construction projects in acorvéesystem.[78]Artists and craftsmen were of higher status than farmers, but they were also under state control, working in the shops attached to the temples and paid directly from the state treasury. Scribes and officials formed the upper class in ancient Egypt, known as the "white kilt class" in reference to the bleached linen garments that served as a mark of their rank.[79]The upper class prominently displayed their social status in art and literature. Below the nobility were the priests, physicians, and engineers with specialized training in their field. It is unclear whetherslaveryas understood today existed in ancient Egypt; there is difference of opinions among authors.[80]
The ancient Egyptians viewed men and women, including people from all social classes, as essentially equal under the law, and even the lowliestpeasantwas entitled to petition thevizierand his court for redress.[81]Although slaves were mostly used as indentured servants, they were able to buy and sell their servitude, work their way to freedom or nobility, and were usually treated bydoctorsin the workplace.[82]Both men and women had the right to own and sell property, make contracts, marry and divorce, receive inheritance, and pursue legal disputes in court. Married couples could own property jointly and protect themselves from divorce by agreeing to marriage contracts, which stipulated the financial obligations of the husband to his wife and children should the marriage end. Compared with their counterparts in ancient Greece, Rome, and even more modern places around the world, ancient Egyptian women had a greater range of personal choices, legal rights, and opportunities for achievement. Women such asHatshepsutandCleopatra VIIeven became pharaohs, while others wielded power asDivine Wives of Amun. Despite these freedoms,ancient Egyptian womendid not often take part in official roles in the administration, aside from the royal high priestesses, apparently served only secondary roles in the temples (not much data for many dynasties), and were not so probably to be as educated as men.[81]
The head of the legal system was officially the pharaoh, who was responsible for enacting laws, delivering justice, and maintaining law and order, a concept the ancient Egyptians referred to asMa'at.[71]Although nolegal codesfrom ancient Egypt survive, court documents show that Egyptian law was based on a common-sense view of right and wrong that emphasized reaching agreements and resolving conflicts rather than strictly adhering to a complicated set of statutes.[81]Local councils of elders, known asKenbetin the New Kingdom, were responsible for ruling in court cases involving small claims and minor disputes.[71]More serious cases involving murder, major land transactions, and tomb robbery were referred to theGreat Kenbet, over which the vizier or pharaoh presided. Plaintiffs and defendants were expected to represent themselves and were required to swear an oath that they had told the truth. In some cases, the state took on both the role of prosecutor and judge, and it could torture the accused with beatings to obtain a confession and the names of any co-conspirators. Whether the charges were trivial or serious, court scribes documented the complaint, testimony, and verdict of the case for future reference.[83]
Punishment for minor crimes involved either imposition of fines, beatings, facial mutilation, or exile, depending on the severity of the offense. Serious crimes such as murder and tomb robbery were punished by execution, carried out by decapitation, drowning, orimpalingthe criminal on a stake. Punishment could also be extended to the criminal's family.[71]Beginning in the New Kingdom,oraclesplayed a major role in the legal system, dispensing justice in both civil and criminal cases. The procedure was to ask the god a "yes" or "no" question concerning the right or wrong of an issue. The god, carried by a number of priests, rendered judgement by choosing one or the other, moving forward or backward, or pointing to one of the answers written on a piece of papyrus or anostracon.[84]
A combination of favorable geographical features contributed to the success of ancient Egyptian culture, the most important of which was the richfertile soilresulting from annual inundations of the Nile River. The ancient Egyptians were thus able to produce an abundance of food, allowing the population to devote more time and resources to cultural, technological, and artistic pursuits.Land managementwas crucial in ancient Egypt because taxes were assessed based on the amount of land a person owned.[85]
Farming in Egypt was dependent on the cycle of the Nile River. The Egyptians recognized three seasons:Akhet(flooding),Peret(planting), andShemu(harvesting). The flooding season lasted from June to September, depositing on the river's banks a layer of mineral-rich silt ideal for growing crops. After the floodwaters had receded, thegrowing seasonlasted from October to February. Farmers plowed and planted seeds in the fields, which were irrigated with ditches and canals. Egypt received little rainfall, so farmers relied on the Nile to water their crops.[86]From March to May, farmers usedsicklesto harvest their crops, which were thenthreshedwith aflailto separate the straw from the grain.Winnowingremoved thechafffrom the grain, and the grain was then ground into flour, brewed to make beer, or stored for later use.[87]
The ancient Egyptians cultivatedemmerandbarley, and several other cereal grains, all of which were used to make the two main food staples of bread and beer.[88]Flaxplants, uprooted before they started flowering, were grown for the fibers of their stems. These fibers were split along their length and spun into thread, which was used to weave sheets oflinenand to make clothing.Papyrusgrowing on the banks of the Nile River was used to make paper. Vegetables and fruits were grown in garden plots, close to habitations and on higher ground, and had to be watered by hand. Vegetables included leeks, garlic, melons, squashes, pulses, lettuce, and other crops, in addition to grapes that were made into wine.[89]
The Egyptians believed that a balanced relationship between people andanimalswas an essential element of the cosmic order; thus humans, animals and plants were believed to be members of a single whole.[90]Animals, bothdomesticatedandwild, were therefore a critical source of spirituality, companionship, and sustenance to the ancient Egyptians.Cattlewere the most important livestock; the administration collected taxes on livestock in regularcensuses, and the size of a herd reflected the prestige and importance of the estate or temple that owned them. In addition to cattle, the ancient Egyptians kept sheep, goats, and pigs.Poultry, such as ducks, geese, and pigeons, were captured in nets and bred on farms, where they were force-fed with dough to fatten them.[91]The Nile provided a plentiful source offish. Bees were also domesticated from at least the Old Kingdom, and provided both honey and wax.[92]
The ancient Egyptians used donkeys andoxenasbeasts of burden, and they were responsible for plowing the fields and trampling seed into the soil. The slaughter of a fattened ox was also a central part of an offering ritual. Horses were introduced by theHyksosin theSecond Intermediate Period. Camels, although known from the New Kingdom, were not used as beasts of burden until the Late Period. There is also evidence to suggest thatelephantswere briefly used in the Late Period but largely abandoned due to lack ofgrazingland.[91]Cats, dogs, and monkeys were common family pets, while more exotic pets imported from the heart of Africa, such asSub-Saharan Africanlions,[93]were reserved for royalty.Herodotusobserved that the Egyptians were the only people to keep their animals with them in their houses.[90]During the Late Period, the worship of the gods in their animal form was extremely popular, such as the cat goddessBastetand the ibis godThoth, and these animals were kept in large numbers for the purpose of ritual sacrifice.[94]
Egypt is rich in building and decorative stone, copper and lead ores, gold, and semiprecious stones. Thesenatural resourcesallowed the ancient Egyptians to build monuments, sculpt statues, make tools, andfashion jewelry.[95]Embalmersused salts from theWadi Natrunformummification, which also provided thegypsumneeded to make plaster.[96]Ore-bearingrock formationswere found in distant, inhospitablewadisin theEastern Desertand the Sinai, requiring large, state-controlled expeditions to obtain natural resources found there. There were extensivegold minesinNubia, and one of the first maps known is of a gold mine in this region. TheWadi Hammamatwas a notable source of granite,greywacke, and gold.Flintwas the first mineral collected and used to make tools, and flint handaxes are the earliest pieces of evidence of habitation in the Nile valley. Nodules of the mineral were carefully flaked to make blades and arrowheads of moderate hardness and durability even after copper was adopted for this purpose.[97]Ancient Egyptians were among the first to use minerals such assulfuras cosmetic substances.[98]
The Egyptians worked deposits of thelead oregalenaat Gebel Rosas to make net sinkers, plumb bobs, and small figurines. Copper was the most important metal for toolmaking in ancient Egypt and was smelted in furnaces frommalachiteore mined in the Sinai.[99]Workers collected gold by washing the nuggets out of sediment inalluvial deposits, or by the more labor-intensive process of grinding and washing gold-bearing quartzite. Iron deposits found in upper Egypt were used in the Late Period.[100]High-quality building stones were abundant in Egypt; the ancient Egyptians quarried limestone all along the Nile valley, granite from Aswan, and basalt and sandstone from the wadis of the Eastern Desert. Deposits of decorative stones such asporphyry, greywacke,alabaster, andcarneliandotted the Eastern Desert and were collected even before the First Dynasty. In the Ptolemaic and Roman Periods, miners worked deposits ofemeraldsin Wadi Sikait andamethystinWadi el-Hudi.[101]
The ancient Egyptians engaged in trade with theirforeign neighborsto obtain rare, exotic goods not found in Egypt. In thePredynastic Period, they established trade with Nubia to obtain gold and incense. They also established trade with Palestine, as evidenced by Palestinian-style oil jugs found in the burials of the First Dynasty pharaohs.[102]An Egyptiancolonystationed in southernCanaandates to slightly before the First Dynasty.[103]Tell es-Sakanin present-day Gaza was established as an Egyptian settlement in the late 4th millennium BC, and is theorised to have been the main Egyptian colonial site in the region.[104]Narmerhad Egyptian pottery produced in Canaan and exported back to Egypt.[105][106]
By the Second Dynasty at latest, ancient Egyptian trade withByblosyielded a critical source of quality timber not found in Egypt. By the Fifth Dynasty, trade withPuntprovided gold, aromatic resins, ebony, ivory, and wild animals such as monkeys and baboons.[107]Egypt relied on trade withAnatoliafor essential quantities of tin as well as supplementary supplies of copper, both metals being necessary for the manufacture of bronze. The ancient Egyptians prized the blue stonelapis lazuli, which had to be imported from far-awayAfghanistan. Egypt's Mediterranean trade partners also includedGreeceand Crete, which provided, among other goods, supplies ofolive oil.[108]
TheEgyptian languageis a northernAfro-Asiaticlanguage closely related to theBerberandSemitic languages.[109]It has the longest known history of any language having been written fromc.3200BC to the Middle Ages and remaining as a spoken language for longer. The phases of ancient Egyptian areOld Egyptian,Middle Egyptian(Classical Egyptian),Late Egyptian,DemoticandCoptic.[110]Egyptian writings do not show dialect differences before Coptic, but it was probably spoken in regional dialects around Memphis and later Thebes.[111]
Ancient Egyptian was asynthetic language, but it became moreanalyticlater on. Late Egyptian developed prefixal definite and indefinitearticles, which replaced the older inflectionalsuffixes. There was a change from the olderverb–subject–objectword ordertosubject–verb–object.[112]The Egyptianhieroglyphic,hieratic, and demotic scripts were eventually replaced by the more phoneticCoptic alphabet. Coptic is still used in the liturgy of theEgyptian Orthodox Church, and traces of it are found in modernEgyptian Arabic.[113]
Ancient Egyptian has 25 consonants similar to those of other Afro-Asiatic languages. These includepharyngealandemphaticconsonants, voiced and voiceless stops, voicelessfricativesand voiced and voicelessaffricates. It has three long and three short vowels, which expanded in Late Egyptian to about nine.[114]The basic word in Egyptian, similar to Semitic and Berber, is atriliteralor biliteral root of consonants and semiconsonants. Suffixes are added to form words. The verb conjugation corresponds to theperson. For example, the triconsonantal skeletonS-Ḏ-Mis the semantic core of the word 'hear'; its basic conjugation issḏm, 'he hears'. If the subject is a noun, suffixes are not added to the verb:[115]sḏm ḥmt, 'the woman hears'.
Adjectives are derived from nouns through a process that Egyptologists callnisbationbecause of its similarity with Arabic.[116]The word order ispredicate–subjectin verbal and adjectival sentences, andsubject–predicatein nominal and adverbial sentences.[117]The subject can be moved to the beginning of sentences if it is long and is followed by a resumptive pronoun.[118]Verbs and nouns are negated by theparticlen, butnnis used for adverbial and adjectival sentences.Stressfalls on the ultimate or penultimate syllable, which can be open (CV) or closed (CVC).[119]
Hieroglyphic writingdates fromc.3000BC, and is composed of hundreds of symbols. A hieroglyph can represent a word, a sound, or a silent determinative; and the same symbol can serve different purposes in different contexts. Hieroglyphs were a formal script, used on stone monuments and in tombs, that could be as detailed as individual works of art. In day-to-day writing, scribes used a cursive form of writing, calledhieratic, which was quicker and easier. While formal hieroglyphs may be read in rows or columns in either direction (though typically written from right to left), hieratic was always written from right to left, usually in horizontal rows. A new form of writing,Demotic, became the prevalent writing style, and it is this form of writing—along with formal hieroglyphs—that accompany the Greek text on the Rosetta Stone.[121]
Around the first century AD, the Coptic alphabet started to be used alongside the Demotic script. Coptic is a modifiedGreek alphabetwith the addition of some Demotic signs.[122]Although formal hieroglyphs were used in a ceremonial role until the fourth century, towards the end only a small handful of priests could still read them. As the traditional religious establishments were disbanded, knowledge of hieroglyphic writing was mostly lost. Attempts to decipher them date to the Byzantine[123]and Islamic periods in Egypt,[124]but only in the 1820s, after the discovery of the Rosetta Stone and years of research byThomas YoungandJean-François Champollion, were hieroglyphssubstantially deciphered.[125]
Writing first appeared in association with kingship on labels and tags for items found in royal tombs. It was primarily an occupation of the scribes, who worked out of thePer Ankhinstitution or the House of Life. The latter comprised offices, libraries (called House of Books), laboratories and observatories.[126]Some of the best-known pieces of ancient Egyptian literature, such as thePyramidandCoffin Texts, were written in Classical Egyptian, which continued to be the language of writing until about 1300BC. Late Egyptian was spoken from the New Kingdom onward and is represented inRamessideadministrative documents, love poetry and tales, as well as in Demotic and Coptic texts. During this period, the tradition of writing had evolved into the tomb autobiography, such as those ofHarkhufandWeni. The genre known asSebayt('instructions') was developed to communicate teachings and guidance from famous nobles; theIpuwer papyrus, a poem of lamentations describingnatural disastersand social upheaval, is a famous example.
TheStory of Sinuhe, written inMiddle Egyptian, might be the classic of Egyptian literature.[127]Also written at this time was theWestcar Papyrus, a set of stories told toKhufuby his sons relating the marvels performed by priests.[128]TheInstruction of Amenemopeis considered a masterpiece of Near Eastern literature.[129]Towards the end of the New Kingdom, thevernacular languagewas more often employed to write popular pieces such as theStory of Wenamunand theInstruction of Any. The former tells the story of a noble who is robbed on his way to buy cedar from Lebanon and of his struggle to return to Egypt. From about 700BC, narrative stories and instructions, such as the popular Instructions of Onchsheshonqy, as well as personal and business documents were written in thedemoticscript and phase of Egyptian. Many stories written in demotic during theGreco-Romanperiod were set in previous historical eras, when Egypt was an independent nation ruled by great pharaohs such asRamesses II.[130]
Most ancient Egyptians were farmers tied to the land. Their dwellings were restricted to immediate family members, and were constructed ofmudbrickdesigned to remain cool in the heat of the day. Each home had a kitchen with an open roof, which contained a grindstone for milling grain and a small oven for baking the bread.[131]Ceramicsserved as household wares for the storage, preparation, transport, and consumption of food, drink, and raw materials. Walls were painted white and could be covered with dyed linen wall hangings. Floors were covered with reed mats, while wooden stools, beds raised from the floor and individual tables comprised the furniture.[132]
The ancient Egyptians placed a great value on hygiene and appearance. Most bathed in the Nile and used a pasty soap made fromanimal fatand chalk. Men shaved their entire bodies for cleanliness; perfumes and aromatic ointments covered bad odors and soothed skin.[133]Clothing was made from simple linen sheets that were bleached white, and both men and women of the upper classes wore wigs, jewelry, andcosmetics. Children went without clothing until maturity, at about age 12, and at this age males were circumcised and had their heads shaved. Mothers were responsible for taking care of the children, while the father provided the family'sincome.[134]
Music and dance were popular entertainments for those who could afford them. Early instruments included flutes and harps, while instruments similar to trumpets, oboes, and pipes developed later and became popular. In the New Kingdom, the Egyptians played on bells, cymbals, tambourines, drums, and importedlutesandlyresfrom Asia.[135]Thesistrumwas a rattle-likemusical instrumentthat was especially important in religious ceremonies.
The ancient Egyptians enjoyed a variety of leisure activities, including games and music.Senet, a board game where pieces moved according to random chance, was particularly popular from the earliest times; another similar game wasmehen, which had a circular gaming board. "Hounds and Jackals" also known as 58 holes is another example of board games played in ancient Egypt. The first complete set of this game was discovered from aTheban tombof the Egyptian pharaohAmenemhat IVthat dates to the13th Dynasty.[136]Juggling andball gameswere popular with children, and wrestling is also documented in a tomb atBeni Hasan.[137]The wealthy members of ancient Egyptian society enjoyedhunting, fishing, and boating as well.
The excavation of the workers' village ofDeir el-Medinahas resulted in one of the most thoroughly documented accounts of community life in the ancient world, which spans almost four hundred years. There is no comparable site in which the organization, social interactions, and working and living conditions of a community have been studied in such detail.[138]
Egyptian cuisine remained remarkably stable over time; indeed, thecuisine of modern Egyptretains some striking similarities to the cuisine of the ancients. The staple diet consisted of bread and beer, supplemented with vegetables such as onions and garlic, and fruit such as dates and figs. Wine and meat were enjoyed by all on feast days while the upper classes indulged on a more regular basis. Fish, meat, and fowl could be salted or dried, and could be cooked in stews or roasted on a grill.[139]
The architecture of ancient Egypt includes some of the most famous structures in the world: theGreat Pyramids of Gizaand the temples at Thebes. Building projects were organized and funded by the state for religious and commemorative purposes, but also to reinforce the wide-ranging power of the pharaoh. The ancient Egyptians were skilled builders; using only simple but effective tools and sighting instruments, architects could build largestone structureswith great accuracy and precision that is still envied today.[140]
The domestic dwellings of elite and ordinary Egyptians alike were constructed from perishable materials such as mudbricks and wood, and have not survived. Peasants lived in simple homes, while the palaces of the elite and the pharaoh were more elaborate structures. A few surviving New Kingdom palaces, such as those inMalkataandAmarna, show richly decorated walls and floors with scenes of people, birds, water pools, deities and geometric designs.[141]Important structures such as temples and tombs that were intended to last forever were constructed of stone instead of mudbricks. The architectural elements used in the world's first large-scale stone building,Djoser's mortuary complex, includepost and lintelsupports in the papyrus and lotus motif.[citation needed]
The earliest preserved ancient Egyptian temples, such as those at Giza, consist of single, enclosed halls with roof slabs supported by columns. In the New Kingdom, architects added thepylon, the opencourtyard, and the enclosedhypostylehall to the front of the temple's sanctuary, a style that was standard until the Greco-Roman period.[142]The earliest and most popular tomb architecture in the Old Kingdom was themastaba, a flat-roofed rectangular structure of mudbrick or stone built over an undergroundburial chamber. Thestep pyramid of Djoseris a series of stone mastabas stacked on top of each other. Pyramids were built during the Old and Middle Kingdoms, but most later rulers abandoned them in favor of less conspicuous rock-cut tombs.[143]The use of the pyramid form continued in private tomb chapels of the New Kingdom and in the royalpyramids of Nubia.[144]
The ancient Egyptians produced art to serve functional purposes. For over 3500 years, artists adhered to artistic forms and iconography that were developed during the Old Kingdom, following a strict set of principles that resisted foreign influence and internal change.[145]These artistic standards—simple lines, shapes, and flat areas of color combined with the characteristic flat projection of figures with no indication of spatial depth—created a sense of order and balance within a composition. Images and text were intimately interwoven on tomb and temple walls, coffins, stelae, and even statues. TheNarmer Palette, for example, displays figures that can also be read as hieroglyphs.[146]Because of the rigid rules that governed its highly stylized and symbolic appearance, ancient Egyptian art served its political and religious purposes with precision and clarity.[147]
Ancient Egyptian artisans used stone as a medium for carving statues and fine reliefs, but used wood as a cheap and easily carved substitute. Paints were obtained from minerals such as iron ores (red and yellow ochres), copper ores (blue and green), soot or charcoal (black), and limestone (white). Paints could be mixed withgum arabicas a binder and pressed into cakes, which could be moistened with water when needed.[148]
Pharaohs usedreliefsto record victories in battle, royal decrees, and religious scenes. Common citizens had access to pieces offunerary art, such asshabtistatues and books of the dead, which they believed would protect them in the afterlife.[149]During the Middle Kingdom,wooden or clay modelsdepicting scenes from everyday life became popular additions to the tomb. In an attempt to duplicate the activities of the living in theafterlife, these models show laborers, houses, boats, and even military formations that are scale representations of the ideal ancient Egyptian afterlife.[150]
Despite the homogeneity of ancient Egyptian art, the styles of particular times and places sometimes reflected changing cultural or political attitudes. After the invasion of the Hyksos in the Second Intermediate Period,Minoan-style frescoes were found inAvaris.[151]The most striking example of a politically driven change in artistic forms comes from theAmarna Period, where figures were radically altered to conform toAkhenaten's revolutionary religious ideas.[152]This style, known asAmarna art, was quickly abandoned after Akhenaten's death and replaced by the traditional forms.[153]
Beliefs in the divine and in the afterlife were ingrained in ancient Egyptian civilization from its inception; pharaonic rule was based on thedivine right of kings. The Egyptian pantheon was populated bygodswho had supernatural powers and were called on for help or protection. However, the gods were not always viewed as benevolent, and Egyptians believed they had to be appeased with offerings and prayers. The structure of thispantheonchanged continually as new deities were promoted in the hierarchy, but priests made no effort to organize the diverse and sometimes conflictingmythsand stories into a coherent system.[154]These various conceptions of divinity were not considered contradictory but rather layers in the multiple facets of reality.[155]
Gods were worshiped in cult temples administered by priests acting on the king's behalf. At the center of the temple was the cult statue in a shrine. Temples were not places of public worship or congregation, and only on select feast days and celebrations was a shrine carrying the statue of the god brought out for public worship. Normally, the god's domain was sealed off from the outside world and was only accessible to temple officials. Common citizens could worship private statues in their homes, and amulets offered protection against the forces of chaos.[156]After the New Kingdom, the pharaoh's role as a spiritual intermediary was de-emphasized as religious customs shifted to direct worship of the gods. As a result, priests developed a system oforaclesto communicate the will of the gods directly to the people.[157]
The Egyptians believed that every human being was composed of physical and spiritual parts oraspects. In addition to the body, each person had ašwt(shadow), aba(personality or soul), aka(life-force), and aname.[158]The heart, rather than the brain, was considered the seat of thoughts and emotions. After death, the spiritual aspects were released from the body and could move at will, but they required the physical remains (or a substitute, such as a statue) as a permanent home. The ultimate goal of the deceased was to rejoin hiskaandbaand become one of the "blessed dead", living on as anakh, or "effective one". For this to happen, the deceased had to be judged worthy in a trial, in which the heart was weighed against a "feather of truth". If deemed worthy, the deceased could continue their existence on earth in spiritual form.[159]If they were not deemed worthy, their heart was eaten byAmmitthe Devourer and they were erased from the Universe.[citation needed]
The ancient Egyptians maintained an elaborate set of burial customs that they believed were necessary to ensure immortality after death. These customs involved preserving the body bymummification, performing burial ceremonies, and interring with the body goods the deceased would use in the afterlife.[149]Before the Old Kingdom, bodies buried in desert pits were naturally preserved bydesiccation. The arid, desert conditions were a boon throughout the history of ancient Egypt for burials of the poor, who could not afford the elaborate burial preparations available to the elite. Wealthier Egyptians began to bury their dead in stone tombs and use artificial mummification, which involved removing theinternal organs, wrapping the body in linen, and burying it in a rectangular stone sarcophagus or wooden coffin. Beginning in the Fourth Dynasty, some parts were preserved separately incanopic jars.[160]
By the New Kingdom, the ancient Egyptians had perfected the art of mummification; the best technique took 70 days and involved removing the internal organs, removing the brain through the nose, and desiccating the body in a mixture of salts callednatron. The body was then wrapped in linen with protective amulets inserted between layers and placed in a decorated anthropoid coffin. Mummies of the Late Period were also placed in paintedcartonnagemummy cases. Actual preservation practices declined during the Ptolemaic and Roman eras, while greater emphasis was placed on the outer appearance of the mummy, which was decorated.[161]
Wealthy Egyptians were buried with larger quantities of luxury items, but all burials, regardless of social status, included goods for the deceased.Funerary textswere often included in the grave, and, beginning in the New Kingdom, so wereshabtistatues that were believed to perform manual labor for them in the afterlife.[162]Rituals in which the deceased was magically re-animated accompanied burials. After burial, living relatives were expected to occasionally bring food to the tomb and recite prayers on behalf of the deceased.[163]
The ancient Egyptian military was responsible for defending Egypt against foreign invasion, and for maintaining Egypt's domination in theancient Near East. The military protected mining expeditions to the Sinai during the Old Kingdom and fought civil wars during the First and Second Intermediate Periods. The military was responsible for maintaining fortifications along important trade routes, such as those found at the city ofBuhenon the way to Nubia. Forts also were constructed to serve as military bases, such as the fortress at Sile, which was a base of operations for expeditions to theLevant. In the New Kingdom, a series of pharaohs used the standing Egyptian army to attack and conquerKushand parts of the Levant.[164]
Typical military equipment includedbows and arrows, spears, and round-topped shields made by stretchinganimal skinover a wooden frame. In the New Kingdom, the military began usingchariotsthat had earlier been introduced by the Hyksos invaders. Weapons and armor continued to improve after the adoption of bronze: shields were now made from solid wood with a bronze buckle, spears were tipped with a bronze point, and thekhopeshwas adopted from Asiatic soldiers.[165]The pharaoh was usually depicted in art and literature riding at the head of the army; it has been suggested that at least a few pharaohs, such asSeqenenre Tao IIand his sons, did do so.[166]However, it has also been argued that "kings of this period did not personally act as frontline war leaders, fighting alongside their troops".[167]Soldiers were recruited from the general population, but during, and especially after, the New Kingdom, mercenaries from Nubia, Kush, and Libya were hired to fight for Egypt.[168]
In technology, medicine, and mathematics, ancient Egypt achieved a relatively high standard of productivity and sophistication. Traditionalempiricism, as evidenced by theEdwin SmithandEbers papyri(c.1600 BC), is first credited to Egypt. The Egyptians created their own alphabet anddecimal system.
Even before the Old Kingdom, the ancient Egyptians had developed a glassy material known asfaience, which they treated as a type of artificial semi-precious stone. Faience is a non-clay ceramic made ofsilica, small amounts oflimeandsoda, and a colorant, typically copper.[169]The material was used to make beads, tiles, figurines, and small wares. Several methods can be used to create faience, but typically production involved application of the powdered materials in the form of a paste over a clay core, which was then fired. By a related technique, the ancient Egyptians produced a pigment known asEgyptian blue, also called blue frit, which is produced by fusing (orsintering) silica, copper, lime, and an alkali such as natron. The product can be ground up and used as a pigment.[170]
The ancient Egyptians could fabricate a wide variety of objects from glass with great skill, but it is not clear whether they developed the process independently.[171]It is also unclear whether they made their own raw glass or merely imported pre-made ingots, which they melted and finished. However, they did have technical expertise in making objects, as well as addingtrace elementsto control the color of the finished glass. A range of colors could be produced, including yellow, red, green, blue, purple, and white, and the glass could be made either transparent or opaque.[172]
The medical problems of the ancient Egyptians stemmed directly from their environment. Living and working close to the Nile brought hazards frommalariaand debilitatingschistosomiasisparasites, which caused liver and intestinal damage. Dangerous wildlife such as crocodiles and hippos were also a common threat. The lifelong labors of farming and building put stress on the spine and joints, and traumatic injuries from construction and warfare all took a significant toll on the body. The grit and sand from stone-ground flour abraded teeth, leaving them susceptible toabscesses(thoughcarieswere rare).[173]
The diets of the wealthy were rich in sugars, which promotedperiodontal disease.[174]Despite the flattering physiques portrayed on tomb walls, the overweight mummies of many of the upper class show the effects of a life of overindulgence.[175]Adultlife expectancywas about 35 for men and 30 for women, but reaching adulthood was difficult as about one-third of the population died in infancy.[e]
Ancient Egyptian physicians were renowned in the ancient Near East for their healing skills, and some, such asImhotep, remained famous long after their deaths.[176]Herodotus remarked that there was a high degree of specialization among Egyptian physicians, with some treating only the head or the stomach, while others were eye-doctors and dentists.[177]Training of physicians took place at thePer Ankhor "House of Life" institution, most notably those headquartered inPer-Bastetduring the New Kingdom and atAbydosandSaïsin the Late period.Medical papyrishowempirical knowledgeof anatomy, injuries, and practical treatments.[178]
Wounds were treated by bandaging with raw meat, white linen, sutures, nets, pads, and swabs soaked with honey to prevent infection,[179]whileopium,thyme, andbelladonawere used to relieve pain. The earliest records of burn treatment describe burn dressings that use the milk from mothers of male babies. Prayers were made to the goddessIsis. Moldy bread, honey, and copper salts were also used to prevent infection from dirt in burns.[180]Garlic and onions were used regularly to promote good health and were thought to relieveasthmasymptoms. Ancient Egyptian surgeons stitched wounds, setbroken bones, and amputated diseased limbs, but they recognized that some injuries were so serious that they could only make the patient comfortable until death occurred.[181]
Early Egyptians knew how to assemble planks of wood into aship hulland had mastered advanced forms ofshipbuildingas early as 3000BC. TheArchaeological Institute of Americareports that the oldestplankedshipsknown are theAbydos boats.[5]A group of 14 discovered ships inAbydoswere constructed of wooden planks "sewn" together. Discovered by Egyptologist David O'Connor ofNew York University,[182]wovenstrapswere found to have been used to lash the planks together,[5]andreedsorgrassstuffed between the planks helped to seal the seams.[5]Because the ships are all buried together and near a mortuary belonging toPharaoh Khasekhemwy, originally they were all thought to have belonged to him, but one of the 14 ships dates to 3000BC, and the associated pottery jars buried with the vessels also suggest earlier dating. The ship dating to 3000BC was 75 feet (23 m) long and is now thought to perhaps have belonged to an earlier pharaoh, perhaps one as early asHor-Aha.[182]
Early Egyptians also knew how to assemble planks of wood withtreenailsto fasten them together, usingpitchforcaulkingthe seams. The "Khufu ship", a 43.6-metre (143 ft) vessel sealed into a pit in the Giza pyramid complex at the foot of theGreat Pyramid of Gizain theFourth Dynastyaround 2500BC, is a full-size surviving example that may have filled the symbolic function of asolar barque. Early Egyptians also knew how to fasten the planks of this ship together withmortise and tenonjoints.[5]
Large seagoing ships are known to have been heavily used by the Egyptians in their trade with the city states of the eastern Mediterranean, especiallyByblos(on the coast of modern-day Lebanon), and in several expeditions down the Red Sea to theLand of Punt. In fact one of the earliest Egyptian words for a seagoing ship is a "Byblos Ship", which originally defined a class of Egyptian seagoing ships used on the Byblos run; however, by the end of the Old Kingdom, the term had come to include large seagoing ships, whatever their destination.[183]
In 1977, an ancient north–south canal was discovered extending fromLake Timsahto the Ballah Lakes.[184]It was dated to theMiddle Kingdom of Egyptby extrapolating dates of ancient sites constructed along its course.[184][f]
In 2011, archaeologists from Italy, the United States, and Egypt, excavating a dried-up lagoon known asMersa Gawasis, unearthed traces of an ancient harbor that once launched early voyages, such asHatshepsut's Punt, expedition onto the open ocean. Some of the site's most evocative evidence for the ancient Egyptians' seafaring prowess include large ship timbers and hundreds of feet of ropes, made from papyrus, coiled in huge bundles.[185]In 2013, a team of Franco-Egyptian archaeologists discovered what is believed to be the world's oldest port, dating back about 4500 years, from the time of King Khufu, on the Red Sea coast, near Wadi el-Jarf (about 110 miles south ofSuez).[186]
The earliest attested examples of mathematical calculations date to the predynasticNaqadaperiod, and show a fully developednumeral system.[g]The importance of mathematics to an educated Egyptian is suggested by a New Kingdom fictional letter in which the writer proposes a scholarly competition between himself and another scribe regarding everyday calculation tasks such as accounting of land, labor, and grain.[188]Texts such as theRhind Mathematical Papyrusand theMoscow Mathematical Papyrusshow that the ancient Egyptians could perform the four basic mathematical operations—addition, subtraction,multiplication, and division—use fractions, calculate the areas of rectangles, triangles, and circles and compute the volumes of boxes, columns and pyramids. They understood basic concepts ofalgebraandgeometry, and could solvesystems of equations.[189]
Mathematical notationwas decimal, and based on hieroglyphic signs for each power of ten up to one million. Each of these could be written as many times as necessary to add up to the desired number; so to write the number eighty or eight hundred, the symbol for ten or one hundred was written eight times respectively.[190]Because their methods of calculation could not handle most fractions with a numerator greater than one, they had to writefractionsas the sum of several fractions. For example, they resolved the fractiontwo-fifthsinto the sum ofone-third+one-fifteenth. Standard tables of values facilitated this.[191]Somecommon fractions, however, were written with a special glyph—the equivalent of the modern two-thirds is shown on the right.[192]
Ancient Egyptian mathematicians knew thePythagorean theoremas an empirical formula. They were aware, for example, that a triangle had a right angle opposite thehypotenusewhen its sides were in a 3–4–5 ratio.[193]They were able to estimate the area of acircleby subtracting one-ninth from its diameter and squaring the result:
Estimates of the size of the population range from 1–1.5 million in the 3rd millennium BC to possibly 2–3 million by the 1st millennium BC, before growing significantly towards the end of that millennium.[195]
According to historian William Stiebling and archaeologist Susan N. Helft, conflicting DNA analysis on recent genetic samples such as theAmarnaroyal mummies has led to a lack of consensus on the genetic makeup of the ancient Egyptians and their geographic origins.[196]
The genetic history of Ancient Egypt remains a developing field, and is relevant for the understanding of population demographic events connecting Africa and Eurasia. To date, the amount of genome-wide aDNA analyses on ancient specimens from Egypt and Sudan remain scarce, although studies on uniparental haplogroups in ancient individuals have been carried out several times, pointing broadly to affinities with other African and Eurasian groups.[197][198]
The currently most advanced full genome analyses was made on three ancient specimens recovered from the Nile River Valley, Abusir el-Meleq, Egypt. Two of the individuals were dated to the Pre-Ptolemaic Period (New Kingdom to Late Period), and one individual to the Ptolemaic Period, spanning around 1300 years of Egyptian history. These results point to a genetic continuity of Ancient Egyptians with modernEgyptians. The results further point to a close genetic affinity between ancient Egyptians andMiddle Easternpopulations, especially ancient groups from theLevant.[197][198]
Ancient Egyptians also displayed affinities toNubiansto the south of Egypt, in modern-daySudan. Archaeological and historical evidence support interactions between Egyptian and Nubian populations more than 5000 years ago, with socio-political dynamics between Egyptians and Nubians ranging from peaceful coexistence to variably successful attempts of conquest. A study on sixty-six ancient Nubian individuals revealed significant contact with ancient Egyptians, characterized by the presence ofc.57% Neolithic/Bronze Age Levantine ancestry in these individuals. Such geneflow of Levantine-like ancestry corresponds with archaeological and botanic evidence, pointing to a Neolithic movement around 7,000 years ago.[197][198]
Modern Egyptians, like modern Nubians, also underwent subsequent admixture events, contributing both "Sub-Saharan" African-like and West Asian-like ancestries, since theRoman period, with significance on theAfrican Slave Tradeand theSpread of Islam.[197][198]
Some scholars, such asChristopher Ehret, caution that a wider sampling area is needed and argue that the current data is inconclusive on the origin of ancient Egyptians. They also point out issues with the previously used methodology such as the sampling size, comparative approach and a "biased interpretation" of the genetic data. They argue in favor for a link between Ancient Egypt and the northernHorn of Africa. This latter view has been attributed to the correspondingarchaeological,genetic,linguisticandbiological anthropologicalsources of evidence which broadly indicate that the earliest Egyptians and Nubians were the descendants of populations in northeast Africa.[199][200][201][196]
The culture and monuments of ancient Egypt have left a lasting legacy on the world. Egyptian civilization significantly influenced theKingdom of KushandMeroëwith both adopting Egyptian religious and architectural norms (hundreds of pyramids (6–30 meters high) were built in Egypt/Sudan), as well as using Egyptian writing as the basis of theMeroitic script.[202]Meroitic is the oldest written language in Africa, other than Egyptian, and was used from the 2nd century BC until the early 5th century AD.[203]The cult of the goddessIsis, for example, became popular in theRoman Empire, as obelisks and other relics were transported back to Rome.[204]The Romans also importedbuilding materialsfrom Egypt to erect Egyptian-style structures. Early historians such as Herodotus,Strabo, andDiodorus Siculusstudied and wrote about the land, which Romans came to view as a place of mystery.[205]
During theMiddle Agesand theRenaissance, Egyptian pagan culture was in decline after the rise of Christianity and laterIslam, but interest in Egyptian antiquity continued in the writings of medieval scholars such asDhul-Nun al-Misriandal-Maqrizi.[206]In the seventeenth and eighteenth centuries, European travelers and tourists brought back antiquities and wrote stories of their journeys, leading to a wave ofEgyptomaniaacross Europe, as evident in symbolism such as theEye of Providenceand theGreat Seal of the United States. This renewed interest sent collectors to Egypt, who took, purchased, or were given many important antiquities.[207]Napoleonarranged the first studies inEgyptologywhen he brought some 150 scientists and artists to study and document Egypt'snatural history, which was published in theDescription de l'Égypte.[208]
In the 20th century, the Egyptian Government and archaeologists alike recognized the importance of cultural respect and integrity in excavations. Since the 2010s, theMinistry of Tourism and Antiquitieshas overseen excavations and the recovery of artifacts.[209]
Evolutionis the change in theheritablecharacteristicsof biological populations over successive generations.[1][2]It occurs when evolutionary processes such asnatural selectionandgenetic driftact on genetic variation, resulting in certain characteristics becoming more or less common within a population over successive generations.[3]The process of evolution has given rise tobiodiversityat every level ofbiological organisation.[4][5]
Thescientific theoryof evolution by natural selection was conceived independently by two British naturalists,Charles DarwinandAlfred Russel Wallace, in the mid-19th century as an explanation for why organisms are adapted to their physical and biological environments. The theory was first set out in detail in Darwin's bookOn the Origin of Species.[6]Evolution by natural selection is established by observable facts about living organisms: (1) more offspring are often produced than can possibly survive; (2)traits varyamong individuals with respect to theirmorphology,physiology, and behaviour; (3) different traits confer different rates of survival and reproduction (differentialfitness); and (4) traits can be passed from generation to generation (heritabilityof fitness).[7]In successive generations, members of a population are therefore more likely to be replaced by theoffspringof parents with favourable characteristics for that environment.
In the early 20th century,competing ideas of evolutionwererefutedand evolution was combined withMendelian inheritanceandpopulation geneticsto give rise to modern evolutionary theory.[8]In this synthesisthe basis for heredity is inDNAmolecules that pass information from generation to generation. The processes that change DNA in a population include natural selection, genetic drift,mutation, andgene flow.[3]
All life on Earth—includinghumanity—shares alast universal common ancestor(LUCA),[9][10][11]which lived approximately 3.5–3.8 billion years ago.[12]Thefossilrecord includes a progression from earlybiogenicgraphite[13]tomicrobial matfossils[14][15][16]to fossilisedmulticellular organisms. Existing patterns of biodiversity have been shaped by repeated formations of new species (speciation), changes within species (anagenesis), and loss of species (extinction) throughout the evolutionaryhistory of lifeon Earth.[17]Morphologicalandbiochemicaltraits tend to be more similar among species that share a morerecent common ancestor, which historically was used to reconstructphylogenetic trees, although direct comparison of genetic sequences is a more common method today.[18][19]
Evolutionary biologistshave continued to study various aspects of evolution by forming and testinghypothesesas well as constructing theories based onevidencefrom the field or laboratory and on data generated by the methods ofmathematical and theoretical biology. Their discoveries have influenced not just the development ofbiologybut also other fields including agriculture, medicine, andcomputer science.[20]
Evolution in organisms occurs through changes in heritable characteristics—the inherited characteristics of an organism. In humans, for example,eye colouris an inherited characteristic and an individual might inherit the "brown-eye trait" from one of their parents.[21]Inherited traits are controlled by genes and the complete set of genes within an organism'sgenome(genetic material) is called itsgenotype.[22]
The complete set of observable traits that make up the structure and behaviour of an organism is called itsphenotype. Some of these traits come from the interaction of its genotype with the environment while others are neutral.[23]Some observable characteristics are not inherited. For example,suntannedskin comes from the interaction between a person's genotype and sunlight; thus, suntans are not passed on to people's children. The phenotype is the ability of the skin to tan when exposed to sunlight. However, some people tan more easily than others, due to differences in genotypic variation; a striking example are people with the inherited trait ofalbinism, who do not tan at all and are very sensitive tosunburn.[24]
Heritable characteristics are passed from one generation to the next viaDNA, amoleculethat encodes genetic information.[22]DNA is a longbiopolymercomposed of four types of bases. The sequence of bases along a particular DNA molecule specifies the genetic information, in a manner similar to a sequence of letters spelling out a sentence. Before a cell divides, the DNA is copied, so that each of the resulting two cells will inherit the DNA sequence. Portions of a DNA molecule that specify a single functional unit are called genes; different genes have different sequences of bases. Within cells, each long strand of DNA is called achromosome. The specific location of a DNA sequence within a chromosome is known as alocus. If the DNA sequence at a locus varies between individuals, the different forms of this sequence are called alleles. DNA sequences can change through mutations, producing new alleles. If a mutation occurs within a gene, the new allele may affect the trait that the gene controls, altering the phenotype of the organism.[25]However, while this simple correspondence between an allele and a trait works in some cases, most traits are influenced by multiple genes in aquantitativeorepistaticmanner.[26][27]
Evolution can occur if there is genetic variation within a population. Variation comes from mutations in the genome, reshuffling of genes throughsexual reproductionand migration between populations (gene flow). Despite the constant introduction of new variation through mutation and gene flow, most of the genome of a species is very similar among all individuals of that species.[28]However, discoveries in the field ofevolutionary developmental biologyhave demonstrated that even relatively small differences in genotype can lead to dramatic differences in phenotype both within and between species.
An individual organism's phenotype results from both its genotype and the influence of the environment it has lived in.[27]The modern evolutionary synthesis defines evolution as the change over time in this genetic variation. The frequency of one particular allele will become more or less prevalent relative to other forms of that gene. Variation disappears when a new allele reaches the point offixation—when it either disappears from the population or replaces the ancestral allele entirely.[29]
Mutations are changes in the DNA sequence of a cell's genome and are the ultimate source of genetic variation in all organisms.[30]When mutations occur, they may alter theproduct of a gene, or prevent the gene from functioning, or have no effect.
About half of the mutations in the coding regions of protein-coding genes are deleterious — the other half are neutral. A small percentage of the total mutations in this region confer a fitness benefit.[31]Some of the mutations in other parts of the genome are deleterious but the vast majority are neutral. A few are beneficial.
Mutations can involve large sections of a chromosome becomingduplicated(usually bygenetic recombination), which can introduce extra copies of a gene into a genome.[32]Extra copies of genes are a major source of the raw material needed for new genes to evolve.[33]This is important because most new genes evolve withingene familiesfrom pre-existing genes that share common ancestors.[34]For example, thehuman eyeuses four genes to make structures that sense light: three forcolour visionand one fornight vision; all four are descended from a single ancestral gene.[35]
New genes can be generated from an ancestral gene when a duplicate copy mutates and acquires a new function. This process is easier once a gene has been duplicated because it increases theredundancyof the system; one gene in the pair can acquire a new function while the other copy continues to perform its original function.[36][37]Other types of mutations can even generate entirely new genes from previously noncoding DNA, a phenomenon termedde novogene birth.[38][39]
The generation of new genes can also involve small parts of several genes being duplicated, with these fragments then recombining to form new combinations with new functions (exon shuffling).[40][41]When new genes are assembled from shuffling pre-existing parts,domainsact as modules with simple independent functions, which can be mixed together to produce new combinations with new and complex functions.[42]For example,polyketide synthasesare largeenzymesthat makeantibiotics; they contain up to 100 independent domains that each catalyse one step in the overall process, like a step in an assembly line.[43]
One example of mutation iswild boarpiglets. They are camouflage coloured and show a characteristic pattern of dark and light longitudinal stripes. However, mutations in themelanocortin 1 receptor(MC1R) disrupt the pattern. The majority of pig breeds carry MC1R mutations disrupting wild-type colour and different mutations causing dominant black colouring.[44]
Inasexualorganisms, genes are inherited together, orlinked, as they cannot mix with genes of other organisms during reproduction. In contrast, the offspring of sexual organisms contain random mixtures of their parents' chromosomes that are produced through independent assortment. In a related process calledhomologous recombination, sexual organisms exchange DNA between two matching chromosomes.[45]Recombination and reassortment do not alter allele frequencies, but instead change which alleles are associated with each other, producing offspring with new combinations of alleles.[46]Sex usually increases genetic variation and may increase the rate of evolution.[47][48]
The two-fold cost of sex was first described byJohn Maynard Smith.[49]The first cost is that in sexually dimorphic species only one of the two sexes can bear young. This cost does not apply to hermaphroditic species, like most plants and manyinvertebrates. The second cost is that any individual who reproduces sexually can only pass on 50% of its genes to any individual offspring, with even less passed on as each new generation passes.[50]Yet sexual reproduction is the more common means of reproduction among eukaryotes and multicellular organisms. TheRed Queen hypothesishas been used to explain the significance of sexual reproduction as a means to enable continual evolution and adaptation in response tocoevolutionwith other species in an ever-changing environment.[50][51][52][53]Another hypothesis is that sexual reproduction is primarily an adaptation for promoting accurate recombinational repair of damage in germline DNA, and that increased diversity is a byproduct of this process that may sometimes be adaptively beneficial.[54][55]
Gene flow is the exchange of genes between populations and between species.[56]It can therefore be a source of variation that is new to a population or to a species. Gene flow can be caused by the movement of individuals between separate populations of organisms, as might be caused by the movement of mice between inland and coastal populations, or the movement ofpollenbetween heavy-metal-tolerant and heavy-metal-sensitive populations of grasses.
Gene transfer between species includes the formation ofhybridorganisms andhorizontal gene transfer. Horizontal gene transfer is the transfer of genetic material from one organism to another organism that is not its offspring; this is most common among bacteria.[57]In medicine, this contributes to the spread ofantibiotic resistance, as when one bacteria acquires resistance genes it can rapidly transfer them to other species.[58]Horizontal transfer of genes from bacteria to eukaryotes such as the yeastSaccharomyces cerevisiaeand the adzuki bean weevilCallosobruchus chinensishas occurred.[59][60]An example of larger-scale transfers are the eukaryoticbdelloid rotifers, which have received a range of genes from bacteria, fungi and plants.[61]Viruses can also carry DNA between organisms, allowing transfer of genes even acrossbiological domains.[62]
Large-scale gene transfer has also occurred between the ancestors ofeukaryotic cellsand bacteria, during the acquisition ofchloroplastsandmitochondria. It is possible that eukaryotes themselves originated from horizontal gene transfers between bacteria andarchaea.[63]
Some heritable changes cannot be explained by changes to the sequence ofnucleotidesin the DNA. These phenomena are classed as epigenetic inheritance systems.[64]DNA methylationmarkingchromatin, self-sustaining metabolic loops, gene silencing byRNA interferenceand the three-dimensionalconformationofproteins(such asprions) are areas where epigenetic inheritance systems have been discovered at the organismic level.[65]Developmental biologists suggest that complex interactions ingenetic networksand communication among cells can lead to heritable variations that may underlay some of the mechanics indevelopmental plasticityandcanalisation.[66]Heritability may also occur at even larger scales. For example, ecological inheritance through the process ofniche constructionis defined by the regular and repeated activities of organisms in their environment. This generates a legacy of effects that modify and feed back into the selection regime of subsequent generations.[67]Other examples of heritability in evolution that are not under the direct control of genes include the inheritance of cultural traits andsymbiogenesis.[68][69]
From aneo-Darwinianperspective, evolution occurs when there are changes in the frequencies of alleles within a population of interbreeding organisms,[70]for example, the allele for black colour in a population of moths becoming more common. Mechanisms that can lead to changes in allele frequencies include natural selection, genetic drift, and mutation bias.
Evolution by natural selection is the process by which traits that enhance survival and reproduction become more common in successive generations of a population. It embodies three principles:[7]
More offspring are produced than can possibly survive, and these conditions produce competition between organisms for survival and reproduction. Consequently, organisms with traits that give them an advantage over their competitors are more likely to pass on their traits to the next generation than those with traits that do not confer an advantage.[71]Thisteleonomyis the quality whereby the process of natural selection creates and preserves traits that areseemingly fittedfor thefunctionalroles they perform.[72]Consequences of selection includenonrandom mating[73]andgenetic hitchhiking.
The central concept of natural selection is theevolutionary fitnessof an organism.[74]Fitness is measured by an organism's ability to survive and reproduce, which determines the size of its genetic contribution to the next generation.[74]However, fitness is not the same as the total number of offspring: instead fitness is indicated by the proportion of subsequent generations that carry an organism's genes.[75]For example, if an organism could survive well and reproduce rapidly, but its offspring were all too small and weak to survive, this organism would make little genetic contribution to future generations and would thus have low fitness.[74]
If an allele increases fitness more than the other alleles of that gene, then with each generation this allele has a higher probability of becoming common within the population. These traits are said to be selectedfor. Examples of traits that can increase fitness are enhanced survival and increasedfecundity. Conversely, the lower fitness caused by having a less beneficial or deleterious allele results in this allele likely becoming rarer—they are selectedagainst.[76]
Importantly, the fitness of an allele is not a fixed characteristic; if the environment changes, previously neutral or harmful traits may become beneficial and previously beneficial traits become harmful.[25]However, even if the direction of selection does reverse in this way, traits that were lost in the past may not re-evolve in an identical form.[77][78]However, a re-activation of dormant genes, as long as they have not been eliminated from the genome and were only suppressed perhaps for hundreds of generations, can lead to the re-occurrence of traits thought to be lost like hindlegs in dolphins, teeth in chickens, wings in wingless stick insects, tails and additional nipples in humans etc. "Throwbacks" such as these are known asatavisms.[79]
Natural selection within a population for a trait that can vary across a range of values, such as height, can be categorised into three different types. The first isdirectional selection, which is a shift in the average value of a trait over time—for example, organisms slowly getting taller.[80]Secondly,disruptive selectionis selection for extreme trait values and often results intwo different valuesbecoming most common, with selection against the average value. This would be when either short or tall organisms had an advantage, but not those of medium height. Finally, instabilising selectionthere is selection against extreme trait values on both ends, which causes a decrease invariancearound the average value and less diversity.[71][81]This would, for example, cause organisms to eventually have a similar height.
Natural selection most generally makes nature the measure against which individuals and individual traits, are more or less likely to survive. "Nature" in this sense refers to anecosystem, that is, a system in which organisms interact with every other element,physicalas well asbiological, in their local environment.Eugene Odum, a founder of ecology, defined an ecosystem as: "Any unit that includes all of the organisms...in a given area interacting with the physical environment so that a flow of energy leads to clearly defined trophic structure, biotic diversity, and material cycles (i.e., exchange of materials between living and nonliving parts) within the system...."[82]Each population within an ecosystem occupies a distinctniche, or position, with distinct relationships to other parts of the system. These relationships involve the life history of the organism, its position in thefood chainand its geographic range. This broad understanding of nature enables scientists to delineate specific forces which, together, comprise natural selection.
Natural selection can act atdifferent levels of organisation, such as genes, cells, individual organisms, groups of organisms and species.[83][84][85]Selection can act at multiple levels simultaneously.[86]An example of selection occurring below the level of the individual organism are genes calledtransposons, which can replicate and spread throughout a genome.[87]Selection at a level above the individual, such asgroup selection, may allow the evolution of cooperation.[88]
Genetic drift is the random fluctuation ofallele frequencieswithin a population from one generation to the next.[89]When selective forces are absent or relatively weak, allele frequencies are equally likely todriftupward or downward[clarification needed]in each successive generation because the alleles are subject tosampling error.[90]This drift halts when an allele eventually becomes fixed, either by disappearing from the population or by replacing the other alleles entirely. Genetic drift may therefore eliminate some alleles from a population due to chance alone. Even in the absence of selective forces, genetic drift can cause two separate populations that begin with the same genetic structure to drift apart into two divergent populations with different sets of alleles.[91]
According to theneutral theory of molecular evolutionmost evolutionary changes are the result of the fixation ofneutral mutationsby genetic drift.[92]In this model, most genetic changes in a population are thus the result of constant mutation pressure and genetic drift.[93]This form of the neutral theory has been debated since it does not seem to fit some genetic variation seen in nature.[94][95]A better-supported version of this model is thenearly neutral theory, according to which a mutation that would be effectively neutral in a small population is not necessarily neutral in a large population.[71]Other theories propose that genetic drift is dwarfed by otherstochasticforces in evolution, such as genetic hitchhiking, also known as genetic draft.[90][96][97]Another concept isconstructive neutral evolution(CNE), which explains that complex systems can emerge and spread into a population through neutral transitions due to the principles of excess capacity, presuppression, and ratcheting,[98][99][100]and it has been applied in areas ranging from the origins of thespliceosometo the complex interdependence ofmicrobial communities.[101][102][103]
The time it takes a neutral allele to become fixed by genetic drift depends on population size; fixation is more rapid in smaller populations.[104]The number of individuals in a population is not critical, but instead a measure known as the effective population size.[105]The effective population is usually smaller than the total population since it takes into account factors such as the level of inbreeding and the stage of the lifecycle in which the population is the smallest.[105]The effective population size may not be the same for every gene in the same population.[106]
It is usually difficult to measure the relative importance of selection and neutral processes, including drift.[107]The comparative importance of adaptive and non-adaptive forces in driving evolutionary change is an area ofcurrent research.[108]
Mutation biasis usually conceived as a difference in expected rates for two different kinds of mutation, e.g., transition-transversion bias, GC-AT bias, deletion-insertion bias. This is related to the idea ofdevelopmental bias.J. B. S. Haldane[109]andRonald Fisher[110]argued that, because mutation is a weak pressure easily overcome by selection, tendencies of mutation would be ineffectual except under conditions of neutral evolution or extraordinarily high mutation rates. This opposing-pressures argument was long used to dismiss the possibility of internal tendencies in evolution,[111]until the molecular era prompted renewed interest in neutral evolution.
Noboru Sueoka[112]andErnst Freese[113]proposed that systematic biases in mutation might be responsible for systematic differences in genomic GC composition between species. The identification of a GC-biasedE. colimutator strain in 1967,[114]along with the proposal of theneutral theory, established the plausibility of mutational explanations for molecular patterns, which are now common in the molecular evolution literature.
For instance, mutation biases are frequently invoked in models of codon usage.[115]Such models also include effects of selection, following the mutation-selection-drift model,[116]which allows both for mutation biases and differential selection based on effects on translation. Hypotheses of mutation bias have played an important role in the development of thinking about the evolution of genome composition, including isochores.[117]Different insertion vs. deletion biases in differenttaxacan lead to the evolution of different genome sizes.[118][119]The hypothesis of Lynch regarding genome size relies on mutational biases toward increase or decrease in genome size.
However, mutational hypotheses for the evolution of composition suffered a reduction in scope when it was discovered that (1) GC-biased gene conversion makes an important contribution to composition in diploid organisms such as mammals[120]and (2) bacterial genomes frequently have AT-biased mutation.[121]
Contemporary thinking about the role of mutation biases reflects a different theory from that of Haldane and Fisher. More recent work[111]showed that the original "pressures" theory assumes that evolution is based on standing variation: when evolution depends on events of mutation that introduce new alleles, mutational and developmentalbiases in the introduction of variation(arrival biases) can impose biases on evolution without requiring neutral evolution or high mutation rates.[111][122]Several studies report that the mutations implicated in adaptation reflect common mutation biases[123][124][125]though others dispute this interpretation.[126]
Recombination allows alleles on the same strand of DNA to become separated. However, the rate of recombination is low (approximately two events per chromosome per generation). As a result, genes close together on a chromosome may not always be shuffled away from each other and genes that are close together tend to be inherited together, a phenomenon known aslinkage.[127]This tendency is measured by finding how often two alleles occur together on a single chromosome compared toexpectations, which is called theirlinkage disequilibrium. A set of alleles that is usually inherited in a group is called ahaplotype. This can be important when one allele in a particular haplotype is strongly beneficial: natural selection can drive aselective sweepthat will also cause the other alleles in the haplotype to become more common in the population; this effect is called genetic hitchhiking or genetic draft.[128]Genetic draft caused by the fact that some neutral genes are genetically linked to others that are under selection can be partially captured by an appropriate effective population size.[96]
A special case of natural selection is sexual selection, which is selection for any trait that increases mating success by increasing the attractiveness of an organism to potential mates.[130]Traits that evolved through sexual selection are particularly prominent among males of several animal species. Although sexually favoured, traits such as cumbersome antlers, mating calls, large body size and bright colours often attract predation, which compromises the survival of individual males.[131][132]This survival disadvantage is balanced by higher reproductive success in males that show thesehard-to-fake, sexually selected traits.[133]
Evolution influences every aspect of the form and behaviour of organisms. Most prominent are the specific behavioural and physical adaptations that are the outcome of natural selection. These adaptations increase fitness by aiding activities such as finding food, avoidingpredatorsor attracting mates. Organisms can also respond to selection bycooperatingwith each other, usually by aiding their relatives or engaging in mutually beneficialsymbiosis. In the longer term, evolution produces new species through splitting ancestral populations of organisms into new groups that cannot or will not interbreed. These outcomes of evolution are distinguished based on time scale asmacroevolutionversus microevolution. Macroevolution refers to evolution that occurs at or above the level of species, in particular speciation and extinction, whereas microevolution refers to smaller evolutionary changes within a species or population, in particular shifts in allele frequency and adaptation.[135]Macroevolution is the outcome of long periods of microevolution.[136]Thus, the distinction between micro- and macroevolution is not a fundamental one—the difference is simply the time involved.[137]However, in macroevolution, the traits of the entire species may be important. For instance, a large amount of variation among individuals allows a species to rapidly adapt to newhabitats, lessening the chance of it going extinct, while a wide geographic range increases the chance of speciation, by making it more likely that part of the population will become isolated. In this sense, microevolution and macroevolution might involve selection at different levels—with microevolution acting on genes and organisms, versus macroevolutionary processes such asspecies selectionacting on entire species and affecting their rates of speciation and extinction.[138][139][140]
A common misconception is that evolution has goals, long-term plans, or an innate tendency for "progress", as expressed in beliefs such asorthogenesisand evolutionism; realistically, however, evolution has no long-term goal and does not necessarily produce greater complexity.[141][142][143]Althoughcomplex specieshave evolved, they occur as a side effect of the overall number of organisms increasing, and simple forms of life still remain more common in the biosphere.[144]For example, the overwhelming majority of species are microscopicprokaryotes, which form about half the world'sbiomassdespite their small size[145]and constitute the vast majority of Earth's biodiversity.[146]Simple organisms have therefore been the dominant form of life on Earth throughout its history and continue to be the main form of life up to the present day, with complex life only appearing more diverse because it ismore noticeable.[147]Indeed, the evolution of microorganisms is particularly important to evolutionary research since their rapid reproduction allows the study ofexperimental evolutionand the observation of evolution and adaptation in real time.[148][149]
Adaptation is the process that makes organisms better suited to their habitat.[150][151]Also, the term adaptation may refer to a trait that is important for an organism's survival. For example, the adaptation of horses' teeth to the grinding of grass. By using the termadaptationfor the evolutionary process andadaptive traitfor the product (the bodily part or function), the two senses of the word may be distinguished. Adaptations are produced by natural selection.[152]The following definitions are due to Theodosius Dobzhansky:
Adaptation may cause either the gain of a new feature, or the loss of an ancestral feature. An example that shows both types of change is bacterial adaptation to antibiotic selection, with genetic changes causing antibiotic resistance by both modifying the target of the drug, or increasing the activity of transporters that pump the drug out of the cell.[156]Other striking examples are the bacteriaEscherichia colievolving the ability to usecitric acidas a nutrient in along-term laboratory experiment,[157]Flavobacteriumevolving a novel enzyme that allows these bacteria to grow on the by-products of nylon manufacturing,[158][159]and the soil bacteriumSphingobiumevolving an entirely newmetabolic pathwaythat degrades the syntheticpesticidepentachlorophenol.[160][161]An interesting but still controversial idea is that some adaptations might increase the ability of organisms to generate genetic diversity and adapt by natural selection (increasing organisms' evolvability).[162][163][164][165]
Adaptation occurs through the gradual modification of existing structures. Consequently, structures with similar internal organisation may have different functions in related organisms. This is the result of a single ancestral structure being adapted to function in different ways. The bones within bat wings, for example, are very similar to those in mice feet andprimatehands, due to the descent of all these structures from a common mammalian ancestor.[167]However, since all living organisms are related to some extent,[168]even organs that appear to have little or no structural similarity, such asarthropod,squidandvertebrateeyes, or the limbs and wings of arthropods and vertebrates, can depend on a common set of homologous genes that control their assembly and function; this is calleddeep homology.[169][170]
During evolution, some structures may lose their original function and become vestigial structures.[171]Such structures may have little or no function in a current species, yet have a clear function in ancestral species, or other closely related species. Examples includepseudogenes,[172]the non-functional remains of eyes in blind cave-dwelling fish,[173]wings in flightless birds,[174]the presence of hip bones in whales and snakes,[166]and sexual traits in organisms that reproduce via asexual reproduction.[175]Examples ofvestigial structures in humansincludewisdom teeth,[176]thecoccyx,[171]thevermiform appendix,[171]and other behavioural vestiges such asgoose bumps[177][178]andprimitive reflexes.[179][180][181]
However, many traits that appear to be simple adaptations are in factexaptations: structures originally adapted for one function, but which coincidentally became somewhat useful for some other function in the process.[182]One example is the African lizardHolaspis guentheri, which developed an extremely flat head for hiding in crevices, as can be seen by looking at its near relatives. However, in this species, the head has become so flattened that it assists in gliding from tree to tree—an exaptation.[182]Within cells,molecular machinessuch as the bacterialflagella[183]andprotein sorting machinery[184]evolved by the recruitment of several pre-existing proteins that previously had different functions.[135]Another example is the recruitment of enzymes fromglycolysisandxenobiotic metabolismto serve as structural proteins calledcrystallinswithin the lenses of organisms' eyes.[185][186]
An area of current investigation in evolutionary developmental biology is the developmental basis of adaptations and exaptations.[187]This research addresses the origin and evolution ofembryonic developmentand how modifications of development and developmental processes produce novel features.[188]These studies have shown that evolution can alter development to produce new structures, such as embryonic bone structures that develop into the jaw in other animals instead forming part of themiddle ear in mammals.[189]It is also possible for structures that have been lost in evolution to reappear due to changes in developmental genes, such as a mutation in chickens causing embryos to grow teeth similar to those of crocodiles.[190]It is now becoming clear that most alterations in the form of organisms are due to changes in a small set of conserved genes.[191]
Interactions between organisms can produce both conflict and cooperation. When the interaction is between pairs of species, such as apathogenand ahost, or a predator and its prey, these species can develop matched sets of adaptations. Here, the evolution of one species causes adaptations in a second species. These changes in the second species then, in turn, cause new adaptations in the first species. This cycle of selection and response is called coevolution.[192]An example is the production oftetrodotoxinin therough-skinned newtand the evolution of tetrodotoxin resistance in its predator, thecommon garter snake. In this predator-prey pair, anevolutionary arms racehas produced high levels of toxin in the newt and correspondingly high levels of toxin resistance in the snake.[193]
Not all co-evolved interactions between species involve conflict.[194]Many cases of mutually beneficial interactions have evolved. For instance, an extreme cooperation exists between plants and themycorrhizalfungi that grow on their roots and aid the plant in absorbing nutrients from the soil.[195]This is areciprocalrelationship as the plants provide the fungi with sugars fromphotosynthesis. Here, the fungi actually grow inside plant cells, allowing them to exchange nutrients with their hosts, while sendingsignalsthat suppress the plantimmune system.[196]
Coalitions between organisms of the same species have also evolved. An extreme case is theeusocialityfound in social insects, such asbees,termitesandants, where sterile insects feed and guard the small number of organisms in acolonythat are able to reproduce. On an even smaller scale, the somatic cells that make up the body of an animal limit their reproduction so they can maintain a stable organism, which then supports a small number of the animal's germ cells to produce offspring. Here, somatic cells respond to specific signals that instruct them whether to grow, remain as they are, or die. If cells ignore these signals and multiply inappropriately, their uncontrolled growthcauses cancer.[197]
Such cooperation within species may have evolved through the process ofkin selection, which is where one organism acts to help raise a relative's offspring.[198]This activity is selected for because if thehelpingindividual contains alleles which promote the helping activity, it is likely that its kin willalsocontain these alleles and thus those alleles will be passed on.[199]Other processes that may promote cooperation include group selection, where cooperation provides benefits to a group of organisms.[200]
Speciation is the process where a species diverges into two or more descendant species.[201]
There are multiple ways to define the concept of "species". The choice of definition is dependent on the particularities of the species concerned.[202]For example, some species concepts apply more readily toward sexually reproducing organisms while others lend themselves better toward asexual organisms. Despite the diversity of various species concepts, these various concepts can be placed into one of three broad philosophical approaches: interbreeding, ecological and phylogenetic.[203]The Biological Species Concept (BSC) is a classic example of the interbreeding approach. Defined by evolutionary biologistErnst Mayrin 1942, the BSC states that "species are groups of actually or potentially interbreeding natural populations, which are reproductively isolated from other such groups."[204]Despite its wide and long-term use, the BSC like other species concepts is not without controversy, for example, because genetic recombination among prokaryotes is not an intrinsic aspect of reproduction;[205]this is called thespecies problem.[202]Some researchers have attempted a unifying monistic definition of species, while others adopt a pluralistic approach and suggest that there may be different ways to logically interpret the definition of a species.[202][203]
Barriers to reproductionbetween two diverging sexual populations are required for the populations to become new species. Gene flow may slow this process by spreading the new genetic variants also to the other populations. Depending on how far two species have diverged since theirmost recent common ancestor, it may still be possible for them to produce offspring, as with horses and donkeys mating to producemules.[206]Such hybrids are generallyinfertile. In this case, closely related species may regularly interbreed, but hybrids will be selected against and the species will remain distinct. However, viable hybrids are occasionally formed and these new species can either have properties intermediate between their parent species, or possess a totally new phenotype.[207]The importance of hybridisation in producingnew speciesof animals is unclear, although cases have been seen in many types of animals,[208]with thegrey tree frogbeing a particularly well-studied example.[209]
Speciation has been observed multiple times under bothcontrolled laboratory conditionsand in nature.[210]In sexually reproducing organisms, speciation results from reproductive isolation followed by genealogical divergence. There are four primary geographic modes of speciation. The most common in animals isallopatric speciation, which occurs in populations initially isolated geographically, such as byhabitat fragmentationor migration. Selection under these conditions can produce very rapid changes in the appearance and behaviour of organisms.[211][212]As selection and drift act independently on populations isolated from the rest of their species, separation may eventually produce organisms that cannot interbreed.[213]
The second mode of speciation isperipatric speciation, which occurs when small populations of organisms become isolated in a new environment. This differs from allopatric speciation in that the isolated populations are numerically much smaller than the parental population. Here, thefounder effectcauses rapid speciation after an increase ininbreedingincreases selection on homozygotes, leading to rapid genetic change.[214]
The third mode isparapatric speciation. This is similar to peripatric speciation in that a small population enters a new habitat, but differs in that there is no physical separation between these two populations. Instead, speciation results from the evolution of mechanisms that reduce gene flow between the two populations.[201]Generally this occurs when there has been a drastic change in the environment within the parental species' habitat. One example is the grassAnthoxanthumodoratum, which can undergo parapatric speciation in response to localised metal pollution from mines.[215]Here, plants evolve that have resistance to high levels of metals in the soil. Selection against interbreeding with the metal-sensitive parental population produced a gradual change in the flowering time of the metal-resistant plants, which eventually produced complete reproductive isolation. Selection against hybrids between the two populations may causereinforcement, which is the evolution of traits that promote mating within a species, as well ascharacter displacement, which is when two species become more distinct in appearance.[216]
Finally, insympatric speciationspecies diverge without geographic isolation or changes in habitat. This form is rare since even a small amount of gene flow may remove genetic differences between parts of a population.[217]Generally, sympatric speciation in animals requires the evolution of bothgenetic differencesand nonrandom mating, to allow reproductive isolation to evolve.[218]
One type of sympatric speciation involvescrossbreedingof two related species to produce a new hybrid species. This is not common in animals as animal hybrids are usually sterile. This is because duringmeiosisthehomologous chromosomesfrom each parent are from different species and cannot successfully pair. However, it is more common in plants because plants often double their number of chromosomes, to formpolyploids.[219]This allows the chromosomes from each parental species to form matching pairs during meiosis, since each parent's chromosomes are represented by a pair already.[220]An example of such a speciation event is when the plant speciesArabidopsis thalianaandArabidopsis arenosacrossbred to give the new speciesArabidopsis suecica.[221]This happened about 20,000 years ago,[222]and the speciation process has been repeated in the laboratory, which allows the study of the genetic mechanisms involved in this process.[223]Indeed, chromosome doubling within a species may be a common cause of reproductive isolation, as half the doubled chromosomes will be unmatched when breeding with undoubled organisms.[224]
Speciation events are important in the theory ofpunctuated equilibrium, which accounts for the pattern in the fossil record of short "bursts" of evolution interspersed with relatively long periods of stasis, where species remain relatively unchanged.[225]In this theory, speciation andrapid evolutionare linked, with natural selection and genetic drift acting most strongly on organisms undergoing speciation in novel habitats or small populations. As a result, the periods of stasis in the fossil record correspond to the parental population and the organisms undergoing speciation and rapid evolution are found in small populations or geographically restricted habitats and therefore rarely being preserved as fossils.[139]
Extinction is the disappearance of an entire species. Extinction is not an unusual event, as species regularly appear through speciation and disappear through extinction.[226]Nearly all animal and plant species that have lived on Earth are now extinct,[227]and extinction appears to be the ultimate fate of all species.[228]These extinctions have happened continuously throughout the history of life, although the rate of extinction spikes in occasional massextinction events.[229]TheCretaceous–Paleogene extinction event, during which the non-avian dinosaurs became extinct, is the most well-known, but the earlierPermian–Triassic extinction eventwas even more severe, with approximately 96% of all marine species driven to extinction.[229]TheHolocene extinctionevent is an ongoing mass extinction associated with humanity's expansion across the globe over the past few thousand years. Present-day extinction rates are 100–1000 times greater than the background rate and up to 30% of current species may be extinct by the mid 21st century.[230]Human activities are now the primary cause of the ongoing extinction event;[231][232]global warmingmay further accelerate it in the future.[233]Despite the estimated extinction of more than 99% of all species that ever lived on Earth,[234][235]about 1 trillion species are estimated to be on Earth currently with only one-thousandth of 1% described.[236]
The role of extinction in evolution is not very well understood and may depend on which type of extinction is considered.[229]The causes of the continuous "low-level" extinction events, which form the majority of extinctions, may be the result of competition between species for limited resources (thecompetitive exclusion principle).[237]If one species can out-compete another, this could produce species selection, with the fitter species surviving and the other species being driven to extinction.[84]The intermittent mass extinctions are also important, but instead of acting as a selective force, they drastically reduce diversity in a nonspecific manner and promote bursts of rapid evolution and speciation in survivors.[238]
Concepts and models used in evolutionary biology, such as natural selection, have many applications.[239]
Artificial selection is the intentional selection of traits in a population of organisms. This has been used for thousands of years in thedomesticationof plants and animals.[240]More recently, such selection has become a vital part ofgenetic engineering, withselectable markerssuch as antibiotic resistance genes being used to manipulate DNA. Proteins with valuable properties have evolved by repeated rounds of mutation and selection (for example modified enzymes and newantibodies) in a process calleddirected evolution.[241]
Understanding the changes that have occurred during an organism's evolution can reveal the genes needed to construct parts of the body, genes which may be involved in humangenetic disorders.[242]For example, theMexican tetrais analbinocavefish that lost its eyesight during evolution. Breeding together different populations of this blind fish produced some offspring with functional eyes, since different mutations had occurred in the isolated populations that had evolved in different caves.[243]This helped identify genes required for vision and pigmentation.[244]
Evolutionary theory has manyapplications in medicine. Many human diseases are not static phenomena, but capable of evolution. Viruses, bacteria, fungi and cancers evolve to be resistant to host immune defences, as well as topharmaceutical drugs.[245][246][247]These same problems occur in agriculture with pesticide[248]andherbicide[249]resistance. It is possible that we are facing the end of the effective life of most of available antibiotics[250]and predicting the evolution and evolvability[251]of our pathogens and devising strategies to slow or circumvent it is requiring deeper knowledge of the complex forces driving evolution at the molecular level.[252]
Incomputer science, simulations of evolution usingevolutionary algorithmsandartificial lifestarted in the 1960s and were extended with simulation of artificial selection.[253]Artificial evolution became a widely recognised optimisation method as a result of the work ofIngo Rechenbergin the 1960s. He usedevolution strategiesto solve complex engineering problems.[254]Genetic algorithmsin particular became popular through the writing ofJohn Henry Holland.[255]Practical applications also includeautomatic evolution of computer programmes.[256]Evolutionary algorithms are now used to solve multi-dimensional problems more efficiently than software produced by human designers and also to optimise the design of systems.[257]
The Earth isabout 4.54 billion years old.[258][259][260]The earliest undisputed evidence of life on Earth dates from at least 3.5 billion years ago,[12][261]during theEoarcheanEra after a geologicalcruststarted to solidify following the earlier moltenHadeanEon. Microbial mat fossils have been found in 3.48 billion-year-old sandstone in Western Australia.[14][15][16]Other early physical evidence of a biogenic substance is graphite in 3.7 billion-year-oldmetasedimentaryrocks discovered in Western Greenland[13]as well as "remains ofbiotic life" found in 4.1 billion-year-old rocks in Western Australia.[262][263]Commenting on the Australian findings,Stephen Blair Hedgeswrote: "If life arose relatively quickly on Earth, then it could be common in the universe."[262][264]In July 2016, scientists reported identifying a set of 355genesfrom thelast universal common ancestor(LUCA) of all organisms living on Earth.[265]
More than 99% of all species, amounting to over five billion species,[266]that ever lived on Earth are estimated to be extinct.[234][235]Estimates on the number of Earth's current species range from 10 million to 14 million,[267][268]of which about 1.9 million are estimated to have been named[269]and 1.6 million documented in a central database to date,[270]leaving at least 80% not yet described.
Highly energetic chemistry is thought to have produced a self-replicating molecule around 4 billion years ago, and half a billion years later the last common ancestor of all life existed.[10]The current scientific consensus is that the complex biochemistry that makes up life came from simpler chemical reactions.[271][272]The beginning of life may have included self-replicating molecules such asRNA[273]and the assembly of simple cells.[274]
All organisms on Earth are descended from a common ancestor or ancestralgene pool.[168][275]Current species are a stage in the process of evolution, with their diversity the product of a long series of speciation and extinction events.[276]The common descent of organisms was first deduced from four simple facts about organisms: First, they have geographic distributions that cannot be explained by local adaptation. Second, the diversity of life is not a set of completely unique organisms, but organisms that share morphological similarities. Third,vestigial traitswith no clear purpose resemble functional ancestral traits. Fourth, organisms can be classified using these similarities into a hierarchy of nested groups, similar to a family tree.[277]
Due to horizontal gene transfer, this "tree of life" may be more complicated than a simple branching tree, since some genes have spread independently between distantly related species.[278][279]To solve this problem and others, some authors prefer to use the "Coral of life" as a metaphor or a mathematical model to illustrate the evolution of life. This view dates back to an idea briefly mentioned by Darwin but later abandoned.[280]
Past species have also left records of their evolutionary history. Fossils, along with the comparative anatomy of present-day organisms, constitute the morphological, or anatomical, record.[281]By comparing the anatomies of both modern and extinct species, palaeontologists can infer the lineages of those species. However, this approach is most successful for organisms that had hard body parts, such as shells, bones or teeth. Further, as prokaryotes such as bacteria and archaea share a limited set of common morphologies, their fossils do not provide information on their ancestry.
More recently, evidence for common descent has come from the study of biochemical similarities between organisms. For example, all living cells use the same basic set of nucleotides andamino acids.[282]The development ofmolecular geneticshas revealed the record of evolution left in organisms' genomes: dating when species diverged through themolecular clockproduced by mutations.[283]For example, these DNA sequence comparisons have revealed that humans and chimpanzees share 98% of their genomes and analysing the few areas where they differ helps shed light on when the common ancestor of these species existed.[284]
Prokaryotes inhabited the Earth from approximately 3–4 billion years ago.[286][287]No obvious changes in morphology or cellular organisation occurred in these organisms over the next few billion years.[288]The eukaryotic cells emerged between 1.6 and 2.7 billion years ago. The next major change in cell structure came when bacteria were engulfed by eukaryotic cells, in a cooperative association calledendosymbiosis.[289][290]The engulfed bacteria and the host cell then underwent coevolution, with the bacteria evolving into either mitochondria orhydrogenosomes.[291]Another engulfment ofcyanobacterial-like organisms led to the formation of chloroplasts in algae and plants.[292]
The history of life was that of theunicellulareukaryotes, prokaryotes and archaea until around 1.7 billion years ago, whenmulticellular organismsbegan to appear, withdifferentiated cellsperforming specialised functions.[293]Theevolution of multicellularityoccurred in multiple independent events, in organisms as diverse assponges,brown algae, cyanobacteria,slime mouldsandmyxobacteria.[294]In January 2016, scientists reported that, about 800 million years ago, a minor genetic change in a single molecule called GK-PID may have allowed organisms to go from a single cell organism to one of many cells.[295]
Approximately 538.8 million years ago, a remarkable amount of biological diversity appeared over a span of around 10 million years in what is called theCambrian explosion. Here, the majority oftypesof modern animals appeared in the fossil record, as well as unique lineages that subsequently became extinct.[296]Various triggers for the Cambrian explosion have been proposed, including the accumulation of oxygen in the atmosphere from photosynthesis.[297]
About 500 million years ago, plants and fungi colonised the land and were soon followed by arthropods and other animals.[298]Insects were particularly successful and even today make up the majority of animal species.[299]Amphibiansfirst appeared around 364 million years ago, followed by earlyamniotesand birds around 155 million years ago (both from "reptile"-like lineages),mammalsaround 129 million years ago,Homininaearound 10 million years ago andmodern humansaround 250,000 years ago.[300][301][302]However, despite the evolution of these large animals, smaller organisms similar to the types that evolved early in this process continue to be highly successful and dominate the Earth, with the majority of both biomass and species being prokaryotes.[146]
The proposal that one type of organism could descend from another type goes back to some of the firstpre-SocraticGreek philosophers, such asAnaximanderandEmpedocles.[304]Such proposals survived into Roman times. The poet and philosopherLucretiusfollowed Empedocles in his masterworkDe rerum natura(lit.'On the Nature of Things').[305][306]
In contrast to thesematerialisticviews,Aristotelianismhad considered all natural things asactualisationsof fixed natural possibilities, known asforms.[307][308]This became part of a medievalteleologicalunderstanding ofnaturein which all things have an intended role to play in adivinecosmicorder. Variations of this idea became the standard understanding of theMiddle Agesand were integrated into Christian learning, but Aristotle did not demand that real types of organisms always correspond one-for-one with exact metaphysical forms and specifically gave examples of how new types of living things could come to be.[309]
A number of Arab Muslim scholars wrote about evolution, most notablyIbn Khaldun, who wrote the bookMuqaddimahin 1377, in which he asserted that humans developed from "the world of the monkeys", in a process by which "species become more numerous".[310]
The"New Science"of the 17th century rejected the Aristotelian approach. It sought to explain natural phenomena in terms ofphysical lawsthat were the same for all visible things and that did not require the existence of any fixed natural categories or divine cosmic order. However, this new approach was slow to take root in the biological sciences: the last bastion of the concept of fixed natural types.John Rayapplied one of the previously more general terms for fixed natural types, "species", to plant and animal types, but he strictly identified each type of living thing as a species and proposed that each species could be defined by the features that perpetuated themselves generation after generation.[311]Thebiological classificationintroduced byCarl Linnaeusin 1735 explicitly recognised the hierarchical nature of species relationships, but still viewed species as fixed according to a divine plan.[312]
Othernaturalistsof this time speculated on the evolutionary change of species over time according to natural laws. In 1751,Pierre Louis Maupertuiswrote of natural modifications occurring during reproduction and accumulating over many generations to produce new species.[313]Georges-Louis Leclerc, Comte de Buffon, suggested that species could degenerate into different organisms, andErasmus Darwinproposed that all warm-blooded animals could have descended from a single microorganism (or "filament").[314]The first full-fledged evolutionary scheme wasJean-Baptiste Lamarck's "transmutation" theory of 1809,[315]which envisagedspontaneous generationcontinually producing simple forms of life that developed greater complexity in parallel lineages with an inherent progressive tendency, and postulated that on a local level, these lineages adapted to the environment by inheriting changes caused by their use or disuse in parents.[316](The latter process was later calledLamarckism.)[316][317][318]These ideas were condemned by established naturalists as speculation lacking empirical support. In particular,Georges Cuvierinsisted that species were unrelated and fixed, their similarities reflecting divine design for functional needs. In the meantime, Ray's ideas of benevolent design had been developed byWilliam Paleyinto theNatural Theology or Evidences of the Existence and Attributes of the Deity(1802), which proposed complex adaptations as evidence of divine design and which was admired by Charles Darwin.[319][320]
The crucial break from the concept of constant typological classes or types in biology came with the theory of evolution through natural selection, which was formulated byCharles DarwinandAlfred Wallacein terms of variable populations. Darwin used the expressiondescent with modificationrather thanevolution.[321]Partly influenced byAn Essay on the Principle of Population(1798) byThomas Robert Malthus, Darwin noted that population growth would lead to a "struggle for existence" in which favourable variations prevailed as others perished. In each generation, many offspring fail to survive to an age of reproduction because of limited resources. This could explain the diversity of plants and animals from a common ancestry through the working of natural laws in the same way for all types of organism.[322][323][324][325]Darwin developed his theory of "natural selection" from 1838 onwards and was writing up his "big book" on the subject whenAlfred Russel Wallacesent him a version of virtually the same theory in 1858. Theirseparate paperswere presented together at an 1858 meeting of theLinnean Society of London.[326]At the end of 1859, Darwin's publication of his "abstract" asOn the Origin of Speciesexplained natural selection in detail and in a way that led to an increasingly wide acceptance ofDarwin's concepts of evolutionat the expense ofalternative theories.Thomas Henry Huxleyapplied Darwin's ideas to humans, usingpalaeontologyandcomparative anatomyto provide strong evidence that humans andapesshared a common ancestry. Some were disturbed by this since it implied that humans did not have a special place in theuniverse.[327]
Othniel C. Marsh, America's first palaeontologist, was the first to provide solid fossil evidence to support Darwin's theory of evolution by unearthing the ancestors of the modern horse.[328]In 1877, Marsh delivered a very influential speech before the annual meeting of the American Association for the Advancement of Science, providing a demonstrative argument for evolution. For the first time, Marsh traced the evolution of vertebrates from fish all the way through humans. Sparing no detail, he listed a wealth of fossil examples of past life forms. The significance of this speech was immediately recognised by the scientific community, and it was printed in its entirety in several scientific journals.[329][330]
In 1880, Marsh caught the attention of the scientific world with the publication ofOdontornithes: a Monograph on Extinct Birds of North America,which included his discoveries of birds with teeth. These skeletons helped bridge the gap between dinosaurs and birds, and provided invaluable support for Darwin's theory of evolution.[331]Darwin wrote to Marsh saying, "Your work on these old birds & on the many fossil animals of N. America has afforded the best support to the theory of evolution, which has appeared within the last 20 years" (since Darwin's publication ofOrigin of Species).[332][333]
The mechanisms of reproductive heritability and the origin of new traits remained a mystery. Towards this end, Darwin developed his provisional theory ofpangenesis.[334]In 1865,Gregor Mendelreported that traits were inherited in a predictable manner through theindependent assortmentand segregation of elements (later known as genes). Mendel's laws of inheritance eventually supplanted most of Darwin's pangenesis theory.[335]August Weismannmade the important distinction betweengerm cellsthat give rise togametes(such asspermandegg cells) and thesomatic cellsof the body, demonstrating that heredity passes through the germ line only.Hugo de Vriesconnected Darwin's pangenesis theory to Weismann's germ/soma cell distinction and proposed that Darwin's pangenes were concentrated in thecell nucleusand when expressed they could move into thecytoplasmto change thecell's structure. De Vries was also one of the researchers who made Mendel's work well known, believing that Mendelian traits corresponded to the transfer of heritable variations along the germline.[336]To explain how new variants originate, de Vries developeda mutation theorythat led to a temporary rift between those who accepted Darwinian evolution and biometricians who allied with de Vries.[337][338]In the 1930s, pioneers in the field ofpopulation genetics, such asRonald Fisher,Sewall WrightandJ. B. S. Haldaneset the foundations of evolution onto a robust statistical philosophy. The false contradiction between Darwin's theory, genetic mutations, andMendelian inheritancewas thus reconciled.[339]
In the 1920s and 1930s, themodern synthesisconnected natural selection and population genetics, based on Mendelian inheritance, into a unified theory that included random genetic drift, mutation, and gene flow. This new version of evolutionary theory focused on changes in allele frequencies in population. It explained patterns observed across species in populations, throughfossil transitionsin palaeontology.[339]
Since then, further syntheses have extended evolution's explanatory power in the light of numerous discoveries, to cover biological phenomena across the whole of thebiological hierarchyfrom genes to populations.[340]
The publication of the structure ofDNAbyJames WatsonandFrancis Crickwith contribution ofRosalind Franklinin 1953 demonstrated a physical mechanism for inheritance.[341]Molecular biologyimproved understanding of the relationship betweengenotypeandphenotype. Advances were also made in phylogeneticsystematics, mapping the transition of traits into a comparative and testable framework through the publication and use ofevolutionary trees.[342]In 1973, evolutionary biologistTheodosius Dobzhanskypenned that "nothing in biology makes sense except in the light of evolution", because it has brought to light the relations of what first seemed disjointed facts in natural history into a coherentexplanatorybody of knowledge that describes and predicts many observable facts about life on this planet.[343]
One extension, known asevolutionary developmental biologyand informally called "evo-devo", emphasises how changes between generations (evolution) act on patterns of change within individual organisms (development).[237][344]Since the beginning of the 21st century, some biologists have argued for anextended evolutionary synthesis, which would account for the effects of non-genetic inheritance modes, such asepigenetics,parental effects, ecological inheritance andcultural inheritance, andevolvability.[345][346]
In the 19th century, particularly after the publication ofOn the Origin of Speciesin 1859, the idea that life had evolved was an active source of academic debate centred on the philosophical, social and religious implications of evolution. Today, the modern evolutionary synthesis is accepted by a vast majority of scientists.[237]However, evolution remains a contentious concept for sometheists.[348]
Whilevarious religions and denominationshave reconciled their beliefs with evolution through concepts such astheistic evolution, there arecreationistswho believe that evolution is contradicted by thecreation mythsfound in their religions and who raise variousobjections to evolution.[135][349][350]As had been demonstrated by responses to the publication ofVestiges of the Natural History of Creationin 1844, the most controversial aspect of evolutionary biology is the implication ofhuman evolutionthat humans share common ancestry with apes and that the mental andmoral facultiesof humanity have the same types of natural causes as other inherited traits in animals.[351]In some countries, notably the United States, these tensions between science and religion have fuelled the current creation–evolution controversy, a religious conflict focusing on politics andpublic education.[352]While other scientific fields such ascosmology[353]andEarth science[354]also conflict with literal interpretations of manyreligious texts, evolutionary biology experiences significantly more opposition from religious literalists.
The teaching of evolution in American secondary school biology classes was uncommon in most of the first half of the 20th century. TheScopes Trialdecision of 1925 caused the subject to become very rare in American secondary biology textbooks for a generation, but it was gradually re-introduced later and became legally protected with the 1968Epperson v. Arkansasdecision. Since then, the competing religious belief of creationism was legally disallowed in secondary school curricula in various decisions in the 1970s and 1980s, but it returned inpseudoscientificform asintelligent design(ID), to be excluded once again in the 2005Kitzmiller v. Dover Area School Districtcase.[355]The debate over Darwin's ideas did not generate significant controversy in China.[356]
Geneticsis the study ofgenes,genetic variation, andheredityinorganisms.[1][2][3]It is an important branch inbiologybecause heredity is vital to organisms'evolution.Gregor Mendel, aMoravianAugustinianfriar working in the 19th century inBrno, was the first to study genetics scientifically. Mendel studied "trait inheritance", patterns in the way traits are handed down from parents to offspring over time. He observed that organisms (pea plants) inherit traits by way of discrete "units of inheritance". This term, still used today, is a somewhat ambiguous definition of what is referred to as a gene.
Traitinheritance andmolecularinheritance mechanisms of genes are still primary principles of genetics in the 21st century, but modern genetics has expanded to study the function and behavior of genes. Gene structure and function, variation, and distribution are studied within the context of thecell, the organism (e.g.dominance), and within the context of a population. Genetics has given rise to a number of subfields, includingmolecular genetics,epigenetics, andpopulation genetics. Organisms studied within the broad field span the domains of life (archaea,bacteria, andeukarya).
Genetic processes work in combination with an organism's environment and experiences to influence development andbehavior, often referred to asnature versus nurture. Theintracellularorextracellularenvironment of a living cell or organism may increase or decrease gene transcription. A classic example is two seeds of genetically identical corn, one placed in atemperate climateand one in anarid climate(lacking sufficient waterfall or rain). While the average height the two corn stalks could grow to is genetically determined, the one in the arid climate only grows to half the height of the one in the temperate climate due to lack of water and nutrients in its environment.
The wordgeneticsstems from theancient Greekγενετικόςgenetikosmeaning "genitive"/"generative", which in turn derives fromγένεσιςgenesismeaning "origin".[4][5][6]
The observation that living things inherittraitsfrom their parents has been used since prehistoric times to improve crop plants and animals throughselective breeding.[7][8]The modern science of genetics, seeking to understand this process, began with the work of theAugustinianfriarGregor Mendelin the mid-19th century.[9]
Prior to Mendel,Imre Festetics, aHungariannoble, who lived in Kőszeg before Mendel, was the first who used the word "genetic" in hereditarian context, and is considered the first geneticist. He described several rules of biological inheritance in his workThe genetic laws of nature(Die genetischen Gesetze der Natur, 1819).[10]His second law is the same as that which Mendel published.[11]In his third law, he developed the basic principles of mutation (he can be considered a forerunner ofHugo de Vries).[12]Festetics argued that changes observed in the generation of farm animals, plants, and humans are the result of scientific laws.[13]Festetics empirically deduced that organisms inherit their characteristics, not acquire them. He recognized recessive traits and inherent variation by postulating that traits of past generations could reappear later, and organisms could produce progeny with different attributes.[14]These observations represent an important prelude to Mendel's theory of particulate inheritance insofar as it features a transition of heredity from its status as myth to that of a scientific discipline, by providing a fundamental theoretical basis for genetics in the twentieth century.[10][15]
Other theories of inheritance preceded Mendel's work. A popular theory during the 19th century, and implied byCharles Darwin's 1859On the Origin of Species, wasblending inheritance: the idea that individuals inherit a smooth blend of traits from their parents.[16]Mendel's work provided examples where traits were definitely not blended after hybridization, showing that traits are produced by combinations of distinct genes rather than a continuous blend. Blending of traits in the progeny is now explained by the action of multiple genes withquantitative effects. Another theory that had some support at that time was theinheritance of acquired characteristics: the belief that individuals inherit traits strengthened by their parents. This theory (commonly associated withJean-Baptiste Lamarck) is now known to be wrong—the experiences of individuals do not affect the genes they pass to their children.[17]Other theories included Darwin'spangenesis(which had both acquired and inherited aspects) andFrancis Galton's reformulation of pangenesis as both particulate and inherited.[18]
Modern genetics started with Mendel's studies of the nature of inheritance in plants. In his paper "Versuche über Pflanzenhybriden" ("Experiments on Plant Hybridization"), presented in 1865 to theNaturforschender Verein(Society for Research in Nature) inBrno, Mendel traced the inheritance patterns of certain traits in pea plants and described them mathematically. Although this pattern of inheritance could only be observed for a few traits, Mendel's work suggested that heredity was particulate, not acquired, and that the inheritance patterns of many traits could be explained through simple rules and ratios.[19]
The importance of Mendel's work did not gain wide understanding until 1900, after his death, whenHugo de Vriesand other scientists rediscovered his research.William Bateson, a proponent of Mendel's work, coined the wordgeneticsin 1905.[20][21]The adjectivegenetic, derived from the Greek wordgenesis—γένεσις, "origin", predates the noun and was first used in a biological sense in 1860.[22]Bateson both acted as a mentor and was aided significantly by the work of other scientists from Newnham College at Cambridge, specifically the work ofBecky Saunders,Nora Darwin Barlow, andMuriel Wheldale Onslow.[23]Bateson popularized the usage of the wordgeneticsto describe the study of inheritance in his inaugural address to the Third International Conference on Plant Hybridization inLondonin 1906.[24]
After the rediscovery of Mendel's work, scientists tried to determine which molecules in the cell were responsible for inheritance. In 1900, Nettie Stevens began studying the mealworm.[25]Over the next 11 years, she discovered that females only had the X chromosome and males had both X and Y chromosomes.[25]She was able to conclude that sex is a chromosomal factor and is determined by the male.[25]In 1911,Thomas Hunt Morganargued that genes are onchromosomes, based on observations of a sex-linkedwhite eyemutation infruit flies.[26]In 1913, his studentAlfred Sturtevantused the phenomenon ofgenetic linkageto show that genes are arranged linearly on the chromosome.[27]
Although genes were known to exist on chromosomes, chromosomes are composed of bothproteinand DNA, and scientists did not know which of the two is responsible for inheritance.In 1928,Frederick Griffithdiscovered the phenomenon oftransformation: dead bacteria could transfergenetic materialto "transform" other still-living bacteria. Sixteen years later, in 1944, theAvery–MacLeod–McCarty experimentidentified DNA as the molecule responsible for transformation.[28]The role of the nucleus as the repository of genetic information in eukaryotes had been established byHämmerlingin 1943 in his work on the single celled algaAcetabularia.[29]TheHershey–Chase experimentin 1952 confirmed that DNA (rather than protein) is the genetic material of the viruses that infect bacteria, providing further evidence that DNA is the molecule responsible for inheritance.[30]
James WatsonandFrancis Crickdetermined the structure of DNA in 1953, using theX-ray crystallographywork ofRosalind FranklinandMaurice Wilkinsthat indicated DNA has ahelicalstructure (i.e., shaped like a corkscrew).[31][32]Their double-helix model had two strands of DNA with the nucleotides pointing inward, each matching a complementary nucleotide on the other strand to form what look like rungs on a twisted ladder.[33]This structure showed that genetic information exists in the sequence of nucleotides on each strand of DNA. The structure also suggested a simple method forreplication: if the strands are separated, new partner strands can be reconstructed for each based on the sequence of the old strand. This property is what gives DNA its semi-conservative nature where one strand of new DNA is from an original parent strand.[34]
Although the structure of DNA showed how inheritance works, it was still not known how DNA influences the behavior of cells. In the following years, scientists tried to understand how DNA controls the process ofprotein production.[35]It was discovered that the cell uses DNA as a template to create matchingmessenger RNA, molecules withnucleotidesvery similar to DNA. The nucleotide sequence of a messenger RNA is used to create anamino acidsequence in protein; this translation between nucleotide sequences and amino acid sequences is known as thegenetic code.[36]
With the newfound molecular understanding of inheritance came an explosion of research.[37]A notable theory arose fromTomoko Ohtain 1973 with her amendment to theneutral theory of molecular evolutionthrough publishing thenearly neutral theory of molecular evolution. In this theory, Ohta stressed the importance of natural selection and the environment to the rate at which geneticevolutionoccurs.[38]One important development was chain-terminationDNA sequencingin 1977 byFrederick Sanger. This technology allows scientists to read the nucleotide sequence of a DNA molecule.[39]In 1983,Kary Banks Mullisdeveloped thepolymerase chain reaction, providing a quick way to isolate and amplify a specific section of DNA from a mixture.[40]The efforts of theHuman Genome Project, Department of Energy, NIH, and parallel private efforts byCelera Genomicsled to the sequencing of thehuman genomein 2003.[41][42]
At its most fundamental level, inheritance in organisms occurs by passing discrete heritable units, calledgenes, from parents to offspring.[43]This property was first observed by Gregor Mendel, who studied the segregation of heritable traits inpeaplants, showing for example that flowers on a single plant were either purple or white—but never an intermediate between the two colors. The discrete versions of the same gene controlling the inherited appearance (phenotypes) are calledalleles.[19][44]
In the case of the pea, which is adiploidspecies, each individual plant has two copies of each gene, one copy inherited from each parent.[45]Many species, including humans, have this pattern of inheritance. Diploid organisms with two copies of the same allele of a given gene are calledhomozygousat thatgene locus, while organisms with two different alleles of a given gene are calledheterozygous. The set of alleles for a given organism is called itsgenotype, while the observable traits of the organism are called itsphenotype. When organisms are heterozygous at a gene, often one allele is calleddominantas its qualities dominate the phenotype of the organism, while the other allele is calledrecessiveas its qualities recede and are not observed. Some alleles do not have complete dominance and instead haveincomplete dominanceby expressing an intermediate phenotype, orcodominanceby expressing both alleles at once.[46]
When a pair of organismsreproduce sexually, their offspring randomly inherit one of the two alleles from each parent. These observations of discrete inheritance and the segregation of alleles are collectively known asMendel's first lawor the Law of Segregation. However, the probability of getting one gene over the other can change due to dominant, recessive, homozygous, or heterozygous genes. For example, Mendel found that if you cross heterozygous organisms your odds of getting the dominant trait is 3:1. Real geneticist study and calculate probabilities by using theoretical probabilities, empirical probabilities, the product rule, the sum rule, and more.[47]
Geneticists use diagrams and symbols to describe inheritance. A gene is represented by one or a few letters. Often a "+" symbol is used to mark the usual,non-mutant allelefor a gene.[48]
In fertilization and breeding experiments (and especially when discussing Mendel's laws) the parents are referred to as the "P" generation and the offspring as the "F1" (first filial) generation. When the F1 offspring mate with each other, the offspring are called the "F2" (second filial) generation. One of the common diagrams used to predict the result of cross-breeding is thePunnett square.[49]
When studying human genetic diseases, geneticists often usepedigree chartsto represent the inheritance of traits.[50]These charts map the inheritance of a trait in a family tree.
Organisms have thousands of genes, and in sexually reproducing organisms these genes generally assort independently of each other. This means that the inheritance of an allele for yellow or green pea color is unrelated to the inheritance of alleles for white or purple flowers. This phenomenon, known as "Mendel's second law" or the "law of independent assortment," means that the alleles of different genes get shuffled between parents to form offspring with many different combinations. Different genes often interact to influence the same trait. In theBlue-eyed Mary(Omphalodes verna), for example, there exists a gene with alleles that determine the color of flowers: blue or magenta. Another gene, however, controls whether the flowers have color at all or are white. When a plant has two copies of this white allele, its flowers are white—regardless of whether the first gene has blue or magenta alleles. This interaction between genes is calledepistasis, with the second gene epistatic to the first.[51]
Many traits are not discrete features (e.g. purple or white flowers) but are instead continuous features (e.g. human height andskin color). Thesecomplex traitsare products of many genes.[52]The influence of these genes is mediated, to varying degrees, by the environment an organism has experienced. The degree to which an organism's genes contribute to a complex trait is calledheritability.[53]Measurement of the heritability of a trait is relative—in a more variable environment, the environment has a bigger influence on the total variation of the trait. For example, human height is a trait with complex causes. It has a heritability of 89% in the United States. In Nigeria, however, where people experience a more variable access to good nutrition andhealth care, height has a heritability of only 62%.[54]
Themolecularbasis for genes isdeoxyribonucleic acid(DNA). DNA is composed ofdeoxyribose(sugar molecule), a phosphate group, and a base (amine group). There are four types of bases:adenine(A),cytosine(C),guanine(G), andthymine(T). The phosphates make phosphodiester bonds with the sugars to make long phosphate-sugar backbones. Bases specifically pair together (T&A, C&G) between two backbones and make like rungs on a ladder. The bases, phosphates, and sugars together make anucleotidethat connects to make long chains of DNA.[55]Genetic information exists in the sequence of these nucleotides, and genes exist as stretches of sequence along the DNA chain.[56]These chains coil into a double a-helix structure and wrap around proteins calledHistoneswhich provide the structural support. DNA wrapped around these histones are called chromosomes.[57]Virusessometimes use the similar moleculeRNAinstead of DNA as their genetic material.[58]
DNA normally exists as a double-stranded molecule, coiled into the shape of adouble helix. Each nucleotide in DNA preferentially pairs with its partner nucleotide on the opposite strand: A pairs with T, and C pairs with G. Thus, in its two-stranded form, each strand effectively contains all necessary information, redundant with its partner strand. This structure of DNA is the physical basis for inheritance: DNA replication duplicates the genetic information by splitting the strands and using each strand as a template for synthesis of a new partner strand.[59]
Genes are arranged linearly along long chains of DNA base-pair sequences. Inbacteria, each cell usually contains a single circulargenophore, whileeukaryoticorganisms (such as plants and animals) have their DNA arranged in multiple linear chromosomes. These DNA strands are often extremely long; the largest human chromosome, for example, is about 247 millionbase pairsin length.[60]The DNA of a chromosome is associated with structural proteins that organize, compact, and control access to the DNA, forming a material calledchromatin; in eukaryotes, chromatin is usually composed ofnucleosomes, segments of DNA wound around cores ofhistoneproteins.[61]The full set of hereditary material in an organism (usually the combined DNA sequences of all chromosomes) is called thegenome.
DNA is most often found in the nucleus of cells, but Ruth Sager helped in the discovery of nonchromosomal genes found outside of the nucleus.[62]In plants, these are often found in the chloroplasts and in other organisms, in the mitochondria.[62]These nonchromosomal genes can still be passed on by either partner in sexual reproduction and they control a variety of hereditary characteristics that replicate and remain active throughout generations.[62]
Whilehaploidorganisms have only one copy of each chromosome, most animals and many plants arediploid, containing two of each chromosome and thus two copies of every gene. The two alleles for a gene are located on identicallociof the twohomologous chromosomes, each allele inherited from a different parent.[45]
Many species have so-calledsex chromosomesthat determine the sex of each organism.[63]In humans and many other animals, theY chromosomecontains the gene that triggers the development of the specifically male characteristics. In evolution, this chromosome has lost most of its content and also most of its genes, while theX chromosomeis similar to the other chromosomes and contains many genes. This being said, Mary Frances Lyon discovered that there is X-chromosome inactivation during reproduction to avoid passing on twice as many genes to the offspring.[64]Lyon's discovery led to the discovery of X-linked diseases.[64]
When cells divide, their full genome is copied and eachdaughter cellinherits one copy. This process, calledmitosis, is the simplest form of reproduction and is the basis for asexual reproduction. Asexual reproduction can also occur in multicellular organisms, producing offspring that inherit their genome from a single parent. Offspring that are genetically identical to their parents are calledclones.[65]
Eukaryoticorganisms often use sexual reproduction to generate offspring that contain a mixture of genetic material inherited from two different parents. The process of sexual reproduction alternates between forms that contain single copies of the genome (haploid) and double copies (diploid).[45]Haploid cells fuse and combine genetic material to create a diploid cell with paired chromosomes. Diploid organisms form haploids by dividing, without replicating their DNA, to create daughter cells that randomly inherit one of each pair of chromosomes. Most animals and many plants are diploid for most of their lifespan, with the haploid form reduced to single cellgametessuch asspermoreggs.[66]
Although they do not use the haploid/diploid method of sexual reproduction,bacteriahave many methods of acquiring new genetic information. Some bacteria can undergoconjugation, transferring a small circular piece of DNA to another bacterium.[67]Bacteria can also take up raw DNA fragments found in the environment and integrate them into their genomes, a phenomenon known astransformation.[68]These processes result inhorizontal gene transfer, transmitting fragments of genetic information between organisms that would be otherwise unrelated.Natural bacterial transformationoccurs in manybacterialspecies, and can be regarded as asexual processfor transferring DNA from one cell to another cell (usually of the same species).[69]Transformation requires the action of numerous bacterialgene products, and its primary adaptive function appears to berepairofDNA damagesin the recipient cell.[69]
The diploid nature of chromosomes allows for genes on different chromosomes toassort independentlyor be separated from their homologous pair during sexual reproduction wherein haploid gametes are formed. In this way new combinations of genes can occur in the offspring of a mating pair. Genes on the same chromosome would theoretically never recombine. However, they do, via the cellular process ofchromosomal crossover. During crossover, chromosomes exchange stretches of DNA, effectively shuffling the gene alleles between the chromosomes.[70]This process of chromosomal crossover generally occurs duringmeiosis, a series of cell divisions that creates haploid cells.Meiotic recombination, particularly in microbialeukaryotes, appears to serve the adaptive function of repair of DNA damages.[69]
The first cytological demonstration of crossing over was performed by Harriet Creighton andBarbara McClintockin 1931. Their research and experiments on corn provided cytological evidence for the genetic theory that linked genes on paired chromosomes do in fact exchange places from one homolog to the other.[71]
The probability of chromosomal crossover occurring between two given points on the chromosome is related to the distance between the points. For an arbitrarily long distance, the probability of crossover is high enough that the inheritance of the genes is effectively uncorrelated.[72]For genes that are closer together, however, the lower probability of crossover means that the genes demonstrate genetic linkage; alleles for the two genes tend to be inherited together. The amounts of linkage between a series of genes can be combined to form a linearlinkage mapthat roughly describes the arrangement of the genes along the chromosome.[73]
Genesexpresstheir functional effect through the production of proteins, which are molecules responsible for most functions in the cell. Proteins are made up of one or more polypeptide chains, each composed of a sequence ofamino acids. The DNA sequence of a gene is used to produce a specificamino acid sequence. This process begins with the production of an RNA molecule with a sequence matching the gene's DNA sequence, a process calledtranscription.
Thismessenger RNAmolecule then serves to produce a corresponding amino acid sequence through a process calledtranslation. Each group of three nucleotides in the sequence, called acodon, corresponds either to one of the twenty possible amino acids in a protein or aninstruction to end the amino acid sequence; this correspondence is called thegenetic code.[74]The flow of information is unidirectional: information is transferred from nucleotide sequences into the amino acid sequence of proteins, but it never transfers from protein back into the sequence of DNA—a phenomenonFrancis Crickcalled thecentral dogma of molecular biology.[75]
The specific sequence of amino acidsresultsin a unique three-dimensional structure for that protein, and the three-dimensional structures of proteins are related to their functions.[76][77]Some are simple structural molecules, like the fibers formed by the proteincollagen. Proteins can bind to other proteins and simple molecules, sometimes acting asenzymesby facilitatingchemical reactionswithin the bound molecules (without changing the structure of the protein itself). Protein structure is dynamic; the proteinhemoglobinbends into slightly different forms as it facilitates the capture, transport, and release of oxygen molecules within mammalian blood.[citation needed]
Asingle nucleotide differencewithin DNA can cause a change in the amino acid sequence of a protein. Because protein structures are the result of their amino acid sequences, some changes can dramatically change the properties of a protein by destabilizing the structure or changing the surface of the protein in a way that changes its interaction with other proteins and molecules. For example,sickle-cell anemiais a humangenetic diseasethat results from a single base difference within thecoding regionfor the β-globin section of hemoglobin, causing a single amino acid change that changes hemoglobin's physical properties.[78]Sickle-cell versions of hemoglobin stick to themselves, stacking to form fibers that distort the shape ofred blood cellscarrying the protein. These sickle-shaped cells no longer flow smoothly throughblood vessels, having a tendency to clog or degrade, causing the medical problems associated with this disease.[79]
Some DNA sequences are transcribed into RNA but are not translated into protein products—such RNA molecules are callednon-coding RNA. In some cases, these products fold into structures which are involved in critical cell functions (e.g.ribosomal RNAandtransfer RNA). RNA can also have regulatory effects through hybridization interactions with other RNA molecules (such asmicroRNA).[80]
Although genes contain all the information an organism uses to function, the environment plays an important role in determining the ultimate phenotypes an organism displays. The phrase "nature and nurture" refers to this complementary relationship. The phenotype of an organism depends on the interaction of genes and the environment. An interesting example is the coat coloration of theSiamese cat. In this case, the body temperature of the cat plays the role of the environment. The cat's genes code for dark hair, thus the hair-producing cells in the cat make cellular proteins resulting in dark hair. But these dark hair-producing proteins are sensitive to temperature (i.e. have a mutation causing temperature-sensitivity) anddenaturein higher-temperature environments, failing to produce dark-hair pigment in areas where the cat has a higher body temperature. In a low-temperature environment, however, the protein's structure is stable and produces dark-hair pigment normally. The protein remains functional in areas of skin that are colder—such as its legs, ears, tail, and face—so the cat has dark hair at its extremities.[81]
Environment plays a major role in effects of the human genetic diseasephenylketonuria. The mutation that causes phenylketonuria disrupts the ability of the body to break down the amino acidphenylalanine, causing a toxic build-up of an intermediate molecule that, in turn, causes severe symptoms of progressive intellectual disability and seizures. However, if someone with the phenylketonuria mutation follows a strict diet that avoids this amino acid, they remain normal and healthy.[82]
A common method for determining how genes and environment ("nature and nurture") contribute to a phenotype involvesstudying identical and fraternal twins, or other siblings ofmultiple births.[83]Identical siblings are genetically the same since they come from the same zygote. Meanwhile, fraternal twins are as genetically different from one another as normal siblings. By comparing how often a certain disorder occurs in a pair of identical twins to how often it occurs in a pair of fraternal twins, scientists can determine whether that disorder is caused by genetic or postnatal environmental factors. One famous example involved the study of theGenain quadruplets, who wereidentical quadrupletsall diagnosed withschizophrenia.[84]
The genome of a given organism contains thousands of genes, but not all these genes need to be active at any given moment. A gene is expressed when it is being transcribed into mRNA and there exist many cellular methods of controlling the expression of genes such that proteins are produced only when needed by the cell.Transcription factorsare regulatory proteins that bind to DNA, either promoting or inhibiting the transcription of a gene.[85]Within the genome ofEscherichia colibacteria, for example, there exists a series of genes necessary for the synthesis of the amino acidtryptophan. However, when tryptophan is already available to the cell, these genes for tryptophan synthesis are no longer needed. The presence of tryptophan directly affects the activity of the genes—tryptophan molecules bind to thetryptophan repressor(a transcription factor), changing the repressor's structure such that the repressor binds to the genes. The tryptophan repressor blocks the transcription and expression of the genes, thereby creatingnegative feedbackregulation of the tryptophan synthesis process.[86]
Differences in gene expression are especially clear withinmulticellular organisms, where cells all contain the same genome but have very different structures and behaviors due to the expression of different sets of genes. All the cells in a multicellular organism derive from a single cell, differentiating into variant cell types in response to external andintercellular signalsand gradually establishing different patterns of gene expression to create different behaviors. As no single gene is responsible for thedevelopmentof structures within multicellular organisms, these patterns arise from the complex interactions between many cells.[citation needed]
Withineukaryotes, there exist structural features ofchromatinthat influence the transcription of genes, often in the form of modifications to DNA and chromatin that are stably inherited by daughter cells.[87]These features are called "epigenetic" because they exist "on top" of the DNA sequence and retain inheritance from one cell generation to the next. Because of epigenetic features, different cell typesgrownwithin the same medium can retain very different properties. Although epigenetic features are generally dynamic over the course of development, some, like the phenomenon ofparamutation, have multigenerational inheritance and exist as rare exceptions to the general rule of DNA as the basis for inheritance.[88]
During the process of DNA replication, errors occasionally occur in the polymerization of the second strand. These errors, called mutations, can affect the phenotype of an organism, especially if they occur within the protein coding sequence of a gene. Error rates are usually very low—1 error in every 10–100 million bases—due to the "proofreading" ability ofDNA polymerases.[89][90]Processes that increase the rate of changes in DNA are calledmutagenic: mutagenic chemicals promote errors in DNA replication, often by interfering with the structure of base-pairing, whileUV radiationinduces mutations by causing damage to the DNA structure.[91]Chemical damage to DNA occurs naturally as well and cells useDNA repairmechanisms to repair mismatches and breaks. The repair does not, however, always restore the original sequence. A particularly important source of DNA damages appears to bereactive oxygen species[92]produced bycellular aerobic respiration, and these can lead to mutations.[93]
In organisms that usechromosomal crossoverto exchange DNA and recombine genes, errors in alignment during meiosis can also cause mutations. Errors in crossover are especially likely when similar sequences cause partner chromosomes to adopt a mistaken alignment; this makes some regions in genomes more prone to mutating in this way. These errors create large structural changes in DNA sequence—duplications,inversions,deletionsof entire regions—or the accidental exchange of whole parts of sequences between different chromosomes,chromosomal translocation.[94]
Mutations alter an organism's genotype and occasionally this causes different phenotypes to appear. Most mutations have little effect on an organism's phenotype, health, or reproductivefitness.[95]Mutations that do have an effect are usually detrimental, but occasionally some can be beneficial.[96]Studies in the flyDrosophila melanogastersuggest that if a mutation changes a protein produced by a gene, about 70 percent of these mutations are harmful with the remainder being either neutral or weakly beneficial.[97]
Population geneticsstudies the distribution of genetic differences within populations and how these distributions change over time.[98]Changes in thefrequency of an allelein a population are mainly influenced bynatural selection, where a given allele provides a selective or reproductive advantage to the organism,[99]as well as other factors such asmutation,genetic drift,genetic hitchhiking,[100]artificial selectionandmigration.[101]
Over many generations, the genomes of organisms can change significantly, resulting in evolution. In the process calledadaptation, selection for beneficial mutations can cause a species to evolve into forms better able to survive in their environment.[102]New species are formed through the process ofspeciation, often caused by geographical separations that prevent populations from exchanging genes with each other.[103]
By comparing thehomologybetween different species' genomes, it is possible to calculate the evolutionary distance between them andwhen they may have diverged. Genetic comparisons are generally considered a more accurate method of characterizing the relatedness between species than the comparison of phenotypic characteristics. The evolutionary distances between species can be used to formevolutionary trees; these trees represent thecommon descentand divergence of species over time, although they do not show the transfer of genetic material between unrelated species (known ashorizontal gene transferand most common in bacteria).[104]
Although geneticists originally studied inheritance in a wide variety of organisms, the range of species studied has narrowed. One reason is that when significant research already exists for a given organism, new researchers are more likely to choose it for further study, and so eventually a fewmodel organismsbecame the basis for most genetics research. Common research topics in model organism genetics include the study ofgene regulationand the involvement of genes indevelopmentandcancer. Organisms were chosen, in part, for convenience—short generation times and easygenetic manipulationmade some organisms popular genetics research tools. Widely used model organisms include the gut bacteriumEscherichia coli, the plantArabidopsis thaliana, baker's yeast (Saccharomyces cerevisiae), the nematodeCaenorhabditis elegans, the common fruit fly (Drosophila melanogaster), the zebrafish (Danio rerio), and the common house mouse (Mus musculus).[105]
Medical geneticsseeks to understand how genetic variation relates to human health and disease.[106]When searching for an unknown gene that may be involved in a disease, researchers commonly use genetic linkage and geneticpedigree chartsto find the location on the genome associated with the disease. At the population level, researchers take advantage ofMendelian randomizationto look for locations in the genome that are associated with diseases, a method especially useful formultigenic traitsnot clearly defined by a single gene.[107]Once a candidate gene is found, further research is often done on the corresponding (orhomologous) genes of model organisms. In addition to studying genetic diseases, the increased availability of genotyping methods has led to the field ofpharmacogenetics: the study of how genotype can affect drug responses.[108]
Individuals differ in their inherited tendency to developcancer, and cancer is a genetic disease. The process of cancer development in the body is a combination of events. Mutations occasionally occur within cells in the body as they divide. Although these mutations will not be inherited by any offspring, they can affect the behavior of cells, sometimes causing them to grow and divide more frequently. There are biological mechanisms that attempt to stop this process; signals are given to inappropriately dividing cells that should triggercell death, but sometimes additional mutations occur that cause cells to ignore these messages. An internal process ofnatural selectionoccurs within the body and eventually mutations accumulate within cells to promote their own growth, creating a canceroustumorthat grows and invades various tissues of the body. Normally, a cell divides only in response to signals calledgrowth factorsandstops growing once in contact with surrounding cellsand in response to growth-inhibitory signals. It usually then divides a limited number of times and dies, staying within theepitheliumwhere it is unable to migrate to other organs. To become a cancer cell, a cell has to accumulate mutations in a number of genes (three to seven). A cancer cell can divide without growth factor and ignores inhibitory signals. Also, it is immortal and can grow indefinitely, even after it makes contact with neighboring cells. It may escape from the epithelium and ultimately from theprimary tumor. Then, the escaped cell can cross the endothelium of a blood vessel and get transported by the bloodstream to colonize a new organ, forming deadlymetastasis. Although there are some genetic predispositions in a small fraction of cancers, the major fraction is due to a set of new genetic mutations that originally appear and accumulate in one or a small number of cells that will divide to form the tumor and are not transmitted to the progeny (somatic mutations). The most frequent mutations are a loss of function ofp53 protein, atumor suppressor, or in the p53 pathway, and gain of function mutations in theRas proteins, or in otheroncogenes.[109][110]
DNA can be manipulated in the laboratory.Restriction enzymesare commonly used enzymes that cut DNA at specific sequences, producing predictable fragments of DNA.[111]DNA fragments can be visualized through use ofgel electrophoresis, which separates fragments according to their length.[112]
The use ofligation enzymesallows DNA fragments to be connected. By binding ("ligating") fragments of DNA together from different sources, researchers can createrecombinant DNA, the DNA often associated withgenetically modified organisms. Recombinant DNA is commonly used in the context ofplasmids: short circular DNA molecules with a few genes on them. In the process known asmolecular cloning, researchers can amplify the DNA fragments by inserting plasmids into bacteria and then culturing them on plates of agar (to isolateclones of bacteria cells). "Cloning" can also refer to the various means of creating cloned ("clonal") organisms.[113]
DNA can also be amplified using a procedure called thepolymerase chain reaction(PCR).[114]By using specific short sequences of DNA, PCR can isolate and exponentially amplify a targeted region of DNA. Because it can amplify from extremely small amounts of DNA, PCR is also often used to detect the presence of specific DNA sequences.[115][116]
DNA sequencing, one of the most fundamental technologies developed to study genetics, allows researchers to determine the sequence of nucleotides in DNA fragments. The technique ofchain-termination sequencing, developed in 1977 by a team led byFrederick Sanger, is still routinely used to sequence DNA fragments. Using this technology, researchers have been able to study the molecular sequences associated with many human diseases.[117]
As sequencing has become less expensive, researchers havesequenced the genomesof many organisms using a process calledgenome assembly, which uses computational tools to stitch together sequences from many different fragments.[118]These technologies were used to sequence the human genome in the Human Genome Project completed in 2003.[41]Newhigh-throughput sequencingtechnologies are dramatically lowering the cost of DNA sequencing, with many researchers hoping to bring the cost of resequencing a human genome down to a thousand dollars.[119]
Next-generation sequencing(or high-throughput sequencing) came about due to the ever-increasing demand for low-cost sequencing. These sequencing technologies allow the production of potentially millions of sequences concurrently.[120][121]The large amount of sequence data available has created the subfield ofgenomics, research that uses computational tools to search for and analyze patterns in the full genomes of organisms. Genomics can also be considered a subfield ofbioinformatics, which uses computational approaches to analyze large sets ofbiological data. A common problem to these fields of research is how to manage and share data that deals with human subject andpersonally identifiable information.[citation needed]
On 19 March 2015, a group of leading biologists urged a worldwide ban on clinical use of methods, particularly the use ofCRISPRandzinc finger, to edit the human genome in a way that can be inherited.[122][123][124][125]In April 2015, Chinese researchersreportedresults ofbasic researchto edit the DNA of non-viablehuman embryosusing CRISPR.[126][127]
Theperiodic table, also known as theperiodic table of the elements, is an ordered arrangement of thechemical elementsinto rows ("periods") and columns ("groups"). It is aniconofchemistryand is widely used inphysicsand other sciences. It is a depiction of theperiodic law, which states that when the elements are arranged in order of theiratomic numbersan approximaterecurrence of their propertiesis evident. The table is divided into four roughly rectangular areas calledblocks. Elements in the same group tend to show similar chemical characteristics.
Vertical, horizontal and diagonaltrendscharacterize the periodic table.Metalliccharacter increases going down a group and from right to left across a period.Nonmetalliccharacter increases going from the bottom left of the periodic table to the top right.
The first periodic table to become generally accepted was that of the Russian chemistDmitri Mendeleevin 1869; he formulated the periodic law as a dependence of chemical properties onatomic mass. As not all elements were then known, there were gaps in his periodic table, and Mendeleev successfully used the periodic law topredict some properties of some of the missing elements. The periodic law was recognized as a fundamental discovery in the late 19th century. It was explained early in the 20th century, with the discovery ofatomic numbersand associated pioneering work inquantum mechanics, both ideas serving to illuminate the internal structure of the atom. A recognisably modern form of the table was reached in 1945 withGlenn T. Seaborg's discovery that theactinideswere in fact f-block rather than d-block elements. The periodic table and law are now a central and indispensable part of modern chemistry.
The periodic table continues to evolve with the progress of science. In nature, only elements up to atomic number 94  exist;[a]to go further, it was necessary tosynthesizenew elements in the laboratory. By 2010, the first 118 elements were known, thereby completing the first seven rows of the table;[1]however, chemical characterization is still needed for the heaviest elements to confirm that their properties match their positions. New discoveries will extend the tablebeyond these seven rows, though it is not yet known how many more elements are possible; moreover, theoretical calculations suggest that this unknown region will not follow the patterns of the known part of the table. Some scientific discussion also continues regarding whether some elements are correctly positioned in today's table. Manyalternative representationsof the periodic law exist, and there is some discussion as to whether there is an optimal form of the periodic table.
Each chemical element has a uniqueatomic number(Z— for "Zahl", German for "number") representing the number ofprotonsin itsnucleus.[4]Each distinct atomic number therefore corresponds to a class of atom: these classes are called thechemical elements.[5]The chemical elements are what the periodic table classifies and organizes.Hydrogenis the element with atomic number 1;helium, atomic number 2;lithium, atomic number 3; and so on. Each of these names can be further abbreviated by a one- or two-letterchemical symbol; those for hydrogen, helium, and lithium are respectively H, He, and Li.[6]Neutrons do not affect the atom's chemical identity, but do affect its weight. Atoms with the same number of protons but different numbers of neutrons are calledisotopesof the same chemical element.[6]Naturally occurring elements usually occur as mixes of different isotopes; since each isotope usually occurs with a characteristic abundance, naturally occurring elements have well-definedatomic weights, defined as the average mass of a naturally occurring atom of that element.[7]All elements have multipleisotopes, variants with the same number of protons but different numbers ofneutrons. For example,carbonhas three naturally occurring isotopes: all of itsatomshave six protons and most have six neutrons as well, but about one per cent have seven neutrons, and a very small fraction have eight neutrons. Isotopes are never separated in the periodic table; they are always grouped together under a single element. When atomic mass is shown, it is usually the weighted average of naturally occurring isotopes; but if no isotopes occur naturally in significant quantities, the mass of the most stable isotope usually appears, often in parentheses.[8]
In the standard periodic table, the elements are listed in order of increasing atomic number. A new row (period) is started when a newelectron shellhas its firstelectron. Columns (groups) are determined by theelectron configurationof the atom; elements with the same number of electrons in a particular subshell fall into the same columns (e.g.oxygen,sulfur, andseleniumare in the same column because they all have four electrons in the outermost p-subshell). Elements with similar chemical properties generally fall into the same group in the periodic table, although in the f-block, and to some respect in the d-block, the elements in the same period tend to have similar properties, as well. Thus, it is relatively easy to predict the chemical properties of an element if one knows the properties of the elements around it.[9]
Today, 118 elements are known, the first 94 of which are known to occur naturally on Earth at present.[10][a]The remaining 24, americium to oganesson (95–118), occur only when synthesized in laboratories. Of the 94 naturally occurring elements, 83 areprimordialand 11 occur only in decay chains of primordial elements. A few of the latter are so rare that they were not discovered in nature, but were synthesized in the laboratory before it was determined that they do exist in nature after all:technetium(element 43),promethium(element 61),astatine(element 85),neptunium(element 93), andplutonium(element 94).[12]No element heavier thaneinsteinium(element 99) has ever been observed in macroscopic quantities in its pure form, nor hasastatine;francium(element 87) has been only photographed in the form oflightemitted from microscopic quantities (300,000 atoms).[14]Of the 94 natural elements, eighty have a stable isotope and one more (bismuth) has an almost-stable isotope (with ahalf-lifeof 2.01×1019years, over a billion times theage of the universe).[15][b]Two more,thoriumanduranium, have isotopes undergoingradioactive decaywith a half-life comparable to theage of the Earth. The stable elements plus bismuth, thorium, and uranium make up the 83primordialelements that survived from the Earth's formation.[c]The remaining eleven natural elements decay quickly enough that their continued trace occurrence rests primarily on being constantly regenerated as intermediate products of the decay of thorium and uranium.[d]All 24 known artificial elements are radioactive.[6]
Under an international naming convention, the groups are numbered numerically from 1 to 18 from the leftmost column (the alkali metals) to the rightmost column (the noble gases). The f-block groups are ignored in this numbering.[22]Groups can also be named by their first element, e.g. the "scandium group" for group 3.[22]Previously, groups were known byRoman numerals. In the United States, the Roman numerals were followed by either an "A" if the group was in thes-orp-block, or a "B" if the group was in thed-block. The Roman numerals used correspond to the last digit of today's naming convention (e.g. thegroup 4 elementswere group IVB, and thegroup 14 elementswere group IVA). In Europe, the lettering was similar, except that "A" was used for groups 1 through 7, and "B" was used for groups 11 through 17. In addition, groups 8, 9 and 10 used to be treated as one triple-sized group, known collectively in both notations as group VIII. In 1988, the newIUPAC(International Union of Pure and Applied Chemistry) naming system (1–18) was put into use, and the old group names (I–VIII) were deprecated.[23]
For reasons of space,[30][31]the periodic table is commonly presented with the f-block elements cut out and positioned as a distinct part below the main body.[32][30][23]This reduces the number of element columns from 32 to 18.[30]
Both forms represent the same periodic table.[6]The form with the f-block included in the main body is sometimes called the 32-column[6]or long form;[33]the form with the f-block cut out the 18-column[6]or medium-long form.[33]The 32-column form has the advantage of showing all elements in their correct sequence, but it has the disadvantage of requiring more space.[34]The form chosen is an editorial choice, and does not imply any change of scientific claim or statement. For example, when discussingthe composition of group 3, the options can be shown equally (unprejudiced) in both forms.[35]
Periodic tables usually at least show the elements' symbols; many also provide supplementary information about the elements, either via colour-coding or as data in the cells. The above table shows the names and atomic numbers of the elements, and also their blocks, natural occurrences andstandard atomic weights. For the short-lived elements without standard atomic weights, the mass number of the most stable known isotope is used instead. Other tables may include properties such as state of matter, melting and boiling points, densities, as well as provide different classifications of the elements.[e]
The periodic table is a graphic description of the periodic law,[36]which states that the properties and atomic structures of the chemical elements are aperiodic functionof theiratomic number.[37]Elements are placed in the periodic table according to theirelectron configurations,[38]the periodic recurrences of which explain thetrendsin properties across the periodic table.[39]
An electron can be thought of as inhabiting anatomic orbital, which characterizes the probability it can be found in any particular region around the atom. Their energies arequantised, which is to say that they can only take discrete values. Furthermore, electrons obey thePauli exclusion principle: different electrons must always be in different states. This allows classification of the possible states an electron can take in various energy levels known as shells, divided into individual subshells, which each contain one or more orbitals. Each orbital can contain up to two electrons: they are distinguished by a quantity known asspin, conventionally labelled "up" or "down".[40][f]In a cold atom (one in its ground state), electrons arrange themselves in such a way that the total energy they have is minimized by occupying the lowest-energy orbitals available.[42]Only the outermost electrons (so-calledvalence electrons) have enough energy to break free of the nucleus and participate in chemical reactions with other atoms. The others are calledcore electrons.[43]
Elements are known with up to the first seven shells occupied. The first shell contains only one orbital, a spherical s orbital. As it is in the first shell, this is called the 1s orbital. This can hold up to two electrons. The second shell similarly contains a 2s orbital, and it also contains three dumbbell-shaped 2p orbitals, and can thus fill up to eight electrons (2×1 + 2×3 = 8). The third shell contains one 3s orbital, three 3p orbitals, and five 3d orbitals, and thus has a capacity of 2×1 + 2×3 + 2×5 = 18. The fourth shell contains one 4s orbital, three 4p orbitals, five 4d orbitals, and seven 4f orbitals, thus leading to a capacity of 2×1 + 2×3 + 2×5 + 2×7 = 32.[30]Higher shells contain more types of orbitals that continue the pattern, but such types of orbitals are not filled in the ground states of known elements.[45]The subshell types are characterized by thequantum numbers. Four numbers describe an orbital in an atom completely: theprincipal quantum numbern, theazimuthal quantum numberℓ (the orbital type), theorbital magnetic quantum numbermℓ, and thespin magnetic quantum numberms.[39]
The sequence in which the subshells are filled is given in most cases by theAufbau principle, also known as the Madelung or Klechkovsky rule (afterErwin MadelungandVsevolod Klechkovskyrespectively). This rule was first observed empirically by Madelung, and Klechkovsky and later authors gave it theoretical justification.[46][47][48][49][g]The shells overlap in energies, and the Madelung rule specifies the sequence of filling according to:[47]
Here the sign ≪ means "much less than" as opposed to < meaning just "less than".[47]Phrased differently, electrons enter orbitals in order of increasingn+ ℓ, and if two orbitals are available with the same value ofn+ ℓ, the one with lowernis occupied first.[45][49]In general, orbitals with the same value ofn+ ℓ are similar in energy, but in the case of the s orbitals (with ℓ = 0), quantum effects raise their energy to approach that of the nextn+ ℓ group. Hence the periodic table is usually drawn to begin each row (often called a period) with the filling of a new s orbital, which corresponds to the beginning of a new shell.[47][48][30]Thus, with the exception of the first row, each period length appears twice:[47]
The overlaps get quite close at the point where the d orbitals enter the picture,[50]and the order can shift slightly with atomic number[51]and atomic charge.[52][h]
Starting from the simplest atom, this lets us build up the periodic table one at a time in order of atomic number, by considering the cases of single atoms. Inhydrogen, there is only one electron, which must go in the lowest-energy orbital 1s. Thiselectron configurationis written 1s1, where the superscript indicates the number of electrons in the subshell.Heliumadds a second electron, which also goes into 1s, completely filling the first shell and giving the configuration 1s2.[39][58][i]
Starting from the third element,lithium, the first shell is full, so its third electron occupies a 2s orbital, giving a 1s22s1configuration. The 2s electron is lithium's only valence electron, as the 1s subshell is now too tightly bound to the nucleus to participate in chemical bonding to other atoms: such a shell is called a "core shell". The 1s subshell is a core shell for all elements from lithium onward. The 2s subshell is completed by the next elementberyllium(1s22s2). The following elements then proceed to fill the 2p subshell.Boron(1s22s22p1) puts its new electron in a 2p orbital;carbon(1s22s22p2) fills a second 2p orbital; and withnitrogen(1s22s22p3) all three 2p orbitals become singly occupied. This is consistent withHund's rule, which states that atoms usually prefer to singly occupy each orbital of the same type before filling them with the second electron.Oxygen(1s22s22p4),fluorine(1s22s22p5), andneon(1s22s22p6) then complete the already singly filled 2p orbitals; the last of these fills the second shell completely.[39][58]
Starting from element 11,sodium, the second shell is full, making the second shell a core shell for this and all heavier elements. The eleventh electron begins the filling of the third shell by occupying a 3s orbital, giving a configuration of 1s22s22p63s1for sodium. This configuration is abbreviated [Ne] 3s1, where [Ne] represents neon's configuration.Magnesium([Ne] 3s2) finishes this 3s orbital, and the following six elementsaluminium,silicon,phosphorus,sulfur,chlorine, andargonfill the three 3p orbitals ([Ne] 3s23p1through [Ne] 3s23p6).[39][58]This creates an analogous series in which the outer shell structures of sodium through argon are analogous to those of lithium through neon, and is the basis for the periodicity of chemical properties that the periodic table illustrates:[39]at regular but changing intervals of atomic numbers, the properties of the chemical elements approximately repeat.[36]
The first 18 elements can thus be arranged as the start of a periodic table. Elements in the same column have the same number of valence electrons and have analogous valence electron configurations: these columns are called groups. The single exception is helium, which has two valence electrons like beryllium and magnesium, but is typically placed in the column of neon and argon to emphasise that its outer shell is full. (Some contemporary authors question even this single exception, preferring to consistently follow the valence configurations and place helium over beryllium.) There are eight columns in this periodic table fragment, corresponding to at most eight outer-shell electrons.[32]A period begins when a new shell starts filling.[30]Finally, the colouring illustrates theblocks: the elements in the s-block (coloured red) are filling s orbitals, while those in the p-block (coloured yellow) are filling p orbitals.[30]
Starting the next row, forpotassiumandcalciumthe 4s subshell is the lowest in energy, and therefore they fill it.[39][58]Potassium adds one electron to the 4s shell ([Ar] 4s1), and calcium then completes it ([Ar] 4s2). However, starting fromscandium([Ar] 3d14s2) the 3d subshell becomes the next highest in energy. The 4s and 3d subshells have approximately the same energy and they compete for filling the electrons, and so the occupation is not quite consistently filling the 3d orbitals one at a time. The precise energy ordering of 3d and 4s changes along the row, and also changes depending on how many electrons are removed from the atom. For example, due to the repulsion between the 3d electrons and the 4s ones, atchromiumthe 4s energy level becomes slightly higher than 3d, and so it becomes more profitable for a chromium atom to have a [Ar] 3d54s1configuration than an [Ar] 3d44s2one. A similar anomaly occurs atcopper, whose atom has a [Ar] 3d104s1configuration rather than the expected [Ar] 3d94s2.[39]These are violations of the Madelung rule. Such anomalies, however, do not have any chemical significance:[52]most chemistry is not about isolated gaseous atoms,[60]and the various configurations are so close in energy to each other[50]that the presence of a nearby atom can shift the balance.[39]Therefore, the periodic table ignores them and considers only idealized configurations.[38]
Atzinc([Ar] 3d104s2), the 3d orbitals are completely filled with a total of ten electrons.[39][58]Next come the 4p orbitals, completing the row, which are filled progressively bygallium([Ar] 3d104s24p1) throughkrypton([Ar] 3d104s24p6), in a manner analogous to the previous p-block elements.[39][58]From gallium onwards, the 3d orbitals form part of the electronic core, and no longer participate in chemistry.[57]The s- and p-block elements, which fill their outer shells, are calledmain-group elements; the d-block elements (coloured blue below), which fill an inner shell, are calledtransition elements(or transition metals, since they are all metals).[61]
The next 18 elements fill the 5s orbitals (rubidiumandstrontium), then 4d (yttriumthroughcadmium, again with a few anomalies along the way), and then 5p (indiumthroughxenon).[30][58]Again, from indium onward the 4d orbitals are in the core.[58][62]Hence the fifth row has the same structure as the fourth.[30]
The sixth row of the table likewise starts with two s-block elements:caesiumandbarium.[58]After this, the first f-block elements (coloured green below) begin to appear, starting withlanthanum. These are sometimes termed inner transition elements.[61]As there are now not only 4f but also 5d and 6s subshells at similar energies, competition occurs once again with many irregular configurations;[50]this resulted in some dispute about where exactly the f-block is supposed to begin, but most who study the matter agree that it starts at lanthanum in accordance with the Aufbau principle.[27]Even though lanthanum does not itself fill the 4f subshell as a single atom, because of repulsion between electrons,[52]its 4f orbitals are low enough in energy to participate in chemistry.[63][53][64]Atytterbium, the seven 4f orbitals are completely filled with fourteen electrons; thereafter, a series of ten transition elements (lutetiumthroughmercury) follows,[58][65][66][67]and finally six main-group elements (thalliumthroughradon) complete the period.[58][68]From lutetium onwards the 4f orbitals are in the core,[58][64]and from thallium onwards so are the 5d orbitals.[58][57][69]
The seventh row is analogous to the sixth row: 7s fills (franciumandradium), then 5f (actiniumtonobelium), then 6d (lawrenciumtocopernicium), and finally 7p (nihoniumtooganesson).[58]Starting from lawrencium the 5f orbitals are in the core,[58]and probably the 6d orbitals join the core starting from nihonium.[58][70][j]Again there are a few anomalies along the way:[30]for example, as single atoms neither actinium northoriumactually fills the 5f subshell, and lawrencium does not fill the 6d shell, but all these subshells can still become filled in chemical environments.[72][73][74]For a very long time, the seventh row was incomplete as most of its elements do not occur in nature. The missingelements beyond uraniumstarted to be synthesized in the laboratory in 1940, when neptunium was made.[75](However, the first element to be discovered by synthesis rather than in nature was technetium in 1937.) The row was completed with the synthesis oftennessinein 2010[76](the last elementoganessonhad already been made in 2002),[77]and the last elements in this seventh row were given names in 2016.[78]
This completes the modern periodic table, with all seven rows completely filled to capacity.[78]
The following table shows the electron configuration of a neutral gas-phase atom of each element. Different configurations can be favoured in different chemical environments.[52]The main-group elements have entirely regular electron configurations; the transition and inner transition elements show twenty irregularities due to the aforementioned competition between subshells close in energy level. For the last ten elements (109–118), experimental data is lacking[79]and therefore calculated configurations have been shown instead.[80]Completely filled subshells have been greyed out.
Although the modern periodic table is standard today, the placement of the period 1 elements hydrogen and helium remains an open issue under discussion, and some variation can be found.[57][81]Following their respective s1and s2electron configurations, hydrogen would be placed in group 1, and helium would be placed in group 2.[57]The group 1 placement of hydrogen is common, but helium is almost always placed in group 18 with the other noble gases.[6]The debate has to do with conflicting understandings of the extent to which chemical or electronic properties should decide periodic table placement.[81]
Like the group 1 metals, hydrogen has one electron in its outermost shell[82]and typically loses its only electron in chemical reactions.[83]Hydrogen has some metal-like chemical properties, being able to displace some metals from theirsalts.[83]But it forms a diatomic nonmetallic gas at standard conditions, unlike the alkali metals which are reactive solid metals. This and hydrogen's formation ofhydrides, in which it gains an electron, brings it close to the properties of thehalogenswhich do the same[83](though it is rarer for hydrogen to form H−than H+).[84]Moreover, the lightest two halogens (fluorineandchlorine) are gaseous like hydrogen at standard conditions.[83]Some properties of hydrogen are not a good fit for either group: hydrogen is neither highly oxidizing nor highly reducing and is not reactive with water.[84]Hydrogen thus has properties corresponding to both those of the alkali metals and the halogens, but matches neither group perfectly, and is thus difficult to place by its chemistry.[83]Therefore, while the electronic placement of hydrogen in group 1 predominates, some rarer arrangements show either hydrogen in group 17,[85]duplicate hydrogen in both groups 1 and 17,[86][87]or float it separately from all groups.[87][88][57]This last option has nonetheless been criticized by the chemist and philosopher of scienceEric Scerrion the grounds that it appears to imply that hydrogen is above the periodic law altogether, unlike all the other elements.[89]
Helium is the only element that routinely occupies a position in the periodic table that is not consistent with its electronic structure. It has two electrons in its outermost shell, whereas the other noble gases have eight; and it is an s-block element, whereas all other noble gases are p-block elements. However it is unreactive at standard conditions, and has a full outer shell: these properties are like the noble gases in group 18, but not at all like the reactive alkaline earth metals of group 2. For these reasons helium is nearly universally placed in group 18[6]which its properties best match;[57]a proposal to move helium to group 2 was rejected by IUPAC in 1988 for these reasons.[23]Nonetheless, helium is still occasionally placed in group 2 today,[90]and some of its physical and chemical properties are closer to the group 2 elements and support the electronic placement.[82][57]Solid helium crystallises in ahexagonal close-packedstructure, which matches beryllium and magnesium in group 2, but not the other noble gases in group 18.[91]Recent theoretical developments in noble gas chemistry, in which helium is expected to show slightly less inertness than neon and to form (HeO)(LiF)2with a structure similar to the analogous beryllium compound (but with no expected neon analogue), have resulted in more chemists advocating a placement of helium in group 2. This relates to the electronic argument, as the reason for neon's greater inertness is repulsion from its filled p-shell that helium lacks, though realistically it is unlikely that helium-containing molecules will be stable outside extreme low-temperature conditions (around 10K).[92][93][94][95]
Thefirst-row anomalyin the periodic table has additionally been cited to support moving helium to group 2. It arises because the first orbital of any type is unusually small, since unlike its higher analogues, it does not experience interelectronic repulsion from a smaller orbital of the same type. This makes the first row of elements in each block unusually small, and such elements tend to exhibit characteristic kinds of anomalies for their group. Some chemists arguing for the repositioning of helium have pointed out that helium exhibits these anomalies if it is placed in group 2, but not if it is placed in group 18: on the other hand, neon, which would be the first group 18 element if helium was removed from that spot, does exhibit those anomalies.[92]The relationship between helium and beryllium is then argued to resemble that between hydrogen and lithium, a placement which is much more commonly accepted.[93]For example, because of this trend in the sizes of orbitals, a large difference in atomic radii between the first and second members of each main group is seen in groups 1 and 13–17: it exists between neon and argon, and between helium and beryllium, but not between helium and neon. This similarly affects the noble gases' boiling points and solubilities in water, where helium is too close to neon, and the large difference characteristic between the first two elements of a group appears only between neon and argon. Moving helium to group 2 makes this trend consistent in groups 2 and 18 as well, by making helium the first group 2 element and neon the first group 18 element: both exhibit the characteristic properties of akainosymmetricfirst element of a group.[96][97]The group 18 placement of helium nonetheless remains near-universal due to its extreme inertness.[98]Additionally, tables that float both hydrogen and helium outside all groups may rarely be encountered.[88][57][58]
In many periodic tables, the f-block is shifted one element to the right, so that lanthanum and actinium become d-block elements in group 3, and Ce–Lu and Th–Lr form the f-block. Thus the d-block is split into two very uneven portions. This is a holdover from early mistaken measurements of electron configurations; modern measurements are more consistent with the form with lutetium and lawrencium in group 3, and with La–Yb and Ac–No as the f-block.[25][99]
The 4f shell is completely filled at ytterbium, and for that reasonLev LandauandEvgeny Lifshitzin 1948 considered it incorrect to group lutetium as an f-block element.[26]They did not yet take the step of removing lanthanum from the d-block as well, butJun Kondōrealized in 1963 that lanthanum's low-temperaturesuperconductivityimplied the activity of its 4f shell.[100]In 1965, David C. Hamilton linked this observation to its position in the periodic table, and argued that the f-block should be composed of the elements La–Yb and Ac–No.[63]Since then, physical, chemical, and electronic evidence has supported this assignment.[25][23][99]The issue was brought to wide attention byWilliam B. Jensenin 1982,[25]and the reassignment of lutetium and lawrencium to group 3 was supported by IUPAC reports dating from 1988 (when the 1–18 group numbers were recommended)[23]and 2021.[24]The variation nonetheless still exists because most textbook writers are not aware of the issue.[25]
A third form can sometimes be encountered in which the spaces below yttrium in group 3 are left empty, such as the table appearing on the IUPAC web site,[6]but this creates an inconsistency with quantum mechanics by making the f-block 15 elements wide (La–Lu and Ac–Lr) even though only 14 electrons can fit in an f-subshell.[24]There is moreover some confusion in the literature on which elements are then implied to be in group 3.[24][33][101][102][103]While the 2021 IUPAC report noted that 15-element-wide f-blocks are supported by some practitioners of a specialized branch ofrelativistic quantum mechanicsfocusing on the properties ofsuperheavy elements, the project's opinion was that such interest-dependent concerns should not have any bearing on how the periodic table is presented to "the general chemical and scientific community".[24]Other authors focusing on superheavy elements since clarified that the "15th entry of the f-block represents the first slot of the d-block which is left vacant to indicate the place of the f-block inserts", which would imply that this form still has lutetium and lawrencium (the 15th entries in question) as d-block elements in group 3.[104]Indeed, when IUPAC publications expand the table to 32 columns, they make this clear and place lutetium and lawrencium under yttrium in group 3.[105][106]
Several arguments in favour of Sc-Y-La-Ac can be encountered in the literature,[107][108]but they have been challenged as being logically inconsistent.[27][28][29]For example, it has been argued that lanthanum and actinium cannot be f-block elements because as individual gas-phase atoms, they have not begun to fill the f-subshells.[109]But the same is true of thorium which is never disputed as an f-block element,[24][25]and this argument overlooks the problem on the other end: that the f-shells complete filling at ytterbium and nobelium, matching the Sc-Y-Lu-Lr form, and not at lutetium and lawrencium as the Sc-Y-La-Ac form would have it.[110]Not only are such exceptional configurations in the minority,[110]but they have also in any case never been considered as relevant for positioning any other elements on the periodic table: in gaseous atoms, the d-shells complete their filling at copper, palladium, and gold, but it is universally accepted by chemists that these configurations are exceptional and that the d-block really ends in accordance with the Madelung rule at zinc, cadmium, and mercury.[33]The relevant fact for placement[38][65]is that lanthanum and actinium (like thorium) have valence f orbitals that can become occupied in chemical environments, whereas lutetium and lawrencium do not:[58][111][74]their f-shells are in the core, and cannot be used for chemical reactions.[64][112]Thus the relationship between yttrium and lanthanum is only a secondary relationship between elements with the same number of valence electrons but different kinds of valence orbitals, such as that between chromium and uranium; whereas the relationship between yttrium and lutetium is primary, sharing both valence electron count and valence orbital type.[58]
As chemical reactions involve the valence electrons,[32]elements with similar outer electron configurations may be expected to react similarly and form compounds with similar proportions of elements in them.[113]Such elements are placed in the same group, and thus there tend to be clear similarities and trends in chemical behaviour as one proceeds down a group.[114]As analogous configurations occur at regular intervals, the properties of the elements thus exhibit periodic recurrences, hence the name of the periodic table and the periodic law. These periodic recurrences were noticed well before the underlying theory that explains them was developed.[115][116]
Historically, the physical size of atoms was unknown until the early 20th century. The first calculated estimate of the atomic radius of hydrogen was published by physicistArthur Haasin 1910 to within an order of magnitude (a factor of 10) of the accepted value, theBohr radius(~0.529 Å). In his model, Haas used a single-electron configuration based on the classical atomic model proposed byJ. J. Thomsonin 1904, often called theplum-pudding model.[117]
Atomic radii(the size of atoms) are dependent on the sizes of their outermost orbitals.[96]They generally decrease going left to right along the main-group elements, because the nuclear charge increases but the outer electrons are still in the same shell. However, going down a column, the radii generally increase, because the outermost electrons are in higher shells that are thus further away from the nucleus.[32][118]The first row of each block is abnormally small, due to an effect calledkainosymmetryor primogenic repulsion:[119]the 1s, 2p, 3d, and 4f subshells have no inner analogues. For example, the 2p orbitals do not experience strong repulsion from the 1s and 2s orbitals, which have quite different angular charge distributions, and hence are not very large; but the 3p orbitals experience strong repulsion from the 2p orbitals, which have similar angular charge distributions. Thus higher s-, p-, d-, and f-subshells experience strong repulsion from their inner analogues, which have approximately the same angular distribution of charge, and must expand to avoid this. This makes significant differences arise between the small 2p elements, which prefermultiple bonding, and the larger 3p and higher p-elements, which do not.[96]Similar anomalies arise for the 1s, 2p, 3d, 4f, and the hypothetical 5g elements:[120]the degree of this first-row anomaly is highest for the s-block, is moderate for the p-block, and is less pronounced for the d- and f-blocks.[121]
In the transition elements, an inner shell is filling, but the size of the atom is still determined by the outer electrons. The increasing nuclear charge across the series and the increased number of inner electrons for shielding somewhat compensate each other, so the decrease in radius is smaller.[118]The 4p and 5d atoms, coming immediately after new types of transition series are first introduced, are smaller than would have been expected,[122]because the added core 3d and 4f subshells provide only incomplete shielding of the nuclear charge for the outer electrons. Hence for example gallium atoms are slightly smaller than aluminium atoms.[96]Together with kainosymmetry, this results in an even-odd difference between the periods (except in the s-block)[k]that is sometimes known as secondary periodicity: elements in even periods have smaller atomic radii and prefer to lose fewer electrons, while elements in odd periods (except the first) differ in the opposite direction. Thus for example many properties in the p-block show a zigzag rather than a smooth trend along the group. For example, phosphorus and antimony in odd periods of group 15 readily reach the +5 oxidation state, whereas nitrogen, arsenic, and bismuth in even periods prefer to stay at +3.[121][123]A similar situation holds for the d-block, with lutetium through tungsten atoms being slightly smaller than yttrium through molybdenum atoms respectively.[124][125]
Thallium and lead atoms are about the same size as indium and tin atoms respectively, but from bismuth to radon the 6p atoms are larger than the analogous 5p atoms. This happens because when atomic nuclei become highly charged,special relativitybecomes needed to gauge the effect of the nucleus on the electron cloud. Theserelativistic effectsresult in heavy elements increasingly having differing properties compared to their lighter homologues in the periodic table.Spin–orbit interactionsplits the p subshell: one p orbital is relativistically stabilized and shrunken (it fills in thallium and lead), but the other two (filling in bismuth through radon) are relativistically destabilized and expanded.[96]Relativistic effects also explain whygoldis golden andmercuryis a liquid at room temperature.[126][127]They are expected to become very strong in the late seventh period, potentially leading to a collapse of periodicity.[128]Electron configurations are only clearly known until element 108 (hassium), and experimental chemistry beyond 108 has only been done for elements 112 (copernicium) through 115 (moscovium), so the chemical characterization of the heaviest elements remains a topic of current research.[129][130]
The trend that atomic radii decrease from left to right is also present inionic radii, though it is more difficult to examine because the most common ions of consecutive elements normally differ in charge. Ions with the same electron configuration decrease in size as their atomic number rises, due to increased attraction from the more positively charged nucleus: thus for example ionic radii decrease in the series Se2−, Br−, Rb+, Sr2+, Y3+, Zr4+, Nb5+, Mo6+, Tc7+. Ions of the same element get smaller as more electrons are removed, because the attraction from the nucleus begins to outweigh the repulsion between electrons that causes electron clouds to expand: thus for example ionic radii decrease in the series V2+, V3+, V4+, V5+.[131]
The firstionisation energyof an atom is the energy required to remove an electron from it. This varies with the atomic radius: ionisation energy increases left to right and down to up, because electrons that are closer to the nucleus are held more tightly and are more difficult to remove. Ionisation energy thus is minimized at the first element of each period – hydrogen and thealkali metals– and then generally rises until it reaches thenoble gasat the right edge of the period.[32]There are some exceptions to this trend, such as oxygen, where the electron being removed is paired and thus interelectronic repulsion makes it easier to remove than expected.[132]
In the transition series, the outer electrons are preferentially lost even though the inner orbitals are filling. For example, in the 3d series, the 4s electrons are lost first even though the 3d orbitals are being filled. The shielding effect of adding an extra 3d electron approximately compensates the rise in nuclear charge, and therefore the ionisation energies stay mostly constant, though there is a small increase especially at the end of each transition series.[133]
As metal atoms tend to lose electrons in chemical reactions, ionisation energy is generally correlated with chemical reactivity, although there are other factors involved as well.[133]
The opposite property to ionisation energy is theelectron affinity, which is the energy released when adding an electron to the atom.[134]A passing electron will be more readily attracted to an atom if it feels the pull of the nucleus more strongly, and especially if there is an available partially filled outer orbital that can accommodate it. Therefore, electron affinity tends to increase down to up and left to right. The exception is the last column, the noble gases, which have a full shell and have no room for another electron. This gives thehalogensin the next-to-last column the highest electron affinities.[32]
Some atoms, like the noble gases, have no electron affinity: they cannot form stable gas-phase anions.[135](They can form metastableresonancesif the incoming electron arrives with enough kinetic energy, but these inevitably and rapidlyautodetach: for example, the lifetime of the most long-lived He−level is about 359 microseconds.)[136]The noble gases, having high ionisation energies and no electron affinity, have little inclination towards gaining or losing electrons and are generally unreactive.[32]
Some exceptions to the trends occur: oxygen and fluorine have lower electron affinities than their heavier homologues sulfur and chlorine, because they are small atoms and hence the newly added electron would experience significant repulsion from the already present ones. For the nonmetallic elements, electron affinity likewise somewhat correlates with reactivity, but not perfectly since other factors are involved. For example, fluorine has a lower electron affinity than chlorine (because of extreme interelectronic repulsion for the very small fluorine atom), but is more reactive.[134]
Thevalenceof an element can be defined either as the number of hydrogen atoms that can combine with it to form a simple binary hydride, or as twice the number of oxygen atoms that can combine with it to form a simple binary oxide (that is, not aperoxideor asuperoxide).[110]The valences of the main-group elements are directly related to the group number: the hydrides in the main groups 1–2 and 13–17 follow the formulae MH, MH2, MH3, MH4, MH3, MH2, and finally MH. The highest oxides instead increase in valence, following the formulae M2O, MO, M2O3, MO2, M2O5, MO3, M2O7.[l]Today the notion of valence has been extended by that of theoxidation state, which is the formal charge left on an element when all other elements in a compound have been removed as their ions.[113]
The electron configuration suggests a ready explanation from the number of electrons available for bonding;[113]indeed, the number of valence electrons starts at 1 in group 1, and then increases towards the right side of the periodic table, only resetting at 3 whenever each new block starts. Thus in period 6, Cs–Ba have 1–2 valence electrons; La–Yb have 3–16; Lu–Hg have 3–12; and Tl–Rn have 3–8.[112]However, towards the right side of the d- and f-blocks, the theoretical maximum corresponding to using all valence electrons is not achievable at all;[137]the same situation affects oxygen, fluorine, and the light noble gases up to krypton.[138]
A full explanation requires considering the energy that would be released in forming compounds with different valences rather than simply considering electron configurations alone.[139]For example, magnesium forms Mg2+rather than Mg+cations when dissolved in water, because the latter would spontaneouslydisproportionateinto Mg0and Mg2+cations. This is because theenthalpyof hydration (surrounding the cation with water molecules) increases in magnitude with the charge and radius of the ion. In Mg+, the outermost orbital (which determines ionic radius) is still 3s, so the hydration enthalpy is small and insufficient to compensate the energy required to remove the electron; but ionizing again to Mg2+uncovers the core 2p subshell, making the hydration enthalpy large enough to allow magnesium(II) compounds to form. For similar reasons, the common oxidation states of the heavier p-block elements (where the ns electrons become lower in energy than the np) tend to vary by steps of 2, because that is necessary to uncover an inner subshell and decrease the ionic radius (e.g. Tl+uncovers 6s, and Tl3+uncovers 5d, so once thallium loses two electrons it tends to lose the third one as well). Analogous arguments based onorbital hybridizationcan be used for the less electronegative p-block elements.[140][m]
For transition metals, common oxidation states are nearly always at least +2 for similar reasons (uncovering the next subshell); this holds even for the metals with anomalous dx+1s1or dx+2s0configurations (except forsilver), because repulsion between d-electrons means that the movement of the second electron from the s- to the d-subshell does not appreciably change its ionisation energy.[142]Because ionizing the transition metals further does not uncover any new inner subshells, their oxidation states tend to vary by steps of 1 instead.[140]The lanthanides and late actinides generally show a stable +3 oxidation state, removing the outer s-electrons and then (usually) one electron from the (n−2)f orbitals, that are similar in energy to ns.[143]The common and maximum oxidation states of the d- and f-block elements tend to depend on the ionisation energies. As the energy difference between the (n−1)d and ns orbitals rises along each transition series, it becomes less energetically favourable to ionize further electrons. Thus, the early transition metal groups tend to prefer higher oxidation states, but the +2 oxidation state becomes more stable for the late transition metal groups. The highest formal oxidation state thus increases from +3 at the beginning of each d-block row, to +7 or +8 in the middle (e.g.OsO4), and then decrease to +2 at the end.[142]The lanthanides and late actinides usually have high fourth ionisation energies and hence rarely surpass the +3 oxidation state, whereas early actinides have low fourth ionisation energies and so for example neptunium and plutonium can reach +7.[110][142][143]The very last actinides go further than the lanthanides towards low oxidation states: mendelevium is more easily reduced to the +2 state than thulium or even europium (the lanthanide with the most stable +2 state, on account of its half-filled f-shell), and nobelium outright favours +2 over +3, in contrast to ytterbium.[54]
As elements in the same group share the same valence configurations, they usually exhibit similar chemical behaviour. For example, thealkali metalsin the first group all have one valence electron, and form a very homogeneous class of elements: they are all soft and reactive metals. However, there are many factors involved, and groups can often be rather heterogeneous. For instance, hydrogen also has one valence electron and is in the same group as the alkali metals, but its chemical behaviour is quite different. The stable elements ofgroup 14comprise a nonmetal (carbon), two semiconductors (siliconandgermanium), and two metals (tinandlead); they are nonetheless united by having four valence electrons.[144]This often leads to similarities in maximum and minimum oxidation states (e.g.sulfurandseleniumingroup 16both have maximum oxidation state +6, as inSO3andSeO3, and minimum oxidation state −2, as insulfidesandselenides); but not always (e.g.oxygenis not known to form oxidation state +6, despite being in the same group as sulfur and selenium).[58]
Another important property of elements is theirelectronegativity. Atoms can formcovalent bondsto each other by sharing electrons in pairs, creating an overlap of valence orbitals. The degree to which each atom attracts the shared electron pair depends on the atom's electronegativity[145]– the tendency of an atom towards gaining or losing electrons.[32]The more electronegative atom will tend to attract the electron pair more, and the less electronegative (or more electropositive) one will attract it less. In extreme cases, the electron can be thought of as having been passed completely from the more electropositive atom to the more electronegative one, though this is a simplification. The bond then binds two ions, one positive (having given up the electron) and one negative (having accepted it), and is termed anionic bond.[32]
Electronegativity depends on how strongly the nucleus can attract an electron pair, and so it exhibits a similar variation to the other properties already discussed: electronegativity tends to fall going up to down, and rise going left to right. The alkali and alkaline earth metals are among the most electropositive elements, while the chalcogens, halogens, and noble gases are among the most electronegative ones.[145]
Electronegativity is generally measured on the Pauling scale, on which the most electronegative reactive atom (fluorine) is given electronegativity 4.0, and the least electronegative atom (caesium) is given electronegativity 0.79.[32]In factneonis the most electronegative element, but the Pauling scale cannot measure its electronegativity because it does not form covalent bonds with most elements.[146]
An element's electronegativity varies with the identity and number of the atoms it is bonded to, as well as how many electrons it has already lost: an atom becomes more electronegative when it has lost more electrons.[145]This sometimes makes a large difference: lead in the +2 oxidation state has electronegativity 1.87 on the Pauling scale, while lead in the +4 oxidation state has electronegativity 2.33.[147]
A simple substance is a substance formed from atoms of one chemical element. The simple substances of the more electronegative atoms tend to share electrons (form covalent bonds) with each other. They form either small molecules (like hydrogen or oxygen, whose atoms bond in pairs) or giant structures stretching indefinitely (like carbon or silicon). The noble gases simply stay as single atoms, as they already have a full shell.[32]Substances composed of discrete molecules or single atoms are held together by weaker attractive forces between the molecules, such as theLondon dispersion force: as electrons move within the molecules, they create momentary imbalances of electrical charge, which induce similar imbalances on nearby molecules and create synchronized movements of electrons across many neighbouring molecules.[149]
The more electropositive atoms, however, tend to instead lose electrons, creating a "sea" of electrons engulfing cations.[32]The outer orbitals of one atom overlap to share electrons with all its neighbours, creating a giant structure of molecular orbitals extending over all the atoms.[150]This negatively charged "sea" pulls on all the ions and keeps them together in ametallic bond. Elements forming such bonds are often calledmetals; those which do not are often callednonmetals.[32]Some elements can form multiple simple substances with different structures: these are calledallotropes. For example,diamondandgraphiteare two allotropes of carbon.[144][n]
The metallicity of an element can be predicted from electronic properties. When atomic orbitals overlap during metallic or covalent bonding, they create both bonding and antibondingmolecular orbitalsof equal capacity, with the antibonding orbitals of higher energy. Net bonding character occurs when there are more electrons in the bonding orbitals than there are in the antibonding orbitals. Metallic bonding is thus possible when the number of electrons delocalized by each atom is less than twice the number of orbitals contributing to the overlap. This is the situation for elements in groups 1 through 13; they also have too few valence electrons to form giant covalent structures where all atoms take equivalent positions, and so almost all of them metallise. The exceptions are hydrogen and boron, which have too high an ionisation energy. Hydrogen thus forms a covalent H2molecule, and boron forms a giant covalent structure based on icosahedral B12clusters. In a metal, the bonding and antibonding orbitals have overlapping energies, creating a single band that electrons can freely flow through, allowing for electrical conduction.[152]
In group 14, both metallic and covalent bonding become possible. In a diamond crystal, covalent bonds between carbon atoms are strong, because they have a small atomic radius and thus the nucleus has more of a hold on the electrons. Therefore, the bonding orbitals that result are much lower in energy than the antibonding orbitals, and there is no overlap, so electrical conduction becomes impossible: carbon is a nonmetal. However, covalent bonding becomes weaker for larger atoms and the energy gap between the bonding and antibonding orbitals decreases. Therefore, silicon and germanium have smallerband gapsand aresemiconductorsat ambient conditions: electrons can cross the gap when thermally excited. (Boron is also a semiconductor at ambient conditions.) The band gap disappears in tin, so that tin and lead become metals.[152]As the temperature rises, all nonmetals develop some semiconducting properties, to a greater or lesser extent depending on the size of the band gap. Thus metals and nonmetals may be distinguished by the temperature dependence of their electrical conductivity: a metal's conductivity lowers as temperature rises (because thermal motion makes it more difficult for the electrons to flow freely), whereas a nonmetal's conductivity rises (as more electrons may be excited to cross the gap).[153]
Elements in groups 15 through 17 have too many electrons to form giant covalent molecules that stretch in all three dimensions. For the lighter elements, the bonds in small diatomic molecules are so strong that a condensed phase is disfavoured: thus nitrogen (N2), oxygen (O2), white phosphorus and yellow arsenic (P4and As4), sulfur and red selenium (S8and Se8), and the stable halogens (F2, Cl2, Br2, and I2) readily form covalent molecules with few atoms. The heavier ones tend to form long chains (e.g. red phosphorus, grey selenium, tellurium) or layered structures (e.g. carbon as graphite, black phosphorus, grey arsenic, antimony, bismuth) that only extend in one or two rather than three dimensions. Both kinds of structures can be found as allotropes of phosphorus, arsenic, and selenium, although the long-chained allotropes are more stable in all three. As these structures do not use all their orbitals for bonding, they end up with bonding, nonbonding, and antibonding bands in order of increasing energy. Similarly to group 14, the band gaps shrink for the heavier elements and free movement of electrons between the chains or layers becomes possible. Thus for example black phosphorus, black arsenic, grey selenium, tellurium, and iodine are semiconductors; grey arsenic, antimony, and bismuth aresemimetals(exhibiting quasi-metallic conduction, with a very small band overlap); and polonium and probably astatine are true metals.[152]Finally, the natural group 18 elements all stay as individual atoms.[152][o]
The dividing line between metals and nonmetals is roughly diagonal from top left to bottom right, with the transition series appearing to the left of this diagonal (as they have many available orbitals for overlap). This is expected, as metallicity tends to be correlated with electropositivity and the willingness to lose electrons, which increases right to left and up to down. Thus the metals greatly outnumber the nonmetals. Elements near the borderline are difficult to classify: they tend to have properties that are intermediate between those of metals and nonmetals, and may have some properties characteristic of both. They are often termed semimetals ormetalloids.[32]The term "semimetal" used in this sense should not be confused with its strict physical sense having to do with band structure: bismuth is physically a semimetal, but is generally considered a metal by chemists.[155]
The following table considers the most stable allotropes at standard conditions. The elements coloured yellow form simple substances that are well-characterised by metallic bonding. Elements coloured light blue form giant network covalent structures, whereas those coloured dark blue form small covalently bonded molecules that are held together by weakervan der Waals forces. The noble gases are coloured in violet: their molecules are single atoms and no covalent bonding occurs. Greyed-out cells are for elements which have not been prepared in sufficient quantities for their most stable allotropes to have been characterized in this way. Theoretical considerations and current experimental evidence suggest that all of those elements would metallise if they could form condensed phases,[152]except perhaps for oganesson.[156][p]
MetallicNetwork covalentMolecularcovalentSingle atomsUnknownBackground colorshows bonding of simple substances in theperiodic table. If there are several, the most stable allotrope is considered.
Generally, metals are shiny and dense.[32]They usually have high melting and boiling points due to the strength of the metallic bond, and are often malleable and ductile (easily stretched and shaped) because the atoms can move relative to each other without breaking the metallic bond.[166]They conduct electricity because their electrons are free to move in all three dimensions. Similarly, they conduct heat, which is transferred by the electrons as extrakinetic energy: they move faster. These properties persist in the liquid state, as although the crystal structure is destroyed on melting, the atoms still touch and the metallic bond persists, though it is weakened.[166]Metals tend to be reactive towards nonmetals.[32]Some exceptions can be found to these generalizations: for example, beryllium, chromium,[84]manganese,[167]antimony,[168]bismuth,[169]and uranium are brittle (not an exhaustive list);[84]chromium is extremely hard;[170]gallium, rubidium, caesium, and mercury are liquid at or close to room temperature;[q]andnoble metalssuch as gold are chemically very inert.[171][172]
Nonmetals exhibit different properties. Those forming giant covalent crystals exhibit high melting and boiling points, as it takes considerable energy to overcome the strong covalent bonds. Those forming discrete molecules are held together mostly by dispersion forces, which are more easily overcome; thus they tend to have lower melting and boiling points,[173]and many are liquids or gases at room temperature.[32]Nonmetals are often dull-looking. They tend to be reactive towards metals, except for the noble gases, which are inert towards most substances.[32]They are brittle when solid as their atoms are held tightly in place. They are less dense and conduct electricity poorly,[32]because there are no mobile electrons.[174]Near the borderline, band gaps are small and thus many elements in that region are semiconductors, such as silicon, germanium,[174]and tellurium.[152]Selenium has both a semiconducting grey allotrope and an insulating red allotrope; arsenic has a metallic grey allotrope, a semiconducting black allotrope, and an insulating yellow allotrope (though the last is unstable at ambient conditions).[153]Again there are exceptions; for example, diamond has the highest thermal conductivity of all known materials, greater than any metal.[175]
It is common to designate a class of metalloids straddling the boundary between metals and nonmetals, as elements in that region are intermediate in both physical and chemical properties.[32]However, no consensus exists in the literature for precisely which elements should be so designated. When such a category is used, silicon, germanium, arsenic, and tellurium are almost always included, and boron and antimony usually are; but most sources include other elements as well, without agreement on which extra elements should be added, and some others subtract from this list instead.[r]For example, unlike all the other elements generally considered metalloids or nonmetals, antimony's only stable form has metallic conductivity. Moreover, the element resembles bismuth and, more generally, the other p-block metals in its physical and chemical behaviour. On this basis some authors have argued that it is better classified as a metal than as a metalloid.[84][180][153]On the other hand, selenium has some semiconducting properties in its most stable form (though it also has insulating allotropes) and it has been argued that it should be considered a metalloid[180]– though this situation also holds for phosphorus,[153]which is a much rarer inclusion among the metalloids.[r]
There are some other relationships throughout the periodic table between elements that are not in the same group, such as thediagonal relationshipsbetween elements that are diagonally adjacent (e.g. lithium and magnesium).[121]Some similarities can also be found between the main groups and the transition metal groups, or between the early actinides and early transition metals, when the elements have the same number of valence electrons. Thus uranium somewhat resembles chromium and tungsten in group 6,[121]as all three have six valence electrons.[181]Relationships between elements with the same number of valence electrons but different types of valence orbital have been called secondary or isodonor relationships: they usually have the same maximum oxidation states, but not the same minimum oxidation states. For example, chlorine and manganese both have +7 as their maximum oxidation state (e.g.Cl2O7andMn2O7), but their respective minimum oxidation states are −1 (e.g.HCl) and −3 (K2[Mn(CO)4]). Elements with the same number of valence vacancies but different numbers of valence electrons are related by a tertiary or isoacceptor relationship: they usually have similar minimum but not maximum oxidation states. For example, hydrogen and chlorine both have −1 as their minimum oxidation state (inhydridesandchlorides), but hydrogen's maximum oxidation state is +1 (e.g.H2O) while chlorine's is +7.[58]
Many other physical properties of the elements exhibit periodic variation in accordance with the periodic law, such asmelting points,boiling points,heats of fusion,heats of vaporization,atomisation energy, and so on. Similar periodic variations appear for the compounds of the elements, which can be observed by comparing hydrides, oxides, sulfides, halides, and so on.[145]Chemical properties are more difficult to describe quantitatively, but likewise exhibit their own periodicities. Examples include the variation in theacidicandbasicproperties of the elements and their compounds, the stabilities of compounds, and methods of isolating the elements.[113]Periodicity is and has been used very widely to predict the properties of unknown new elements and new compounds, and is central to modern chemistry.[182]
Many terms have been used in the literature to describe sets of elements that behave similarly. The group namesalkali metal,alkaline earth metal,triel,tetrel,pnictogen,chalcogen,halogen, andnoble gasare acknowledged by IUPAC; the other groups can be referred to by their number, or by their first element (e.g., group 6 is the chromium group).[22][183]Some divide the p-block elements from groups 13 to 16 by metallicity,[178][176]although there is neither an IUPAC definition nor a precise consensus on exactly which elements should be considered metals, nonmetals, or semi-metals (sometimes called metalloids).[178][176][22]Neither is there a consensus on what the metals succeeding the transition metals ought to be called, withpost-transition metalandpoor metalbeing among the possibilities having been used. Some advanced monographs exclude the elements of group 12 from the transition metals on the grounds of their sometimes quite different chemical properties, but this is not a universal practice[184]and IUPAC does not presently mention it as allowable in itsPrinciples of Chemical Nomenclature.[185]
Thelanthanidesare considered to be the elements La–Lu, which are all very similar to each other: historically they included only Ce–Lu, but lanthanum became included by common usage.[22]Therare earth elements(or rare earth metals) add scandium and yttrium to the lanthanides.[22]Analogously, theactinidesare considered to be the elements Ac–Lr (historically Th–Lr),[22]although variation of properties in this set is much greater than within the lanthanides.[52]IUPAC recommends the nameslanthanoidsandactinoidsto avoid ambiguity, as the -ide suffix typically denotes a negative ion; howeverlanthanidesandactinidesremain common.[22]With the increasing recognition of lutetium and lawrencium as d-block elements, some authors began to define the lanthanides as La–Yb and the actinides as Ac–No, matching the f-block.[57][25][186][187][188][189]Thetransactinidesorsuperheavy elementsare the short-lived elements beyond the actinides, starting at lawrencium or rutherfordium (depending on where the actinides are taken to end).[189][190][191][192][193]
Many more categorizations exist and are used according to certain disciplines. In astrophysics, a metal is defined as any element with atomic number greater than 2, i.e. anything except hydrogen and helium.[194]The term "semimetal" has a different definition in physics than it does in chemistry: bismuth is a semimetal by physical definitions, but chemists generally consider it a metal.[195]A few terms are widely used, but without any very formal definition, such as "heavy metal", which has been given such a wide range of definitions that it has been criticized as "effectively meaningless".[196]
The scope of terms varies significantly between authors. For example, according to IUPAC, the noble gases extend to include the whole group, including the very radioactive superheavy element oganesson.[197]However, among those who specialize in the superheavy elements, this is not often done: in this case "noble gas" is typically taken to imply the unreactive behaviour of the lighter elements of the group. Since calculations generally predict that oganesson should not be particularly inert due to relativistic effects, and may not even be a gas at room temperature if it could be produced in bulk, its status as a noble gas is often questioned in this context.[198]Furthermore, national variations are sometimes encountered: in Japan, alkaline earth metals often do not include beryllium and magnesium as their behaviour is different from the heavier group 2 metals.[199]
In 1817, German physicistJohann Wolfgang Döbereinerbegan to formulate one of the earliest attempts to classify the elements.[200]In 1829, he found that he could form some of the elements into groups of three, with the members of each group having related properties. He termed these groupstriads.[201][202]Chlorine, bromine, and iodine formed a triad; as did calcium, strontium, and barium; lithium, sodium, and potassium; and sulfur, selenium, and tellurium. Today, all these triads form part of modern-day groups: the halogens, alkaline earth metals, alkali metals, and chalcogens.[203]Various chemists continued his work and were able to identify more and more relationships between small groups of elements. However, they could not build one scheme that encompassed them all.[204]
John Newlandspublished a letter in theChemical Newsin February 1863 on the periodicity among the chemical elements.[205]In 1864 Newlands published an article in theChemical Newsshowing that if the elements are arranged in the order of their atomic weights, those having consecutive numbers frequently either belong to the same group or occupy similar positions in different groups, and he pointed out that each eighth element starting from a given one is in this arrangement a kind of repetition of the first, like the eighth note of an octave in music (The Law of Octaves).[205]However, Newlands's formulation only worked well for the main-group elements, and encountered serious problems with the others.[58]
German chemistLothar Meyernoted the sequences of similar chemical and physical properties repeated at periodic intervals. According to him, if the atomic weights were plotted as ordinates (i.e. vertically) and the atomic volumes as abscissas (i.e. horizontally)—the curve obtained a series of maximums and minimums—the mostelectropositiveelements would appear at the peaks of the curve in the order of their atomic weights. In 1864, a book of his was published; it contained an early version of the periodic table containing 28 elements, and classified elements into six families by theirvalence—for the first time, elements had been grouped according to their valence. Works on organizing the elements by atomic weight had until then been stymied by inaccurate measurements of the atomic weights.[206]In 1868, he revised his table, but this revision was published as a draft only after his death.[207]
The definitive breakthrough came from the Russian chemistDmitri Mendeleev. Although other chemists (including Meyer) had found some other versions of the periodic system at about the same time, Mendeleev was the most dedicated to developing and defending his system, and it was his system that most affected the scientific community.[208]On 17 February 1869 (1 March 1869 in the Gregorian calendar), Mendeleev began arranging the elements and comparing them by their atomic weights. He began with a few elements, and over the course of the day his system grew until it encompassed most of the known elements. After he found a consistent arrangement, his printed table appeared in May 1869 in the journal of the Russian Chemical Society.[209]When elements did not appear to fit in the system, he boldly predicted that either valencies or atomic weights had been measured incorrectly, or that there was a missing element yet to be discovered.[58]In 1871, Mendeleev published a long article, including an updated form of his table, that made his predictions for unknown elements explicit. Mendeleev predicted the properties of three of these unknown elements in detail: as they would be missing heavier homologues of boron, aluminium, and silicon, he named them eka-boron, eka-aluminium, and eka-silicon ("eka" being Sanskrit for "one").[209][210]: 45In 1875, the French chemistPaul-Émile Lecoq de Boisbaudran, working without knowledge of Mendeleev's prediction, discovered a new element in a sample of the mineralsphalerite, and named it gallium. He isolated the element and began determining its properties. Mendeleev, reading de Boisbaudran's publication, sent a letter claiming that gallium was his predicted eka-aluminium. Although Lecoq de Boisbaudran was initially sceptical, and suspected that Mendeleev was trying to take credit for his discovery, he later admitted that Mendeleev was correct.[211]In 1879, the Swedish chemistLars Fredrik Nilsondiscovered a new element, which he named scandium: it turned out to be eka-boron. Eka-silicon was found in 1886 by German chemistClemens Winkler, who named it germanium. The properties of gallium, scandium, and germanium matched what Mendeleev had predicted.[212]In 1889, Mendeleev noted at the Faraday Lecture to the Royal Institution in London that he had not expected to live long enough "to mention their discovery to the Chemical Society of Great Britain as a confirmation of the exactitude and generality of the periodic law".[213]Even the discovery of the noble gases at the close of the 19th century, which Mendeleev had not predicted, fitted neatly into his scheme as an eighth main group.[214]
Mendeleev nevertheless had some trouble fitting the known lanthanides into his scheme, as they did not exhibit the periodic change in valencies that the other elements did. After much investigation, the Czech chemistBohuslav Braunersuggested in 1902 that the lanthanides could all be placed together in one group on the periodic table. He named this the "asteroid hypothesis" as an astronomical analogy: just as there is anasteroid beltinstead of a single planet between Mars and Jupiter, so the place below yttrium was thought to be occupied by all the lanthanides instead of just one element.[33]
After the internal structure of the atom was probed, amateur Dutch physicistAntonius van den Broekproposed in 1913 that the nuclear charge determined the placement of elements in the periodic table.[215][216]The New Zealand physicistErnest Rutherfordcoined the word "atomic number" for this nuclear charge.[217]In van den Broek's published article he illustrated the first electronic periodic table showing the elements arranged according to the number of their electrons.[218]Rutherford confirmed in his 1914 paper that Bohr had accepted the view of van den Broek.[219]
The same year, English physicistHenry MoseleyusingX-ray spectroscopyconfirmed van den Broek's proposal experimentally. Moseley determined the value of the nuclear charge of each element fromaluminiumtogoldand showed that Mendeleev's ordering actually places the elements in sequential order by nuclear charge.[220]Nuclear charge is identical toprotoncount and determines the value of theatomic number(Z) of each element. Using atomic number gives a definitive, integer-based sequence for the elements. Moseley's research immediately resolved discrepancies between atomic weight and chemical properties; these were cases such as tellurium and iodine, where atomic number increases but atomic weight decreases.[215]Although Moseley was soon killed in World War I, the Swedish physicistManne Siegbahncontinued his work up touranium, and established that it was the element with the highest atomic number then known (92).[221]Based on Moseley and Siegbahn's research, it was also known which atomic numbers corresponded to missing elements yet to be found: 43, 61, 72, 75, 85, and 87.[215](Element 75 had in fact already been found by Japanese chemistMasataka Ogawain 1908 and namednipponium, but he mistakenly assigned it as element 43 instead of 75 and so his discovery was not generally recognized until later. The contemporarily accepted discovery of element 75 came in 1925, whenWalter Noddack,Ida Tacke, andOtto Bergindependently rediscovered it and gave it its present name,rhenium.)[222]
The dawn of atomic physics also clarified the situation ofisotopes. In thedecay chainsof the primordial radioactive elements thorium and uranium, it soon became evident that there were many apparent new elements that had different atomic weights but exactly the same chemical properties. In 1913,Frederick Soddycoined the term "isotope" to describe this situation, and considered isotopes to merely be different forms of the same chemical element. This furthermore clarified discrepancies such as tellurium and iodine: tellurium's natural isotopic composition is weighted towards heavier isotopes than iodine's, but tellurium has a lower atomic number.[223]
The Danish physicistNiels BohrappliedMax Planck's idea of quantization to the atom. He concluded that the energy levels of electrons were quantised: only a discrete set of stable energy states were allowed. Bohr then attempted to understand periodicity through electron configurations, surmising in 1913 that the inner electrons should be responsible for the chemical properties of the element.[224][225]In 1913, he produced the first electronic periodic table based on a quantum atom.[226]
Bohr called his electron shells "rings" in 1913: atomic orbitals within shells did not exist at the time of his planetary model. Bohr explains in Part 3 of his famous 1913 paper that the maximum electrons in a shell is eight, writing, "We see, further, that a ring ofnelectrons cannot rotate in a single ring round a nucleus of charge ne unlessn< 8." For smaller atoms, the electron shells would be filled as follows: "rings of electrons will only join if they contain equal numbers of electrons; and that accordingly the numbers of electrons on inner rings will only be 2, 4, 8." However, in larger atoms the innermost shell would contain eight electrons: "on the other hand, the periodic system of the elements strongly suggests that already in neonN= 10 an inner ring of eight electrons will occur." His proposed electron configurations for the atoms (shown to the right) mostly do not accord with those now known.[227][228]They were improved further after the work ofArnold SommerfeldandEdmund Stonerdiscovered more quantum numbers.[223]
The first one to systematically expand and correct the chemical potentials of Bohr's atomic theory wasWalther Kosselin 1914 and in 1916. Kossel explained that in the periodic table new elements would be created as electrons were added to the outer shell. In Kossel's paper, he writes:
This leads to the conclusion that the electrons, which are added further, should be put into concentric rings or shells, on each of which ... only a certain number of electrons—namely, eight in our case—should be arranged. As soon as one ring or shell is completed, a new one has to be started for the next element; the number of electrons, which are most easily accessible, and lie at the outermost periphery, increases again from element to element and, therefore, in the formation of each new shell the chemical periodicity is repeated.[229][230]
In a 1919 paper,Irving Langmuirpostulated the existence of "cells" which we now call orbitals, which could each only contain eight electrons each, and these were arranged in "equidistant layers" which we now call shells. He made an exception for the first shell to only contain two electrons.[231]The chemistCharles Rugeley Burysuggested in 1921 that eight and eighteen electrons in a shell form stable configurations. Bury proposed that the electron configurations in transitional elements depended upon the valence electrons in their outer shell.[232]He introduced the wordtransitionto describe the elements now known astransition metalsor transition elements.[233]Bohr's theory was vindicated by the discovery of element 72:Georges Urbainclaimed to have discovered it as therare earth elementceltium, but Bury and Bohr had predicted that element 72 could not be a rare earth element and had to be a homologue ofzirconium.Dirk CosterandGeorg von Hevesysearched for the element in zirconium ores and found element 72, which they namedhafniumafter Bohr's hometown ofCopenhagen(Hafniain Latin).[234][235]Urbain's celtium proved to be simply purifiedlutetium(element 71).[236]Hafnium and rhenium thus became the last stable elements to be discovered.[223]
Prompted by Bohr,Wolfgang Paulitook up the problem of electron configurations in 1923. Pauli extended Bohr's scheme to use fourquantum numbers, and formulated hisexclusion principlewhich stated that no two electrons could have the same four quantum numbers. This explained the lengths of the periods in the periodic table (2, 8, 18, and 32), which corresponded to the number of electrons that each shell could occupy.[237]In 1925,Friedrich Hundarrived at configurations close to the modern ones.[238]As a result of these advances, periodicity became based on the number of chemically active or valence electrons rather than by the valences of the elements.[58]TheAufbau principlethat describes the electron configurations of the elements was first empirically observed byErwin Madelungin 1926,[45]though the first to publish it wasVladimir Karapetoffin 1930.[239][240]In 1961,Vsevolod Klechkovskyderived the first part of the Madelung rule (that orbitals fill in order of increasingn+ ℓ) from theThomas–Fermi model;[241]the complete rule was derived from a similar potential in 1971 by Yury N. Demkov and Valentin N. Ostrovsky.[242][s]
The quantum theory clarified the transition metals and lanthanides as forming their own separate groups, transitional between the main groups, although some chemists had already proposed tables showing them this way before then: the English chemist Henry Bassett did so in 1892, the Danish chemistJulius Thomsenin 1895, and the Swiss chemistAlfred Wernerin 1905. Bohr used Thomsen's form in his 1922 Nobel Lecture; Werner's form is very similar to the modern 32-column form. In particular, this supplanted Brauner's asteroidal hypothesis.[33]
The exact position of the lanthanides, and thus the composition ofgroup 3, remained under dispute for decades longer because their electron configurations were initially measured incorrectly.[25][92]On chemical grounds Bassett, Werner, and Bury grouped scandium and yttrium with lutetium rather than lanthanum (the former two left an empty space below yttrium as lutetium had not yet been discovered).[33][232]Hund assumed in 1927 that all the lanthanide atoms had configuration [Xe]4f0−145d16s2, on account of their prevailing trivalency. It is now known that the relationship between chemistry and electron configuration is more complicated than that.[t][54]Early spectroscopic evidence seemed to confirm these configurations, and thus the periodic table was structured to have group 3 as scandium, yttrium, lanthanum, and actinium, with fourteen f-elements breaking up the d-block between lanthanum and hafnium.[25]But it was later discovered that this is only true for four of the fifteen lanthanides (lanthanum, cerium, gadolinium, and lutetium), and that the other lanthanide atoms do not have a d-electron. In particular, ytterbium completes the 4f shell and thus Soviet physicists Lev Landau and Evgeny Lifshitz noted in 1948 that lutetium is correctly regarded as a d-block rather than an f-block element;[26]that bulk lanthanum is an f-metal was first suggested byJun Kondōin 1963, on the grounds of its low-temperaturesuperconductivity.[100]This clarified the importance of looking at low-lying excited states of atoms that can play a role in chemical environments when classifying elements by block and positioning them on the table.[63][65][25]Many authors subsequently rediscovered this correction based on physical, chemical, and electronic concerns and applied it to all the relevant elements, thus making group 3 contain scandium, yttrium, lutetium, and lawrencium[63][23][92]and having lanthanum through ytterbium and actinium through nobelium as the f-block rows:[63][23]this corrected version achieves consistency with the Madelung rule and vindicates Bassett, Werner, and Bury's initial chemical placement.[33]
In 1988, IUPAC released a report supporting this composition of group 3,[23]a decision that was reaffirmed in 2021.[24]Variation can still be found in textbooks on the composition of group 3,[35]and some argumentation against this format is still published today,[27]but chemists and physicists who have considered the matter largely agree on group 3 containing scandium, yttrium, lutetium, and lawrencium and challenge the counterarguments as being inconsistent.[27]
By 1936, the pool of missing elements from hydrogen to uranium had shrunk to four: elements 43, 61, 85, and 87 remained missing. Element 43 eventually became the first element to be synthesized artificially via nuclear reactions rather than discovered in nature. It was discovered in 1937 by Italian chemistsEmilio SegrèandCarlo Perrier, who named their discoverytechnetium, after the Greek word for "artificial".[243]Elements 61 (promethium) and 85 (astatine) were likewise produced artificially in 1945 and 1940 respectively; element 87 (francium) became the last element to be discovered in nature, by French chemistMarguerite Pereyin 1939.[244][u]The elements beyond uranium were likewise discovered artificially, starting withEdwin McMillanandPhilip Abelson's 1940 discovery ofneptunium(via bombardment of uranium with neutrons).[75]Glenn T. Seaborgand his team at theLawrence Berkeley National Laboratory(LBNL) continued discovering transuranium elements, starting withplutoniumin 1941, and discovered that contrary to previous thinking, the elements from actinium onwards were congeners of the lanthanides rather than transition metals.[245]Bassett (1892), Werner (1905), and the French engineerCharles Janet(1928) had previously suggested this, but their ideas did not then receive general acceptance.[33]Seaborg thus called them the actinides.[245]Elements up to 101 (named mendelevium in honour of Mendeleev) were synthesized up to 1955, either through neutron or alpha-particle irradiation, or in nuclear explosions in the cases of 99 (einsteinium) and 100 (fermium).[75]
A significant controversy arose with elements 102 through 106 in the 1960s and 1970s, as competition arose between the LBNL team (now led byAlbert Ghiorso) and a team of Soviet scientists at theJoint Institute for Nuclear Research(JINR) led byGeorgy Flyorov. Each team claimed discovery, and in some cases each proposed their own name for the element, creating anelement naming controversythat lasted decades. These elements were made by bombardment of actinides with light ions.[246]IUPAC at first adopted a hands-off approach, preferring to wait and see if a consensus would be forthcoming. But as it was also the height of theCold War, it became clear that this would not happen. As such, IUPAC and theInternational Union of Pure and Applied Physics(IUPAP) created aTransfermium Working Group(TWG, fermium being element 100) in 1985 to set out criteria for discovery,[247]which were published in 1991.[248]After some further controversy, these elements received their final names in 1997, including seaborgium (106) in honour of Seaborg.[249]
The TWG's criteria were used to arbitrate later element discovery claims from LBNL and JINR, as well as from research institutes in Germany (GSI) and Japan (Riken).[250]Currently, consideration of discovery claims is performed by aIUPAC/IUPAP Joint Working Party. After priority was assigned, the elements were officially added to the periodic table, and the discoverers were invited to propose their names.[6]By 2016, this had occurred for all elements up to 118, therefore completing the periodic table's first seven rows.[6][251]The discoveries of elements beyond 106 were made possible by techniques devised byYuri Oganessianat the JINR: cold fusion (bombardment of lead and bismuth by heavy ions) made possible the 1981–2004 discoveries of elements 107 through 112 at GSI and 113 at Riken, and he led the JINR team (in collaboration with American scientists) to discover elements 114 through 118 using hot fusion (bombardment of actinides by calcium ions) in 1998–2010.[252][253]The heaviest known element, oganesson (118), is named in Oganessian's honour. Element 114 is named flerovium in honour of his predecessor and mentor Flyorov.[253]
In celebration of the periodic table's 150th anniversary, theUnited Nationsdeclared the year 2019 as the International Year of the Periodic Table, celebrating "one of the most significant achievements in science".[254]The discovery criteria set down by the TWG were updated in 2020 in response to experimental and theoretical progress that had not been foreseen in 1991.[255]Today, the periodic table is among the most recognisable icons of chemistry.[81]IUPAC is involved today with many processes relating to the periodic table: the recognition and naming of new elements, recommending group numbers and collective names, and the updating of atomic weights.[6]
The most recently named elements – nihonium (113), moscovium (115), tennessine (117), and oganesson (118) – completed the seventh row of the periodic table.[6]Future elements would have to begin aneighth row. These elements may be referred to either by their atomic numbers (e.g. "element 164"), or by the IUPACsystematic element namesadopted in 1978, which directly relate to the atomic numbers (e.g. "unhexquadium" for element 164, derived from Latinunus"one", Greekhexa"six", Latinquadra"four", and the traditional-iumsuffix for metallic elements).[6]All attempts to synthesize such elements have failed so far. An attempt to makeelement 119has been ongoing since 2018 at the Riken research institute in Japan. The LBNL in the United States, the JINR in Russia, and the Heavy Ion Research Facility inLanzhou(HIRFL) in China also plan to make their own attempts at synthesizing the first few period 8 elements.[258][259][260][261][262][263]
If the eighth period followed the pattern set by the earlier periods, then it would contain fifty elements, filling the 8s, 5g, 6f, 7d, and finally 8p subshells in that order. But by this point, relativistic effects should result in significant deviations from the Madelung rule. Various different models have been suggested for the configurations of eighth-period elements, as well as how to show the results in a periodic table. All agree that the eighth period should begin like the previous ones with two 8s elements, 119 and120. However, after that the massive energetic overlaps between the 5g, 6f, 7d, and 8p subshells means that they all begin to fill together, and it is not clear how to separate out specific 5g and 6f series.[59][264][265][266][267]Elements121through 156 thus do not fit well as chemical analogues of any previous group in the earlier parts of the table,[128]although they have sometimes been placed as 5g, 6f, and other series to formally reflect their electron configurations.[128]Eric Scerri has raised the question of whether an extended periodic table should take into account the failure of the Madelung rule in this region, or if such exceptions should be ignored.[264]The shell structure may also be fairly formal at this point: already the electron distribution in an oganesson atom is expected to be rather uniform, with no discernible shell structure.[268]
The situation from elements 157 to 172 should return to normalcy and be more reminiscent of the earlier rows.[257]The heavy p-shells are split by thespin–orbit interaction: one p orbital (p1/2) is more stabilized, and the other two (p3/2) are destabilized. (Such shifts in the quantum numbers happen for all types of shells, but it makes the biggest difference to the order for the p-shells.) It is likely that by element 157, the filled 8s and 8p1/2shells with four electrons in total have sunk into the core. Beyond the core, the next orbitals are 7d and 9s at similar energies, followed by 9p1/2and 8p3/2at similar energies, and then a large gap.[257]Thus, the 9s and 9p1/2orbitals in essence replace the 8s and 8p1/2ones, making elements 157–172 probably chemically analogous to groups 3–18: for example, element 164 would appear two places below lead in group 14 under the usual pattern, but is calculated to be very analogous to palladium in group 10 instead.[54][266][59][256][128]Thus, it takes fifty-four elements rather than fifty to reach the next noble element after 118.[269]However, while these conclusions about elements 157 through 172's chemistry are generally agreed by models,[128][59]there is disagreement on whether the periodic table should be drawn to reflect chemical analogies, or if it should reflect likely formal electron configurations, which should be quite different from earlier periods and are not agreed between sources. Discussion about the format of the eighth row thus continues.[59][266][267][104]
Beyond element 172, calculation is complicated by the 1s electron energy level becomingimaginary. Such a situation does have a physical interpretation and does not in itself pose an electronic limit to the periodic table, but the correct way to incorporate such states into multi-electron calculations is still an open question needing to be solved to calculate the periodic table's structure beyond this point.[270]
Nuclear stability will likely prove a decisive factor constraining the number of possible elements. It depends on the balance between the electric repulsion between protons and the strong force binding protons and neutrons together.[271]Protons and neutrons are arranged inshells, just like electrons, and so a closed shell can significantly increase stability: the known superheavy nuclei exist because of such a shell closure, probably at around 114–126protons and 184 neutrons.[270]They are probably close to a predictedisland of stability, where superheavy nuclides should be more long-lived than expected: predictions for the longest-lived nuclides on the island range from microseconds to millions of years.[104][272][273]It should nonetheless be noted that these are essentially extrapolations into an unknown part of the chart of nuclides, and systematic model uncertainties need to be taken into account.[104]
As the closed shells are passed, the stabilizing effect should vanish.[274]Thus, superheavy nuclides with more than 184 neutrons are expected to have much shorter lifetimes, spontaneously fissioning within 10−15seconds. If this is so, then it would not make sense to consider them chemical elements: [IUPAC/IUPAP theorizes and recommends] an element to exist only if the nucleus lives longer than 10−14seconds, the time needed for it to gather an electron cloud. Nonetheless, theoretical estimates of half-lives are very model-dependent, ranging over many orders of magnitude.[270]The extreme repulsion between protons is predicted to result in exotic nuclear topologies, with bubbles, rings, and tori expected: this further complicates extrapolation.[104]It is not clear if any further-out shell closures exist, due to an expected smearing out of distinct nuclear shells (as is already expected for the electron shells at oganesson).[274]Furthermore, even if later shell closures exist, it is not clear if they would allow such heavy elements to exist.[275][276][277][178]As such, it may be that the periodic table practically ends around element 120, as elements become too short-lived to observe, and then too short-lived to have chemistry; the era of discovering new elements would thus be close to its end.[178][278]If another proton shell closure beyond 126 does exist, then it probably occurs around 164;[275]thus the region where periodicity fails more or less matches the region of instability between the shell closures.[128]
Alternatively,quark mattermay become stable at high mass numbers, in which the nucleus is composed of freely flowingupanddown quarksinstead of binding them into protons and neutrons; this would create acontinent of stabilityinstead of an island.[279][280]Other effects may come into play: for example, in very heavy elements the 1s electrons are likely to spend a significant amount of time so close to the nucleus that they are actually inside it, which would make them vulnerable toelectron capture.[281]
Even if eighth-row elements can exist, producing them is likely to be difficult, and it should become even more difficult as atomic number rises.[282]Although the 8s elements 119 and 120 are expected to be reachable with present means, the elements beyond that are expected to require new technology,[283]if they can be produced at all.[284]Experimentally characterizing these elements chemically would also pose a great challenge.[258]
The periodic law may be represented in multiple ways, of which the standard periodic table is only one.[285]Within 100 years of the appearance of Mendeleev's table in 1869,Edward G. Mazurshad collected an estimated 700 different published versions of the periodic table.[181][286]Many forms retain the rectangular structure, includingCharles Janet's left-step periodic table (pictured below), and the modernised form of Mendeleev's original 8-column layout that is still common in Russia. Other periodic table formats have been shaped much more exotically, such as spirals (Otto Theodor Benfey's pictured to the right), circles and triangles.[287]
Alternative periodic tables are often developed to highlight or emphasize chemical or physical properties of the elements that are not as apparent in traditional periodic tables, with different ones skewed more towards emphasizing chemistry or physics at either end.[288]The standard form, which remains by far the most common, is somewhere in the middle.[288]
The many different forms of the periodic table have prompted the questions of whether there is an optimal or definitive form of the periodic table, and if so, what it might be. There are no current consensus answers to either question.[289][288]Janet's left-step table is being increasingly discussed as a candidate for being the optimal or most fundamental form; Scerri has written in support of it, as it clarifies helium's nature as an s-block element, increases regularity by having all period lengths repeated, faithfully follows Madelung's rule by making each period correspond to one value ofn+ℓ,[g]and regularises atomic number triads and the first-row anomaly trend. While he notes that its placement of helium atop the alkaline earth metals can be seen a disadvantage from a chemical perspective, he counters this by appealing to the first-row anomaly, pointing out that the periodic table "fundamentally reduces to quantum mechanics", and that it is concerned with "abstract elements" and hence atomic properties rather than macroscopic properties.[295]
Achemical bondis the association ofatomsorionsto formmolecules,crystals, and other structures. The bond may result from theelectrostatic forcebetween oppositely charged ions as inionic bondsor through the sharing of electrons as incovalent bonds, or some combination of these effects. Chemical bonds are described as having different strengths: there are "strong bonds" or "primary bonds" such ascovalent,ionicandmetallicbonds, and "weak bonds" or "secondary bonds" such asdipole–dipole interactions, theLondon dispersion force, andhydrogen bonding.
Since oppositeelectric chargesattract, the negatively chargedelectronssurrounding the nucleus and the positively chargedprotonswithin anucleusattract each other. Electrons shared between twonucleiwill be attracted to both of them. "Constructivequantum mechanicalwavefunctioninterference"[1]stabilizes the paired nuclei (seeTheories of chemical bonding). Bonded nuclei maintain an optimal distance (the bond distance) balancing attractive and repulsive effects explained quantitatively byquantum theory.[2][3]
The atoms inmolecules,crystals,metalsand other forms of matter are held together by chemical bonds, which determine the structure and properties of matter.
All bonds can be described byquantum theory, but, in practice, simplified rules and other theories allow chemists to predict the strength, directionality, and polarity of bonds.[4]Theoctet ruleandVSEPR theoryare examples. More sophisticated theories arevalence bond theory, which includesorbital hybridization[5]andresonance,[6]andmolecular orbital theory[7]which includes thelinear combination of atomic orbitalsandligand field theory.Electrostaticsare used to describe bond polarities and the effects they have on chemical substances.
A chemical bond is an attraction between atoms. This attraction may be seen as the result of different behaviors of the outermost orvalence electronsof atoms. These behaviors merge into each other seamlessly in various circumstances, so that there is no clear line to be drawn between them. However it remains useful and customary to differentiate between different types of bond, which result in different properties ofcondensed matter.
In the simplest view of acovalent bond, one or more electrons (often a pair of electrons) are drawn into the space between the two atomic nuclei. Energy is released by bond formation.[8]This is not as a result of reduction in potential energy, because the attraction of the two electrons to the two protons is offset by the electron-electron and proton-proton repulsions. Instead, the release of energy (and hence stability of the bond) arises from the reduction in kinetic energy due to the electrons being in a more spatially distributed (i.e. longerde Broglie wavelength) orbital compared with each electron being confined closer to its respective nucleus.[9]These bonds exist between two particular identifiable atoms and have a direction in space, allowing them to be shown as single connecting lines between atoms in drawings, or modeled as sticks between spheres in models.
In apolar covalent bond, one or more electrons are unequally shared between two nuclei. Covalent bonds often result in the formation of small collections of better-connected atoms calledmolecules, which in solids and liquids are bound to other molecules by forces that are often much weaker than the covalent bonds that hold the molecules internally together. Such weak intermolecular bonds give organic molecular substances, such as waxes and oils, their soft bulk character, and their low melting points (in liquids, molecules must cease most structured or oriented contact with each other). When covalent bonds link long chains of atoms in large molecules, however (as in polymers such asnylon), or when covalent bonds extend in networks through solids that are not composed of discrete molecules (such asdiamondorquartzor thesilicate mineralsin many types of rock) then the structures that result may be both strong and tough, at least in the direction oriented correctly with networks of covalent bonds.[10]Also, the melting points of such covalent polymers and networks increase greatly.
In a simplified view of anionicbond, the bonding electron is not shared at all, but transferred. In this type of bond, the outeratomic orbitalof one atom has a vacancy which allows the addition of one or more electrons. These newly added electrons potentially occupy a lower energy-state (effectively closer to more nuclear charge) than they experience in a different atom. Thus, one nucleus offers a more tightly bound position to an electron than does another nucleus, with the result that one atom may transfer an electron to the other. This transfer causes one atom to assume a net positive charge, and the other to assume a net negative charge. Thebondthen results from electrostatic attraction between the positive and negatively chargedions. Ionic bonds may be seen as extreme examples of polarization in covalent bonds. Often, such bonds have no particular orientation in space, since they result from equal electrostatic attraction of each ion to all ions around them. Ionic bonds are strong (and thus ionic substances require high temperatures to melt) but also brittle, since the forces between ions are short-range and do not easily bridge cracks and fractures. This type of bond gives rise to the physical characteristics of crystals of classic mineral salts, such as table salt.
A less often mentioned type of bonding ismetallicbonding. In this type of bonding, each atom in a metal donates one or more electrons to a "sea" of electrons that reside between many metal atoms. In this sea, each electron is free (by virtue of itswave nature) to be associated with a great many atoms at once. The bond results because the metal atoms become somewhat positively charged due to loss of their electrons while the electrons remain attracted to many atoms, without being part of any given atom. Metallic bonding may be seen as an extreme example ofdelocalizationof electrons over a large system of covalent bonds, in which every atom participates. This type of bonding is often very strong (resulting in thetensile strengthof metals). However, metallic bonding is more collective in nature than other types, and so they allow metal crystals to more easily deform, because they are composed of atoms attracted to each other, but not in any particularly-oriented ways. This results in the malleability of metals. The cloud of electrons in metallic bonding causes the characteristically good electrical and thermal conductivity of metals, and also their shinylustrethat reflects most frequencies of white light.
Early speculations about the nature of thechemical bond, from as early as the 12th century, supposed that certain types ofchemical specieswere joined by a type ofchemical affinity. In 1704,Sir Isaac Newtonfamously outlined his atomic bonding theory, in "Query 31" of hisOpticks, wherebyatomsattach to each other by some "force". Specifically, after acknowledging the various popular theories in vogue at the time, of how atoms were reasoned to attach to each other, i.e. "hooked atoms", "glued together by rest", or "stuck together by conspiring motions", Newton states that he would rather infer from their cohesion, that "particles attract one another by someforce, which in immediate contact is exceedingly strong, at small distances performs the chemical operations, and reaches not far from the particles with any sensible effect."
In 1819, on the heels of the invention of thevoltaic pile,Jöns Jakob Berzeliusdeveloped a theory of chemical combination stressing the electronegative and electropositive characters of the combining atoms. By the mid 19th century,Edward Frankland,F.A. Kekulé, A.S. Couper,Alexander Butlerov, andHermann Kolbe, building on thetheory of radicals, developed thetheory of valency, originally called "combining power", in which compounds were joined owing to an attraction of positive and negative poles. In 1904,Richard Abeggproposedhis rulethat the difference between the maximum and minimum valencies of an element is often eight. At this point, valency was still an empirical number based only on chemical properties.
However the nature of the atom became clearer withErnest Rutherford's 1911 discovery that of anatomic nucleussurrounded by electrons in which he quoted Nagaoka rejected Thomson's model on the grounds that opposite charges are impenetrable. In 1904, Nagaoka proposed an alternativeplanetary modelof theatomin which a positively charged center is surrounded by a number of revolving electrons, in the manner of Saturn and its rings.[11]
Rutherford mentions Nagaoka's model in his 1911 paper in which theatomic nucleusis proposed.[12]
At the 1911 Solvay Conference, in the discussion of what could regulate energy differences between atoms, Max Planck stated: "The intermediaries could be the electrons."[13]These nuclear models suggested that electrons determine chemical behavior.
Next cameNiels Bohr's1913 modelof a nuclear atom with electron orbits. In 1916, chemistGilbert N. Lewisdeveloped the concept ofelectron-pair bonds, in which two atoms may share one to six electrons, thus forming thesingle electron bond, asingle bond, adouble bond, or atriple bond; in Lewis's own words, "An electron may form a part of the shell of two different atoms and cannot be said to belong to either one exclusively."[14]
Also in 1916,Walther Kosselput forward a theory similar to Lewis' only his model assumed complete transfers of electrons between atoms, and was thus a model ofionic bonding. Both Lewis and Kossel structured their bonding models on that ofAbegg's rule(1904).
Niels Bohralso proposeda model of the chemical bondin 1913. According to his model for adiatomic molecule, the electrons of the atoms of the molecule form a rotating ring whose plane is perpendicular to the axis of the molecule and equidistant from the atomic nuclei. Thedynamic equilibriumof the molecular system is achieved through the balance of forces between the forces of attraction of nuclei to the plane of the ring of electrons and the forces of mutual repulsion of the nuclei. The Bohr model of the chemical bond took into account theCoulomb repulsion– the electrons in the ring are at the maximum distance from each other.[15][16]
In 1927, the first mathematically complete quantum description of a simple chemical bond, i.e. that produced by one electron in the hydrogen molecular ion,H2+, was derived by the Danish physicistØyvind Burrau.[17]This work showed that the quantum approach to chemical bonds could be fundamentally and quantitatively correct, but the mathematical methods used could not be extended to molecules containing more than one electron. A more practical, albeit less quantitative, approach was put forward in the same year byWalter HeitlerandFritz London. The Heitler–London method forms the basis of what is now calledvalence bond theory.[18]In 1929, thelinear combination of atomic orbitals molecular orbital method(LCAO) approximation was introduced by SirJohn Lennard-Jones, who also suggested methods to derive electronic structures of molecules of F2(fluorine) and O2(oxygen) molecules, from basic quantum principles. Thismolecular orbitaltheory represented a covalent bond as an orbital formed by combining the quantum mechanicalSchrödingeratomic orbitals which had been hypothesized for electrons in single atoms. The equations for bonding electrons in multi-electron atoms could not be solved to mathematical perfection (i.e.,analytically), but approximations for them still gave many good qualitative predictions and results. Most quantitative calculations in modernquantum chemistryuse either valence bond or molecular orbital theory as a starting point, although a third approach,density functional theory, has become increasingly popular in recent years.
In 1933, H. H. James and A. S. Coolidge carried out a calculation on the dihydrogen molecule that, unlike all previous calculation which used functions only of the distance of the electron from the atomic nucleus, used functions which also explicitly added the distance between the two electrons.[19]With up to 13 adjustable parameters they obtained a result very close to the experimental result for the dissociation energy. Later extensions have used up to 54 parameters and gave excellent agreement with experiments. This calculation convinced the scientific community that quantum theory could give agreement with experiment. However this approach has none of the physical pictures of the valence bond and molecular orbital theories and is difficult to extend to larger molecules.
Because atoms and molecules are three-dimensional, it is difficult to use a single method to indicate orbitals and bonds. Inmolecular formulasthe chemical bonds (binding orbitals) between atoms are indicated in different ways depending on the type of discussion. Sometimes, some details are neglected. For example, inorganic chemistryone is sometimes concerned only with thefunctional groupof the molecule. Thus, the molecular formula ofethanolmay be written inconformationalform, three-dimensional form, full two-dimensional form (indicating every bond with no three-dimensional directions), compressed two-dimensional form (CH3–CH2–OH), by separating the functional group from another part of the molecule (C2H5OH), or by its atomic constituents (C2H6O), according to what is discussed. Sometimes, even the non-bonding valence shell electrons (with the two-dimensional approximate directions) are marked, e.g. for elemental carbon.'C'. Some chemists may also mark the respective orbitals, e.g. the hypothetical ethene−4anion (\/C=C/\−4) indicating the possibility of bond formation.
Strong chemical bonds are theintramolecularforces that hold atoms together inmolecules. A strong chemical bond is formed from the transfer or sharing ofelectronsbetween atomic centers and relies on theelectrostatic attractionbetween the protons in nuclei and the electrons in the orbitals.
The types of strong bond differ due to the difference inelectronegativityof the constituent elements. Electronegativity is the tendency for anatomof a givenchemical elementto attract shared electrons when forming a chemical bond, where the higher the associated electronegativity then the more it attracts electrons. Electronegativity serves as a simple way to quantitatively estimate thebond energy, which characterizes a bond along the continuous scale fromcovalenttoionic bonding. A large difference in electronegativity leads to more polar (ionic) character in the bond.
Ionic bonding is a type of electrostatic interaction between atoms that have a large electronegativity difference. There is no precise value that distinguishes ionic from covalent bonding, but an electronegativity difference of over 1.7 is likely to be ionic while a difference of less than 1.7 is likely to be covalent.[21]Ionic bonding leads to separate positive and negativeions. Ionic charges are commonly between −3eto +3e. Ionic bonding commonly occurs inmetal saltssuch assodium chloride(table salt). A typical feature of ionic bonds is that the species form into ionic crystals, in which no ion is specifically paired with any single other ion in a specific directional bond. Rather, each species of ion is surrounded by ions of the opposite charge, and the spacing between it and each of the oppositely charged ions near it is the same for all surrounding atoms of the same type. It is thus no longer possible to associate an ion with any specific other single ionized atom near it. This is a situation unlike that in covalent crystals, where covalent bonds between specific atoms are still discernible from the shorter distances between them, as measured via such techniques asX-ray diffraction.
Ionic crystals may contain a mixture of covalent and ionic species, as for example salts of complex acids such assodium cyanide, NaCN. X-ray diffraction shows that in NaCN, for example, the bonds between sodiumcations(Na+) and the cyanideanions(CN−) areionic, with nosodiumion associated with any particularcyanide. However, the bonds between thecarbon(C) andnitrogen(N) atoms in cyanide are of thecovalenttype, so that each carbon is strongly bound tojust onenitrogen, to which it is physically much closer than it is to other carbons or nitrogens in a sodium cyanide crystal.
When such crystals are melted into liquids, the ionic bonds are broken first because they are non-directional and allow the charged species to move freely. Similarly, when such salts dissolve into water, the ionic bonds are typically broken by the interaction with water but the covalent bonds continue to hold. For example, in solution, the cyanide ions, still bound together as single CN−ions, move independently through the solution, as do sodium ions, as Na+. In water, charged ions move apart because each of them are more strongly attracted to a number of water molecules than to each other. The attraction between ions and water molecules in such solutions is due to a type of weakdipole-dipoletype chemical bond. In melted ionic compounds, the ions continue to be attracted to each other, but not in any ordered or crystalline way.
Covalent bonding is a common type of bonding in which two or more atoms sharevalence electronsmore or less equally. The simplest and most common type is asingle bondin which two atoms share two electrons. Other types include thedouble bond, thetriple bond,one- and three-electron bonds, thethree-center two-electron bondandthree-center four-electron bond.
In non-polar covalent bonds, the electronegativity difference between the bonded atoms is small, typically 0 to 0.3. Bonds within mostorganic compoundsare described as covalent. The figure shows methane (CH4), in which each hydrogen forms a covalent bond with the carbon. Seesigma bondsandpi bondsfor LCAO descriptions of such bonding.[22]
Molecules that are formed primarily from non-polar covalent bonds are oftenimmisciblein water or otherpolar solvents, but much more soluble innon-polar solventssuch ashexane.
Apolar covalent bondis a covalent bond with a significantionic character. This means that the two shared electrons are closer to one of the atoms than the other, creating an imbalance of charge. Such bonds occur between two atoms with moderately different electronegativities and give rise todipole–dipole interactions. The electronegativity difference between the two atoms in these bonds is 0.3 to 1.7.
Asingle bondbetween two atoms corresponds to the sharing of one pair of electrons. The Hydrogen (H) atom has one valence electron. Two Hydrogen atoms can then form a molecule, held together by the shared pair of electrons. Each H atom now has the noble gas electron configuration of helium (He). The pair of shared electrons forms a single covalent bond. The electron density of these two bonding electrons in the region between the two atoms increases from the density of two non-interacting H atoms.
Adouble bondhas two shared pairs of electrons, one in a sigma bond and one in api bondwith electron density concentrated on two opposite sides of the internuclear axis. Atriple bondconsists of three shared electron pairs, forming one sigma and two pi bonds. An example is nitrogen.Quadrupleand higher bonds are very rare and occur only between certaintransition metalatoms.
Acoordinate covalent bondis a covalent bond in which the two shared bonding electrons are from the same one of the atoms involved in the bond. For example,boron trifluoride(BF3) andammonia(NH3) form anadductorcoordination complexF3B←NH3with a B–N bond in which alone pairof electrons on N is shared with an empty atomic orbital on B. BF3with an empty orbital is described as an electron pair acceptor orLewis acid, while NH3with a lone pair that can be shared is described as an electron-pair donor orLewis base. The electrons are shared roughly equally between the atoms in contrast to ionic bonding. Such bonding is shown by an arrow pointing to the Lewis acid. (In the Figure, solid lines are bonds in the plane of the diagram,wedged bondspoint towards the observer, and dashed bonds point away from the observer.)
Transition metal complexesare generally bound by coordinate covalent bonds. For example, the ion Ag+reacts as a Lewis acid with two molecules of the Lewis base NH3to form the complex ion Ag(NH3)2+, which has two Ag←N coordinate covalent bonds.
In metallic bonding, bonding electrons are delocalized over a lattice of atoms. By contrast, in ionic compounds, the locations of the binding electrons and their charges are static. The free movement or delocalization of bonding electrons leads to classical metallic properties such asluster(surface lightreflectivity),electricalandthermal conductivity,ductility, and hightensile strength.
There are several types of weak bonds that can be formed between two or more molecules which are not covalently bound.Intermolecular forcescause molecules to attract or repel each other. Often, these forces influence physical characteristics (such as themelting point) of a substance.
Van der Waals forcesare interactions betweenclosed-shellmolecules. They include both Coulombic interactions between partial charges inpolarmolecules, andPauli repulsionsbetween closed electrons shells.[23]: 696
Keesom forcesare the forces between the permanentdipolesof two polar molecules.[23]: 701London dispersion forcesare the forces between induced dipoles of different molecules.[23]: 703There can also be an interaction between a permanent dipole in one molecule and an induced dipole in another molecule.[23]: 702
Hydrogen bondsof the form A--H•••B occur when A and B are two highly electronegative atoms (usually N, O or F) such that A forms a highly polar covalent bond with H so that H has a partial positive charge, and B has alone pairof electrons which is attracted to this partial positive charge and forms a hydrogen bond.[23]: 702Hydrogen bonds are responsible for the high boiling points of water andammoniawith respect to their heavier analogues. In some cases a similarhalogen bondcan be formed by a halogen atom located between two electronegative atoms on different molecules.
At short distances, repulsive forces between atoms also become important.[23]: 705-6
In the (unrealistic) limit of "pure"ionic bonding, electrons are perfectly localized on one of the two atoms in the bond. Such bonds can be understood byclassical physics. The force between the atoms depends onisotropiccontinuum electrostatic potentials. The magnitude of the force is in simple proportion to the product of the two ionic charges according toCoulomb's law.[citation needed]
Covalent bonds are better understood byvalence bond (VB) theoryormolecular orbital (MO) theory. The properties of the atoms involved can be understood using concepts such asoxidation number,formal charge, andelectronegativity. The electron density within a bond is not assigned to individual atoms, but is instead delocalized between atoms. In valence bond theory, bonding is conceptualized as being built up from electron pairs that are localized and shared by two atoms via the overlap of atomic orbitals. The concepts oforbital hybridizationandresonanceaugment this basic notion of the electron pair bond. In molecular orbital theory, bonding is viewed as being delocalized and apportioned in orbitals that extend throughout the molecule and are adapted to its symmetry properties, typically by consideringlinear combinations of atomic orbitals(LCAO). Valence bond theory is more chemically intuitive by being spatially localized, allowing attention to be focused on the parts of the molecule undergoing chemical change. In contrast, molecular orbitals are more "natural" from a quantum mechanical point of view, with orbital energies being physically significant and directly linked to experimental ionization energies fromphotoelectron spectroscopy. Consequently, valence bond theory and molecular orbital theory are often viewed as competing but complementary frameworks that offer different insights into chemical systems. As approaches for electronic structure theory, both MO and VB methods can give approximations to any desired level of accuracy, at least in principle. However, at lower levels, the approximations differ, and one approach may be better suited for computations involving a particular system or property than the other.[citation needed]
Unlike the spherically symmetrical Coulombic forces in pure ionic bonds, covalent bonds are generally directed andanisotropic. These are often classified based on their symmetry with respect to a molecular plane assigma bondsandpi bonds. In the general case, atoms form bonds that are intermediate between ionic and covalent, depending on the relativeelectronegativityof the atoms involved. Bonds of this type are known aspolar covalent bonds.[24]
Plate tectonics(fromLatintectonicus, fromAncient Greekτεκτονικός(tektonikós)'pertaining to building')[1]is thescientific theorythat theEarth'slithospherecomprises a number of largetectonic plates, which have been slowly moving since 3–4 billion years ago.[2][3][4]The model builds on the concept ofcontinental drift, an idea developed during the first decades of the 20th century. Plate tectonics came to be accepted bygeoscientistsafterseafloor spreadingwas validated in the mid-to-late 1960s. The processes that result in plates and shapeEarth's crustare calledtectonics.
Tectonic plates also occur in other planets and moons.
Earth's lithosphere, the rigid outer shell of the planet including thecrustandupper mantle, is fractured into seven or eight major plates (depending on how they are defined) and many minor plates or "platelets". Where the plates meet, their relative motion determines the type ofplate boundary(orfault):convergent,divergent, ortransform. The relative movement of the plates typically ranges from zero to 10 cm annually.[5]Faults tend to be geologically active, experiencingearthquakes,volcanic activity,mountain-building, andoceanic trenchformation.
Tectonic plates are composed of the oceanic lithosphere and the thicker continental lithosphere, each topped by its own kind of crust. Alongconvergent plate boundaries, the process ofsubductioncarries the edge of one plate down under the other plate and into themantle. This process reduces the total surface area (crust) of the Earth. The lost surface is balanced by the formation of newoceanic crustalong divergent margins by seafloor spreading, keeping the totalsurface areaconstant in a tectonic "conveyor belt".
Tectonic plates are relativelyrigidand float across the ductileasthenospherebeneath. Lateral density variations in the mantle result inconvectioncurrents, the slow creeping motion of Earth's solid mantle. At a seafloorspreading ridge, plates move away from the ridge, which is atopographichigh, and the newly formed crust cools as it moves away, increasing itsdensityand contributing to the motion. At asubductionzone, the relatively cold, dense oceanic crust sinks down into the mantle, forming the downward convecting limb of amantle cell,[6]which is the strongest driver of plate motion.[7][8]The relative importance and interaction of other proposed factors such as active convection, upwelling inside the mantle, and tidal drag of the Moon is still the subject of debate.
Theouter layers of Earthare divided into thelithosphereandasthenosphere. The division is based on differences inmechanical propertiesand in the method forthe transfer of heat. The lithosphere is cooler and more rigid, while the asthenosphere is hotter and flows more easily. In terms of heat transfer, the lithosphere loses heat byconduction, whereas the asthenosphere also transfers heat byconvectionand has a nearlyadiabatictemperature gradient. This division should not be confused with thechemicalsubdivision of these same layers into the mantle (comprising both the asthenosphere and the mantle portion of the lithosphere) and the crust: a given piece of mantle may be part of the lithosphere or the asthenosphere at different times depending on its temperature and pressure.
The key principle of plate tectonics is that the lithosphere exists as separate and distincttectonic plates, which ride on theviscoelasticasthenosphere. Plate motions range from 10 to 40 millimetres per year (0.4 to 1.6 in/year) at theMid-Atlantic Ridge(about as fast asfingernailsgrow), to about 160 millimetres per year (6.3 in/year) for theNazca plate(about as fast ashairgrows).[9][10]
Tectonic lithospheric plates consist of lithospheric mantle overlain by one or two types of crustal material:oceanic crust(in older texts calledsimafromsiliconandmagnesium) andcontinental crust(sialfrom silicon andaluminium). The distinction between oceanic crust and continental crust is based on their modes of formation. Oceanic crust is formed at sea-floor spreading centers. Continental crust is formed througharc volcanismandaccretionofterranesthrough plate tectonic processes. Oceanic crust is denser than continental crust because it has less silicon and more of theheavier elementsthancontinental crust.[11][12]As a result of this density difference, oceanic crust generally lies belowsea level, while continental crustbuoyantly projectsabove sea level.
Average oceanic lithosphere is typically 100 km (62 mi) thick.[13]Its thickness is a function of its age. As time passes, it cools by conducting heat from below, and releasing it radiatively into space. The adjacent mantle below is cooled by this process and added to its base. Because it is formed at mid-ocean ridges and spreads outwards, its thickness is therefore a function of its distance from the mid-ocean ridge where it was formed. For a typical distance that oceanic lithosphere must travel before being subducted, the thickness varies from about 6 km (4 mi) thick at mid-ocean ridges to greater than 100 km (62 mi) atsubductionzones. For shorter or longer distances, the subduction zone, and therefore also the mean, thickness becomes smaller or larger, respectively.[14]Continental lithosphere is typically about 200 km (120 mi) thick, though this varies considerably between basins, mountain ranges, and stablecratonicinteriors of continents.
The location where two plates meet is called aplate boundary. Plate boundaries are where geological events occur, such asearthquakesand the creation of topographic features such asmountains,volcanoes,mid-ocean ridges, andoceanic trenches. The vast majority of the world's active volcanoes occur along plate boundaries, with the Pacific plate'sRing of Firebeing the most active and widely known. Some volcanoes occur in the interiors of plates, and these have been variously attributed to internal plate deformation[15]and to mantle plumes.
Tectonic plates may include continental crust or oceanic crust, or both. For example, theAfrican plateincludes the continent and parts of the floor of theAtlanticandIndianOceans.
Some pieces of oceanic crust, known asophiolites, failed to be subducted under continental crust at destructive plate boundaries; instead these oceanic crustal fragments were pushed upward and were preserved within continental crust.
Three types of plate boundaries exist,[16]characterized by the way the plates move relative to each other. They are associated with different types of surface phenomena. The different types of plate boundaries are:[17][18]
Tectonic plates are able to move because of the relative density ofoceanic lithosphereand the relative weakness of theasthenosphere.Dissipation of heat from the mantleis the original source of the energy required to drive plate tectonics through convection or large scale upwelling and doming. As a consequence, a powerful source generating plate motion is the excess density of the oceanic lithosphere sinking in subduction zones. When the new crust forms at mid-ocean ridges, this oceanic lithosphere is initially less dense than the underlying asthenosphere, but it becomes denser with age as it conductively cools and thickens. The greaterdensityof old lithosphere relative to the underlying asthenosphere allows it to sink into the deep mantle at subduction zones, providing most of the driving force for plate movement. The weakness of the asthenosphere allows the tectonic plates to move easily towards a subduction zone.[20]
For much of the first quarter of the 20th century, the leading theory of the driving force behind tectonic plate motions envisaged large scale convection currents in the upper mantle, which can be transmitted through the asthenosphere. This theory was launched byArthur Holmesand some forerunners in the 1930s[21]and was immediately recognized as the solution for the acceptance of the theory as originally discussed in the papers ofAlfred Wegenerin the early years of the 20th century. However, despite its acceptance, it was long debated in the scientific community because the leading theory still envisaged a static Earth without moving continents up until the major breakthroughs of the early sixties.
Two- and three-dimensional imaging of Earth's interior (seismic tomography) shows a varying lateral density distribution throughout the mantle. Such density variations can be material (from rock chemistry), mineral (from variations in mineral structures), or thermal (through thermal expansion and contraction from heat energy). The manifestation of this varying lateral density ismantle convectionfrom buoyancy forces.[22]
How mantle convection directly and indirectly relates to plate motion is a matter of ongoing study and discussion ingeodynamics. Somehow, thisenergymust be transferred to the lithosphere for tectonic plates to move. There are essentially two main types of mechanisms that are thought to exist related to the dynamics of the mantle that influence plate motion which are primary (through the large scale convection cells) or secondary. The secondary mechanisms view plate motion driven by friction between the convection currents in the asthenosphere and the more rigid overlying lithosphere. This is due to the inflow of mantle material related to the downward pull on plates in subduction zones at ocean trenches. Slab pull may occur in a geodynamic setting where basal tractions continue to act on the plate as it dives into the mantle (although perhaps to a greater extent acting on both the under and upper side of the slab). Furthermore, slabs that are broken off and sink into the mantle can cause viscous mantle forces driving plates through slab suction.
In the theory ofplume tectonicsfollowed by numerous researchers during the 1990s, a modified concept of mantle convection currents is used. It asserts that super plumes rise from the deeper mantle and are the drivers or substitutes of the major convection cells. These ideas find their roots in the early 1930s in the works ofBeloussovandvan Bemmelen, which were initially opposed to plate tectonics and placed the mechanism in a fixed frame of vertical movements. Van Bemmelen later modified the concept in his "Undation Models" and used "Mantle Blisters" as the driving force for horizontal movements, invoking gravitational forces away from the regional crustal doming.[23][24]
The theories find resonance in the modern theories which envisagehot spotsormantle plumeswhich remain fixed and are overridden by oceanic and continental lithosphere plates over time and leave their traces in the geological record (though these phenomena are not invoked as real driving mechanisms, but rather as modulators).
The mechanism is still advocated to explain the break-up of supercontinents during specific geological epochs.[25]It has followers amongst the scientists involved in thetheory of Earth expansion.[26][27][28]
Another theory is that the mantle flows neither in cells nor large plumes but rather as a series of channels just below Earth's crust, which then provide basal friction to the lithosphere. This theory, called "surge tectonics", was popularized during the 1980s and 1990s.[29]Recent research, based on three-dimensional computer modelling, suggests that plate geometry is governed by a feedback between mantle convection patterns and the strength of the lithosphere.[30]
Forces related to gravity are invoked as secondary phenomena within the framework of a more general driving mechanism such as the various forms of mantle dynamics described above. In modern views, gravity is invoked as the major driving force, through slab pull along subduction zones.
Gravitational sliding away from a spreading ridge is one of the proposed driving forces: plate motion is driven by the higher elevation of plates at ocean ridges.[31][32]As oceanic lithosphere is formed at spreading ridges from hot mantle material, it gradually cools and thickens with age (and thus adds distance from the ridge). Cool oceanic lithosphere is significantly denser than the hot mantle material from which it is derived and so with increasing thickness it gradually subsides into the mantle to compensate the greater load. The result is a slight lateral incline with increased distance from the ridge axis.
This force is regarded as a secondary force and is often referred to as "ridge push". This is a misnomer as there is no force "pushing" horizontally, indeed tensional features are dominant along ridges. It is more accurate to refer to this mechanism as "gravitational sliding", since the topography across the whole plate can vary considerably and spreading ridges are only the most prominent feature. Other mechanisms generating this gravitational secondary force includeflexural bulgingof the lithosphere before it dives underneath an adjacent plate, producing a clear topographical feature that can offset, or at least affect, the influence of topographical ocean ridges.Mantle plumesand hot spots are also postulated to impinge on the underside of tectonic plates.
Slab pull: Scientific opinion is that the asthenosphere is insufficiently competent or rigid to directly cause motion by friction along the base of the lithosphere. Slab pull is therefore most widely thought to be the greatest force acting on the plates. In this understanding, plate motion is mostly driven by the weight of cold, dense plates sinking into the mantle at trenches.[8]Recent models indicate thattrench suctionplays an important role as well. However, the fact that theNorth American plateis nowhere being subducted, although it is in motion, presents a problem. The same holds for the African,Eurasian, andAntarcticplates.
Gravitational sliding away from mantle doming: According to older theories, one of the driving mechanisms of the plates is the existence of large scale asthenosphere/mantle domes which cause the gravitational sliding of lithosphere plates away from them (see the paragraph on Mantle Mechanisms). This gravitational sliding represents a secondary phenomenon of this basically vertically oriented mechanism. It finds its roots in the Undation Model ofvan Bemmelen. This can act on various scales, from the small scale of one island arc up to the larger scale of an entire ocean basin.[31][32][25]
Alfred Wegener, being ameteorologist, had proposedtidal forcesandcentrifugal forcesas the main driving mechanisms behindcontinental drift; however, these forces were considered far too small to cause continental motion as the concept was of continents plowing through oceanic crust.[33]Therefore, Wegener later changed his position and asserted that convection currents are the main driving force of plate tectonics in the last edition of his book in 1929.
However, in the plate tectonics context (accepted since theseafloor spreadingproposals of Heezen, Hess, Dietz, Morley, Vine, and Matthews (see below) during the early 1960s), the oceanic crust is suggested to be in motionwiththe continents, which caused the proposals related to Earth rotation to be reconsidered. In more recent literature, these driving forces are:
Forces that are small and generally negligible are:
For these mechanisms to be overall valid, systematic relationships should exist all over the globe between the orientation and kinematics of deformation and the geographicallatitudinalandlongitudinalgrid of Earth itself. Systematic relations studies in the second half of the nineteenth century and the first half of the twentieth century underlined exactly the opposite: that the plates had not moved in time, that the deformation grid was fixed with respect to Earth'sequatorand axis, and that gravitational driving forces were generally acting vertically and caused only local horizontal movements (the so-called pre-plate tectonic, "fixist theories"). Later studies (discussed below on this page), therefore, invoked many of the relationships recognized during this pre-plate tectonics period to support their theories (see reviews of these various mechanisms related to Earth rotation in the work of van Dijk and collaborators).[37]
Of the many forces discussed above, tidal force is still highly debated and defended as a possible principal driving force of plate tectonics. The other forces are only used in global geodynamic models not using plate tectonics concepts (therefore beyond the discussions treated in this section) or proposed as minor modulations within the overall plate tectonics model.
In 1973, George W. Moore[38]of theUSGSand R. C. Bostrom[39]presented evidence for a general westward drift of Earth's lithosphere with respect to the mantle, based on the steepness of the subduction zones (shallow dipping towards the east, steeply dipping towards the west). They concluded that tidal forces (the tidal lag or "friction") caused by Earth's rotation and the forces acting upon it by the Moon are a driving force for plate tectonics. As Earth spins eastward beneath the Moon, the Moon's gravity ever so slightly pulls Earth's surface layer back westward, just as proposed by Alfred Wegener (see above). Since 1990 this theory has been mainly advocated by Doglioni and co-workers (Doglioni 1990), such as in a more recent 2006 study,[40]where scientists reviewed and advocated these ideas. It has been suggested inLovett (2006)that this observation may also explain whyVenusandMarshave no plate tectonics, as Venus has no moon and Mars' moons are too small to have significant tidal effects on the planet. In a paper by Torsvik et al.,[41]it was suggested that, on the other hand, it can easily be observed that many plates are moving north and eastward, and that the dominantly westward motion of the Pacific Ocean basins derives simply from the eastward bias of the Pacific spreading center (which is not a predicted manifestation of such lunar forces). In the same paper the authors admit, however, that relative to the lower mantle, there is a slight westward component in the motions of all the plates. They demonstrated though that the westward drift, seen only for the past 30 Ma, is attributed to the increased dominance of the steadily growing and accelerating Pacific plate. The debate is still open, and a 2022 paper by Hofmeister et al.[42]revived the idea of the interaction between the Earth's rotation and the Moon as the main driving force for plate movement.
The role of water has been proposed to be crucial in plate tectonics on Earth.[43][44][45]
Thevectorof a plate's motion is a function of all the forces acting on the plate; however, therein lies the problem regarding the degree to which each process contributes to the overall motion of each tectonic plate.
The diversity of geodynamic settings and the properties of each plate result from the impact of the various processes actively driving each individual plate. One method of dealing with this problem is to consider the relative rate at which each plate is moving as well as the evidence related to the significance of each process to the overall driving force on the plate.
One of the most significant correlations discovered to date is that lithospheric plates attached to downgoing (subducting) plates move much faster than other types of plates. The Pacific plate, for instance, is essentially surrounded by zones of subduction (the so-called Ring of Fire) and moves much faster than the plates of the Atlantic basin, which are attached (perhaps one could say 'welded') to adjacent continents instead of subducting plates. It is thus thought that forces associated with the downgoing plate (slab pull and slab suction) are the driving forces which determine the motion of plates, except for those plates which are not being subducted.[8]This view however has been contradicted by a recent study which found that the actual motions of the Pacific plate and other plates associated with the East Pacific Rise do not correlate mainly with either slab pull or slab push, but rather with a mantle convection upwelling whose horizontal spreading along the bases of the various plates drives them along via viscosity-related traction forces.[46]The driving forces of plate motion continue to be active subjects of on-going research withingeophysicsandtectonophysics.
The development of the theory of plate tectonics was the scientific and cultural change which occurred during a period of 50 years of scientific debate. The event of the acceptance itself was aparadigm shiftand can therefore be classified as a scientific revolution,[47]now described as thePlate Tectonics Revolution.
Around the start of the twentieth century, various theorists unsuccessfully attempted to explain the many geographical, geological, and biological continuities between continents. In 1912, the meteorologistAlfred Wegenerdescribed what he called continental drift, an idea that culminated fifty years later in the modern theory of plate tectonics.[48]
Wegener expanded his theory in his 1915 bookThe Origin of Continents and Oceans.[49]Starting from the idea (also expressed by his forerunners) that the present continents once formed a single land mass (later calledPangaea), Wegener suggested that these separated and drifted apart, likening them to "icebergs" of low densitysialfloating on a sea of densersima.[50][51]Supporting evidence for the idea came from the dove-tailing outlines of South America's east coast and Africa's west coastAntonio Snider-Pellegrinihad drawn on his maps, and from the matching of the rock formations along these edges. Confirmation of their previous contiguous nature also came from the fossil plantsGlossopterisandGangamopteris, and thetherapsidormammal-like reptileLystrosaurus, all widely distributed over South America, Africa, Antarctica, India, and Australia. The evidence for such an erstwhile joining of these continents was patent to field geologists working in the southern hemisphere. The South AfricanAlex du Toitput together a mass of such information in his 1937 publicationOur Wandering Continents, and went further than Wegener in recognising the strong links between theGondwanafragments.
Wegener's work was initially not widely accepted, in part due to a lack of detailed evidence but mostly because of the lack of a reasonable physically supported mechanism. Earth might have a solid crust and mantle and a liquid core, but there seemed to be no way that portions of the crust could move around. Many distinguished scientists of the time, such asHarold JeffreysandCharles Schuchert, were outspoken critics of continental drift.
Despite much opposition, the view of continental drift gained support and a lively debate started between "drifters" or "mobilists" (proponents of the theory) and "fixists" (opponents). During the 1920s, 1930s and 1940s, the former reached important milestones proposing thatconvection currentsmight have driven the plate movements, and that spreading may have occurred below the sea within the oceanic crust. Concepts close to the elements of plate tectonics were proposed by geophysicists and geologists (both fixists and mobilists) like Vening-Meinesz, Holmes, and Umbgrove. In 1941,Otto Ampfererdescribed, in his publication "Thoughts on the motion picture of the Atlantic region",[52]processes that anticipatedseafloor spreadingandsubduction.[53][54]One of the first pieces of geophysical evidence that was used to support the movement of lithospheric plates came frompaleomagnetism. This is based on the fact that rocks of different ages show a variablemagnetic fielddirection, evidenced by studies since the mid–nineteenth century. The magnetic north and south poles reverse through time, and, especially important in paleotectonic studies, the relative position of the magnetic north pole varies through time. Initially, during the first half of the twentieth century, the latter phenomenon was explained by introducing what was called "polar wander" (seeapparent polar wander) (i.e., it was assumed that the north pole location had been shifting through time). An alternative explanation, though, was that the continents had moved (shifted and rotated) relative to the north pole, and each continent, in fact, shows its own "polar wander path". During the late 1950s, it was successfully shown on two occasions that these data could show the validity of continental drift: by Keith Runcorn in a paper in 1956,[55]and by Warren Carey in a symposium held in March 1956.[56]
The second piece of evidence in support of continental drift came during the late 1950s and early 60s from data on the bathymetry of the deepocean floorsand the nature of the oceanic crust such as magnetic properties and, more generally, with the development ofmarine geology[57]which gave evidence for the association of seafloor spreading along themid-oceanic ridgesandmagnetic field reversals, published between 1959 and 1963 by Heezen, Dietz, Hess, Mason, Vine & Matthews, and Morley.[58]
Simultaneous advances in earlyseismicimaging techniques in and aroundWadati–Benioff zonesalong the trenches bounding many continental margins, together with many other geophysical (e.g., gravimetric) and geological observations, showed how the oceanic crust could disappear into the mantle, providing the mechanism to balance the extension of the ocean basins with shortening along its margins.
All this evidence, both from the ocean floor and from the continental margins, made it clear around 1965 that continental drift was feasible. The theory of plate tectonics was defined in a series of papers between 1965 and 1967. The theory revolutionized the Earth sciences, explaining a diverse range of geological phenomena and their implications in other studies such aspaleogeographyandpaleobiology.
In the late 19th and early 20th centuries, geologists assumed that Earth's major features were fixed, and that most geologic features such as basin development and mountain ranges could be explained by vertical crustal movement, described in what is called thegeosynclinal theory. Generally, this was placed in the context of a contracting planet Earth due to heat loss in the course of a relatively short geological time.
It was observed as early as 1596 that the oppositecoastsof the Atlantic Ocean—or, more precisely, the edges of thecontinental shelves—have similar shapes and seem to have once fitted together.[59]
Since that time many theories were proposed to explain this apparent complementarity, but the assumption of a solid Earth made these various proposals difficult to accept.[60]
The discovery ofradioactivityand its associatedheatingproperties in 1895 prompted a re-examination of the apparentage of Earth.[61]This had previously been estimated by its cooling rate under the assumption that Earth's surface radiated like ablack body.[62]Those calculations had implied that, even if it started atred heat, Earth would have dropped to its present temperature in a few tens of millions of years. Armed with the knowledge of a new heat source, scientists realized that Earth would be much older, and thatits corewas still sufficiently hot to be liquid.
By 1915, after having published a first article in 1912,[63]Alfred Wegener was making serious arguments for the idea of continental drift in the first edition ofThe Origin of Continents and Oceans.[49]In that book (re-issued in four successive editions up to the final one in 1936), he noted how the east coast ofSouth Americaand the west coast ofAfricalooked as if they were once attached. Wegener was not the first to note this (Abraham Ortelius,Antonio Snider-Pellegrini,Eduard Suess,Roberto MantovaniandFrank Bursley Taylorpreceded him just to mention a few), but he was the first to marshal significantfossiland paleo-topographical and climatological evidence to support this simple observation (and was supported in this by researchers such asAlex du Toit). Furthermore, when the rockstrataof the margins of separate continents are very similar it suggests that these rocks were formed in the same way, implying that they were joined initially. For instance, parts ofScotlandandIrelandcontain rocks very similar to those found inNewfoundlandandNew Brunswick. Furthermore, theCaledonian Mountainsof Europe and parts of theAppalachian Mountainsof North America are very similar instructureandlithology.
However, his ideas were not taken seriously by many geologists, who pointed out that there was no apparent mechanism for continental drift. Specifically, they did not see how continental rock could plow through the much denser rock that makes up oceanic crust. Wegener could not explain the force that drove continental drift, and his vindication did not come until after his death in 1930.[64]
As it was observed early that althoughgraniteexisted on continents, seafloor seemed to be composed of denserbasalt, the prevailing concept during the first half of the twentieth century was that there were two types of crust, named "sial" (continental type crust) and "sima" (oceanic type crust). Furthermore, it was supposed that a static shell of strata was present under the continents. It therefore looked apparent that a layer of basalt (sial) underlies the continental rocks.
However, based on abnormalities inplumb line deflectionby theAndesin Peru,Pierre Bouguerhad deduced that less-dense mountains must have a downward projection into the denser layer underneath. The concept that mountains had "roots" was confirmed byGeorge B. Airya hundred years later, during study ofHimalayangravitation, and seismic studies detected corresponding density variations. Therefore, by the mid-1950s, the question remained unresolved as to whether mountain roots were clenched in surrounding basalt or were floating on it like an iceberg.
During the 20th century, improvements in and greater use of seismic instruments such asseismographsenabled scientists to learn that earthquakes tend to be concentrated in specific areas, most notably along theoceanic trenchesand spreading ridges. By the late 1920s, seismologists were beginning to identify several prominent earthquake zones parallel to the trenches that typically were inclined 40–60° from the horizontal and extended several hundred kilometers into Earth. These zones later became known as Wadati–Benioff zones, or simply Benioff zones, in honor of the seismologists who first recognized them,Kiyoo Wadatiof Japan andHugo Benioffof the United States. The study of global seismicity greatly advanced in the 1960s with the establishment of the Worldwide Standardized Seismograph Network (WWSSN)[65]to monitor the compliance of the 1963 treaty banning above-ground testing of nuclear weapons. The much improved data from the WWSSN instruments allowed seismologists to map precisely the zones of earthquake concentration worldwide.
Meanwhile, debates developed around the phenomenon of polar wander. Since the early debates of continental drift, scientists had discussed and used evidence that polar drift had occurred because continents seemed to have moved through different climatic zones during the past. Furthermore, paleomagnetic data had shown that the magnetic pole had also shifted during time. Reasoning in an opposite way, the continents might have shifted and rotated, while the pole remained relatively fixed. The first time the evidence of magnetic polar wander was used to support the movements of continents was in a paper byKeith Runcornin 1956,[55]and successive papers by him and his studentsTed Irving(who was actually the first to be convinced of the fact that paleomagnetism supported continental drift) and Ken Creer.
This was immediately followed by a symposium on continental drift inTasmaniain March 1956 organised byS. Warren Careywho had been one of the supporters and promotors of Continental Drift since the thirties[66]During this symposium, some of the participants used the evidence in the theory of anexpansion of the global crust, a theory which had been proposed by other workers decades earlier. In this hypothesis, the shifting of the continents is explained by a large increase in the size of Earth since its formation. However, although the theory still has supporters in science, this is generally regarded as unsatisfactory because there is no convincing mechanism to produce a significant expansion of Earth. Other work during the following years would soon show that the evidence was equally in support of continental drift on a globe with a stable radius.
During the 1930s up to the late 1950s, works byVening-Meinesz, Holmes,Umbgrove, and numerous others outlined concepts that were close or nearly identical to modern plate tectonics theory. In particular, the English geologistArthur Holmesproposed in 1920 that plate junctions might lie beneath thesea, and in 1928 that convection currents within the mantle might be the driving force.[67]Often, these contributions are forgotten because:
In 1947, a team of scientists led byMaurice Ewingutilizing theWoods Hole Oceanographic Institution's research vesselAtlantisand an array of instruments, confirmed the existence of a rise in the central Atlantic Ocean, and found that the floor of the seabed beneath the layer of sediments consisted of basalt, not the granite which is the main constituent of continents. They also found that the oceanic crust was much thinner than continental crust. All these new findings raised important and intriguing questions.[68]
The new data that had been collected on the ocean basins also showed particular characteristics regarding the bathymetry. One of the major outcomes of these datasets was that all along the globe, a system of mid-oceanic ridges was detected. An important conclusion was that along this system, new ocean floor was being created, which led to the concept of the "Great Global Rift". This was described in the crucial paper ofBruce Heezen(1960) based on his work withMarie Tharp,[69]which would trigger a real revolution in thinking. A profound consequence of seafloor spreading is that new crust was, and still is, being continually created along the oceanic ridges. For this reason, Heezen initially advocated the so-called "expanding Earth" hypothesis of S. Warren Carey (see above). Therefore, the question remained as to how new crust could continuously be added along the oceanic ridges without increasing the size of Earth. In reality, this question had been solved already by numerous scientists during the 1940s and the 1950s, like Arthur Holmes, Vening-Meinesz, Coates and many others: The crust in excess disappeared along what were called the oceanic trenches, where so-called "subduction" occurred. Therefore, when various scientists during the early 1960s started to reason on the data at their disposal regarding the ocean floor, the pieces of the theory quickly fell into place.
The question particularly intriguedHarry Hammond Hess, aPrinceton Universitygeologist and a Naval Reserve Rear Admiral, andRobert S. Dietz, a scientist with theUnited States Coast and Geodetic Surveywho coined the termseafloor spreading. Dietz and Hess (the former published the same idea one year earlier inNature,[70]but priority belongs to Hess who had already distributed an unpublished manuscript of his 1962 article by 1960)[71]were among the small number who really understood the broad implications of sea floor spreading and how it would eventually agree with the, at that time, unconventional and unaccepted ideas of continental drift and the elegant and mobilistic models proposed by previous workers like Holmes.
In the same year,Robert R. Coatsof the U.S. Geological Survey described the main features ofisland arcsubduction in theAleutian Islands.[72]His paper, though little noted (and sometimes even ridiculed) at the time, has since been called "seminal" and "prescient". In reality, it shows that the work by the European scientists on island arcs and mountain belts performed and published during the 1930s up until the 1950s was applied and appreciated also in the United States.
If Earth's crust was expanding along the oceanic ridges, Hess and Dietz reasoned like Holmes and others before them, it must be shrinking elsewhere. Hess followed Heezen, suggesting that new oceanic crust continuously spreads away from the ridges in a conveyor belt–like motion. And, using the mobilistic concepts developed before, he correctly concluded that many millions of years later, the oceanic crust eventually descends along the continental margins where oceanic trenches—very deep, narrow canyons—are formed, e.g. alongthe rim of the Pacific Ocean basin. The important step Hess made was that convection currents would be the driving force in this process, arriving at the same conclusions as Holmes had decades before with the only difference that the thinning of the ocean crust was performed using Heezen's mechanism of spreading along the ridges. Hess therefore concluded that the Atlantic Ocean was expanding while thePacific Oceanwas shrinking. As old oceanic crust is "consumed" in the trenches (like Holmes and others, he thought this was done by thickening of the continental lithosphere, not, as later understood, by underthrusting at a larger scale of the oceanic crust itself into the mantle), new magma rises and erupts along the spreading ridges to form new crust. In effect, the ocean basins are perpetually being "recycled", with the forming of new crust and the destruction of old oceanic lithosphere occurring simultaneously. Thus, the new mobilistic concepts neatly explained why Earth does not get bigger with sea floor spreading, why there is so little sediment accumulation on the ocean floor, and why oceanic rocks are much younger than continental rocks.
Beginning in the 1950s, scientists likeVictor Vacquier, using magnetic instruments (magnetometers) adapted from airborne devices developed duringWorld War IIto detectsubmarines, began recognizing odd magnetic variations across the ocean floor. This finding, though unexpected, was not entirely surprising because it was known thatbasalt—the iron-rich, volcanic rock making up the ocean floor—contains a strongly magnetic mineral (magnetite) and can locally distort compass readings. This distortion was recognized by Icelandic mariners as early as the late 18th century. More importantly, because the presence of magnetite gives the basalt measurable magnetic properties, these newly discovered magnetic variations provided another means to study the deep ocean floor. When newly formed rock cools, such magnetic materials recordedEarth's magnetic fieldat the time.
As more and more of the seafloor was mapped during the 1950s, the magnetic variations turned out not to be random or isolated occurrences, but instead revealed recognizable patterns. When these magnetic patterns were mapped over a wide region, the ocean floor showed azebra-like pattern: one stripe with normal polarity and the adjoining stripe with reversed polarity. The overall pattern, defined by these alternating bands of normally and reversely polarized rock, became known as magnetic striping, and was published byRon G. Masonand co-workers in 1961, who did not find, though, an explanation for these data in terms of sea floor spreading, like Vine, Matthews and Morley a few years later.[73]
The discovery of magnetic striping called for an explanation. In the early 1960s scientists such as Heezen, Hess and Dietz had begun to theorise that mid-ocean ridges mark structurally weak zones where the ocean floor was being ripped in two lengthwise along the ridge crest (see the previous paragraph). Newmagmafrom deep within Earth rises easily through these weak zones and eventually erupts along the crest of the ridges to create new oceanic crust. This process, at first denominated the "conveyer belt hypothesis" and later called seafloor spreading, operating over many millions of years continues to form new ocean floor all across the 50,000 km-long system of mid-ocean ridges.
Only four years after the maps with the "zebra pattern" of magnetic stripes were published, the link between sea floor spreading and these patterns was recognized independently byLawrence Morley, and byFred VineandDrummond Matthews, in 1963,[74](theVine–Matthews–Morley hypothesis). This hypothesis linked these patterns to geomagnetic reversals and was supported by several lines of evidence:[75]
By explaining both the zebra-like magnetic striping and the construction of the mid-ocean ridge system, the seafloor spreading hypothesis (SFS) quickly gained converts and represented another major advance in the development of the plate-tectonics theory. Furthermore, the oceanic crust came to be appreciated as a natural "tape recording" of the history of the geomagnetic field reversals (GMFR) of Earth's magnetic field. Extensive studies were dedicated to the calibration of the normal-reversal patterns in the oceanic crust on one hand and known timescales derived from the dating of basalt layers in sedimentary sequences (magnetostratigraphy) on the other, to arrive at estimates of past spreading rates and plate reconstructions.
After all these considerations, plate tectonics (or, as it was initially called "New Global Tectonics") became quickly accepted and numerous papers followed that defined the concepts:
According to a hypothesis proposed by Robert Stern and Taras Gerya, plate tectonics are a necessary criterion for a planet to be able to sustain complex life because of the role plate tectonics plays in regulating the carbon cycle.[86]
Continental drift theory helps biogeographers to explain the disjunctbiogeographicdistribution of present-day life found on different continents but havingsimilar ancestors.[87]
Reconstruction is used to establish past (and future) plate configurations, helping determine the shape and make-up of ancient supercontinents and providing a basis for paleogeography.
Active plate boundaries are defined by their seismicity.[88]Past plate boundaries within existing plates are identified from a variety of evidence, such as the presence ofophiolitesthat are indicative of vanished oceans.[89]
The timing of the emergence of plate tectonics on Earth has been the subject of considerable controversy, with the estimated time varying wildly between researchers, spanning 85% of Earth's history.[90]Some authors have suggested that during at least part of theArcheanperiod (~4-2.5 billion years ago) the mantle was between 100 and 250 °C warmer than at present, which is thought to be incompatible with modern-style plate tectonics, and that Earth may have had astagnant lidor other kinds of regimes. The increasinglyfelsicnature of preserved rocks between 3 and 2.5 billion years ago implies that subduction zones had emerged by this time, with preserved zircons suggesting that subduction may have begun as early as 3.8 billion years ago. Early subduction zones appear to have been temporary and localized, though to what degree is controversial. Modern plate tectonics are suggested to have emerged by at least 2.2 billion years ago with the formation of the first recognised supercontinent Columbia, though some authors have suggested that modern-style plate tectonics did not appear until 800 million years ago based on the late appearance of rock types likeblueschistwhich require cold subducted material.[90]Other authors have suggested that plate tectonics were already functional in theHadean, over 4 billion years ago.[91]
Various types of quantitative and semi-quantitative information are available to constrain past plate motions. The geometric fit between continents, such as between west Africa and South America is still an important part of plate reconstruction. Magnetic stripe patterns provide a reliable guide to relative plate motions going back into theJurassicperiod.[92]The tracks of hotspots give absolute reconstructions, but these are only available back to theCretaceous.[93]Older reconstructions rely mainly onpaleomagnetic poledata, although these only constrain the latitude and rotation, but not the longitude. Combining poles of different ages in a particular plate to produce apparent polar wander paths provides a method for comparing the motions of different plates through time.[94]Additional evidence comes from the distribution of certainsedimentary rocktypes,[95]faunal provinces shown by particular fossil groups, and the position oforogenic belts.[93]
The movement of plates has caused the formation and break-up of continents over time, including occasional formation of asupercontinentthat contains most or all of the continents. The supercontinentColumbiaor Nuna formed during a period of2,000 to 1,800million years agoand broke up about1,500 to 1,300million years ago.[96][97]The supercontinentRodiniais thought to have formed about 1billion years ago and to have embodied most or all of Earth's continents, and broken up into eight continents around600million years ago. The eight continents later re-assembled into another supercontinent calledPangaea; Pangaea broke up intoLaurasia(which became North America and Eurasia) andGondwana(which became the remaining continents).
TheHimalayas, the world's tallest mountain range, are assumed to have been formed by the collision of two major plates. Before uplift, the area where they stand was covered by theTethys Ocean.
Depending on how they are defined, there are usually seven or eight "major" plates:African,Antarctic,Eurasian,North American,South American,Pacific, andIndo-Australian. The latter is sometimes subdivided into theIndianandAustralianplates.
There are dozens of smaller plates, the eight largest of which are theArabian,Caribbean,Juan de Fuca,Cocos,Nazca,Philippine Sea,ScotiaandSomali.
During the 2020s, new proposals have come forward that divide the Earth's crust into many smaller plates, called terranes, which reflects the fact that Plate reconstructions show that the larger plates have been internally deformed and oceanic and continental plates have been fragmented over time. This has resulted in the definition of roughly 1200 terranes inside the oceanic plates, continental blocks and the mobile zones (mountainous belts) that separate them.[98][99]
The motion of the tectonic plates is determined by remote sensing satellite data sets, calibrated with ground station measurements.
The appearance of plate tectonics onterrestrial planetsis related to planetary mass, withmore massive planets than Earthexpected to exhibit plate tectonics. Earth may be a borderline case, owing its tectonic activity to abundant water (silica and water form a deepeutectic).[100]
Venus shows no evidence of active plate tectonics. There is debatable evidence of active tectonics in the planet's distant past; however, events taking place since then (such as the plausible and generally accepted hypothesis that the Venusianlithospherehas thickened greatly over the course of several hundred million years) has made constraining the course of its geologic record difficult. However, the numerous well-preservedimpact cratershave been used as adating methodto approximately date the Venusian surface (since there are thus far no known samples of Venusian rock to be dated by more reliable methods). Dates derived are dominantly in the range500 to 750million years ago, although ages of up to1,200million years agohave been calculated. This research has led to the fairly well accepted hypothesis that Venus has undergone an essentially complete volcanic resurfacing at least once in its distant past, with the last event taking place approximately within the range of estimated surface ages. While the mechanism of such an impressive thermal event remains a debated issue in Venusian geosciences, some scientists are advocates of processes involving plate motion to some extent.
One explanation for Venus's lack of plate tectonics is that on Venus temperatures are too high for significant water to be present.[101][102]Earth's crust is soaked with water, and water plays an important role in the development ofshear zones. Plate tectonics requires weak surfaces in the crust along which crustal slices can move, and it may well be that such weakening never took place on Venus because of the absence of water. However, some researchers[103]remain convinced that plate tectonics is or was once active on this planet.
Mars is considerably smaller than Earth and Venus, and there is evidence for ice on its surface and in its crust.
In the 1990s, it was proposed thatMartian Crustal Dichotomywas created by plate tectonic processes.[104]Scientists have since determined that it was created either by upwelling within the Martianmantlethat thickened the crust of the Southern Highlands and formedTharsis[105]or by a giant impact that excavated theNorthern Lowlands.[106]
Observations made of the magnetic field of Mars by theMars Global Surveyorspacecraft in 1999 showed patterns of magnetic striping discovered on this planet. Some scientists interpreted these as requiring plate tectonic processes, such as seafloor spreading.[108]However, their data failed a "magnetic reversal test", which is used to see if they were formed by flipping polarities of a global magnetic field.[109]
Some of thesatellitesofJupiterhave features that may be related to plate-tectonic style deformation, although the materials and specific mechanisms may be different from plate-tectonic activity on Earth. On 8 September 2014, NASA reported finding evidence of plate tectonics onEuropa, a satellite of Jupiter—the first sign of subduction activity on another world other than Earth.[110]Titan, the largest moon ofSaturn, was reported to show tectonic activity in images taken by theHuygensprobe, which landed on Titan on January 14, 2005.[111]
On Earth-sized planets, plate tectonics is more likely if there are oceans of water. However, in 2007, two independent teams of researchers came to opposing conclusions about the likelihood of plate tectonics on largersuper-Earths[114][115]with one team saying that plate tectonics would be episodic or stagnant[116]and the other team saying that plate tectonics is very likely on super-earths even if the planet is dry.[100]
Consideration of plate tectonics is a part of thesearch for extraterrestrial intelligenceandextraterrestrial life.[117]
TheBig Bangis aphysical theorythat describes how theuniverse expandedfrom an initial state of highdensityandtemperature.[1]Variouscosmological modelsbased on the Big Bang concept explain a broad range of phenomena,[2][3][4]including the abundance oflight elements, thecosmic microwave background(CMB)radiation, andlarge-scale structure. The uniformity of the universe, known as thehorizonandflatness problems, is explained throughcosmic inflation: a phase of accelerated expansion during the earliest stages. A wide range of empirical evidence strongly favors the Big Bang event, which is now essentially universally accepted.[5]Detailed measurements of the expansion rate of the universe place the Big Bang singularity at an estimated13.787±0.02billion years ago, which is considered theage of the universe.[6]
Extrapolating this cosmic expansion backward in time using the knownlaws of physics, the models describe an extraordinarily hot and dense primordial universe. Physics lacks a widely accepted theory that can model the earliest conditions of the Big Bang.[7]As the universe expanded, it cooled sufficiently to allow the formation ofsubatomic particles, and lateratoms. These primordial elements—mostlyhydrogen, with someheliumandlithium—then coalesced under the force ofgravityaided bydark matter, forming earlystarsand galaxies.  Measurements of the redshifts ofsupernovaeindicate that theexpansion of the universe is accelerating, an observation attributed to a concept calleddark energy.
The concept of an expandinguniversewas scientifically originated by thephysicistAlexander Friedmannin 1922 with the mathematical derivation of theFriedmann equations.[8][9][10][11]The earliest empirical observation of an expanding universe is known asHubble's law, published in work by physicistEdwin Hubblein 1929, which discerned that galaxies are moving away from Earth at a rate that accelerates proportionally with distance.Independentof Friedmann's work, and independent of Hubble's observations, physicistGeorges Lemaîtreproposed that the universe emerged from a "primevalatom" in 1931, introducing the modern notion of the Big Bang. In 1964, the CMB was discovered, which convinced many cosmologists that the competingsteady-state modelof cosmic evolution wasfalsified, since the Big Bang models predict a uniform background radiation caused by high temperatures and densities in the distant past.[12]
There remain aspects of the observed universe that are not yet adequately explained by the Big Bang models. These include the unequal abundances of matter andantimatterknown asbaryon asymmetry, the detailed nature ofdark mattersurrounding galaxies, and the origin ofdark energy.[13]
Big Bang cosmology models depend on three major assumptions: the universality of physical laws, thecosmological principle, and that the matter content can be modeled as aperfect fluid.[14]The universality of physical laws is one of the underlying principles of thetheory of relativity. The cosmological principle states that on large scales theuniverseishomogeneousandisotropic—appearing the same in all directions regardless of location.[15]A perfect fluid has no viscosity; the pressure of a perfect fluid is proportional to its density.[16]: 49
These ideas were initially taken as postulates, but later efforts were made to test each of them. For example, the first assumption has been tested by observations showing that the largest possible deviation of thefine-structure constantover much of the age of the universe is of order 10−5.[17]The key physical law behind these models,general relativityhas passed stringenttestson the scale of theSolar Systemandbinary stars.[18][19]The cosmological principle has been confirmed to a level of 10−5via observations of the temperature of the CMB. At the scale of the CMB horizon, the universe has been measured to be homogeneous with an upper boundon the order of10% inhomogeneity, as of 1995.[20]
The cosmological principle dramatically simplifies the equations of general relativity, giving theFriedmann–Lemaître–Robertson–Walker metricto describe the geometry of the universe and, with the assumption of a perfect fluid, theFriedmann equationsgiving the time dependence of that geometry. The only parameter at this level of description is the mass-energy density: thegeometry of the universeand itsexpansionis a direct consequence of its density.[21]: 73All of the major features of Big Bang cosmology are related to these results.[16]: 49
In Big Bang cosmology, themass–energydensity controls the shape and evolution of the universe. By combining astronomical observations with known laws ofthermodynamicsandparticle physics, cosmologists have worked out the components of the density over the lifespan of the universe. In the current universe, luminousmatter, the stars, planets, and so on makes up less than 5% of the density.Dark matteraccounts for 27%  anddark energythe remaining 68%.[22]
An important feature of the Big Bang spacetime is the presence ofparticle horizons. Since the universe has a finite age, andlighttravels at a finite speed, there may be events in the past whose light has not yet had time to reach earth. This places a limit or apast horizonon the most distant objects that can be observed. Conversely, because space is expanding, and more distant objects are receding ever more quickly, light emitted by us today may never "catch up" to very distant objects. This defines afuture horizon, which limits the events in the future that we will be able to influence. The presence of either type of horizon depends on the details of theFriedmann–Lemaître–Robertson–Walker (FLRW) metricthat describes the expansion of the universe.[23]
Our understanding of the universe back to very early times suggests that there is a past horizon, though in practice our view is also limited by the opacity of the universe at early times. So our view cannot extend further backward in time, though the horizon recedes in space. If the expansion of the universe continues to accelerate, there is a future horizon as well.[23]
Some processes in the early universe occurred too slowly, compared to the expansion rate of the universe, to reach approximatethermodynamic equilibrium. Others were fast enough to reachthermalization. The parameter usually used to find out whether a process in the very early universe has reached thermal equilibrium is the ratio between the rate of the process (usually rate of collisions between particles) and theHubble parameter. The larger the ratio, the more time particles had to thermalize before they were too far away from each other.[24]
According to the Big Bang models, the universe at the beginning was very hot and very compact, and since then it has been expanding and cooling.
Existing theories of physics cannot tell us about the moment of the Big Bang.[7]Extrapolation of the expansion of the universe backwards in time using only general relativity yields agravitational singularitywithinfinitedensityandtemperatureat a finite time in the past,[25]but the meaning of this extrapolation in the context of the Big Bang is unclear.[26]Moreover,classicalgravitational theories are expected to be inadequate to describe physics under these conditions.[21]: 275Quantum gravityeffects are expected to be dominant during thePlanck epoch, when the temperature of the universe was close to thePlanck scale(around 1032K or 1028eV).
Even below the Planck scale, undiscovered physics could greatly influence the expansion history of the universe. The Standard Model of particle physics is only tested up to temperatures of order 1017K (10 TeV) in particle colliders, such as theLarge Hadron Collider. Moreover, new physical phenomena decoupled from the Standard Model could have been important before the time ofneutrino decoupling, when the temperature of the universe was only about 1010K (1 MeV).[27]
The earliest phases of the Big Bang are subject to much speculation, given the lack of available data. In the most common models the universe was filled homogeneously and isotropically with a very highenergy densityand huge temperatures andpressures, and was very rapidly expanding and cooling. The period up to 10−43seconds into the expansion, thePlanck epoch, was a phase in which the fourfundamental forces—theelectromagnetic force, thestrong nuclear force, theweak nuclear force, and thegravitational force, were unified as one.[28]In this stage, thecharacteristic scale lengthof the universe was thePlanck length,1.6×10−35m, and consequently had a temperature of approximately 1032degrees Celsius. Even the very concept of a particle breaks down in these conditions. A proper understanding of this period awaits the development of a theory ofquantum gravity.[29][30]The Planck epoch was succeeded by thegrand unification epochbeginning at 10−43seconds, where gravitation separated from the other forces as the universe's temperature fell.[28]
At approximately 10−37seconds into the expansion, aphase transitioncaused acosmic inflation, during which the universe grewexponentially, unconstrained by thelight speed invariance, and temperatures dropped by a factor of 100,000. This concept is motivated by theflatness problem, where thedensity of matter and energyis very close to the critical density needed to produce aflat universe. That is, theshape of the universehas no overallgeometric curvaturedue to gravitational influence. Microscopicquantum fluctuationsthat occurred because ofHeisenberg's uncertainty principlewere "frozen in" by inflation, becoming amplified into the seeds that would later form the large-scale structure of the universe.[31]At a time around 10−36seconds, theelectroweak epochbegins when the strong nuclear force separates from the other forces, with only the electromagnetic force and weak nuclear force remaining unified.[32]
Inflation stopped locally at around 10−33to 10−32seconds, with the observable universe's volume having increased by a factor of at least 1078. Reheating followed as theinflaton fielddecayed, until the universe obtained the temperatures required for theproductionof aquark–gluon plasmaas well as all otherelementary particles.[33][34]Temperatures were so high that the random motions of particles were atrelativisticspeeds, andparticle–antiparticle pairsof all kinds were being continuously created and destroyed in collisions.[1]At some point, an unknown reaction calledbaryogenesisviolated the conservation ofbaryon number, leading to a very small excess ofquarksandleptonsover antiquarks and antileptons—of the order of one part in 30 million. This resulted in the predominance of matter over antimatter in the present universe.[35]
The universe continued to decrease in density and fall in temperature, hence the typical energy of each particle was decreasing.Symmetry-breakingphase transitions put thefundamental forcesof physics and the parameters of elementary particles into their present form, with the electromagnetic force and weak nuclear force separating at about 10−12seconds.[32][36]
After about 10−11seconds, the picture becomes less speculative, since particle energies drop to values that can be attained inparticle accelerators. At about 10−6seconds,quarksandgluonscombined to formbaryonssuch asprotonsandneutrons. The small excess of quarks over antiquarks led to a small excess of baryons over antibaryons. The temperature was no longer high enough to create either new proton–antiproton or neutron–antineutron pairs. A massannihilationimmediately followed, leaving just one in 108of the original matter particles and none of theirantiparticles.[37]A similar process happened at about 1 second for electrons and positrons. After these annihilations, the remaining protons, neutrons and electrons were no longer moving relativistically and the energy density of the universe was dominated byphotons(with a minor contribution fromneutrinos).
A few minutes into the expansion, when the temperature was about a billionkelvinand the density of matter in the universe was comparable to the current density of Earth's atmosphere, neutrons combined with protons to form the universe'sdeuteriumandheliumnucleiin a process calledBig Bang nucleosynthesis(BBN).[38]Most protons remained uncombined as hydrogen nuclei.[39]
As the universe cooled, therest energydensity of matter came to gravitationally dominate over that of the photon and neutrino radiation at a time of about 50,000 years. At a time of about 380,000 years, the universe cooled enough that electrons and nuclei combined into neutralatoms(mostlyhydrogen) in an event calledrecombination. This process made the previously opaque universe transparent, and the photons that last scattered during this epoch comprise the cosmic microwave background.[39]
After the recombination epoch, the slightly denser regions of the uniformly distributed matter gravitationally attracted nearby matter and thus grew even denser, forming gas clouds, stars, galaxies, and the other astronomical structures observable today.[1]The details of this process depend on the amount and type of matter in the universe. The four possible types of matter are known ascold dark matter(CDM),warm dark matter,hot dark matter, andbaryonic matter. The best measurements available, from theWilkinson Microwave Anisotropy Probe(WMAP), show that the data is well-fit by aLambda-CDM modelin which dark matter is assumed to be cold. This CDM is estimated to make up about 23% of the matter/energy of the universe, while baryonic matter makes up about 4.6%.[41]
Independent lines of evidence from Type Ia supernovae and the CMB imply that the universe today is dominated by a mysterious form of energy known asdark energy, which appears to homogeneously permeate all of space. Observations suggest that 73% of the total energy density of the present day universe is in this form. When the universe was very young it was likely infused with dark energy, but with everything closer together,gravitypredominated, braking the expansion. Eventually, after billions of years of expansion, the declining density of matter relative to the density of dark energy allowed the expansion of the universe to begin to accelerate.[13]
Dark energy in its simplest formulation is modeled by acosmological constantterm inEinstein field equationsof general relativity, but its composition and mechanism are unknown. More generally, the details of its equation of state and relationship with the Standard Model of particle physics continue to be investigated both through observation and theory.[13]
All of this cosmic evolution after theinflationary epochcan be rigorously described and modeled by the lambda-CDM model of cosmology, which uses the independent frameworks ofquantum mechanicsand general relativity. There are no easily testable models that would describe the situation prior to approximately 10−15seconds.[42]Understanding this earliest of eras in the history of the universe is one of the greatestunsolved problems in physics.
EnglishastronomerFred Hoyleis credited with coining the term "Big Bang" during a talk for a March 1949BBC Radiobroadcast,[43]saying: "These theories were based on the hypothesis that all the matter in the universe was created in one big bang at a particular time in the remote past."[44][45]However, it did not catch on until the 1970s.[45]
It is popularly reported that Hoyle, who favored an alternative "steady-state" cosmological model, intended this to be pejorative,[46][47][48]but Hoyle explicitly denied this and said it was just a striking image meant to highlight the difference between the two models.[49][50][52]Helge Kraghwrites that the evidence for the claim that it was meant as a pejorative is "unconvincing", and mentions a number of indications that it was not a pejorative.[45]
A primordial singularity is sometimes called "the Big Bang",[53]but the term can also refer to a more generic early hot, dense phase.[54]The term itself has been argued to be a misnomer because it evokes an explosion.[45][55]The argument is that whereas an explosion suggests expansion into a surrounding space, the Big Bang only describes the intrinsic expansion of the contents of the universe.[56][57]Another issue pointed out by Santhosh Mathew is that bang implies sound, which is not an important feature of the model.[47]However, an attempt to find a more suitable alternative was not successful.[45]According toTimothy Ferris:[48][58]
The term 'big bang' was coined with derisive intent by Fred Hoyle, and its endurance testifies to Sir Fred's creativity and wit. Indeed, the term survived an international competition in which three judges — the television science reporterHugh Downs, the astronomerCarl Sagan, and myself — sifted through 13,099 entries from 41 countries and concluded that none was apt enough to replace it. No winner was declared, and like it or not, we are stuck with 'big bang'.
Early cosmological models developed from observations of the structure of the universe and from theoretical considerations. In 1912,Vesto Sliphermeasured the firstDoppler shiftof a "spiral nebula" (spiral nebula is the obsolete term for spiral galaxies), and soon discovered that almost all such nebulae were receding from Earth. He did not grasp the cosmological implications of this fact, and indeed at the time it washighly controversialwhether or not these nebulae were "island universes" outside ourMilky Way.[60][61]Ten years later,Alexander Friedmann, aRussiancosmologistandmathematician, derived theFriedmann equationsfrom the Einstein field equations, showing that the universe might be expanding in contrast to thestatic universemodel advocated byAlbert Einsteinat that time.[62][63]
In 1924,AmericanastronomerEdwin Hubble's measurement of the great distance to the nearest spiral nebulae showed that these systems were indeed other galaxies. Starting that same year, Hubble painstakingly developed a series of distance indicators, the forerunner of thecosmic distance ladder, using the 100-inch (2.5 m)Hooker telescopeatMount Wilson Observatory. This allowed him to estimate distances to galaxies whoseredshiftshad already been measured, mostly by Slipher. In 1929, Hubble discovered a correlation between distance andrecessional velocity—now known as Hubble's law.[64][65]
Independently deriving Friedmann's equations in 1927,Georges Lemaître, aBelgianphysicistandRoman Catholic priest, proposed that the recession of the nebulae was due to the expansion of the universe.[66][67]He inferred the relation that Hubble would later observe, given the cosmological principle.[13]In 1931, Lemaître went further and suggested that the evident expansion of the universe, if projected back in time, meant that the further in the past the smaller the universe was, until at some finite time in the past all the mass of the universe was concentrated into a single point, a "primeval atom" where and when the fabric of time and space came into existence.[68]
In the 1920s and 1930s, almost every major cosmologist preferred an eternal steady-state universe, and several complained that the beginning of time implied by an expanding universe imported religious concepts into physics; this objection was later repeated by supporters of the steady-state theory.[69]This perception was enhanced by the fact that the originator of the expanding universe concept, Lemaître, was a Roman Catholic priest.[70]Arthur Eddingtonagreed withAristotlethat the universe did not have a beginning in time,viz., thatmatter is eternal. A beginning in time was "repugnant" to him.[71][72]Lemaître, however, disagreed:
If the world has begun with a singlequantum, the notions of space and time would altogether fail to have any meaning at the beginning; they would only begin to have a sensible meaning when the original quantum had been divided into a sufficient number of quanta. If this suggestion is correct, the beginning of the world happened a little before the beginning of space and time.[73]
During the 1930s, other ideas were proposed asnon-standard cosmologiesto explain Hubble's observations, including theMilne model,[74]theoscillatory universe(originally suggested by Friedmann, but advocated by Albert Einstein andRichard C. Tolman)[75]andFritz Zwicky'stired lighthypothesis.[76]
AfterWorld War II, two distinct possibilities emerged. One was Fred Hoyle's steady-state model, whereby new matter would be created as the universe seemed to expand. In this model the universe is roughly the same at any point in time.[77]The other was Lemaître's expanding universe theory, advocated and developed byGeorge Gamow, who used it to develop a theory for the abundance of chemical elements in the universe.[78]and whose associates,Ralph AlpherandRobert Herman, predicted the cosmic background radiation.[79]
Ironically, it was Hoyle who coined the phrase that came to be applied to Lemaître's theory, referring to it as "thisbig bangidea" during a BBC Radio broadcast in March 1949.[50][45][notes 1]For a while, support was split between these two theories. Eventually, the observational evidence, most notably from radiosource counts, began to favor Big Bang over steady state. The discovery and confirmation of the CMB in 1964 secured the Big Bang as the best theory of the origin and evolution of the universe.[80]
In 1968 and 1970,Roger Penrose,Stephen Hawking, andGeorge F. R. Ellispublished papers where they showed thatmathematical singularitieswere an inevitable initial condition of relativistic models of the Big Bang.[81][82]Then, from the 1970s to the 1990s, cosmologists worked on characterizing the features of the Big Bang universe and resolving outstanding problems. In 1981,Alan Guthmade a breakthrough in theoretical work on resolving certain outstanding theoretical problems in the Big Bang models with the introduction of an epoch of rapid expansion in the early universe he called "inflation".[83]Meanwhile, during these decades, two questions inobservational cosmologythat generated much discussion and disagreement were over the precise values of the Hubble Constant[84]and the matter-density of the universe (before the discovery of dark energy, thought to be the key predictor for the eventualfate of the universe).[85]
Significant progress in Big Bang cosmology has been made since the late 1990s as a result of advances intelescopetechnology as well as the analysis of data from satellites such as theCosmic Background Explorer(COBE),[86]theHubble Space Telescopeand WMAP.[87]Cosmologists now have fairly precise and accurate measurements of many of the parameters of the Big Bang model, and have made the unexpected discovery that the expansion of the universe appears to be accelerating.[88][89]
"[The] big bang picture is too firmly grounded in data from every area to be proved invalid in its general features."
The Big Bang models offer a comprehensive explanation for a broad range of observed phenomena, including the abundances of thelight elements, thecosmic microwave background,large-scale structure, andHubble's law.[91]The earliest and most direct observational evidence of the validity of the theory are the expansion of the universe according to Hubble's law (as indicated by the redshifts of galaxies), discovery and measurement of the cosmic microwave background and the relative abundances of light elements produced byBig Bang nucleosynthesis(BBN). More recent evidence includes observations ofgalaxy formation and evolution, and the distribution oflarge-scale cosmic structures.[92]These are sometimes called the "four pillars" of the Big Bang models.[93][94]
Precise modern models of the Big Bang appeal to various exotic physical phenomena that have not been observed in terrestrial laboratory experiments or incorporated into the Standard Model of particle physics. Of these features,dark matteris currently the subject of most active laboratory investigations.[95]Remaining issues include thecuspy halo problem[96]and thedwarf galaxy problem[97]of cold dark matter. Dark energy is also an area of intense interest for scientists, but it is not clear whether direct detection of dark energy will be possible.[98]Inflation and baryogenesis remain more speculative features of current Big Bang models. Viable, quantitative explanations for such phenomena are still being sought. These are unsolved problems in physics.
Observations of distant galaxies andquasarsshow that these objects are redshifted: the light emitted from them has been shifted to longer wavelengths. This can be seen by taking afrequency spectrumof an object and matching thespectroscopicpattern ofemission or absorption linescorresponding to atoms of the chemical elements interacting with the light. These redshifts areuniformlyisotropic, distributed evenly among the observed objects in all directions. If the redshift is interpreted as a Doppler shift, the recessional velocity of the object can be calculated. For some galaxies, it is possible to estimate distances via thecosmic distance ladder. When the recessional velocities are plotted against these distances, a linear relationship known asHubble's lawis observed:[64]v=H0D{\displaystyle v=H_{0}D}where
Hubble's law implies that the universe is uniformly expanding everywhere. This cosmic expansion was predicted from general relativity by Friedmann in 1922[62]and Lemaître in 1927,[66]well before Hubble made his 1929 analysis and observations, and it remains the cornerstone of the Big Bang model as developed by Friedmann, Lemaître, Robertson, and Walker.
The theory requires the relationv=HD{\displaystyle v=HD}to hold at all times, whereD{\displaystyle D}is the proper distance,v{\displaystyle v}is the recessional velocity, andv{\displaystyle v},H{\displaystyle H}, andD{\displaystyle D}vary as the universe expands (hence we writeH0{\displaystyle H_{0}}to denote the present-day Hubble "constant"). For distances much smaller than the size of theobservable universe, the Hubble redshift can be thought of as the Doppler shift corresponding to the recession velocityv{\displaystyle v}. For distances comparable to the size of the observable universe, the attribution of the cosmological redshift becomes more ambiguous, although its interpretation as a kinematic Doppler shift remains the most natural one.[99]
An unexplained discrepancy with the determination of the Hubble constant is known asHubble tension. Techniques based on observation of the CMB suggest a lower value of this constant compared to the quantity derived from measurements based on the cosmic distance ladder.[100]
In 1964,Arno PenziasandRobert Wilsonserendipitously discovered the cosmic background radiation, an omnidirectional signal in themicrowaveband.[80]Their discovery provided substantial confirmation of the big-bang predictions by Alpher, Herman and Gamow around 1950. Through the 1970s, the radiation was found to be approximately consistent with ablackbodyspectrum in all directions; this spectrum has been redshifted by the expansion of the universe, and today corresponds to approximately 2.725 K. This tipped the balance of evidence in favor of the Big Bang model, and Penzias and Wilson were awarded the 1978Nobel Prize in Physics.
Thesurface of last scatteringcorresponding to emission of the CMB occurs shortly afterrecombination, the epoch when neutral hydrogen becomes stable. Prior to this, the universe comprised a hot dense photon-baryon plasma sea where photons were quicklyscatteredfrom free charged particles. Peaking at around372±14kyr,[102]the mean free path for a photon becomes long enough to reach the present day and the universe becomes transparent.
In 1989,NASAlaunched COBE, which made two major advances: in 1990, high-precision spectrum measurements showed that the CMB frequency spectrum is an almost perfect blackbody with no deviations at a level of 1 part in 104, and measured a residual temperature of 2.726 K (more recent measurements have revised this figure down slightly to 2.7255 K); then in 1992, further COBE measurements discovered tiny fluctuations (anisotropies) in the CMB temperature across the sky, at a level of about one part in 105.[86]John C. MatherandGeorge Smootwere awarded the 2006 Nobel Prize in Physics for their leadership in these results.
During the following decade, CMB anisotropies were further investigated by a large number of ground-based and balloon experiments. In 2000–2001, several experiments, most notablyBOOMERanG, found theshape of the universeto be spatially almost flat by measuring the typical angular size (the size on the sky) of the anisotropies.[106][107][108]
In early 2003, the first results of the Wilkinson Microwave Anisotropy Probe were released, yielding what were at the time the most accurate values for some of the cosmological parameters. The results disproved several specific cosmic inflation models, but are consistent with the inflation theory in general.[87]ThePlanckspace probe was launched in May 2009. Other ground and balloon-basedcosmic microwave background experimentsare ongoing.
Using Big Bang models, it is possible to calculate the expected concentration of the isotopeshelium-4(4He),helium-3(3He), deuterium (2H), andlithium-7(7Li) in the universe as ratios to the amount of ordinary hydrogen.[38]The relative abundances depend on a single parameter, the ratio of photons to baryons. This value can be calculated independently from the detailed structure of CMB fluctuations. The ratios predicted (by mass, not by abundance) are about 0.25 for4He:H, about 10−3for2H:H, about 10−4for3He:H, and about 10−9for7Li:H.[38]
The measured abundances all agree at least roughly with those predicted from a single value of the baryon-to-photon ratio. The agreement is excellent for deuterium, close but formally discrepant for4He, and off by a factor of two for7Li (this anomaly is known as thecosmological lithium problem); in the latter two cases, there are substantialsystematic uncertainties. Nonetheless, the general consistency with abundances predicted by BBN is strong evidence for the Big Bang, as the theory is the only known explanation for the relative abundances of light elements, and it is virtually impossible to "tune" the Big Bang to produce much more or less than 20–30% helium.[109]Indeed, there is no obvious reason outside of the Big Bang that, for example, the young universe beforestar formation, as determined by studying matter supposedly free ofstellar nucleosynthesisproducts, should have more helium than deuterium or more deuterium than3He, and in constant ratios, too.[110]: 182–185
Detailed observations of themorphologyand distribution of galaxies andquasarsare in agreement with the current Big Bang models. A combination of observations and theory suggest that the first quasars and galaxies formed within a billion years after the Big Bang,[111]and since then, larger structures have been forming, such asgalaxy clustersandsuperclusters.[112]
Populations of stars have been aging and evolving, so that distant galaxies (which are observed as they were in the early universe) appear very different from nearby galaxies (observed in a more recent state). Moreover, galaxies that formed relatively recently appear markedly different from galaxies formed at similar distances but shortly after the Big Bang. These observations are strong arguments against the steady-state model. Observations of star formation, galaxy and quasar distributions and larger structures, agree well with Big Bang simulations of the formation of structure in the universe, and are helping to complete details of the theory.[112][113]
In 2011, astronomers found what they believe to be pristine clouds of primordial gas by analyzing absorption lines in the spectra of distant quasars. Before this discovery, all other astronomical objects have been observed to contain heavy elements that are formed in stars. Despite being sensitive to carbon, oxygen, and silicon, these three elements were not detected in these two clouds.[118][119]Since the clouds of gas have no detectable levels of heavy elements, they likely formed in the first few minutes after the Big Bang, during BBN.
The age of the universe as estimated from the Hubble expansion and the CMB is now in agreement with other estimates using the ages of the oldest stars, both as measured by applying the theory ofstellar evolutionto globular clusters and throughradiometric datingof individualPopulation IIstars.[120]It is also in agreement with age estimates based on measurements of the expansion usingType Ia supernovaeand measurements of temperature fluctuations in the cosmic microwave background.[121]The agreement of independent measurements of this age supports theLambda-CDM(ΛCDM) model, since the model is used to relate some of the measurements to an age estimate, and all estimates turn agree. Still, some observations of objects from the relatively early universe (in particular quasarAPM 08279+5255) raise concern as to whether these objects had enough time to form so early in the ΛCDM model.[122][123]
The prediction that the CMB temperature was higher in the past has been experimentally supported by observations of very low temperature absorption lines in gas clouds at high redshift.[124]This prediction also implies that the amplitude of theSunyaev–Zel'dovich effectin clusters of galaxies does not depend directly on redshift. Observations have found this to be roughly true, but this effect depends on cluster properties that do change with cosmic time, making precise measurements difficult.[125][126]
Futuregravitational-wave observatoriesmight be able to detect primordialgravitational waves, relics of the early universe, up to less than a second after the Big Bang.[127][128]
As with any theory, a number of mysteries and problems have arisen as a result of the development of the Big Bang models. Some of these mysteries and problems have been resolved while others are still outstanding. Proposed solutions to some of the problems in the Big Bang model have revealed new mysteries of their own. For example, thehorizon problem, themagnetic monopole problem, and theflatness problemare most commonly resolved with inflation theory, but the details of the inflationary universe are still left unresolved and many, including some founders of the theory, say it has been disproven.[129][130][131][132]What follows are a list of the mysterious aspects of the Big Bang concept still under intense investigation by cosmologists andastrophysicists.
It is not yet understood why the universe has more matter than antimatter.[35]It is generally assumed that when the universe was young and very hot it was instatistical equilibriumand contained equal numbers of baryons and antibaryons. However, observations suggest that the universe, including its most distant parts, is made almost entirely of normal matter, rather than antimatter. A process called baryogenesis was hypothesized to account for the asymmetry. For baryogenesis to occur, theSakharov conditionsmust be satisfied. These require that baryon number is not conserved, thatC-symmetryandCP-symmetryare violated and that the universe depart fromthermodynamic equilibrium.[133][134]All these conditions occur in the Standard Model, but the effects are not strong enough to explain the present baryon asymmetry.
Measurements of the redshift–magnituderelation fortype Ia supernovaeindicate that the expansion of the universe has been accelerating since the universe was about half its present age. To explain this acceleration, cosmological models require that much of the energy in the universe consists of a component with large negative pressure, dubbed "dark energy".[13]
Dark energy, though speculative, solves numerous problems. Measurements of the cosmic microwave background indicate that the universe is very nearly spatially flat, and therefore according to general relativity the universe must have almost exactly thecritical densityof mass/energy. But the mass density of the universe can be measured from its gravitational clustering, and is found to have only about 30% of the critical density.[13]Since theory suggests that dark energy does not cluster in the usual way it is the best explanation for the "missing" energy density. Dark energy also helps to explain two geometrical measures of the overall curvature of the universe, one using the frequency ofgravitational lenses,[135]and the other using the characteristic pattern of the large-scale structure--baryon acoustic oscillations--as a cosmic ruler.[136][137]
Negative pressure is believed to be a property ofvacuum energy, but the exact nature and existence of dark energy remains one of the great mysteries of the Big Bang. Results from the WMAP team in 2008 are in accordance with a universe that consists of 73% dark energy, 23% dark matter, 4.6% regular matter and less than 1% neutrinos.[41]According to theory, the energy density in matter decreases with the expansion of the universe, but the dark energy density remains constant (or nearly so) as the universe expands. Therefore, matter made up a larger fraction of the total energy of the universe in the past than it does today, but its fractional contribution will fall in thefar futureas dark energy becomes even more dominant.[citation needed]
The dark energy component of the universe has been explained by theorists using a variety of competing theories including Einstein's cosmological constant but also extending to more exotic forms ofquintessenceor other modified gravity schemes.[138]Acosmological constant problem, sometimes called the "most embarrassing problem in physics", results from the apparent discrepancy between the measured energy density of dark energy, and the one naively predicted fromPlanck units.[139]
During the 1970s and the 1980s, various observations showed that there is not sufficient visible matter in the universe to account for the apparent strength of gravitational forces within and between galaxies. This led to the idea that up to 90% of the matter in the universe is dark matter that does not emit light or interact with normal baryonic matter. In addition, the assumption that the universe is mostly normal matter led to predictions that were strongly inconsistent with observations. In particular, the universe today is far more lumpy and contains far less deuterium than can be accounted for without dark matter. While dark matter has always been controversial, it is inferred by various observations: the anisotropies in the CMB, thegalaxy rotation problem,galaxy clustervelocity dispersions, large-scale structure distributions,gravitational lensingstudies, andX-ray measurementsof galaxy clusters.[140]
Indirect evidence for dark matter comes from its gravitational influence on other matter, as no dark matter particles have been observed in laboratories. Many particle physics candidates for dark matter have been proposed, and several projects to detect them directly are underway.[141]
Additionally, there are outstanding problems associated with the currently favored cold dark matter model which include thedwarf galaxy problem[97]and thecuspy halo problem.[96]Alternative theories have been proposed that do not require a large amount of undetected matter, but instead modify the laws of gravity established by Newton and Einstein; yet no alternative theory has been as successful as the cold dark matter proposal in explaining all extant observations.[142]
The horizon problem results from the premise that information cannot travelfaster than light. In a universe of finite age this sets a limit—the particle horizon—on the separation of any two regions of space that are incausalcontact.[143]The observed isotropy of the CMB is problematic in this regard: if the universe had been dominated by radiation or matter at all times up to the epoch of last scattering, the particle horizon at that time would correspond to about 2 degrees on the sky. There would then be no mechanism to cause wider regions to have the same temperature.[110]: 191–202
A resolution to this apparent inconsistency is offered by inflation theory in which a homogeneous and isotropicscalar energy fielddominates the universe at some very early period (before baryogenesis). During inflation, the universe undergoesexponentialexpansion, and the particle horizon expands much more rapidly than previously assumed, so that regions presently on opposite sides of the observable universe are well inside each other's particle horizon. The observed isotropy of the CMB then follows from the fact that this larger region was in causal contact before the beginning of inflation.[31]: 180–186
Heisenberg's uncertainty principle predicts that during the inflationary phase there would bequantum thermal fluctuations, which would be magnified to a cosmic scale. These fluctuations served as the seeds for all the current structures in the universe.[110]: 207Inflation predicts that the primordial fluctuations are nearlyscale invariantandGaussian, which has been confirmed by measurements of the CMB.[87]: sec 6
A related issue to the classic horizon problem arises because in most standard cosmological inflation models, inflation ceases well beforeelectroweak symmetry breakingoccurs, so inflation should not be able to prevent large-scale discontinuities in theelectroweak vacuumsince distant parts of the observable universe were causally separate when theelectroweak epochended.[144]
The magnetic monopole objection was raised in the late 1970s.Grand unified theories(GUTs) predictedtopological defectsin space that would manifest asmagnetic monopoles. These objects would be produced efficiently in the hot early universe, resulting in a density much higher than is consistent with observations, given that no monopoles have been found. This problem is resolved by cosmic inflation, which removes all point defects from the observable universe, in the same way that it drives the geometry to flatness.[143]
The flatness problem (also known as the oldness problem) is an observational problem associated with a FLRW.[143]The universe may have positive, negative, or zero spatialcurvaturedepending on its total energy density. Curvature is negative if its density is less than the critical density; positive if greater; and zero at the critical density, in which case space is said to beflat. Observations indicate the universe is consistent with being flat.[145][146]
The problem is that any small departure from the critical density grows with time, and yet the universe today remains very close to flat.[notes 2]Given that a natural timescale for departure from flatness might be thePlanck time, 10−43seconds,[1]the fact that the universe has reached neither aheat deathnor aBig Crunchafter billions of years requires an explanation. For instance, even at the relatively late age of a few minutes (the time of nucleosynthesis), the density of the universe must have been within one part in 1014of its critical value, or it would not exist as it does today.[147]
One of the common misconceptions about the Big Bang model is that it fully explains theorigin of the universe. However, the Big Bang model does not describe how energy, time, and space were caused, but rather it describes the emergence of the present universe from an ultra-dense and high-temperature initial state.[148]It is misleading to visualize the Big Bang by comparing its size to everyday objects. When the size of the universe at Big Bang is described, it refers to the size of the observable universe, and not the entire universe.[149]
Another common misconception is that the Big Bang must be understood as the expansion of space and not in terms of the contents of space exploding apart. In fact, either description can be accurate. The expansion of space (implied by the FLRW metric) is only a mathematical convention, corresponding to a choice ofcoordinateson spacetime. There is nogenerally covariantsense in which space expands.[150]
The recession speeds associated with Hubble's law are not velocities in a relativistic sense (for example, they are not related to the spatial components of4-velocities). Therefore, it is not remarkable that according to Hubble's law, galaxies farther than the Hubble distance recede faster than the speed of light. Such recession speeds do not correspond tofaster-than-lighttravel.
Many popular accounts attribute the cosmological redshift to the expansion of space. This can be misleading because the expansion of space is only a coordinate choice. The most natural interpretation of the cosmological redshift is that it is aDoppler shift.[99]
Given current understanding, scientific extrapolations about the future of the universe are only possible for finite durations, albeit for much longer periods than the current age of the universe. Anything beyond that becomes increasingly speculative. Likewise, at present, a proper understanding of the origin of the universe can only be subject to conjecture.[151]
The Big Bang explains the evolution of the universe from a starting density and temperature that is well beyond humanity's capability to replicate, so extrapolations to the most extreme conditions and earliest times are necessarily more speculative. Lemaître called this initial state the "primeval atom" while Gamow called the material "ylem". How the initial state of the universe originated is still an open question, but the Big Bang model does constrain some of its characteristics. For example, if specificlaws of naturewere to come to existence in a random way, inflation models show, some combinations of these are far more probable,[152]partly explaining why our Universe is rather stable. Another possible explanation for the stability of the Universe could be a hypothetical multiverse, which assumes every possible universe to exist, and thinking species could only emerge in those stable enough.[153]A flat universe implies a balance betweengravitational potential energyand other energy forms, requiring no additional energy to be created.[145][146]
The Big Bang theory is built upon the equations of classical general relativity, which are not expected to be valid at the origin of cosmic time, as the temperature of the universe approaches the Planck scale. Correcting this will require the development of a correct treatment of quantum gravity.[25]Certain quantum gravity treatments, such as theWheeler–DeWitt equation, imply that time itself could be anemergent property.[154]As such, physics may conclude thattimedid not exist before the Big Bang.[155][156][157][158][159]
While it is not known what could have preceded the hot dense state of the early universe or how and why it originated, or even whether such questions are sensible, speculation abounds on the subject of "cosmogony".
Some speculative proposals in this regard, each of which entails untested hypotheses, are:
Proposals in the last two categories see the Big Bang as an event in either a much larger andolder universeor in amultiverse.
Before observations of dark energy, cosmologists considered two scenarios for the future of the universe. If the mass density of the universe were greater than the critical density, then the universe would reach a maximum size and then begin to collapse. It would become denser and hotter again, ending with a state similar to that in which it started—aBig Crunch.[23]
Alternatively, if the density in the universe were equal to or below the critical density, the expansion would slow down but never stop. Star formation would cease with the consumption of interstellar gas in each galaxy; stars would burn out, leavingwhite dwarfs,neutron stars, and black holes. Collisions between these would result in mass accumulating into larger and larger black holes. The average temperature of the universe would very gradually asymptotically approachabsolute zero—aBig Freeze.[174]Moreover, if protons areunstable, then baryonic matter would disappear, leaving only radiation and black holes. Eventually, black holes would evaporate by emittingHawking radiation. Theentropyof the universe would increase to the point where no organized form of energy could be extracted from it, a scenario known as heat death.[175]
Modern observations of accelerating expansion imply that more and more of the currently visible universe will pass beyond ourevent horizonand out of contact with us. The eventual result is not known. The ΛCDM model of the universe contains dark energy in the form of a cosmological constant. This theory suggests that only gravitationally bound systems, such as galaxies, will remain together, and they too will be subject to heat death as the universe expands and cools. Other explanations of dark energy, calledphantom energytheories, suggest that ultimately galaxy clusters, stars, planets, atoms, nuclei, and matter itself will be torn apart by the ever-increasing expansion in a so-calledBig Rip.[176]
As a description of the origin of the universe, the Big Bang has significant bearing on religion and philosophy.[177][178]As a result, it has become one of the liveliest areas in the discourse betweenscience and religion.[179]Some believe the Big Bang implies a creator,[180][181][182][183]while others argue that Big Bang cosmology makes the notion of a creator superfluous.[178][184]
Aphotosynthetic pigment(accessory pigment;chloroplast pigment;antenna pigment) is apigmentthat is present inchloroplastsor photosyntheticbacteriaand captures thelightenergy necessary forphotosynthesis.
List of photosynthetic pigments (in order of increasing polarity):
Chlorophyllais the most common of the six, present in every plant that performs photosynthesis. Each pigment absorbs light more efficiently in a different part of theelectromagnetic spectrum. Chlorophyllaabsorbs well in the ranges of 400–450 nm and at 650–700 nm; chlorophyllbat 450–500 nm and at 600–650 nm. Xanthophyll absorbs well at 400–530 nm. However, none of the pigments[2][3]absorb well in the green-yellow region; thediffuse reflectionof the unabsorbed green light is responsible for the abundant green seen in nature.
Like plants, thecyanobacteriause water as an electron donor for photosynthesis and therefore liberateoxygen; they also use chlorophyll as a pigment. In addition, most cyanobacteria usephycobiliproteins, water-soluble pigments which occur in the cytoplasm of the chloroplast, to capture light energy and pass it on to the chlorophylls. (Some cyanobacteria, the prochlorophytes, use chlorophyll b instead of phycobilin.) It is thought that the chloroplasts in plants and algae all evolved from cyanobacteria.
Several other groups of bacteria use thebacteriochlorophyllpigments (similar to the chlorophylls) for photosynthesis. Unlike the cyanobacteria, these bacteria do not produce oxygen; they typically usehydrogen sulfiderather than water as the electron donor.
Recently, a very different pigment has been found in some marineGammaproteobacteria:proteorhodopsin. It is similar to and probably originated from bacteriorhodopsin (see below: under#Archaea).
Halobacteriause the pigmentbacteriorhodopsinwhich acts directly as aproton pumpwhen exposed to light.
Atomsare the basic particles of thechemical elements. An atom consists of anucleusofprotonsand generallyneutrons, surrounded by an electromagnetically bound swarm ofelectrons.  The chemical elements are distinguished from each other by the number of protons that are in their atoms. For example, any atom that contains 11 protons issodium, and any atom that contains 29 protons iscopper. Atoms with the same number of protons but a different number of neutrons are calledisotopesof the same element.
Atoms are extremely small, typically around 100picometersacross. A human hair is about a millioncarbonatoms wide. Atoms are smaller than the shortest wavelength of visible light, which means humans cannot see atoms with conventional microscopes. They are so small that accurately predicting their behavior usingclassical physicsis not possible due toquantum effects.
More than 99.9994%[1]of an atom'smassis in the nucleus. Protons have a positiveelectric chargeand neutrons have no charge, so the nucleus is positively charged. The electrons are negatively charged, and this opposing charge is what binds them to the nucleus. If the numbers ofprotonsand electrons are equal, as they normally are, then the atom is electrically neutral as a whole. If an atom has more electrons than protons, then it has an overall negative charge and is called a negativeion(or anion). Conversely, if it has more protons than electrons, it has a positive charge and is called a positive ion (or cation).
The electrons of an atom are attracted to the protons in an atomic nucleus by theelectromagnetic force. The protons and neutrons in the nucleus are attracted to each other by thenuclear force. This force is usually stronger than the electromagnetic force that repels the positively charged protons from one another. Under certain circumstances, the repelling electromagnetic force becomes stronger than the nuclear force. In this case, the nucleussplitsand leaves behind different elements. This is a form ofnuclear decay.
Atoms can attach to one or more other atoms bychemical bondsto formchemical compoundssuch asmoleculesorcrystals. The ability of atoms to attach and detach from each other is responsible for most of the physical changes observed in nature.Chemistryis the science that studies these changes.
The basic idea that matter is made up of tiny indivisible particles is an old idea that appeared in many ancient cultures. The wordatomis derived from theancient Greekwordatomos,[a]which means "uncuttable". But this ancient idea was based in philosophical reasoning rather than scientific reasoning. Modern atomic theory is not based on these old concepts.[2][3]In the early 19th century, the scientistJohn Daltonfound evidence that matter really is composed of discrete units, and so applied the wordatomto those units.[4]
In the early 1800s, John Dalton compiled experimental data gathered by him and other scientists and discovered a pattern now known as the "law of multiple proportions". He noticed that in any group of chemical compounds which all contain two particular chemical elements, the amount of Element A per measure of Element B will differ across these compounds by ratios of small whole numbers. This pattern suggested that each element combines with other elements in multiples of a basic unit of weight, with each element having a unit of unique weight. Dalton decided to call these units "atoms".[5]
For example, there are two types oftin oxide: one is a grey powder that is 88.1% tin and 11.9%oxygen, and the other is a white powder that is 78.7% tin and 21.3% oxygen. Adjusting these figures, in the grey powder there is about 13.5 g of oxygen for every 100 g of tin, and in the white powder there is about 27 g of oxygen for every 100 g of tin. 13.5 and 27 form a ratio of 1:2. Dalton concluded that in the grey oxide there is one atom of oxygen for every atom of tin, and in the white oxide there are two atoms of oxygen for every atom of tin (SnOandSnO2).[6][7]
Dalton also analyzediron oxides. There is one type of iron oxide that is a black powder which is 78.1% iron and 21.9% oxygen; and there is another iron oxide that is a red powder which is 70.4% iron and 29.6% oxygen. Adjusting these figures, in the black powder there is about 28 g of oxygen for every 100 g of iron, and in the red powder there is about 42 g of oxygen for every 100 g of iron. 28 and 42 form a ratio of 2:3. Dalton concluded that in these oxides, for every two atoms of iron, there are two or three atoms of oxygen respectively. These substances are known today asiron(II) oxideandiron(III) oxide, and their formulas are FeO and Fe2O3respectively. Iron(II) oxide's formula is normally written as FeO, but since it is a crystalline substance we could alternately write it as Fe2O2, and when we contrast that with Fe2O3, the 2:3 ratio for the oxygen is plain to see.[8][9]
As a final example:nitrous oxideis 63.3%nitrogenand 36.7% oxygen,nitric oxideis 44.05% nitrogen and 55.95% oxygen, andnitrogen dioxideis 29.5% nitrogen and 70.5% oxygen. Adjusting these figures, in nitrous oxide there is 80 g of oxygen for every 140 g of nitrogen, in nitric oxide there is about 160 g of oxygen for every 140 g of nitrogen, and in nitrogen dioxide there is 320 g of oxygen for every 140 g of nitrogen. 80, 160, and 320 form a ratio of 1:2:4. The respective formulas for these oxides areN2O,NO, andNO2.[10][11]
In 1897,J. J. Thomsondiscovered thatcathode rayscan be deflected by electric and magnetic fields, which meant that cathode rays are not a form of light but made of electrically charged particles, and their charge was negative given the direction the particles were deflected in.[12]He measured these particles to be 1,700 times lighter thanhydrogen(the lightest atom).[13]He called these new particlescorpusclesbut they were later renamedelectronssince these are the particles that carry electricity.[14]Thomson also showed that electrons were identical to particles given off byphotoelectricand radioactive materials.[15]Thomson explained that an electric current is the passing of electrons from one atom to the next, and when there was no current the electrons embedded themselves in the atoms. This in turn meant that atoms were not indivisible as scientists thought. The atom was composed of electrons whose negative charge was balanced out by some source of positive charge to create an electrically neutral atom. Ions, Thomson explained, must be atoms which have an excess or shortage of electrons.[16]
The electrons in the atom logically had to be balanced out by a commensurate amount of positive charge, but Thomson had no idea where this positive charge came from, so he tentatively proposed that it was everywhere in the atom, the atom being in the shape of a sphere. This was the mathematically simplest hypothesis to fit the available evidence, or lack thereof. Following from this, Thomson imagined that the balance of electrostatic forces would distribute the electrons throughout the sphere in a more or less even manner.[17]Thomson's model is popularly known as theplum pudding model, though neither Thomson nor his colleagues used this analogy.[18]Thomson's model was incomplete, it was unable to predict any other properties of the elements such asemission spectraandvalencies. It was soon rendered obsolete by the discovery of theatomic nucleus.
Between 1908 and 1913,Ernest Rutherfordand his colleaguesHans GeigerandErnest Marsdenperformed a series of experiments in which they bombarded thin foils of metal with a beam ofalpha particles. They did this to measure the scattering patterns of the alpha particles. They spotted a small number of alpha particles being deflected by angles greater than 90°. This shouldn't have been possible according to the Thomson model of the atom, whose charges were too diffuse to produce a sufficiently strong electric field. The deflections should have all been negligible. Rutherford proposed that the positive charge of the atom is concentrated in a tiny volume at the center of the atom and that the electrons surround this nucleus in a diffuse cloud. This nucleus carried almost all of the atom's mass. Only such an intense concentration of charge, anchored by its high mass, could produce an electric field that could deflect the alpha particles so strongly.[19]
A problem in classical mechanics is that an accelerating charged particle radiateselectromagnetic radiation, causing the particle to losekinetic energy. Circular motion counts as acceleration, which means that an electron orbiting a central charge should spiral down into that nucleus as it loses speed. In 1913, the physicistNiels Bohrproposed a new model in which the electrons of an atom were assumed to orbit the nucleus but could only do so in a finite set of orbits, and could jump between these orbits only in discrete changes of energy corresponding to absorption or radiation of aphoton.[20]This quantization was used to explain why the electrons' orbits are stable and why elements absorb and emit electromagnetic radiation in discrete spectra.[21]Bohr's model could only predict the emission spectra of hydrogen, not atoms with more than one electron.
Back in 1815,William Proutobserved that the atomic weights of many elements were multiples of hydrogen's atomic weight, which is in fact true for all of them if one takesisotopesinto account. In 1898,J. J. Thomsonfound that the positive charge of a hydrogen ion is equal to the negative charge of an electron, and these were then the smallest known charged particles.[22]Thomson later found that the positive charge in an atom is a positive multiple of an electron's negative charge.[23]In 1913,Henry Moseleydiscovered that the frequencies of X-ray emissions from anexcitedatom were a mathematical function of itsatomic numberand hydrogen's nuclear charge. In 1919,Rutherfordbombardednitrogengas withalpha particlesand detectedhydrogenions being emitted from the gas, and concluded that they were produced by alpha particles hitting and splitting the nuclei of the nitrogen atoms.[24]
These observations led Rutherford to conclude that the hydrogen nucleus is a singular particle with a positive charge equal to the electron's negative charge.[25]He named this particle "proton" in 1920.[26]The number of protons in an atom (which Rutherford called the "atomic number"[27][28]) was found to be equal to the element's ordinal number on theperiodic tableand therefore provided a simple and clear-cut way of distinguishing the elements from each other. The atomic weight of each element is higher than its proton number, so Rutherford hypothesized that the surplus weight was carried by unknown particles with no electric charge and a mass equal to that of the proton.
In 1928,Walter Botheobserved thatberylliumemitted a highly penetrating, electrically neutral radiation when bombarded with alpha particles. It was later discovered that this radiation could knock hydrogen atoms out ofparaffin wax. Initially it was thought to be high-energygamma radiation, since gamma radiation had a similar effect on electrons in metals, butJames Chadwickfound that theionizationeffect was too strong for it to be due to electromagnetic radiation, so long as energy and momentum were conserved in the interaction. In 1932, Chadwick exposed various elements, such as hydrogen and nitrogen, to the mysterious "beryllium radiation", and by measuring the energies of the recoiling charged particles, he deduced that the radiation was actually composed of electrically neutral particles which could not be massless like the gamma ray, but instead were required to have a mass similar to that of a proton. Chadwick now claimed these particles as Rutherford's neutrons.[29]
In 1925,Werner Heisenbergpublished the first consistent mathematical formulation of quantum mechanics (matrix mechanics).[30]One year earlier,Louis de Brogliehad proposed that all particles behave like waves to some extent,[31]and in 1926Erwin Schrödingerused this idea to develop theSchrödinger equation, which describes electrons as three-dimensionalwaveformsrather than points in space.[32]A consequence of using waveforms to describe particles is that it is mathematically impossible to obtain precise values for both thepositionandmomentumof a particle at a given point in time. This became known as theuncertainty principle, formulated by Werner Heisenberg in 1927.[30]In this concept, for a given accuracy in measuring a position one could only obtain a range of probable values for momentum, and vice versa.[33]Thus, the planetary model of the atom was discarded in favor of one that describedatomic orbitalzones around the nucleus where a given electron is most likely to be found.[34][35]This model was able to explain observations of atomic behavior that previous models could not, such as certain structural andspectralpatterns of atoms larger than hydrogen.
Though the wordatomoriginally denoted a particle that cannot be cut into smaller particles, in modern scientific usage the atom is composed of varioussubatomic particles. The constituent particles of an atom are theelectron, theproton, and theneutron.
The electron is the least massive of these particles by four orders of magnitude at9.11×10−31kg, with a negativeelectrical chargeand a size that is too small to be measured using available techniques.[36]It was the lightest particle with a positive rest mass measured, until the discovery ofneutrinomass. Under ordinary conditions, electrons are bound to the positively charged nucleus by the attraction created from opposite electric charges. If an atom has more or fewer electrons than its atomic number, then it becomes respectively negatively or positively charged as a whole; a charged atom is called anion. Electrons have been known since the late 19th century, mostly thanks toJ.J. Thomson; seehistory of subatomic physicsfor details.
Protons have a positive charge and a mass of1.6726×10−27kg. The number of protons in an atom is called itsatomic number.Ernest Rutherford(1919) observed that nitrogen under alpha-particle bombardment ejects what appeared to be hydrogen nuclei. By 1920 he had accepted that the hydrogen nucleus is a distinct particle within the atom and named itproton.
Neutrons have no electrical charge and have a mass of1.6749×10−27kg.[37][38]Neutrons are the heaviest of the three constituent particles, but their mass can be reduced by thenuclear binding energy. Neutrons and protons (collectively known asnucleons) have comparable dimensions—on the order of2.5×10−15m—although the 'surface' of these particles is not sharply defined.[39]The neutron was discovered in 1932 by the English physicistJames Chadwick.
In theStandard Modelof physics, electrons are truly elementary particles with no internal structure, whereas protons and neutrons are composite particles composed ofelementary particlescalledquarks. There are two types of quarks in atoms, each having a fractional electric charge. Protons are composed of twoup quarks(each with charge +⁠2/3⁠) and onedown quark(with a charge of −⁠1/3⁠). Neutrons consist of one up quark and two down quarks. This distinction accounts for the difference in mass and charge between the two particles.[40][41]
The quarks are held together by thestrong interaction(or strong force), which is mediated bygluons. The protons and neutrons, in turn, are held to each other in the nucleus by thenuclear force, which is a residuum of the strong force that has somewhat different range-properties (see the article on the nuclear force for more). The gluon is a member of the family ofgauge bosons, which are elementary particles that mediate physical forces.[40][41]
All the bound protons and neutrons in an atom make up a tinyatomic nucleus, and are collectively callednucleons. The radius of a nucleus is approximately equal to1.07A3{\displaystyle 1.07{\sqrt[{3}]{A}}}femtometres, whereA{\displaystyle A}is the total number of nucleons.[42]This is much smaller than the radius of the atom, which is on the order of 105fm. The nucleons are bound together by a short-ranged attractive potential called theresidual strong force. At distances smaller than 2.5 fm this force is much more powerful than theelectrostatic forcethat causes positively charged protons to repel each other.[43]
Atoms of the sameelementhave the same number of protons, called theatomic number. Within a single element, the number of neutrons may vary, determining theisotopeof that element. The total number of protons and neutrons determine thenuclide. The number of neutrons relative to the protons determines the stability of the nucleus, with certain isotopes undergoingradioactive decay.[44]
The proton, the electron, and the neutron are classified asfermions. Fermions obey thePauli exclusion principlewhich prohibitsidenticalfermions, such as multiple protons, from occupying the same quantum state at the same time. Thus, every proton in the nucleus must occupy a quantum state different from all other protons, and the same applies to all neutrons of the nucleus and to all electrons of the electron cloud.[45]
A nucleus that has a different number of protons than neutrons can potentially drop to a lower energy state through a radioactive decay that causes the number of protons and neutrons to more closely match. As a result, atoms with matching numbers of protons and neutrons are more stable against decay, but with increasing atomic number, the mutual repulsion of the protons requires an increasing proportion of neutrons to maintain the stability of the nucleus.[45]
The number of protons and neutrons in the atomic nucleus can be modified, although this can require very high energies because of the strong force.Nuclear fusionoccurs when multiple atomic particles join to form a heavier nucleus, such as through the energetic collision of two nuclei. For example, at the core of the Sun protons require energies of 3 to 10 keV to overcome their mutual repulsion—thecoulomb barrier—and fuse together into a single nucleus.[46]Nuclear fissionis the opposite process, causing a nucleus to split into two smaller nuclei—usually through radioactive decay. The nucleus can also be modified through bombardment by high energy subatomic particles or photons. If this modifies the number of protons in a nucleus, the atom changes to a different chemical element.[47][48]
If the mass of the nucleus following a fusion reaction is less than the sum of the masses of the separate particles, then the difference between these two values can be emitted as a type of usable energy (such as agamma ray, or the kinetic energy of abeta particle), as described byAlbert Einstein'smass–energy equivalenceformula,E = mc2, wheremis the mass loss andcis thespeed of light. This deficit is part of thebinding energyof the new nucleus, and it is the non-recoverable loss of the energy that causes the fused particles to remain together in a state that requires this energy to separate.[49]
The fusion of two nuclei that create larger nuclei with lower atomic numbers thanironandnickel—a total nucleon number of about 60—is usually anexothermic processthat releases more energy than is required to bring them together.[50]It is this energy-releasing process that makes nuclear fusion instarsa self-sustaining reaction.  For heavier nuclei, the binding energy pernucleonbegins to decrease. That means that a fusion process producing a nucleus that has an atomic number higher than about 26, and amass numberhigher than about 60, is anendothermic process. Thus, more massive nuclei cannot undergo an energy-producing fusion reaction that can sustain thehydrostatic equilibriumof a star.[45]
The electrons in an atom are attracted to the protons in the nucleus by theelectromagnetic force. This force binds the electrons inside anelectrostaticpotential wellsurrounding the smaller nucleus, which means that an external source of energy is needed for the electron to escape. The closer an electron is to the nucleus, the greater the attractive force. Hence electrons bound near the center of the potential well require more energy to escape than those at greater separations.
Electrons, like other particles, have properties of both aparticle and a wave. The electron cloud is a region inside the potential well where each electron forms a type of three-dimensionalstanding wave—a wave form that does not move relative to the nucleus. This behavior is defined by anatomic orbital, a mathematical function that characterises the probability that an electron appears to be at a particular location when its position is measured.[51]Only a discrete (orquantized) set of these orbitals exist around the nucleus, as other possible wave patterns rapidly decay into a more stable form.[52]Orbitals can have one or more ring or node structures, and differ from each other in size, shape and orientation.[53]
Each atomic orbital corresponds to a particularenergy levelof the electron. The electron can change its state to a higher energy level by absorbing aphotonwith sufficient energy to boost it into the new quantum state. Likewise, throughspontaneous emission, an electron in a higher energy state can drop to a lower energy state while radiating the excess energy as a photon. These characteristic energy values, defined by the differences in the energies of the quantum states, are responsible foratomic spectral lines.[52]
The amount of energy needed to remove or add an electron—theelectron binding energy—is far less than thebinding energy of nucleons. For example, it requires only 13.6 eV to strip aground-stateelectron from a hydrogen atom,[54]compared to 2.23millioneV for splitting adeuteriumnucleus.[55]Atoms areelectricallyneutral if they have an equal number of protons and electrons. Atoms that have either a deficit or a surplus of electrons are calledions. Electrons that are farthest from the nucleus may be transferred to other nearby atoms or shared between atoms. By this mechanism, atoms are able tobondintomoleculesand other types ofchemical compoundslikeionicandcovalentnetworkcrystals.[56]
By definition, any two atoms with an identical number ofprotonsin their nuclei belong to the samechemical element. Atoms with equal numbers of protons but a different number ofneutronsare different isotopes of the same element. For example, all hydrogen atoms admit exactly one proton, but isotopes exist with no neutrons (hydrogen-1, by far the most common form,[57]also called protium), one neutron (deuterium), two neutrons (tritium) andmore than two neutrons. The known elements form a set of atomic numbers, from the single-proton elementhydrogenup to the 118-proton elementoganesson.[58]All known isotopes of elements with atomic numbers greater than 82 are radioactive, although the radioactivity of element 83 (bismuth) is so slight as to be practically negligible.[59][60]
About 339 nuclides occur naturally onEarth,[61]of which 251 (about 74%) have not been observed to decay, and are referred to as "stable isotopes". Only 90 nuclides are stabletheoretically, while another 161 (bringing the total to 251) have not been observed to decay, even though in theory it is energetically possible. These are also formally classified as "stable". An additional 35 radioactive nuclides have half-lives longer than 100 million years, and are long-lived enough to have been present since the birth of theSolar System. This collection of 286 nuclides are known asprimordial nuclides. Finally, an additional 53 short-lived nuclides are known to occur naturally, as daughter products of primordial nuclide decay (such asradiumfromuranium), or as products of natural energetic processes on Earth, such as cosmic ray bombardment (for example, carbon-14).[62][note 1]
For 80 of the chemical elements, at least onestable isotopeexists. As a rule, there is only a handful of stable isotopes for each of these elements, the average being 3.1 stable isotopes per element. Twenty-six "monoisotopic elements" have only a single stable isotope, while the largest number of stable isotopes observed for any element is ten, for the elementtin. Elements43,61, and all elements numbered83or higher have no stable isotopes.[63]: 1–12
Stability of isotopes is affected by the ratio of protons to neutrons, and also by the presence of certain "magic numbers" of neutrons or protons that represent closed and filled quantum shells. These quantum shells correspond to a set of energy levels within theshell modelof the nucleus; filled shells, such as the filled shell of 50 protons for tin, confers unusual stability on the nuclide. Of the 251 known stable nuclides, only four have both an odd number of protonsandodd number of neutrons:hydrogen-2(deuterium),lithium-6,boron-10, andnitrogen-14. (Tantalum-180mis odd-odd and observationally stable, but is predicted to decay with a very long half-life.) Also, only four naturally occurring, radioactive odd-odd nuclides have a half-life over a billion years:potassium-40,vanadium-50,lanthanum-138, andlutetium-176. Most odd-odd nuclei are highly unstable with respect tobeta decay, because the decay products are even-even, and are therefore more strongly bound, due tonuclear pairing effects.[64]
The large majority of an atom's mass comes from the protons and neutrons that make it up. The total number of these particles (called "nucleons") in a given atom is called themass number. It is a positive integer and dimensionless (instead of having dimension of mass), because it expresses a count. An example of use of a mass number is "carbon-12," which has 12 nucleons (six protons and six neutrons).
The actualmass of an atom at restis often expressed indaltons(Da), also called the unified atomic mass unit (u). This unit is defined as a twelfth of the mass of a free neutral atom ofcarbon-12, which is approximately1.66×10−27kg.[65]Hydrogen-1(the lightest isotope of hydrogen which is also the nuclide with the lowest mass) has an atomic weight of 1.007825 Da.[66]The value of this number is called theatomic mass. A given atom has an atomic mass approximately equal (within 1%) to its mass number times the atomic mass unit (for example the mass of a nitrogen-14 is roughly 14 Da), but this number will not be exactly an integer except (by definition) in the case of carbon-12.[67]The heavieststable atomis lead-208,[59]with a mass of207.9766521Da.[68]
As even the most massive atoms are far too light to work with directly, chemists instead use the unit ofmoles. One mole of atoms of any element always has the same number of atoms (about6.022×1023). This number was chosen so that if an element has an atomic mass of 1 u, a mole of atoms of that element has a mass close to one gram. Because of the definition of theunified atomic mass unit, each carbon-12 atom has an atomic mass of exactly 12 Da, and so a mole of carbon-12 atoms weighs exactly 0.012 kg.[65]
Atoms lack a well-defined outer boundary, so their dimensions are usually described in terms of anatomic radius. This is a measure of the distance out to which the electron cloud extends from the nucleus.[69]This assumes the atom to exhibit a spherical shape, which is only obeyed for atoms in vacuum or free space. Atomic radii may be derived from the distances between two nuclei when the two atoms are joined in achemical bond. The radius varies with the location of an atom on the atomic chart, the type of chemical bond, the number of neighboring atoms (coordination number) and aquantum mechanicalproperty known asspin.[70]On theperiodic tableof the elements, atom size tends to increase when moving down columns, but decrease when moving across rows (left to right).[71]Consequently, the smallest atom isheliumwith a radius of 32pm, while one of the largest iscaesiumat 225 pm.[72]
When subjected to external forces, likeelectrical fields, the shape of an atom may deviate fromspherical symmetry. The deformation depends on the field magnitude and the orbital type of outer shell electrons, as shown bygroup-theoreticalconsiderations. Aspherical deviations might be elicited for instance incrystals, where large crystal-electrical fields may occur atlow-symmetrylattice sites.[73][74]Significantellipsoidaldeformations have been shown to occur for sulfur ions[75]andchalcogenions[76]inpyrite-type compounds.
Atomic dimensions are thousands of times smaller than the wavelengths oflight(400–700nm) so they cannot be viewed using anoptical microscope, although individual atoms can be observed using ascanning tunneling microscope. To visualize the minuteness of the atom, consider that a typical human hair is about 1 million carbon atoms in width.[77]A single drop of water contains about 2sextillion(2×1021) atoms of oxygen, and twice the number of hydrogen atoms.[78]A singlecaratdiamondwith a mass of2×10−4kgcontains about 10 sextillion (1022) atoms ofcarbon.[note 2]If an apple were magnified to the size of the Earth, then the atoms in the apple would be approximately the size of the original apple.[79]
Every element has one or more isotopes that have unstable nuclei that are subject to radioactive decay, causing the nucleus to emit particles or electromagnetic radiation. Radioactivity can occur when the radius of a nucleus is large compared with the radius of the strong force, which only acts over distances on the order of 1 fm.[80]
The most common forms of radioactive decay are:[81][82]
Other more rare types ofradioactive decayinclude ejection of neutrons or protons or clusters ofnucleonsfrom a nucleus, or more than onebeta particle. An analog of gamma emission which allows excited nuclei to lose energy in a different way, isinternal conversion—a process that produces high-speed electrons that are not beta rays, followed by production of high-energy photons that are not gamma rays. A few large nuclei explode into two or more charged fragments of varying masses plus several neutrons, in a decay calledspontaneous nuclear fission.
Eachradioactive isotopehas a characteristic decay time period—thehalf-life—that is determined by the amount of time needed for half of a sample to decay. This is anexponential decayprocess that steadily decreases the proportion of the remaining isotope by 50% every half-life. Hence after two half-lives have passed only 25% of the isotope is present, and so forth.[80]
Elementary particles possess an intrinsic quantum mechanical property known asspin. This is analogous to theangular momentumof an object that is spinning around itscenter of mass, although strictly speaking these particles are believed to be point-like and cannot be said to be rotating. Spin is measured in units of the reducedPlanck constant(ħ), with electrons, protons and neutrons all having spin1⁄2ħ, or "spin-1⁄2". In an atom, electrons in motion around thenucleuspossess orbitalangular momentumin addition to their spin, while the nucleus itself possesses angular momentum due to its nuclear spin.[83]
Themagnetic fieldproduced by an atom—itsmagnetic moment—is determined by these various forms of angular momentum, just as a rotating charged object classically produces a magnetic field, but the most dominant contribution comes from electron spin. Due to the nature of electrons to obey thePauli exclusion principle, in which no two electrons may be found in the samequantum state, bound electrons pair up with each other, with one member of each pair in a spin up state and the other in the opposite, spin down state. Thus these spins cancel each other out, reducing the total magnetic dipole moment to zero in some atoms with even number of electrons.[84]
Inferromagneticelements such as iron, cobalt and nickel, an odd number of electrons leads to an unpaired electron and a net overall magnetic moment. The orbitals of neighboring atoms overlap and a lower energy state is achieved when the spins of unpaired electrons are aligned with each other, a spontaneous process known as anexchange interaction. When the magnetic moments of ferromagnetic atoms are lined up, the material can produce a measurable macroscopic field.Paramagnetic materialshave atoms with magnetic moments that line up in random directions when no magnetic field is present, but the magnetic moments of the individual atoms line up in the presence of a field.[84][85]
The nucleus of an atom will have no spin when it has even numbers of both neutrons and protons, but for other cases of odd numbers, the nucleus may have a spin. Normally nuclei with spin are aligned in random directions because ofthermal equilibrium, but for certain elements (such asxenon-129) it is possible topolarizea significant proportion of the nuclear spin states so that they are aligned in the same direction—a condition calledhyperpolarization. This has important applications inmagnetic resonance imaging.[86][87]
Thepotential energyof an electron in an atom isnegativerelative to when thedistancefrom the nucleusgoes to infinity; its dependence on the electron'spositionreaches theminimuminside the nucleus, roughly ininverse proportionto the distance. In the quantum-mechanical model, a bound electron can occupy only a set ofstatescentered on the nucleus, and each state corresponds to a specificenergy level; seetime-independent Schrödinger equationfor a theoretical explanation. An energy level can be measured by theamount of energy needed to unbindthe electron from the atom, and is usually given in units ofelectronvolts(eV). The lowest energy state of a bound electron is called the ground state, i.e.,stationary state, while an electron transition to a higher level results in an excited state.[88]The electron's energy increases along withnbecause the (average) distance to the nucleus increases. Dependence of the energy onℓis caused not by theelectrostatic potentialof the nucleus, but by interaction between electrons.
For an electron totransition between two different states, e.g.ground stateto firstexcited state, it must absorb or emit aphotonat an energy matching the difference in the potential energy of those levels, according to theNiels Bohrmodel, what can be precisely calculated by theSchrödinger equation. Electrons jump between orbitals in a particle-like fashion. For example, if a single photon strikes the electrons, only a single electron changes states in response to the photon; seeElectron properties.
The energy of an emitted photon is proportional to itsfrequency, so these specific energy levels appear as distinct bands in theelectromagnetic spectrum.[89]Each element has a characteristic spectrum that can depend on the nuclear charge, subshells filled by electrons, the electromagnetic interactions between the electrons and other factors.[90]
When a continuousspectrum of energyis passed through a gas or plasma, some of the photons are absorbed by atoms, causing electrons to change their energy level. Those excited electrons that remain bound to their atom spontaneously emit this energy as a photon, traveling in a random direction, and so drop back to lower energy levels. Thus the atoms behave like a filter that forms a series of darkabsorption bandsin the energy output. (An observer viewing the atoms from a view that does not include the continuous spectrum in the background, instead sees a series ofemission linesfrom the photons emitted by the atoms.)Spectroscopicmeasurements of the strength and width ofatomic spectral linesallow the composition and physical properties of a substance to be determined.[91]
Close examination of the spectral lines reveals that some display afine structuresplitting. This occurs because ofspin–orbit coupling, which is an interaction between the spin and motion of the outermost electron.[92]When an atom is in an external magnetic field, spectral lines become split into three or more components; a phenomenon called theZeeman effect. This is caused by the interaction of the magnetic field with the magnetic moment of the atom and its electrons. Some atoms can have multipleelectron configurationswith the same energy level, which thus appear as a single spectral line. The interaction of the magnetic field with the atom shifts these electron configurations to slightly different energy levels, resulting in multiple spectral lines.[93]The presence of an externalelectric fieldcan cause a comparable splitting and shifting of spectral lines by modifying the electron energy levels, a phenomenon called theStark effect.[94]
If a bound electron is in an excited state, an interacting photon with the proper energy can causestimulated emissionof a photon with a matching energy level. For this to occur, the electron must drop to a lower energy state that has an energy difference matching the energy of the interacting photon. The emitted photon and the interacting photon then move off in parallel and with matching phases. That is, the wave patterns of the two photons are synchronized. This physical property is used to makelasers, which can emit a coherent beam of light energy in a narrow frequency band.[95]
Valency is the combining power of an element. It is determined by the number of bonds it can form to other atoms or groups.[96]The outermost electron shell of an atom in its uncombined state is known as thevalence shell, and the electrons in
that shell are calledvalence electrons. The number of valence electrons determines thebondingbehavior with other atoms. Atoms tend tochemically reactwith each other in a manner that fills (or empties) their outer valence shells.[97]For example, a transfer of a single electron between atoms is a useful approximation for bonds that form between atoms with one-electron more than a filled shell, and others that are one-electron short of a full shell, such as occurs in the compoundsodium chlorideand other chemical ionic salts. Many elements display multiple valences, or tendencies to share differing numbers of electrons in different compounds. Thus,chemical bondingbetween these elements takes many forms of electron-sharing that are more than simple electron transfers. Examples include the element carbon and theorganic compounds.[98]
Thechemical elementsare often displayed in aperiodic tablethat is laid out to display recurring chemical properties, and elements with the same number of valence electrons form a group that is aligned in the same column of the table. (The horizontal rows correspond to the filling of a quantum shell of electrons.) The elements at the far right of the table have their outer shell completely filled with electrons, which results in chemically inert elements known as thenoble gases.[99][100]
Quantities of atoms are found in different states of matter that depend on the physical conditions, such astemperatureandpressure. By varying the conditions, materials can transition betweensolids,liquids,gases, andplasmas.[101]Within a state, a material can also exist in differentallotropes. An example of this is solid carbon, which can exist asgraphiteordiamond.[102]Gaseous allotropes exist as well, such asdioxygenandozone.
At temperatures close toabsolute zero, atoms can form aBose–Einstein condensate, at which point quantum mechanical effects, which are normally only observed at the atomic scale, become apparent on a macroscopic scale.[103][104]This super-cooled collection of atoms then behaves as a singlesuper atom, which may allow fundamental checks of quantum mechanical behavior.[105]
While atoms are too small to be seen, devices such as thescanning tunneling microscope(STM) enable their visualization at the surfaces of solids. The microscope uses thequantum tunnelingphenomenon, which allows particles to pass through a barrier that would be insurmountable in the classical perspective. Electrons tunnel through the vacuum between twobiasedelectrodes, providing a tunneling current that is exponentially dependent on their separation. One electrode is a sharp tip ideally ending with a single atom. At each point of the scan of the surface the tip's height is adjusted so as to keep the tunneling current at a set value. How much the tip moves to and away from the surface is interpreted as the height profile. For low bias, the microscope images the averaged electron orbitals across closely packed energy levels—the localdensity of the electronic statesnear theFermi level.[106][107]Because of the distances involved, both electrodes need to be extremely stable; only then periodicities can be observed that correspond to individual atoms. The method alone is not chemically specific, and cannot identify the atomic species present at the surface.
Atoms can be easily identified by their mass. If an atom isionizedby removing one of its electrons, its trajectory when it passes through amagnetic fieldwill bend. The radius by which the trajectory of a moving ion is turned by the magnetic field is determined by the mass of the atom. Themass spectrometeruses this principle to measure themass-to-charge ratioof ions. If a sample contains multiple isotopes, the mass spectrometer can determine the proportion of each isotope in the sample by measuring the intensity of the different beams of ions. Techniques to vaporize atoms includeinductively coupled plasma atomic emission spectroscopyandinductively coupled plasma mass spectrometry, both of which use a plasma to vaporize samples for analysis.[108]
Theatom-probe tomographhas sub-nanometer resolution in 3-D and can chemically identify individual atoms usingtime-of-flight mass spectrometry.[109]
Electron emission techniques such asX-ray photoelectron spectroscopy(XPS) andAuger electron spectroscopy(AES), which measure the binding energies of thecore electrons, are used to identify the atomic species present in a sample in a non-destructive way. With proper focusing both can be made area-specific. Another such method iselectron energy loss spectroscopy(EELS), which measures the energy loss of anelectron beamwithin atransmission electron microscopewhen it interacts with a portion of a sample.
Spectra ofexcited statescan be used to analyze the atomic composition of distantstars. Specific lightwavelengthscontained in the observed light from stars can be separated out and related to the quantized transitions in free gas atoms. These colors can be replicated using agas-discharge lampcontaining the same element.[110]Heliumwas discovered in this way in the spectrum of the Sun 23 years before it was found on Earth.[111]
Baryonic matterforms about 4% of the total energy density of theobservable universe, with an average density of about 0.25 particles/m3(mostlyprotonsand electrons).[112]Within a galaxy such as theMilky Way, particles have a much higher concentration, with the density of matter in theinterstellar medium(ISM) ranging from 105to 109atoms/m3.[113]The Sun is believed to be inside theLocal Bubble, so the density in thesolar neighborhoodis only about 103atoms/m3.[114]Stars form from dense clouds in the ISM, and the evolutionary processes of stars result in the steady enrichment of the ISM with elements more massive than hydrogen and helium.
Up to 95% of the Milky Way's baryonic matter are concentrated inside stars, where conditions are unfavorable for atomic matter. The total baryonic mass is about 10% of the mass of the galaxy;[115]the remainder of the mass is an unknowndark matter.[116]Hightemperatureinside stars makes most "atoms" fully ionized, that is, separatesallelectrons from the nuclei. Instellar remnants—with exception of their surface layers—an immensepressuremake electron shells impossible.
Electrons are thought to exist in the Universe since early stages of theBig Bang. Atomic nuclei forms innucleosynthesisreactions. In about three minutesBig Bang nucleosynthesisproduced most of thehelium,lithium, anddeuteriumin the Universe, and perhaps some of theberylliumandboron.[117][118][119]
Ubiquitousness and stability of atoms relies on theirbinding energy, which means that an atom has a lower energy than an unbound system of the nucleus and electrons. Where thetemperatureis much higher thanionization potential, the matter exists in the form ofplasma—a gas of positively charged ions (possibly, bare nuclei) and electrons. When the temperature drops below the ionization potential, atoms becomestatisticallyfavorable. Atoms (complete with bound electrons) became to dominate overcharged particles380,000 years after the Big Bang—an epoch calledrecombination, when the expanding Universe cooled enough to allow electrons to become attached to nuclei.[120]
Since the Big Bang, which produced nocarbonorheavier elements, atomic nuclei have been combined instarsthrough the process ofnuclear fusionto produce more of the elementhelium, and (via thetriple-alpha process) the sequence of elements from carbon up toiron;[121]seestellar nucleosynthesisfor details.
Isotopes such as lithium-6, as well as some beryllium and boron are generated in space throughcosmic ray spallation.[122]This occurs when a high-energy proton strikes an atomic nucleus, causing large numbers of nucleons to be ejected.
Elements heavier than iron were produced insupernovaeand collidingneutron starsthrough ther-process, and inAGB starsthrough thes-process, both of which involve the capture of neutrons by atomic nuclei.[123]Elements such asleadformed largely through the radioactive decay of heavier elements.[124]
Most of the atoms that make up theEarthand its inhabitants were present in their current form in thenebulathat collapsed out of amolecular cloudto form theSolar System. The rest are the result of radioactive decay, and their relative proportion can be used to determine theage of the Earththroughradiometric dating.[125][126]Most of theheliumin the crust of the Earth (about 99% of the helium from gas wells, as shown by its lower abundance ofhelium-3) is a product ofalpha decay.[127]
There are a few trace atoms on Earth that were not present at the beginning (i.e., not "primordial"), nor are results of radioactive decay.Carbon-14is continuously generated by cosmic rays in the atmosphere.[128]Some atoms on Earth have been artificially generated either deliberately or as by-products of nuclear reactors or explosions.[129][130]Of thetransuranic elements—those with atomic numbers greater than 92—onlyplutoniumandneptuniumoccur naturally on Earth.[131][132]Transuranic elements have radioactive lifetimes shorter than the current age of the Earth[133]and thus identifiable quantities of these elements have long since decayed, with the exception of traces ofplutonium-244possibly deposited by cosmic dust.[125]Natural deposits of plutonium and neptunium are produced byneutron capturein uranium ore.[134]
The Earth contains approximately1.33×1050atoms.[135]Although small numbers of independent atoms ofnoble gasesexist, such asargon,neon, andhelium, 99% ofthe atmosphereis bound in the form of molecules, includingcarbon dioxideanddiatomicoxygenandnitrogen. At the surface of the Earth, an overwhelming majority of atoms combine to form various compounds, includingwater,salt,silicates, andoxides. Atoms can also combine to create materials that do not consist of discrete molecules, includingcrystalsand liquid or solidmetals.[136][137]This atomic matter forms networked arrangements that lack the particular type of small-scale interrupted order associated with molecular matter.[138]
All nuclides with atomic numbers higher than 82 (lead) are known to be radioactive. No nuclide with an atomic number exceeding 92 (uranium) exists on Earth as aprimordial nuclide, and heavier elements generally have shorter half-lives. Nevertheless, an "island of stability" encompassing relatively long-lived isotopes of superheavy elements[139]with atomic numbers110to114might exist.[140]Predictions for the half-life of the most stable nuclide on the island range from a few minutes to millions of years.[141]In any case, superheavy elements (withZ> 104) would not exist due to increasingCoulombrepulsion (which results inspontaneous fissionwith increasingly short half-lives) in the absence of any stabilizing effects.[142]
Each particle of matter has a correspondingantimatterparticle with the opposite electrical charge. Thus, thepositronis a positively chargedantielectronand theantiprotonis a negatively charged equivalent of aproton. When a matter and corresponding antimatter particle meet, they annihilate each other. Because of this, along with an imbalance between the number of matter and antimatter particles, the latter are rare in the universe. The first causes of this imbalance are not yet fully understood, although theories ofbaryogenesismay offer an explanation. As a result, no antimatter atoms have been discovered in nature.[143][144]In 1996, the antimatter counterpart of the hydrogen atom (antihydrogen) was synthesized at theCERNlaboratory inGeneva.[145][146]
Otherexotic atomshave been created by replacing one of the protons, neutrons or electrons with other particles that have the same charge. For example, an electron can be replaced by a more massivemuon, forming amuonic atom. These types of atoms can be used to test fundamental predictions of physics.[147][148][149]
Thermodynamicsis a branch ofphysicsthat deals withheat,work, andtemperature, and their relation toenergy,entropy, and the physical properties ofmatterandradiation. The behavior of these quantities is governed by the fourlaws of thermodynamics, which convey a quantitative description using measurable macroscopicphysical quantitiesbut may be explained in terms ofmicroscopicconstituents bystatistical mechanics. Thermodynamics applies to various topics inscienceandengineering, especiallyphysical chemistry,biochemistry,chemical engineering, andmechanical engineering, as well as other complex fields such asmeteorology.
Historically, thermodynamics developed out of a desire to increase theefficiencyof earlysteam engines, particularly through the work of French physicistSadi Carnot(1824) who believed that engine efficiency was the key that could help France win theNapoleonic Wars.[1]Scots-Irish physicistLord Kelvinwas the first to formulate a concise definition of thermodynamics in 1854[2]which stated, "Thermo-dynamics is the subject of the relation of heat to forces acting between contiguous parts of bodies, and the relation of heat to electrical agency."  German physicist and mathematicianRudolf Clausiusrestated Carnot's principle known as theCarnot cycleand gave thetheory of heata truer and sounder basis. His most important paper, "On the Moving Force of Heat",[3]published in 1850, first stated thesecond law of thermodynamics. In 1865 he introduced the concept of entropy. In 1870 he introduced thevirial theorem, which applied to heat.[4]
The initial application of thermodynamics tomechanical heat engineswas quickly extended to the study of chemical compounds and chemical reactions.Chemical thermodynamicsstudies the nature of the role of entropy in the process ofchemical reactionsand has provided the bulk of expansion and knowledge of the field. Other formulations of thermodynamics emerged.Statistical thermodynamics, or statistical mechanics, concerns itself withstatisticalpredictions of the collective motion of particles from their microscopic behavior. In 1909,Constantin Carathéodorypresented a purely mathematical approach in anaxiomaticformulation, a description often referred to asgeometrical thermodynamics.
A description of any thermodynamic system employs the fourlaws of thermodynamicsthat form an axiomatic basis.The first lawspecifies that energy can be transferred between physical systems asheat, aswork, and with transfer of matter.[5]The second lawdefines the existence of a quantity calledentropy, that describes the direction, thermodynamically, that a system can evolve and quantifies the state of order of a system and that can be used to quantify the useful work that can be extracted from the system.[6]
In thermodynamics, interactions between large ensembles of objects are studied and categorized. Central to this are the concepts of the thermodynamicsystemand itssurroundings. A system is composed of particles, whose average motions define its properties, and those properties are in turn related to one another throughequations of state. Properties can be combined to expressinternal energyandthermodynamic potentials, which are useful for determining conditions forequilibriumandspontaneous processes.
With these tools, thermodynamics can be used to describe how systems respond to changes in their environment. This can be applied to a wide variety of topics inscienceandengineering, such asengines,phase transitions,chemical reactions,transport phenomena, and evenblack holes. The results of thermodynamics are essential for other fields ofphysicsand forchemistry,chemical engineering,corrosion engineering,aerospace engineering,mechanical engineering,cell biology,biomedical engineering,materials science, andeconomics, to name a few.[7][8]
This article is focused mainly on classical thermodynamics which primarily studies systems inthermodynamic equilibrium.Non-equilibrium thermodynamicsis often treated as an extension of the classical treatment, but statistical mechanics has brought many advances to that field.
Thehistory of thermodynamicsas a scientific discipline generally begins withOtto von Guerickewho, in 1650, built and designed the world's firstvacuum pumpand demonstrated avacuumusing hisMagdeburg hemispheres. Guericke was driven to make a vacuum in order to disproveAristotle's long-held supposition that 'nature abhors a vacuum'. Shortly after Guericke, the Anglo-Irish physicist and chemistRobert Boylehad learned of Guericke's designs and, in 1656, in coordination with English scientistRobert Hooke, built an air pump.[10]Using this pump, Boyle and Hooke noticed a correlation betweenpressure,temperature, andvolume. In time,Boyle's Lawwas formulated, which states that pressure and volume areinversely proportional. Then, in 1679, based on these concepts, an associate of Boyle's namedDenis Papinbuilt asteam digester, which was a closed vessel with a tightly fitting lid that confined steam until a high pressure was generated.
Later designs implemented a steam release valve that kept the machine from exploding. By watching the valve rhythmically move up and down, Papin conceived of the idea of apistonand a cylinder engine. He did not, however, follow through with his design. Nevertheless, in 1697, based on Papin's designs, engineerThomas Saverybuilt the first engine, followed byThomas Newcomenin 1712. Although these early engines were crude and inefficient, they attracted the attention of the leading scientists of the time.
The fundamental concepts ofheat capacityandlatent heat, which were necessary for the development of thermodynamics, were developed by ProfessorJoseph Blackat the University of Glasgow, whereJames Wattwas employed as an instrument maker. Black and Watt performed experiments together, but it was Watt who conceived the idea of theexternal condenserwhich resulted in a large increase insteam engineefficiency.[11]Drawing on all the previous work ledSadi Carnot, the "father of thermodynamics", to publishReflections on the Motive Power of Fire(1824), a discourse on heat, power, energy and engine efficiency. The book outlined the basic energetic relations between theCarnot engine, theCarnot cycle, and motive power. It marked the start of thermodynamics as a modern science.[12]
The first thermodynamic textbook was written in 1859 byWilliam Rankine, originally trained as a physicist and a civil and mechanical engineering professor at theUniversity of Glasgow.[13]The first and second laws of thermodynamics emerged simultaneously in the 1850s, primarily out of the works of William Rankine,Rudolf Clausius, andWilliam Thomson(Lord Kelvin).
The foundations of statistical thermodynamics were set out by physicists such asJames Clerk Maxwell,Ludwig Boltzmann,Max Planck,Rudolf ClausiusandJ. Willard Gibbs.
Clausius, who first stated the basic ideas of the second law in his paper "On the Moving Force of Heat",[3]published in 1850, and is called "one of the founding fathers of thermodynamics",[14]introduced the concept ofentropyin 1865.
During the years 1873–76 the American mathematical physicistJosiah Willard Gibbspublished a series of three papers, the most famous beingOn the Equilibrium of Heterogeneous Substances,[15]in which he showed howthermodynamic processes, includingchemical reactions, could be graphically analyzed, by studying theenergy,entropy,volume,temperatureandpressureof thethermodynamic systemin such a manner, one can determine if a process would occur spontaneously.[16]AlsoPierre Duhemin the 19th century wrote about chemical thermodynamics.[17]During the early 20th century, chemists such asGilbert N. Lewis,Merle Randall,[18]andE. A. Guggenheim[19][20]applied the mathematical methods of Gibbs to the analysis of chemical processes.
By a surface-level analysis, the word consists of two parts that can be traced back to Ancient Greek. Firstly,thermo-("of heat"; used in words such asthermometer) can be traced back to the rootθέρμηtherme, meaning "heat". Secondly, the worddynamics("science of force [or power]")[22]can be traced back to the rootδύναμιςdynamis, meaning "power".[23][24]
In 1849, the adjectivethermo-dynamicis used by William Thomson.[25][26]
In 1854, the nounthermo-dynamicsis used by Thomson and William Rankine to represent the science of generalized heat engines.[26][21]
Pierre Perrot claims that the termthermodynamicswas coined byJames Joulein 1858 to designate the science of relations between heat and power,[12]however, Joule never used that term, but used instead the termperfect thermo-dynamic enginein reference to Thomson's 1849[25]phraseology.[21]
The study of thermodynamical systems has developed into several related branches, each using a different fundamental model as a theoretical or experimental basis, or applying the principles to varying types of systems.
Classical thermodynamics is the description of the states of thermodynamic systems at near-equilibrium, that uses macroscopic, measurable properties. It is used to model exchanges of energy, work and heat based on thelaws of thermodynamics. The qualifierclassicalreflects the fact that it represents the first level of understanding of the subject as it developed in the 19th century and describes the changes of a system in terms of macroscopic empirical (large scale, and measurable) parameters. A microscopic interpretation of these concepts was later provided by the development ofstatistical mechanics.
Statistical mechanics, also known as statistical thermodynamics, emerged with the development of atomic and molecular theories in the late 19th century and early 20th century, and supplemented classical thermodynamics with an interpretation of the microscopic interactions between individual particles or quantum-mechanical states. This field relates the microscopic properties of individual atoms and molecules to the macroscopic, bulk properties of materials that can be observed on the human scale, thereby explaining classical thermodynamics as a natural result of statistics, classical mechanics, andquantum theoryat the microscopic level.
Chemical thermodynamicsis the study of the interrelation ofenergywithchemical reactionsor with a physical change ofstatewithin the confines of thelaws of thermodynamics. The primary objective of chemical thermodynamics is determining the spontaneity of a given transformation.[27]
Equilibrium thermodynamicsis the study of transfers of matter and energy in systems or bodies that, by agencies in their surroundings, can be driven from one state of thermodynamic equilibrium to another. The term 'thermodynamic equilibrium' indicates a state of balance, in which all macroscopic flows are zero; in the case of the simplest systems or bodies, their intensive properties are homogeneous, and their pressures are perpendicular to their boundaries. In an equilibrium state there are no unbalanced potentials, or driving forces, between macroscopically distinct parts of the system. A central aim in equilibrium thermodynamics is: given a system in a well-defined initial equilibrium state, and given its surroundings, and given its constitutive walls, to calculate what will be the final equilibrium state of the system after a specified thermodynamic operation has changed its walls or surroundings.
Non-equilibrium thermodynamicsis a branch of thermodynamics that deals with systems that are not inthermodynamic equilibrium. Most systems found in nature are not in thermodynamic equilibrium because they are not in stationary states, and are continuously and discontinuously subject to flux of matter and energy to and from other systems. The thermodynamic study of non-equilibrium systems requires more general concepts than are dealt with by equilibrium thermodynamics.[28]Many natural systems still today remain beyond the scope of currently known macroscopic thermodynamic methods.
Thermodynamics is principally based on a set of four laws which are universally valid when applied to systems that fall within the constraints implied by each. In the various theoretical descriptions of thermodynamics these laws may be expressed in seemingly differing forms, but the most prominent formulations are the following.
Thezeroth law of thermodynamicsstates:If two systems are each in thermal equilibrium with a third, they are also in thermal equilibrium with each other.
This statement implies that thermal equilibrium is anequivalence relationon the set ofthermodynamic systemsunder consideration. Systems are said to be in equilibrium if the small, random exchanges between them (e.g.Brownian motion) do not lead to a net change in energy. This law is tacitly assumed in every measurement of temperature. Thus, if one seeks to decide whether two bodies are at the sametemperature, it is not necessary to bring them into contact and measure any changes of their observable properties in time.[29]The law provides an empirical definition of temperature, and justification for the construction of practical thermometers.
The zeroth law was not initially recognized as a separate law of thermodynamics, as its basis in thermodynamical equilibrium was implied in the other laws. The first, second, and third laws had been explicitly stated already, and found common acceptance in the physics community before the importance of the zeroth law for the definition of temperature was realized. As it was impractical to renumber the other laws, it was named thezeroth law.
Thefirst law of thermodynamicsstates:In a process without transfer of matter, the change ininternal energy,ΔU{\displaystyle \Delta U}, of athermodynamic systemis equal to the energy gained as heat,Q{\displaystyle Q}, less the thermodynamic work,W{\displaystyle W}, done by the system on its surroundings.[33][nb 1]
whereΔU{\displaystyle \Delta U}denotes the change in the internal energy of aclosed system(for which heat or work through the system boundary are possible, but matter transfer is not possible),Q{\displaystyle Q}denotes the quantity of energy suppliedtothe system as heat, andW{\displaystyle W}denotes the amount of thermodynamic work donebythe systemonits surroundings. An equivalent statement is thatperpetual motion machinesof the first kind are impossible; workW{\displaystyle W}done by a system on its surrounding requires that the system's internal energyU{\displaystyle U}decrease or be consumed, so that the amount of internal energy lost by that work must be resupplied as heatQ{\displaystyle Q}by an external energy source or as work by an external machine acting on the system (so thatU{\displaystyle U}is recovered) to make the system work continuously.
For processes that include transfer of matter, a further statement is needed:With due account of the respective fiducial reference states of the systems, when two systems, which may be of different chemical compositions, initially separated only by an impermeable wall, and otherwise isolated, are combined into a new system by the thermodynamic operation of removal of the wall, then
whereU0denotes the internal energy of the combined system, andU1andU2denote the internal energies of the respective separated systems.
Adapted for thermodynamics, this law is an expression of the principle ofconservation of energy, which states that energy can be transformed (changed from one form to another), but cannot be created or destroyed.[34]
Internal energy is a principal property of thethermodynamic state, while heat and work are modes of energy transfer by which a process may change this state. A change of internal energy of a system may be achieved by any combination of heat added or removed and work performed on or by the system. As afunction of state, the internal energy does not depend on the manner, or on the path through intermediate steps, by which the system arrived at its state.
A traditional version of thesecond law of thermodynamicsstates:Heat does not spontaneously flow from a colder body to a hotter body.
The second law refers to a system of matter and radiation, initially with inhomogeneities in temperature, pressure, chemical potential, and otherintensive properties, that are due to internal 'constraints', or impermeable rigid walls, within it, or to externally imposed forces. The law observes that, when the system is isolated from the outside world and from those forces, there is a definite thermodynamic quantity, itsentropy, that increases as the constraints are removed, eventually reaching a maximum value at thermodynamic equilibrium, when the inhomogeneities practically vanish. For systems that are initially far from thermodynamic equilibrium, though several have been proposed, there is known no general physical principle that determines the rates of approach to thermodynamic equilibrium, and thermodynamics does not deal with such rates. The many versions of the second law all express the generalirreversibilityof the transitions involved in systems approaching thermodynamic equilibrium.
In macroscopic thermodynamics, the second law is a basic observation applicable to any actual thermodynamic process; in statistical thermodynamics, the second law is postulated to be a consequence of molecular chaos.
Thethird law of thermodynamicsstates:As the temperature of a system approaches absolute zero, all processes cease and the entropy of the system approaches a minimum value.
This law of thermodynamics is a statistical law of nature regarding entropy and the impossibility of reachingabsolute zeroof temperature. This law provides an absolute reference point for the determination of entropy. The entropy determined relative to this point is the absolute entropy. Alternate definitions include "the entropy of all systems and of all states of a system is smallest at absolute zero," or equivalently "it is impossible to reach the absolute zero of temperature by any finite number of processes".
Absolute zero, at which all activity would stop if it were possible to achieve, is −273.15 °C (degrees Celsius), or −459.67 °F (degrees Fahrenheit), or 0 K (kelvin), or 0° R (degreesRankine).
An important concept in thermodynamics is thethermodynamic system, which is a precisely defined region of the universe under study. Everything in the universe except the system is called thesurroundings. A system is separated from the remainder of the universe by aboundarywhich may be a physical or notional, but serve to confine the system to a finite volume. Segments of theboundaryare often described aswalls; they have respective defined 'permeabilities'. Transfers of energy aswork, or asheat, or ofmatter, between the system and the surroundings, take place through the walls, according to their respective permeabilities.
Matter or energy that pass across the boundary so as to effect a change in the internal energy of the system need to be accounted for in the energy balance equation. The volume contained by the walls can be the region surrounding a single atom resonating energy, such as Max Planck defined in 1900; it can be a body of steam or air in asteam engine, such as Sadi Carnot defined in 1824. The system could also be just onenuclide(i.e. a system ofquarks) as hypothesized inquantum thermodynamics. When a looser viewpoint is adopted, and the requirement of thermodynamic equilibrium is dropped, the system can be the body of atropical cyclone, such asKerry Emanueltheorized in 1986 in the field ofatmospheric thermodynamics, or theevent horizonof ablack hole.
Boundaries are of four types: fixed, movable, real, and imaginary. For example, in an engine, a fixed boundary means the piston is locked at its position, within which a constant volume process might occur. If the piston is allowed to move that boundary is movable while the cylinder and cylinder head boundaries are fixed. For closed systems, boundaries are real while for open systems boundaries are often imaginary. In the case of a jet engine, a fixed imaginary boundary might be assumed at the intake of the engine, fixed boundaries along the surface of the case and a second fixed imaginary boundary across the exhaust nozzle.
Generally, thermodynamics distinguishes three classes of systems, defined in terms of what is allowed to cross their boundaries:
As time passes in an isolated system, internal differences of pressures, densities, and temperatures tend to even out. A system in which all equalizing processes have gone to completion is said to be in astateofthermodynamic equilibrium.
Once in thermodynamic equilibrium, a system's properties are, by definition, unchanging in time. Systems in equilibrium are much simpler and easier to understand than are systems which are not in equilibrium. Often, when analysing a dynamic thermodynamic process, the simplifying assumption is made that each intermediate state in the process is at equilibrium, producing thermodynamic processes which develop so slowly as to allow each intermediate step to be an equilibrium state and are said to bereversible processes.
When a system is at equilibrium under a given set of conditions, it is said to be in a definitethermodynamic state. The state of the system can be described by a number ofstate quantitiesthat do not depend on the process by which the system arrived at its state. They are calledintensive variablesorextensive variablesaccording to how they change when the size of the system changes. The properties of the system can be described by anequation of statewhich specifies the relationship between these variables. State may be thought of as the instantaneous quantitative description of a system with a set number of variables held constant.
Athermodynamic processmay be defined as the energetic evolution of a thermodynamic system proceeding from an initial state to a final state. It can be described byprocess quantities. Typically, each thermodynamic process is distinguished from other processes in energetic character according to what parameters, such as temperature, pressure, or volume, etc., are held fixed; Furthermore, it is useful to group these processes into pairs, in which each variable held constant is one member of aconjugatepair.
Several commonly studied thermodynamic processes are:
There are two types ofthermodynamic instruments, the meter and the reservoir. A thermodynamic meter is any device which measures any parameter of athermodynamic system. In some cases, the thermodynamic parameter is actually defined in terms of an idealized measuring instrument. For example, thezeroth lawstates that if two bodies are in thermal equilibrium with a third body, they are also in thermal equilibrium with each other. This principle, as noted byJames Maxwellin 1872, asserts that it is possible to measure temperature. An idealizedthermometeris a sample of an ideal gas at constant pressure. From theideal gas lawpV=nRT, the volume of such a sample can be used as an indicator of temperature; in this manner it defines temperature. Although pressure is defined mechanically, a pressure-measuring device, called abarometermay also be constructed from a sample of an ideal gas held at a constant temperature. Acalorimeteris a device which is used to measure and define the internal energy of a system.
A thermodynamic reservoir is a system which is so large that its state parameters are not appreciably altered when it is brought into contact with the system of interest. When the reservoir is brought into contact with the system, the system is brought into equilibrium with the reservoir. For example, a pressure reservoir is a system at a particular pressure, which imposes that pressure upon the system to which it is mechanically connected. The Earth's atmosphere is often used as a pressure reservoir. The ocean can act as temperature reservoir when used to cool power plants.
The central concept of thermodynamics is that ofenergy, the ability to dowork. By theFirst Law, the total energy of a system and its surroundings is conserved. Energy may be transferred into a system by heating, compression, or addition of matter, and extracted from a system by cooling, expansion, or extraction of matter. Inmechanics, for example, energy transfer equals the product of the force applied to a body and the resulting displacement.
Conjugate variablesare pairs of thermodynamic concepts, with the first being akin to a "force" applied to somethermodynamic system, the second being akin to the resulting "displacement", and the product of the two equaling the amount of energy transferred. The common conjugate variables are:
Thermodynamic potentialsare different quantitative measures of the stored energy in a system. Potentials are used to measure the energy changes in systems as they evolve from an initial state to a final state. The potential used depends on the constraints of the system, such as constant temperature or pressure. For example, the Helmholtz and Gibbs energies are the energies available in a system to do useful work when the temperature and volume or the pressure and temperature are fixed, respectively. Thermodynamic potentials cannot be measured in laboratories, but can be computed using molecular thermodynamics.[35][36]
whereT{\displaystyle T}is thetemperature,S{\displaystyle S}theentropy,p{\displaystyle p}thepressure,V{\displaystyle V}thevolume,μ{\displaystyle \mu }thechemical potential,N{\displaystyle N}the number of particles in the system, andi{\displaystyle i}is the count of particles types in the system.
Thermodynamic potentials can be derived from the energy balance equation applied to a thermodynamic system. Other thermodynamic potentials can also be obtained throughLegendre transformation.
Axiomatic thermodynamics is amathematical disciplinethat aims to describe thermodynamics in terms of rigorousaxioms, for example by finding a mathematically rigorous way to express the familiarlaws of thermodynamics.
The first attempt at an axiomatic theory of thermodynamics wasConstantin Carathéodory's 1909 workInvestigations on the Foundations of Thermodynamics, which made use ofPfaffian systemsand the concept ofadiabatic accessibility, a notion that was introduced by Carathéodory himself.[37][38]In this formulation, thermodynamic concepts such asheat,entropy, andtemperatureare derived from quantities that are more directly measurable.[39]Theories that came after, differed in the sense that they made assumptions regardingthermodynamic processeswith arbitrary initial and final states, as opposed to considering only neighboring states.
Astrophysicsis a science that employs the methods and principles ofphysicsandchemistryin the study ofastronomical objectsand phenomena.[1][2]As one of  the founders of the discipline,James Keeler, said, astrophysics "seeks to ascertain the nature of the heavenly bodies, rather than their positions or motions in space—whatthey are, rather thanwherethey are",[3]which is studied incelestial mechanics.
Among the subjects studied are theSun(solar physics), otherstars,galaxies,extrasolar planets, theinterstellar medium, and thecosmic microwave background.[4][5]Emissions from these objects are examined across all parts of theelectromagnetic spectrum, and the properties examined includeluminosity,density,temperature, andchemicalcomposition. Because astrophysics is a very broad subject,astrophysicistsapply concepts and methods from many disciplines of physics, includingclassical mechanics,electromagnetism,statistical mechanics,thermodynamics,quantum mechanics,relativity,nuclearandparticle physics, andatomic and molecular physics.
In practice, modern astronomical research often involves substantial work in the realms oftheoreticaland observational physics. Some areas of study for astrophysicists include the properties ofdark matter,dark energy,black holes, and othercelestial bodies; and theoriginandultimate fate of the universe.[4]Topics also studied by theoretical astrophysicists includeSolar System formation and evolution;stellar dynamicsandevolution;galaxy formation and evolution;magnetohydrodynamics;large-scale structureofmatterin the universe; origin ofcosmic rays;general relativity,special relativity, andquantumandphysical cosmology(the physical study of the largest-scale structures of the universe), includingstringcosmology andastroparticle physics.
Astronomy is an ancient science, long separated from the study of terrestrial physics. In theAristotelianworldview, bodies in the sky appeared to be unchangingsphereswhose only motion was uniform motion in a circle, while the earthly world was the realm which underwentgrowth and decayand in which natural motion was in a straight line and ended when the moving object reached itsgoal.  Consequently, it was held that the celestial region was made of a fundamentally different kind of matter from that found in the terrestrial sphere; eitherFireas maintained byPlato, orAetheras maintained byAristotle.[6][7]During the 17th century, natural philosophers such asGalileo,[8]Descartes,[9]andNewton[10]began to maintain that the celestial and terrestrial regions were made of similar kinds of material and were subject to the samenatural laws.[11]Their challenge was that the tools had not yet been invented with which to prove these assertions.[12]
For much of the nineteenth century, astronomical research was focused on the routine work of measuring the positions and computing the motions of astronomical objects.[13][14]A new astronomy, soon to be called astrophysics, began to emerge whenWilliam Hyde WollastonandJoseph von Fraunhoferindependently discovered that, when decomposing the light from the Sun, a multitude ofdark lines(regions where there was less or no light) were observed in thespectrum.[15]By 1860 the physicist,Gustav Kirchhoff, and the chemist,Robert Bunsen, had demonstrated that thedark linesin the solar spectrum corresponded tobright linesin the spectra of known gases, specific lines corresponding to uniquechemical elements.[16]Kirchhoff deduced that the dark lines in the solar spectrum are caused byabsorptionbychemical elementsin the Solar atmosphere.[17]In this way it was proved that the chemical elements found in the Sun and stars were also found on Earth.
Among those who extended the study of solar and stellar spectra wasNorman Lockyer, who in 1868 detected radiant, as well as dark lines in solar spectra. Working with chemistEdward Franklandto investigate the spectra of elements at various temperatures and pressures, he could not associate a yellow line in the solar spectrum with any known elements.  He thus claimed the line represented a new element, which was calledhelium, after the GreekHelios, the Sun personified.[18][19]
In 1885,Edward C. Pickeringundertook an ambitious program of stellar spectral classification atHarvard College Observatory, in which a team ofwoman computers, notablyWilliamina Fleming,Antonia Maury, andAnnie Jump Cannon, classified the spectra recorded on photographic plates. By 1890, a catalog of over 10,000 stars had been prepared that grouped them into thirteen spectral types. Following Pickering's vision, by 1924 Cannon expanded thecatalogto nine volumes and over a quarter of a million stars, developing theHarvard Classification Schemewhich was accepted for worldwide use in 1922.[20]
In 1895,George Ellery HaleandJames E. Keeler, along with a group of ten associate editors from Europe and the United States,[21]establishedThe Astrophysical Journal: An International Review of Spectroscopy and Astronomical Physics.[22]It was intended that the journal would fill the gap between journals in astronomy and physics, providing a venue for publication of articles on astronomical applications of the spectroscope; on laboratory research closely allied to astronomical physics, including wavelength determinations of metallic and gaseous spectra and experiments on radiation and absorption; on theories of the Sun, Moon, planets, comets, meteors, and nebulae; and on instrumentation for telescopes and laboratories.[21]
Around 1920, following the discovery of theHertzsprung–Russell diagramstill used as the basis for classifying stars and their evolution,Arthur Eddingtonanticipated the discovery and mechanism ofnuclear fusionprocesses instars, in his paperThe Internal Constitution of the Stars.[23][24]At that time, the source of stellar energy was a complete mystery; Eddington correctly speculated that the source wasfusionof hydrogen into helium, liberating enormous energy according to Einstein's equationE = mc2. This was a particularly remarkable development since at that time fusion and thermonuclear energy, and even that stars are largely composed ofhydrogen(seemetallicity), had not yet been discovered.[25]
In 1925 Cecilia Helena Payne (laterCecilia Payne-Gaposchkin) wrote an influential doctoral dissertation atRadcliffe College, in which she appliedSaha's ionization theoryto stellar atmospheres to relate the spectral classes to the temperature of stars.[26]Most significantly, she discovered that hydrogen and helium were the principal components of stars, not the composition of Earth. Despite Eddington's suggestion, discovery was so unexpected that her dissertation readers (includingRussell) convinced her to modify the conclusion before publication. However, later research confirmed her discovery.[27][28]
By the end of the 20th century, studies of astronomical spectra had expanded to cover wavelengths extending from radio waves through optical, x-ray, and gamma wavelengths.[29]In the 21st century, it further expanded to include observations based ongravitational waves.
Observational astronomyis a division of the astronomical science that is concerned with recording and interpreting data, in contrast withtheoretical astrophysics, which is mainly concerned with finding out the measurable implications of physicalmodels. It is the practice of observingcelestial objectsby usingtelescopesand other astronomical apparatus.
Most astrophysical observations are made using theelectromagnetic spectrum.
Other than electromagnetic radiation, few things may be observed from the Earth that originate from great distances. A fewgravitational waveobservatories have been constructed, but gravitational waves are extremely difficult to detect.Neutrinoobservatories have also been built, primarily to study the Sun. Cosmic rays consisting of very high-energy particles can be observed hitting the Earth's atmosphere.
Observations can also vary in their time scale. Most optical observations take minutes to hours, so phenomena that change faster than this cannot readily be observed. However, historical data on some objects is available, spanningcenturiesormillennia. On the other hand, radio observations may look at events on a millisecond timescale (millisecond pulsars) or combine years of data (pulsar decelerationstudies). The information obtained from these different timescales is very different.
The study of the Sun has a special place in observational astrophysics. Due to the tremendous distance of all other stars, the Sun can be observed in a kind of detail unparalleled by any other star. Understanding the Sun serves as a guide to understanding of other stars.
The topic of how stars change, or stellar evolution, is often modeled by placing the varieties of star types in their respective positions on theHertzsprung–Russell diagram, which can be viewed as representing the state of a stellar object, from birth to destruction.
Theoretical astrophysicists use a wide variety of tools which includeanalytical models(for example,polytropesto approximate the behaviors of a star) andcomputationalnumerical simulations. Each has some advantages. Analytical models of a process are generally better for giving insight into the heart of what is going on. Numerical models can reveal the existence of phenomena and effects that would otherwise not be seen.[30][31]
Theorists in astrophysics endeavor to create theoretical models and figure out the observational consequences of those models. This helps allow observers to look for data that can refute a model or help in choosing between several alternate or conflicting models.
Theorists also try to generate or modify models to take into account new data. In the case of an inconsistency, the general tendency is to try to make minimal modifications to the model to fit the data. In some cases, a large amount of inconsistent data over time may lead to total abandonment of a model.
Topics studied by theoretical astrophysicists include stellar dynamics and evolution; galaxy formation and evolution; magnetohydrodynamics; large-scale structure of matter in the universe; origin of cosmic rays; general relativity and physical cosmology, includingstringcosmology and astroparticle physics. Relativistic astrophysics serves as a tool to gauge the properties of large-scale structures for which gravitation plays a significant role in physical phenomena investigated and as the basis forblack hole(astro)physics and the study ofgravitational waves.
Some widely accepted and studied theories and models in astrophysics, now included in theLambda-CDM model, are theBig Bang,cosmic inflation, dark matter, dark energy and fundamental theories of physics.
The roots of astrophysics can be found in the seventeenth century emergence of a unified physics, in which the same laws applied to the celestial and terrestrial realms.[11]There were scientists who were qualified in both physics and astronomy who laid the firm foundation for the current science of astrophysics.  In modern times, students continue to be drawn to astrophysics due to its popularization by theRoyal Astronomical Societyand notableeducatorssuch as prominent professorsLawrence Krauss,Subrahmanyan Chandrasekhar,Stephen Hawking,Hubert Reeves,Carl SaganandPatrick Moore. The efforts of the early, late, and present scientists continue to attract young people to study the history and science of astrophysics.[32][33][34]The television sitcom showThe Big Bang Theorypopularized the field of astrophysics with the general public, and featured some well known scientists likeStephen HawkingandNeil deGrasse Tyson.
World War I[b]or theFirst World War(28 July 1914 – 11 November 1918), also known as theGreat War, was aglobal conflictbetween two coalitions: theAllies(or Entente) and theCentral Powers. Fighting took place mainly inEuropeand theMiddle East, as well as in parts ofAfricaand theAsia-Pacific, and in Europe was characterised bytrench warfare; the widespread use ofartillery, machine guns, andchemical weapons(gas); and the introductions oftanksandaircraft. World War I was one of thedeadliest conflicts in history, resulting in an estimated10 million military dead and more than 20 million wounded, plus some 10 million civilian dead from causes includinggenocide. The movement of large numbers of people was a major factor in the deadlySpanish flupandemic.
Thecauses of World War Iincluded the rise ofGermanyanddecline of the Ottoman Empire, which disturbed the long-standingbalance of powerin Europe, and rising economic competition between nations driven byindustrialisationandimperialism. Growing tensions between thegreat powersand in theBalkansreacheda breaking pointon 28 June 1914, whenGavrilo Princip, aBosnian Serb,assassinated the heir to the Austro-Hungarian throne.Austria-HungaryblamedSerbia, and declared war on 28 July. AfterRussiamobilised in Serbia's defence, Germany declared war on Russia andFrance, who hadan alliance. TheUnited Kingdomentered after Germanyinvaded Belgium, and the Ottomans joined the Central Powers in November.Germany's strategy in 1914was to quickly defeat France then transfer its forces to the east, but its advancewas halted in September, and by the end of the year theWestern Frontconsisted of a near-continuous line of trenches from the English Channel to Switzerland. TheEastern Frontwas more dynamic, but neither side gained a decisive advantage, despite costly offensives.Italy,Bulgaria,Romania,Greeceand others joined in from 1915 onward.
Major battles, including atVerdun,the Somme, andPasschendaele, failed to break the stalemate on the Western Front. In April 1917, theUnited States joined the Alliesafter Germany resumedunrestricted submarine warfareagainst Atlantic shipping. Later that year, theBolsheviksseized power in Russia in theOctober Revolution;Soviet Russiasignedan armisticewith the Central Powers in December, followed bya separate peacein March 1918. That month, Germany launcheda spring offensive in the west, which despite initial successes left theGerman Armyexhausted and demoralised. The AlliedHundred Days Offensivebeginning in August 1918 caused a collapse of the German front line. Following theVardar Offensive, Bulgaria signed anarmisticein late September. By early November, theOttoman EmpireandAustria-Hungaryhad each signed armistices with the Allies, leaving Germany isolated. Facinga revolution at home, KaiserWilhelm IIabdicated on 9 November, and the war ended with theArmistice of 11 November 1918.
TheParis Peace Conferenceof 1919–1920 imposed settlements on the defeated powers, most notably theTreaty of Versailles, by which Germany lost significant territories, was disarmed, and was required to pay largewar reparationsto the Allies. The dissolution of the Russian, German, Austro-Hungarian, and Ottoman Empires redrew national boundaries and resulted in the creation of new independent states, includingPoland,Finland, theBaltic states,Czechoslovakia, andYugoslavia. TheLeague of Nationswas established to maintain world peace, but its failure to manage instability during theinterwar periodcontributed to the outbreak ofWorld War IIin 1939.
BeforeWorld War II, the events of 1914–1918 were generally known as theGreat Waror simply theWorld War.[1]In August 1914, the magazineThe Independentwrote "This is the Great War. It names itself".[2]In October 1914, the Canadian magazineMaclean'ssimilarly wrote, "Some wars name themselves. This is the Great War."[3]Contemporary Europeans also referred to it as "the war to end war" and it was also described as "the war to end all wars" due to their perception of its unparalleled scale, devastation, and loss of life.[4]The first recorded use of the termFirst World Warwas in September 1914 by German biologist and philosopherErnst Haeckelwho stated, "There is no doubt that the course and character of the feared 'European War' ... will become the first world war in the full sense of the word."[5]
For much of the 19th century, the major European powers maintained a tenuousbalance of power, known as theConcert of Europe.[6]After 1848, this was challenged by Britain's withdrawal into so-calledsplendid isolation, thedecline of the Ottoman Empire,New Imperialism, and the rise ofPrussiaunderOtto von Bismarck. Victory in the 1870–1871Franco-Prussian Warallowed Bismarck toconsolidateaGerman Empire. Post-1871, the primary aim of French policy was toavengethis defeat,[7]but by the early 1890s, this had switched to the expansion of theFrench colonial empire.[8]
In 1873, Bismarck negotiated theLeague of the Three Emperors, which includedAustria-Hungary,Russia, and Germany. After the 1877–1878Russo-Turkish War, the League was dissolved due to Austrian concerns over the expansion of Russian influence in theBalkans, an area they considered to be of vital strategic interest.Germanyand Austria-Hungary then formed the 1879Dual Alliance, which became theTriple Alliancewhen Italy joined in 1882.[9]For Bismarck, the purpose of these agreements was to isolate France by ensuring the three empires resolved any disputes between themselves. In 1887, Bismarck set up theReinsurance Treaty, a secret agreement between Germany and Russia to remain neutral if either were attacked by France or Austria-Hungary.[10]
For Bismarck, peace with Russia was the foundation of German foreign policy, but in 1890, he was forced to retire byWilhelm II. The latter was persuaded not to renew the Reinsurance Treaty by his newChancellor,Leo von Caprivi.[11]This gave France an opening to agree to theFranco-Russian Alliancein 1894, which was then followed by the 1904Entente Cordialewith Britain. TheTriple Ententewas completed by the 1907Anglo-Russian Convention. While not formal alliances, by settlinglongstanding colonial disputes in Asiaand Africa, British support for France or Russia in any future conflict became a possibility.[12]This was accentuated by British and Russian support for France against Germany during the 1911Agadir Crisis.[13]
German economic and industrial strength continued to expand rapidly post-1871. Backed by Wilhelm II, AdmiralAlfred von Tirpitzsought to use this growth to build anImperial German Navy, that could compete with the BritishRoyal Navy.[14]This policy was based on the work of US naval authorAlfred Thayer Mahan, who argued that possession of ablue-water navywas vital for global power projection; Tirpitz had his books translated into German, while Wilhelm made them required reading for his advisors and senior military personnel.[15]
However, it was also an emotional decision, driven by Wilhelm's simultaneous admiration for the Royal Navy and desire to surpass it. Bismarck thought that the British would not interfere in Europe, as long as its maritime supremacy remained secure, but his dismissal in 1890 led to a change in policy and anAnglo-German naval arms racebegan.[16]Despite the vast sums spent by Tirpitz, the launch ofHMSDreadnoughtin 1906 gave the British a technological advantage.[14]Ultimately, the race diverted huge resources into creating a German navy large enough to antagonise Britain, but not defeat it; in 1911, ChancellorTheobald von Bethmann Hollwegacknowledged defeat, leading to theRüstungswendeor 'armaments turning point', when he switched expenditure from the navy to the army.[17]
This decision was not driven by a reduction in political tensions but by German concern over Russia's quick recovery from its defeat in theRusso-Japanese Warand the subsequent1905 Russian Revolution. Economic reforms led to a significant post-1908 expansion of railways and transportation infrastructure, particularly in its western border regions.[18]Since Germany and Austria-Hungary relied on faster mobilisation to compensate for their numerical inferiority compared to Russia, the threat posed by the closing of this gap was more important than competing with the Royal Navy. After Germany expanded its standing army by 170,000 troops in 1913, France extended compulsory military service from two to three years; similar measures were taken by theBalkan powersand Italy, which led to increased expenditure by theOttomansand Austria-Hungary. Absolute figures are difficult to calculate due to differences in categorising expenditure since they often omit civilian infrastructure projects like railways which had logistical importance and military use. It is known, however, that from 1908 to 1913, military spending by the six major European powers increased by over 50% in real terms.[19]
The years before 1914 were marked by a series of crises in the Balkans, as other powers sought to benefit from the Ottoman decline. WhilePan-SlavicandOrthodoxRussia considered itself the protector ofSerbiaand otherSlavstates, they preferred the strategically vitalBosporusstraits to be controlled by a weak Ottoman government, rather than an ambitious Slav power likeBulgaria. Russia had ambitions in northeasternAnatoliawhile its clients had overlapping claims in the Balkans. These competing interests divided Russian policy-makers and added to regional instability.[20]
Austrian statesmen viewed the Balkans as essential for the continued existence of their Empire and saw Serbian expansion as a direct threat. The 1908–1909Bosnian Crisisbegan when Austria annexed the former Ottoman territory ofBosnia and Herzegovina, which ithad occupiedsince 1878. Timed to coincide with theBulgarian Declaration of Independencefrom the Ottoman Empire, this unilateral action was denounced by the European powers, but accepted as there was no consensus on how to resolve the situation. Some historians see this as a significant escalation, ending any chance of Austria cooperating with Russia in the Balkans, while also damaging diplomatic relations between Serbia and Italy.[21]
Tensions increased after the 1911–1912Italo-Turkish Wardemonstrated Ottoman weakness and led to the formation of theBalkan League, an alliance of Serbia, Bulgaria,Montenegro, andGreece.[22]The League quickly overran most of the Ottomans' territory in the Balkans during the 1912–1913First Balkan War, much to the surprise of outside observers.[23]The Serbian capture of ports on theAdriaticresulted in partial Austrian mobilisation, starting on 21 November 1912, including units along the Russian border inGalicia. TheRussian governmentdecided not to mobilise in response, unprepared to precipitate a war.[24]
The Great Powers sought to re-assert control through the 1913Treaty of London, which had created an independentAlbaniawhile enlarging the territories of Bulgaria, Serbia, Montenegro and Greece. However, disputes between the victors sparked the 33-daySecond Balkan War, when Bulgaria attacked Serbia and Greece on 16 June 1913; it was defeated, losing most ofMacedoniato Serbia and Greece, andSouthern Dobrujato Romania.[25]The result was that even countries which benefited from the Balkan Wars, such as Serbia and Greece, felt cheated of their "rightful gains", while for Austria it demonstrated the apparent indifference with which other powers viewed their concerns, including Germany.[26]This complex mix of resentment, nationalism and insecurity helps explain why the pre-1914 Balkans became known as the "powder keg of Europe".[27][28][29][30]
On 28 June 1914,Archduke Franz Ferdinand of Austria, heir presumptive to EmperorFranz Joseph I of Austria, visitedSarajevo, the capital of the recently annexedBosnia and Herzegovina.Cvjetko Popović,Gavrilo Princip,Nedeljko Čabrinović,Trifko Grabež,Vaso Čubrilović(Bosnian Serbs) andMuhamed Mehmedbašić(from theBosniakscommunity),[33]from the movement known asYoung Bosnia, took up positions along the Archduke's motorcade route, to assassinate him. Supplied with arms by extremists within the SerbianBlack Handintelligence organisation, they hoped his death would free Bosnia from Austrian rule.[34]
Čabrinović threw agrenadeat the Archduke's car and injured two of his aides. The other assassins were also unsuccessful. An hour later, as Ferdinand was returning from visiting the injured officers in hospital, his car took a wrong turn into a street whereGavrilo Principwas standing. He fired two pistol shots, fatally wounding Ferdinand and his wifeSophie.[35]
According to historianZbyněk Zeman, in Vienna "the event almost failed to make any impression whatsoever. On 28 and 29 June, the crowds listened to music and drank wine, as if nothing had happened."[36]Nevertheless, the impact of the murder of the heir to the throne was significant, and has been described by historianChristopher Clarkas a "9/11 effect, a terrorist event charged with historic meaning, transforming the political chemistry in Vienna".[37]
Austro-Hungarian authorities encouraged subsequentanti-Serb riots in Sarajevo.[38][39]Violent actions against ethnic Serbs were also organised outside Sarajevo, in other cities in Austro-Hungarian-controlled Bosnia and Herzegovina, Croatia and Slovenia. Austro-Hungarian authorities in Bosnia and Herzegovina imprisoned approximately 5,500 prominent Serbs, 700 to 2,200 of whom died in prison. A further 460 Serbs were sentenced to death. A predominantly Bosniak special militia known as theSchutzkorpswas established, and carried out the persecution of Serbs.[40][41][42][43]
The assassination initiated the July Crisis, a month of diplomatic manoeuvring between Austria-Hungary, Germany, Russia, France and Britain. Believing that Serbian intelligence helped organise Franz Ferdinand's murder, Austrian officials wanted to use the opportunity to end their interference in Bosnia and saw war as the best way of achieving this.[44]However, theForeign Ministryhad no solid proof of Serbian involvement.[45]On 23July, Austria delivered anultimatumto Serbia, listing ten demands made intentionally unacceptable to provide an excuse for starting hostilities.[46]
Serbia ordered generalmobilisationon 25July, but accepted all the terms, except for those empowering Austrian representatives to suppress "subversive elements" inside Serbia, and take part in the investigation and trial of Serbians linked to the assassination.[47][48]Claiming this amounted to rejection, Austria broke off diplomatic relations and ordered partial mobilisation the next day; on 28 July, they declared war on Serbia and began shellingBelgrade. Russia ordered general mobilisation in support of Serbia on 30 July.[49]
Anxious to ensure backing from theSPDpolitical opposition by presenting Russia as the aggressor, German Chancellor Bethmann Hollweg delayed the commencement of war preparations until 31 July.[50]That afternoon, the Russian government were handed a note requiring them to "cease all war measures against Germany and Austria-Hungary" within 12 hours.[51]A further German demand for neutrality was refused by the French who orderedgeneral mobilisationbut delayed declaring war.[52]TheGerman General Staffhad long assumed they faced a war on two fronts; theSchlieffen Planenvisaged using 80% of the army to defeat France, then switching to Russia. Since this required them to move quickly, mobilisation orders were issued that afternoon.[53]Once the German ultimatum to Russia expired on the morning of 1 August, the two countries were at war.
At a meeting on 29 July, the British cabinet had narrowly decided its obligations to Belgium under the 1839Treaty of Londondid not require it to oppose a German invasion with military force; however, Prime MinisterAsquithand his senior Cabinet ministers were already committed to supporting France, the Royal Navy had been mobilised, and public opinion was strongly in favour of intervention.[54]On 31 July, Britain sent notes to Germany and France, asking them to respect Belgian neutrality; France pledged to do so, but Germany did not reply.[55]Aware of German plans to attack through Belgium, French Commander-in-ChiefJoseph Joffreasked his government for permission to cross the border and pre-empt such a move. To avoid violating Belgian neutrality, he was told any advance could come only after a German invasion.[56]Instead, the French cabinet ordered its Army to withdraw 10 km behind the German frontier, to avoid provoking war. On 2 August,Germany occupied Luxembourgand exchanged fire with French units when German patrols entered French territory; on 3August, they declared war on France and demanded free passage across Belgium, which was refused. Early on the morning of 4August, the Germans invaded, andAlbert I of Belgiumcalled for assistance under theTreaty of London.[57][58]Britain sent Germany an ultimatum demanding they withdraw from Belgium; when this expired at midnight, without a response, the two empires were at war.[59]
Germany promised to support Austria-Hungary's invasion of Serbia, but interpretations of what this meant differed. Previously tested deployment plans had been replaced early in 1914, but those had never been tested in exercises. Austro-Hungarian leaders believed Germany would cover its northern flank against Russia.[60]
Beginning on 12 August, the Austrians and Serbs clashed at the battles of theCerandKolubara; over the next two weeks, Austrian attacks were repulsed with heavy losses. As a result, Austria had to keep sizeable forces on the Serbian front, weakening their efforts against Russia.[61]Serbia's victory against Austria-Hungary in the 1914 invasion has been called one of the major upset victories of the twentieth century.[62]In 1915, the campaign saw the first use ofanti-aircraft warfareafter an Austrian plane was shot down withground-to-airfire, as well as the firstmedical evacuationby the Serbian army.[63][64]
Upon mobilisation, in accordance with theSchlieffen Plan, 80% of theGerman Armywas located on the Western Front, with the remainder acting as a screening force in the East. Rather than a direct attack across their shared frontier, the German right wing would sweep through theNetherlandsandBelgium, then swing south, encircling Paris and trapping the French army against the Swiss border. The plan's creator,Alfred von Schlieffen, head of theGerman General Stafffrom 1891 to 1906, estimated that this would take six weeks, after which the German army would transfer to the East and defeat the Russians.[65]
The plan was substantially modified by his successor,Helmuth von Moltke the Younger. Under Schlieffen, 85% of German forces in the west were assigned to the right wing, with the remainder holding along the frontier. By keeping his left-wing deliberately weak, he hoped to lure the French into an offensive into the "lost provinces" ofAlsace-Lorraine, which was the strategy envisaged by theirPlan XVII.[65]However, Moltke grew concerned that the French might push too hard on his left flank and as the German Army increased in size from 1908 to 1914, he changed the allocation of forces between the two wings to 70:30.[66]He also considered Dutch neutrality essential for German trade and cancelled the incursion into the Netherlands, which meant any delays in Belgium threatened the viability of the plan.[67]HistorianRichard Holmesargues that these changes meant the right wing was not strong enough to achieve decisive success.[68]
The initial German advance in the West was very successful. By the end of August, the Allied left, which included theBritish Expeditionary Force(BEF), was infull retreat, and the French offensive in Alsace-Lorraine was a disastrous failure, with casualties exceeding 260,000.[69]German planning provided broad strategic instructions while allowing army commanders considerable freedom in carrying them out at the front, butAlexander von Kluckused this freedom to disobey orders, opening a gap between the German armies as they closed on Paris.[70]The French army, reinforced by the British expeditionary corps, seized this opportunity to counter-attack and pushed the German army 40 to 80 km back. Both armies were then so exhausted that no decisive move could be implemented, so they settled in trenches, with the vain hope of breaking through as soon as they could build local superiority.
In 1911, the RussianStavkaagreed with the French to attack Germany within fifteen days of mobilisation, ten days before the Germans had anticipated, although it meant the two Russian armies that enteredEast Prussiaon 17 August did so without many of their support elements.[71]
By the end of 1914, German troops held strong defensive positions inside France, controlled the bulk of France's domestic coalfields, and inflicted 230,000 more casualties than it lost itself. However, communications problems and questionable command decisions cost Germany the chance of a decisive outcome, while it had failed to achieve the primary objective of avoiding a long, two-front war.[72]As was apparent to several German leaders, this amounted to a strategic defeat; shortly after theFirst Battle of the Marne,Crown Prince Wilhelmtold an American reporter "We have lost the war. It will go on for a long time but lost it is already."[73]
On 30 August 1914, New Zealandoccupied German Samoa(nowSamoa). On 11 September, theAustralian Naval and Military Expeditionary Forcelanded on the island ofNew Britain, then part ofGerman New Guinea. On 28 October, the German cruiserSMSEmdensank theRussian cruiserZhemchugin theBattle of Penang. Japan declared war on Germany before seizing territories in the Pacific, which later became theSouth Seas Mandate, as well as GermanTreaty portson the ChineseShandongpeninsula atTsingtao. After Vienna refused to withdraw its cruiserSMSKaiserin Elisabethfrom Tsingtao, Japan declared war on Austria-Hungary, and the ship was sunk in November 1914.[74]Within a few months, Allied forces had seized all German territories in the Pacific, leaving only isolated commerce raiders and a few holdouts in New Guinea.[75][76]
Some of the first clashes of the war involved British, French, and German colonial forces in Africa. On 6–7 August, French and British troops invaded the German protectorates ofTogolandandKamerun. On 10 August, German forces inSouth-West Africaattacked South Africa; sporadic and fierce fighting continued for the rest of the war. The German colonial forces inGerman East Africa, led by ColonelPaul von Lettow-Vorbeck, fought aguerrilla warfarecampaign and only surrendered two weeks after the armistice took effect in Europe.[77]
Before the war, Germany had attempted to use Indian nationalism and pan-Islamism to its advantage, a policy continued post-1914 byinstigating uprisings in India, while theNiedermayer–Hentig Expeditionurged Afghanistan to join the war on the side of Central Powers. However, contrary to British fears of a revolt in India, the outbreak of the war saw a reduction in nationalist activity.[78][79]Leaders from theIndian National Congressand other groups believed support for the British war effort would hastenIndian Home Rule, a promise allegedly made explicit in 1917 byEdwin Montagu, theSecretary of State for India.[80]
In 1914, theBritish Indian Armywas larger than the British Army itself, and between 1914 and 1918 an estimated 1.3 million Indian soldiers and labourers served in Europe, Africa, and the Middle East. In all, 140,000 soldiers served on the Western Front and nearly 700,000 in the Middle East, with 47,746 killed and 65,126 wounded.[81]The suffering engendered by the war, as well as the failure of the British government to grant self-government to India afterward, bred disillusionment, resulting inthe campaign for full independenceled byMahatma Gandhi.[82]
Pre-war military tactics that had emphasised open warfare and individual riflemen proved obsolete when confronted with conditions prevailing in 1914. Technological advances allowed the creation of strong defensive systems largely impervious to massed infantry advances, such asbarbed wire, machine guns and above all far more powerfulartillery, which dominated the battlefield and made crossing open ground extremely difficult.[83]Both sides struggled to develop tactics for breaching entrenched positions without heavy casualties. In time, technology enabled the production of new offensive weapons, such asgas warfareand thetank.[84]
After theFirst Battle of the Marnein September 1914, Allied and German forces unsuccessfully tried to outflank each other, a series of manoeuvres later known as the "Race to the Sea". By the end of 1914, the opposing forces confronted each other along an uninterrupted line of entrenched positions from theChannelto the Swiss border.[85]Since the Germans were normally able to choose where to stand, they generally held the high ground, while their trenches tended to be better built; those constructed by the French and English were initially considered "temporary", only needed until an offensive would destroy the German defences.[86]Both sides tried to break the stalemate using scientific and technological advances. On 22 April 1915, at theSecond Battle of Ypres, the Germans (violating theHague Convention) usedchlorinegas for the first time on the Western Front. Several types of gas soon became widely used by both sides and though it never proved a decisive, battle-winning weapon, it became one of the most feared and best-remembered horrors of the war.[87][88]
In February 1916, the Germans attacked French defensive positions at theBattle of Verdun, lasting until December 1916. Casualties were greater for the French, but the Germans bled heavily as well, with anywhere from 700,000[89]to 975,000[90]casualties between the two combatants. Verdun became a symbol of French determination and self-sacrifice.[91]
TheBattle of the Sommewas an Anglo-French offensive from July to November 1916. Theopening dayon 1 July 1916, was the bloodiest single day in the history of theBritish Army, which suffered 57,500 casualties, including 19,200 dead. As a whole, the Somme offensive led to an estimated 420,000 British casualties, along with 200,000 French and 500,000 Germans.[92]The diseases that emerged in the trenches were a major killer on both sides. The living conditions led to disease and infection, such astrench foot,lice,typhus,trench fever, and the 'Spanish flu'.[93]
At the start of the war, Germancruiserswere scattered across the globe, some of which were subsequently used to attack Alliedmerchant shipping. These were systematically hunted down by the Royal Navy, though not before causing considerable damage. One of the most successful was theSMSEmden, part of the GermanEast Asia Squadronstationed atQingdao, which seized or sank 15 merchantmen, a Russian cruiser and a French destroyer. Most of the squadron was returning to Germany when it sank two British armoured cruisers at theBattle of Coronelin November 1914, before being virtually destroyed at theBattle of the Falkland Islandsin December. TheSMSDresdenescaped with a few auxiliaries, but after theBattle of Más a Tierra, these too were either destroyed or interned.[94]
Soon after the outbreak of hostilities, Britain began a navalblockade of Germany. This proved effective in cutting off vital supplies, though it violated accepted international law.[95]Britain also mined international waters which closed off entire sections of the ocean, even to neutral ships.[96]Since there was limited response to this tactic, Germany expected a similar response to its unrestricted submarine warfare.[97]
TheBattle of Jutland[d]in May/June 1916 was the only full-scale clash of battleships during the war, and one of the largest in history. The clash was indecisive, though the Germans inflicted more damage than they received; thereafter the bulk of the GermanHigh Seas Fleetwas confined to port.[98]
GermanU-boatsattempted to cut the supply lines between North America and Britain.[99]The nature ofsubmarine warfaremeant that attacks often came without warning, giving the crews of the merchant ships little hope of survival.[99][100]The United States launched a protest, and Germany changed its rules of engagement. Afterthe sinkingof the passenger shipRMSLusitaniain 1915, Germany promised not to target passenger liners, while Britain armed its merchant ships, placing them beyond the protection of the "cruiser rules", which demanded warning and movement of crews to "a place of safety" (a standard that lifeboats did not meet).[101]Finally, in early 1917, Germany adopted a policy ofunrestricted submarine warfare, realising the Americans would eventually enter the war.[99][102]Germany sought to strangle Alliedsea lanesbefore the United States could transport a large army overseas, but, after initial successes, eventually failed to do so.[99]
The U-boat threat lessened in 1917, when merchant ships began travelling inconvoys, escorted bydestroyers. This tactic made it difficult for U-boats to find targets, which significantly lessened losses; after thehydrophoneanddepth chargeswere introduced, destroyers could potentially successfully attack a submerged submarine. Convoys slowed the flow of supplies since ships had to wait as convoys were assembled; the solution was an extensive program of building new freighters. Troopships were too fast for the submarines and did not travel the North Atlantic in convoys.[103]The U-boats sunk more than 5,000 Allied ships, at the cost of 199 submarines.[104]
World War I also saw the first use ofaircraft carriersin combat, withHMSFuriouslaunchingSopwith Camelsin a successful raid against theZeppelinhangars atTondernin July 1918, as well asblimpsfor antisubmarine patrol.[105]
Faced with Russia in the east, Austria-Hungary could spare only one-third of its army to attack Serbia. After suffering heavy losses, the Austrians briefly occupied the Serbian capital,Belgrade. A Serbian counter-attack in the Battle of Kolubara succeeded in driving them from the country by the end of 1914. For the first 10 months of 1915, Austria-Hungary used most of its military reserves to fight Italy. German and Austro-Hungarian diplomats scored a coup by persuading Bulgaria to join the attack on Serbia.[106]The Austro-Hungarian provinces ofSlovenia, Croatia andBosniaprovided troops for Austria-Hungary. Montenegro allied itself with Serbia.[107]
Bulgaria declared war on Serbia on 14 October 1915, and joined in the attack by the Austro-Hungarian army under Mackensen's army of 250,000 that was already underway. Serbia was conquered in a little more than a month, as the Central Powers, now including Bulgaria, sent in 600,000 troops in total. The Serbian army, fighting on two fronts and facing certain defeat, retreated into northernAlbania. The Serbs suffered defeat in theBattle of Kosovo. Montenegro covered the Serbian retreat toward the Adriatic coast in theBattle of Mojkovacon 6–7 January 1916, but ultimately the Austrians also conquered Montenegro. The surviving Serbian soldiers were evacuated to Greece.[108]After the conquest, Serbia was divided between Austro-Hungary and Bulgaria.[109]
In late 1915, a Franco-British force landed atSalonicain Greece to offer assistance and to pressure its government to declare war against the Central Powers. However, the pro-GermanKing Constantine Idismissed the pro-Allied government ofEleftherios Venizelosbefore the Allied expeditionary force arrived.[110]
The Macedonian front was at first mostly static. French and Serbian forces retook limited areas of Macedonia by recapturingBitolaon 19 November 1916, following the costlyMonastir offensive, which brought stabilisation of the front.[111]
Serbian and French troops finally made a breakthrough in September 1918 in theVardar offensive, after most German and Austro-Hungarian troops had been withdrawn. The Bulgarians were defeated at theBattle of Dobro Pole, and by 25 September British and French troops had crossed the border into Bulgaria proper as the Bulgarian army collapsed. Bulgaria capitulated four days later, on 29 September 1918.[113]The German high command responded by despatching troops to hold the line, but these forces were too weak to re-establish a front.[114]
The disappearance of the Macedonian front meant that the road toBudapestand Vienna was now opened to Allied forces. Hindenburg and Ludendorff concluded that the strategic and operational balance had now shifted decidedly against theCentral Powersand, a day after the Bulgarian collapse, insisted on an immediate peace settlement.[115]
The Ottomans threatened Russia'sCaucasianterritories and Britain's communications with India via theSuez Canal. The Ottoman Empire took advantage of the European powers' preoccupation with the war and conducted large-scale ethnic cleansing of theArmenian,Greek, andAssyrianChristian populations—theArmenian genocide,Greek genocide, andAssyrian genociderespectively.[116][117][118]
The British and French opened overseas fronts with theGallipoli(1915) andMesopotamian campaigns(1914). In Gallipoli, the Ottoman Empire successfully repelled the British, French, andAustralian and New Zealand Army Corps(ANZACs). InMesopotamia, by contrast, after the defeat of the British defenders in thesiege of Kutby the Ottomans (1915–1916), British Imperial forces reorganised and capturedBaghdadin March 1917. The British were aided in Mesopotamia by local Arab and Assyrian fighters, while the Ottomans employed localKurdishandTurcomantribes.[119]
TheSuez Canalwas defended from Ottoman attacks in 1915 and 1916; in August 1916, a German and Ottoman force was defeated at theBattle of Romaniby theANZAC Mounted Divisionand the52nd (Lowland) Infantry Division. Following this victory, anEgyptian Expeditionary Forceadvanced across theSinai Peninsula, pushing Ottoman forces back in theBattle of Magdhabain December and theBattle of Rafaon the border between the EgyptianSinaiand Ottoman Palestine in January 1917.[120]
Russian armies generally had success in theCaucasus campaign.Enver Pasha, supreme commander of the Ottoman armed forces, dreamed of re-conquering central Asia and areas that had been previously lost to Russia. He was, however, a poor commander.[121]He launched an offensive against the Russians in the Caucasus in December 1914 with 100,000 troops, insisting on a frontal attack against mountainous Russian positions in winter. He lost 86% of his force at theBattle of Sarikamish.[122]GeneralNikolai Yudenich, the Russian commander from 1915 to 1916, drove the Turks out of most of the southernCaucasus.[122]
The Ottoman Empire, with German support, invaded Persia (modernIran) in December 1914 to cut off British and Russian access topetroleum reservoirsaroundBaku.[123]Persia, ostensibly neutral, had long been under British and Russian influence. The Ottomans and Germans were aided byKurdishandAzeriforces, together with a large number of major Iranian tribes, while the Russians and British had the support of Armenian andAssyrian forces. ThePersian campaignlasted until 1918 and ended in failure for the Ottomans and their allies. However, the Russian withdrawal from the war in 1917 led Armenian and Assyrian forces to be cut off from supply lines, outnumbered, outgunned and isolated, forcing them to fight and flee towards British lines in northern Mesopotamia.[124]
TheArab Revolt, instigated by the BritishForeign Office, started in June 1916 with theBattle of Mecca, led bySharif Hussein. The Sharif declared the independence of theKingdom of Hejazand, with British assistance, conquered much of Ottoman-held Arabia, resulting finally in the Ottoman surrender of Damascus.Fakhri Pasha, the Ottoman commander ofMedina, resisted for more than2+1⁄2years during thesiege of Medinabefore surrendering in January 1919.[125]
TheSenussitribe, along the border ofItalian LibyaandBritish Egypt, incited and armed by the Turks, waged a small-scaleguerrilla waragainst Allied troops. The British were forced to dispatch 12,000 troops to oppose them in theSenussi campaign. Their rebellion was finally crushed in mid-1916.[126]
Total Allied casualties on the Ottoman fronts amounted to 650,000 men. Total Ottoman casualties were 725,000, with 325,000 dead and 400,000 wounded.[127]
Though Italy joined the Triple Alliance in 1882, a treaty with its traditional Austrian enemy was so controversial that subsequent governments denied its existence and the terms were only made public in 1915.[128]This arose fromnationalistdesigns on Austro-Hungarian territory inTrentino, theAustrian Littoral,RijekaandDalmatia, considered vital to secure the borders established in1866.[129]In 1902, Rome secretly had agreed with France to remain neutral if the latter was attacked by Germany, effectively nullifying its role in the Triple Alliance.[130]
When the war began in 1914, Italy argued the Triple Alliance was defensive and it was not obliged to support an Austrian attack on Serbia. Opposition to joining the Central Powers increased when Turkey became a member in September, since in1911 Italy had occupiedOttoman possessions inLibyaand theDodecaneseislands.[131]To secure Italian neutrality, the Central Powers offered themTunisia, while in return for an immediate entry into the war, the Allies agreed to their demands for Austrian territory and sovereignty over the Dodecanese.[132]Although they remained secret, these provisions were incorporated into the April 1915Treaty of London; Italy joined the Triple Entente and, on 23 May, declared war on Austria-Hungary,[133]followed by Germany fifteen months later.
The pre-1914 Italian army was short of officers, trained men, adequate transport and modern weapons; by April 1915, some of these deficiencies had been remedied but it was still unprepared for the major offensive required by the Treaty of London.[134]The advantage of superior numbers was offset by the difficult terrain; much of the fighting took place high in theAlpsandDolomites, where trench lines had to be cut through rock and ice and keeping troops supplied was a major challenge. These issues were exacerbated by unimaginative strategies and tactics.[135]Between 1915 and 1917, the Italian commander,Luigi Cadorna, undertooka series of frontal assaults along the Isonzo, which made little progress and cost many lives; by the end of the war, Italian combat deaths totalled around 548,000.[136]
In the spring of 1916, the Austro-Hungarians counterattacked inAsiagoin theStrafexpedition, but made little progress and were pushed by the Italians back to Tyrol.[137]Although Italy occupied southernAlbaniain May 1916, their main focus was the Isonzo front which, after thecapture of Goriziain August 1916, remained static until October 1917. After a combined Austro-German force won a major victory atCaporetto, Cadorna was replaced byArmando Diazwho retreated more than 100 kilometres (62 mi) before holding positions along thePiave River.[138]A second Austrianoffensive was repulsedin June 1918. On 24 October, Diaz launched theBattle of Vittorio Venetoand initially met stubborn resistance,[139]but with Austria-Hungary collapsing, Hungarian divisions in Italy demanded they be sent home.[140]When this was granted, many others followed and the Imperial army disintegrated, the Italians taking over 300,000 prisoners.[141]On 3November, theArmistice of Villa Giustiended hostilities between Austria-Hungary and Italy which occupiedTriesteand areas along theAdriatic Seaawarded to it in 1915.[142]
As previously agreed with French presidentRaymond Poincaré, Russian plans at the start of the war were to simultaneously advance intoAustrian Galiciaand East Prussia as soon as possible. Although theirattack on Galiciawas largely successful, and the invasions achieved their aim of forcing Germany to divert troops from the Western Front, the speed of mobilisation meant they did so without much of their heavy equipment and support functions. These weaknesses contributed to Russian defeats atTannenbergand theMasurian Lakesin August and September 1914, forcing them to withdraw from East Prussia with heavy losses.[143][144]By spring 1915, they had also retreated from Galicia, and the May 1915Gorlice–Tarnów offensiveallowed the Central Powers to invadeRussian-occupied Poland.[145]
Despite the successful June 1916Brusilov offensiveagainst the Austrians in eastern Galicia,[146]shortages of supplies, heavy losses and command failures prevented the Russians from fully exploiting their victory. However, it was one of the most significant offensives of the war, diverting German resources fromVerdun, relieving Austro-Hungarian pressure on the Italians, and convincing Romania to enter the war on the side of the Allies on 27 August. It also fatally weakened both the Austrian and Russian armies, whose offensive capabilities were badly affected by their losses and increased disillusion with the war that ultimately led to the Russian revolutions.[147]
Meanwhile, unrest grew in Russia as TsarNicholas IIremained at the front, with the home front controlled byEmpress Alexandra. Her increasingly incompetent rule and food shortages in urban areas led to widespread protests and the murder of her favourite,Grigori Rasputin, at the end of 1916.[148]
Despite secretly agreeing to support the Triple Alliance in 1883, Romania increasingly found itself at odds with the Central Powers over their support for Bulgaria in the Balkan Wars and the status of ethnic Romanian communities inHungarian-controlledTransylvania,[149]which comprised an estimated 2.8 million of the 5.0 million population.[150]With the ruling elite split into pro-German and pro-Entente factions,[151]Romania remained neutral for two years while allowing Germany and Austria to transport military supplies and advisors across Romanian territory.[152]
In September 1914, Russia acknowledged Romanian rights to Austro-Hungarian territories including Transylvania andBanat, whose acquisition had widespread popular support,[150]and Russian success against Austria led Romania to join the Entente in the August 1916Treaty of Bucharest.[152]Under the strategic plan known asHypothesis Z, the Romanian army planned an offensive into Transylvania, while defending SouthernDobrujaandGiurgiuagainst a possible Bulgarian counterattack.[153]On 27 August 1916, theyattacked Transylvaniaand occupied substantial parts of the province before being driven back by the recently formedGerman 9th Army, led by former Chief of StaffErich von Falkenhayn.[154]A combined German-Bulgarian-Turkish offensive captured Dobruja and Giurgiu, although the bulk of the Romanian army managed to escape encirclement and retreated toBucharest, whichsurrenderedto the Central Powers on 6 December 1916.[155]
In the summer of 1917, a Central Powers offensive began in Romania under the command of August von Mackensen to knock Romania out of the war, resulting in the battles ofOituz,MărăștiandMărășești, where up to 1,000,000 Central Powers troops were present. The battles lasted from 22 July to 3 September and eventually, the Romanian army was victorious advancing 500 km2. August von Mackensen could not plan for another offensive as he had to transfer troops to the Italian Front.[156]Following the Russian revolution, Romania found itself alone on the Eastern Front and signed theTreaty of Bucharestwith the Central Powers, which recognised Romanian sovereignty overBessarabiain return for ceding control of passes in the Carpathian Mountains to Austria-Hungary and leasing its oil wells to Germany. Although approved byParliament,King Ferdinand Irefused to sign it, hoping for an Allied victory in the west.[157]Romania re-entered the war on 10 November 1918, on the side of the Allies and the Treaty of Bucharest was formally annulled by the Armistice of 11 November 1918.[158][e]
On 12 December 1916, after ten brutal months of theBattle of Verdunand asuccessful offensive against Romania, Germany attempted to negotiate a peace with the Allies.[160]However, this attempt was rejected out of hand as a "duplicitous war ruse".[160]
US presidentWoodrow Wilsonattempted to intervene as a peacemaker, asking for both sides to state their demands.Lloyd George'sWar Cabinet considered the German offer to be a ploy to create divisions among the Allies. After initial outrage and much deliberation, they took Wilson's note as a separate effort, signalling that the US was on the verge of entering the war against Germany following the "submarine outrages". While the Allies debated a response to Wilson's offer, the Germans chose to rebuff it in favour of "a direct exchange of views". Learning of the German response, the Allied governments were free to make clear demands in their response of 14 January. They sought restoration of damages, the evacuation of occupied territories, reparations for France, Russia and Romania, and a recognition of the principle of nationalities.[161]The Allies sought guarantees that would prevent or limit future wars.[162]The negotiations failed and the Entente powers rejected the German offer on the grounds of honour, and noted Germany had not put forward any specific proposals.[160]
By the end of 1916, Russian casualties totalled nearly five million killed, wounded or captured, with major urban areas affected by food shortages and high prices. In March 1917, Tsar Nicholas ordered the military to forcibly suppress strikes inPetrogradbut the troops refused to fire on the crowds.[163]Revolutionaries set up thePetrograd Sovietand fearing a left-wing takeover, theState Dumaforced Nicholas to abdicate and established theRussian Provisional Government, which confirmed Russia's willingness to continue the war. However, the Petrograd Soviet refused to disband, creatingcompeting power centresand causing confusion and chaos, with frontline soldiers becoming increasingly demoralised.[164]
Following the tsar's abdication,Vladimir Lenin—with the help of the German government—was ushered from Switzerland into Russia on 16 April 1917. Discontent and the weaknesses of the Provisional Government led to a rise in the popularity of the Bolshevik Party, led by Lenin, which demanded an immediate end to the war. The Revolution of November was followed in December by an armistice and negotiations with Germany. At first, the Bolsheviks refused the German terms, but when German troops began marching across Ukraine unopposed, they acceded to theTreaty of Brest-Litovskon 3March 1918. The treaty ceded vast territories, including Finland, Estonia, Latvia, Lithuania, and parts of Poland and Ukraine to the Central Powers.[165]
With theRussian Empireout of the war, Romania found itself alone on the Eastern Front and signed theTreaty of Bucharestwith the Central Powers in May 1918. Under the terms of the treaty, Romania ceded territory to Austria-Hungary and Bulgaria and leased its oil reserves to Germany. However, the terms also included the Central Powers' recognition of the union ofBessarabiawith Romania.[166][157]
The United States was a major supplier of war material to the Allies but remained neutral in 1914, in large part due to domestic opposition.[167]The most significant factor in creating the support Wilson needed was the German submarine offensive, which not only cost American lives but paralysed trade as ships were reluctant to put to sea.[168]
On 6 April 1917, Congressdeclared war on Germanyas an "Associated Power" of the Allies.[169]TheUS Navysent abattleship grouptoScapa Flowto join the Grand Fleet, and provided convoy escorts. In April 1917, theUS Armyhad fewer than 300,000 men, includingNational Guardunits, compared to British and French armies of 4.1 and 8.3 million respectively. TheSelective Service Act of 1917drafted 2.8 million men, though training and equipping such numbers was a huge logistical challenge. By June 1918, over 667,000 members of theAmerican Expeditionary Forces(AEF) were transported to France, a figure which reached 2 million by the end of November.[170]
Despite his conviction that Germany must be defeated, Wilson went to war to ensure the US played a leading role in shaping the peace, which meant preserving the AEF as a separate military force, rather than being absorbed into British or French units as his Allies wanted.[171]He was strongly supported by AEF commander GeneralJohn J. Pershing, a proponent of pre-1914 "open warfare" who considered the French and British emphasis on artillery misguided and incompatible with American "offensive spirit".[172]Much to the frustration of his Allies, who had suffered heavy losses in 1917, he insisted on retaining control of American troops, and refused to commit them to the front line until able to operate as independent units. As a result, the first significant US involvement was theMeuse–Argonne offensivein late September 1918.[173]
In December 1916,Robert Nivellereplaced Pétain as commander of French armies on the Western Front and began planning aspring attackinChampagne, part of a joint Franco-British operation.[174]Poor security meant German intelligence was well informed on tactics and timetables, but despite this, when the attack began on 16 April the French made substantial gains, before being brought to a halt by the newly built and extremely strong defences of the Hindenburg Line. Nivelle persisted with frontal assaults and, by 25 April, the French had suffered nearly 135,000 casualties, including 30,000 dead, most incurred in the first two days.[175]
Concurrent British attacks atArraswere more successful, though ultimately of little strategic value.[176]Operating as a separate unit for the first time, theCanadian Corps' capture ofVimy Ridgeis viewed by many Canadians as a defining moment in creating a sense of national identity.[177][178]Though Nivelle continued the offensive, on 3 May the21st Division, which had been involved in some of the heaviest fighting at Verdun, refused orders to go into battle, initiating theFrench Army mutinies; within days, "collective indiscipline" had spread to 54 divisions, while over 20,000 deserted.[179]
In March and April 1917, at theFirstandSecond Battles of Gaza, German and Ottoman forces stopped the advance of the Egyptian Expeditionary Force, which had begun in August 1916 at the Battle of Romani.[180][181]At the end of October 1917, theSinai and Palestine campaignresumed, when GeneralEdmund Allenby'sXXth Corps,XXI CorpsandDesert Mounted Corpswon theBattle of Beersheba.[182]Two Ottoman armies were defeated a few weeks later at theBattle of Mughar Ridgeand, early in December,Jerusalemhad been captured following another Ottoman defeat at theBattle of Jerusalem.[183][184][185]
About this time,Friedrich Freiherr Kress von Kressensteinwas relieved of his duties as the Eighth Army's commander, replaced byDjevad Pasha, and a few months later the commander of theOttoman Armyin Palestine,Erich von Falkenhayn, was replaced byOtto Liman von Sanders.[186][187]
In early 1918, the front line wasextendedand theJordan Valleywas occupied, following theFirst Transjordanand theSecond Transjordanattacks by British Empire forces in March and April 1918.[188]
In December 1917, the Central Powers signed an armistice with Russia, thus freeing large numbers of German troops for use in the West. With German reinforcements and new American troops pouring in, the outcome was to be decided on the Western Front. The Central Powers knew that they could not win a protracted war, but they held high hopes for success in a final quick offensive.[190]Ludendorff drew up plans (Operation Michael) for the 1918 offensive on the Western Front. The operation commenced on 21 March 1918, with an attack on British forces nearSaint-Quentin. German forces achieved an unprecedented advance of 60 kilometres (37 mi).[191]The initial offensive was a success; after heavy fighting, however, the offensive was halted. Lacking tanks ormotorised artillery, the Germans were unable to consolidate their gains. The problems of re-supply were also exacerbated by increasing distances that now stretched over terrain that was shell-torn and often impassable to traffic.[192]Germany launchedOperation Georgetteagainst the northernEnglish Channelports. The Allies halted the drive after limited territorial gains by Germany. The German Army to the south then conductedOperations Blücher and Yorck, pushing broadly towards Paris. Germany launched Operation Marne (Second Battle of the Marne) on 15 July, in an attempt to encircleReims. The resulting counter-attack, which started theHundred Days Offensiveon 8 August,[193]led to a marked collapse in German morale.[194][195][196]
By September, the Germans had fallen back to the Hindenburg Line. The Allies hadadvanced to the Hindenburg Linein the north and centre. German forces launched numerous counterattacks, but positions and outposts of the Line continued falling, with the BEF alone taking 30,441 prisoners in the last week of September. On 24 September, the Supreme Army Command informed the leaders in Berlin that armistice talks were inevitable.[197]
Thefinal assaulton the Hindenburg Line began with theMeuse-Argonne offensive, launched by American and French troops on 26 September. Two days later the Belgians, French and Britishattacked around Ypres, and the day after the British at St Quentin in the centre of the line. The following week, cooperating American and French units broke through inChampagneat theBattle of Blanc Mont Ridge( 3–27 October), forcing the Germans off the commanding heights, and closing towards the Belgian frontier.[198]On 8October, the Hindenburg Line was pierced by British and Dominion troops of the First and Third British Armies at theSecond Battle of Cambrai.[199]
Allied forces started theVardar offensiveon 15 September at two key points:Dobro Poleand nearDojran Lake. In theBattle of Dobro Pole, the Serbian and French armies had success after a three-day-long battle with relatively small casualties, and subsequently made a breakthrough in the front, something which was rarely seen in World War I. After the front was broken, Allied forces started to liberate Serbia and reachedSkopjeat 29 September, after whichBulgariasigned an armistice with the Allies on 30 September.[200][201]
The collapse of the Central Powers came swiftly. Bulgaria was the first to sign an armistice, theArmistice of Salonicaon 29 September 1918.[202]Wilhelm II, in a telegram to TsarFerdinand I of Bulgariadescribed the situation thus: "Disgraceful! 62,000 Serbs decided the war!".[203][204]On the same day, theGerman Supreme Army Commandinformed Wilhelm II and theImperial ChancellorCountGeorg von Hertling, that the military situation facing Germany was hopeless.[205]
On 24 October, the Italians began a push that rapidly recovered territory lost after the Battle of Caporetto. This culminated in the Battle of Vittorio Veneto, marking the end of the Austro-Hungarian Army as an effective fighting force. The offensive also triggered the disintegration of the Austro-Hungarian Empire. During the last week of October, declarations of independence were made in Budapest, Prague, and Zagreb. On 29 October, the imperial authorities asked Italy for an armistice, but the Italians continued advancing, reaching Trento, Udine, and Trieste. On 3November, Austria-Hungary sent aflag of truceand accepted theArmistice of Villa Giusti, arranged with the Allied Authorities in Paris. Austria and Hungary signed separate armistices following the overthrow of theHabsburg monarchy. In the following days, the Italian Army occupiedInnsbruckand allTyrol, with over 20,000 soldiers.[206]On 30 October, the Ottoman Empire capitulated, and signed the Armistice of Mudros.[202]
With the military faltering and with widespread loss of confidence in the kaiser, Germany moved towards surrender. Prince Maximilian of Baden took charge on 3 October as Chancellor of Germany. Negotiations with President Wilson began immediately, in the hope that he would offer better terms than the British and French. Wilson demanded a constitutional monarchy and parliamentary control over the German military.[208]
TheGerman Revolution of 1918–1919began at the end of October 1918. Units of the German Navy refused to set sail for a large-scale operation in a war they believed to be as good as lost. Thesailors' revolt, which then ensued in the naval ports ofWilhelmshavenandKiel, spread across the whole country within days and led to theproclamation of a republicon 9November 1918, shortly thereafter to theabdication of Wilhelm II, and German surrender.[209][210][211][212][213]
In the aftermath of the war, the German, Austro-Hungarian, Ottoman, and Russian empires disappeared.[f]Numerous nations regained their former independence, and new ones were created. Four dynasties fell as a result of the war: theRomanovs, theHohenzollerns, theHabsburgs, and theOttomans. Belgium and Serbia were badly damaged, as was France, with 1.4 million soldiers dead,[214]not counting other casualties. Germany and Russia were similarly affected.[215]
A formal state of war between the two sides persisted for another seven months, until the signing of theTreaty of Versailleswith Germany on 28 June 1919. The US Senate did not ratify the treaty despite public support for it,[216][217]and did not formally end its involvement in the war until theKnox–Porter Resolutionwas signed on 2July 1921 by PresidentWarren G. Harding.[218]For the British Empire, the state of war ceased under the provisions of theTermination of the Present War (Definition) Act 1918concerning:
Somewar memorialsdate the end of the war as being when the Versailles Treaty was signed in 1919, which was when many of the troops serving abroad finally returned home; by contrast, most commemorations of the war's end concentrate on the armistice of 11 November 1918.[224]
TheParis Peace Conferenceimposed a series of peace treaties on the Central Powers officially ending the war. The 1919Treaty of Versaillesdealt with Germany and, building onWilson's 14th point, established theLeague of Nationson 28 June 1919.[225][226]
The Central Powers had to acknowledge responsibility for "all the loss and damage to which the Allied and Associated Governments and their nationals have been subjected as a consequence of the war imposed upon them by" their aggression. In the Treaty of Versailles, this statement wasArticle 231. This article became known as the "War Guilt Clause", as the majority of Germans felt humiliated and resentful.[227]The Germans felt they had been unjustly dealt with by what they called the "diktatof Versailles". German historian Hagen Schulze said the Treaty placed Germany "under legal sanctions, deprived of military power, economically ruined, and politically humiliated."[228]Belgian historian Laurence Van Ypersele emphasises the central role played by memory of the war and the Versailles Treaty in German politics in the 1920s and 1930s:
Active denial of war guilt in Germany and German resentment at both reparations and continued Allied occupation of the Rhineland made widespread revision of the meaning and memory of the war problematic. The legend of the "stab in the back" and the wish to revise the "Versailles diktat", and the belief in an international threat aimed at the elimination of the German nation persisted at the heart of German politics. Even a man of peace such as [Gustav] Stresemann publicly rejected German guilt. As for the Nazis, they waved the banners of domestic treason and international conspiracy in an attempt to galvanise the German nation into a spirit of revenge. Like a Fascist Italy, Nazi Germany sought to redirect the memory of the war to the benefit of its policies.[229]
Meanwhile, new nations liberated from German rule viewed the treaty as a recognition of wrongs committed against small nations by much larger aggressive neighbours.[230]
Austria-Hungary was partitioned into several successor states, largely but not entirely along ethnic lines. Apart from Austria and Hungary, Czechoslovakia, Italy, Poland, Romania and Yugoslavia received territories from the Dual Monarchy (the formerly separate and autonomousKingdom of Croatia-Slavoniawas incorporated into Yugoslavia). The details were contained in the treaties ofSaint-Germain-en-LayeandTrianon. As a result, Hungary lost 64% of its total population, decreasing from 20.9 million to 7.6 million, and losing 31% (3.3 out of 10.7 million) of its ethnicHungarians.[231]According to the 1910 census, speakers of the Hungarian language included approximately 54% of the entire population of theKingdom of Hungary. Within the country, numerous ethnic minorities were present: 16.1%Romanians, 10.5%Slovaks, 10.4%Germans, 2.5%Ruthenians, 2.5%Serbsand 8% others.[232]Between 1920 and 1924, 354,000 Hungarians fled former Hungarian territories attached to Romania, Czechoslovakia, and Yugoslavia.[233]
The Russian Empire lost much of its western frontier as the newly independent nations ofEstonia,Finland,Latvia,Lithuania, andPolandwere carved from it. Romania took control of Bessarabia in April 1918.[234]
After 123 years, Poland re-emerged as an independent country. The Kingdom of Serbia and its dynasty, as a "minor Entente nation" and the country with the most casualties per capita,[235][236][237]became the backbone of a new multinational state, theKingdom of Serbs, Croats and Slovenes, later renamed Yugoslavia. Czechoslovakia, combining theKingdom of Bohemiawith parts of the Kingdom of Hungary, became a new nation. Romania woulduniteall Romanian-speaking people under a single state, leading toGreater Romania.[238]
In Australia and New Zealand, the Battle of Gallipoli became known as those nations' "Baptism of Fire". It was the first major war in which the newly established countries fought, and it was one of the first times that Australian troops fought as Australians, not just subjects of theBritish Crown, and independent national identities for these nations took hold.Anzac Day, named after the Australian and New Zealand Army Corps (ANZAC), commemorates this defining moment.[239][240]
In the aftermath of World War I, Greecefoughtagainst Turkish nationalists led byMustafa Kemal, a war that eventually resulted in amassive population exchange between the two countriesunder the Treaty of Lausanne.[241]According to various sources,[242]several hundred thousand Greeks died during this period, which was tied in with the Greek genocide.[243]
The total number of military andcivilian casualtiesin World War I was about 40 million: estimates range from around 15 to 22 million deaths[244]and about 23 million wounded military personnel, ranking it among the deadliest conflicts in human history. The total number of deaths includes between 9 and 11 millionmilitary personnel, with an estimated civilian death toll of about 6 to 13 million.[244][245]
Of the 60 million European military personnel who were mobilised from 1914 to 1918, an estimated 8 million were killed, 7 million were permanently disabled, and 15 million were seriously injured. Germany lost 15.1% of its active male population, Austria-Hungary lost 17.1%, and France lost 10.5%.[246]France mobilised 7.8 million men, of which 1.4 million died and 3.2 million were injured.[247]Approximately 15,000 deployed men sustained gruesome facial injuries, causing social stigma and marginalisation; they were called thegueules cassées(broken faces). In Germany, civilian deaths were 474,000 higher than in peacetime, due in large part to food shortages and malnutrition that had weakened disease resistance. These excess deaths are estimated as 271,000 in 1918, plus another 71,000 in the first half of 1919 when the blockade was still in effect.[248]Starvation caused by famine killed approximately 100,000 people in Lebanon.[249]
Diseases flourished in the chaotic wartime conditions. In 1914 alone, louse-borneepidemic typhuskilled 200,000 in Serbia.[250]Starting in early 1918, a major influenza epidemic known asSpanish fluspread across the world, accelerated by the movement of large numbers of soldiers, often crammed together in camps and transport ships with poor sanitation. The Spanish flu killed at least 17 to 25 million people,[251][252]including an estimated 2.64 million Europeans and as many as 675,000 Americans.[253]Between 1915 and 1926, an epidemic ofencephalitis lethargicaaffected nearly 5 million people worldwide.[254][255]
Eight millionequinesmostly horses, donkeys and mules died, three-quarters of them from the extreme conditions they worked in.[256]
The German army was the first to successfully deploy chemical weapons during the Second Battle of Ypres (April–May 1915), after German scientists under the direction ofFritz Haberat theKaiser Wilhelm Institutedeveloped a method to weaponisechlorine.[g][258]The use of chemical weapons had been sanctioned by the German High Command to force Allied soldiers out of their entrenched positions, complementing rather than supplanting more lethal conventional weapons.[258]Chemical weapons were deployed by all major belligerents throughout the war, inflicting approximately 1.3 million casualties, of which about 90,000 were fatal.[258]The use of chemical weapons in warfare was a direct violation of the1899 Hague Declaration Concerning Asphyxiating Gasesand the1907 Hague Convention on Land Warfare, which prohibited their use.[259][260]
Theethnic cleansingof the Ottoman Empire's Armenian population, including mass deportations and executions, during the final years of the Ottoman Empire is consideredgenocide.[262]The Ottomans carried out organised and systematic massacres of the Armenian population at the beginning of the war and manipulated acts of Armenian resistance by portraying them as rebellions to justify further extermination.[263]In early 1915, several Armenians volunteered to join the Russian forces and the Ottoman government used this as a pretext to issue theTehcir Law(Law on Deportation), which authorised the deportation of Armenians from the Empire's eastern provinces to Syria between 1915 and 1918. The Armenians were intentionallymarched to deathand a number were attacked by Ottoman brigands.[264]While the exact number of deaths is unknown, theInternational Association of Genocide Scholarsestimates around 1.5 million.[262][265]The government of Turkey continues todeny the genocideto the present day, arguing that those who died were victims of inter-ethnic fighting, famine, or disease during World War I; these claims are rejected by most historians.[266]
Other ethnic groups were similarly attacked by the Ottoman Empire during this period, including Assyrians andGreeks, and some scholars consider those events to be part of the same policy of extermination.[267][268][269]At least 250,000 Assyrian Christians, about half of the population, and 350,000–750,000AnatolianandPontic Greekswere killed between 1915 and 1922.[270]
About 8 million soldiers surrendered and were held inPOW campsduring the war. All nations pledged to follow theHague Conventionson fair treatment ofprisoners of war, and the survival rate for POWs was generally much higher than that of combatants at the front.[271]
Around 25–31% of Russian losses (as a proportion of those captured, wounded, or killed) were to prisoner status; for Austria-Hungary 32%; for Italy 26%; for France 12%; for Germany 9%; for Britain 7%. Prisoners from the Allied armies totalled about 1.4 million (not including Russia, which lost 2.5–3.5 million soldiers as prisoners). From the Central Powers, about 3.3 million soldiers became prisoners; most of them surrendered to Russians.[272]
Allied personnel was around 42,928,000, while Central personnel was near 25,248,000.[215]British soldiers of the war were initially volunteers but were increasinglyconscripted. Surviving veterans returning home often found they could discuss their experiences only among themselves, so formed "veterans' associations" or "Legions".
Conscription was common in most European countries. However, it was controversial in English-speaking countries,[273]It was especially unpopular among minority ethnicities—especially the Irish Catholics in Ireland,[274]Australia,[275][page needed][276]and the French Catholics in Canada.[277][278]
In the US, conscription began in 1917 and was generally well-received, with a few pockets of opposition in isolated rural areas.[279][page needed]The administration decided to rely primarily on conscription, rather than voluntary enlistment, to raise military manpower after only 73,000 volunteers enlisted out of the initial 1 million target in the first six weeks of war.[280]
Military and civilian observers from every major power closely followed the course of the war.[281]Many were able to report on events from a perspective somewhat akin to modern "embedded" positions within the opposing land and naval forces.[282][283]
Macro- and micro-economic consequences devolved from the war. Families were altered by the departure of many men. With the death or absence of the primary wage earner, women were forced into the workforce in unprecedented numbers. At the same time, the industry needed to replace the lost labourers sent to war. This aided the struggle forvoting rights for women.[284]
In all nations, the government's share of GDP increased, surpassing 50% in both Germany and France and nearly reaching that level in Britain. To pay for purchases in the US, Britain cashed in its extensive investments in American railroads and then began borrowing heavily fromWall Street. President Wilson was on the verge of cutting off the loans in late 1916 but allowed a great increase inUS governmentlending to the Allies. After 1919, the US demanded repayment of these loans. The repayments were, in part, funded by German reparations that, in turn, were supported by American loans to Germany. This circular system collapsed in 1931 and some loans were never repaid. Britain still owed the United States $4.4billion[h]of World WarI debt in 1934; the last installment was finally paid in 2015.[285]
Britain turned to her colonies for help in obtaining essential war materials whose supply from traditional sources had become difficult. Geologists such asAlbert Kitsonwere called on to find new resources of precious minerals in the African colonies. Kitson discovered important new deposits ofmanganese, used in munitions production, in theGold Coast.[286]
Article 231 of the Treaty of Versailles (the so-called "war guilt" clause) stated Germany accepted responsibility for "all the loss and damage to which the Allied and Associated Governments and their nationals have been subjected as a consequence of the war imposed upon them by the aggression of Germany and her allies."[287]It was worded as such to lay a legal basis for reparations, and a similar clause was inserted in the treaties with Austria and Hungary. However, neither of them interpreted it as an admission of war guilt.[288]In 1921, the total reparation sum was placed at 132 billion gold marks. However, "Allied experts knew that Germany could not pay" this sum. The total sum was divided into three categories, with the third being "deliberately designed to be chimerical" and its "primary function was to mislead public opinion ... into believing the 'total sum was being maintained.'"[289]Thus, 50 billion gold marks (12.5 billion dollars) "represented the actual Allied assessment of German capacity to pay" and "therefore ... represented the total German reparations" figure that had to be paid.[289]
This figure could be paid in cash or in-kind (coal, timber, chemical dyes, etc.). Some of the territory lost—via the Treaty of Versailles—was credited towards the reparation figure as were other acts such as helping to restore the Library of Louvain.[290]By 1929, theGreat Depressioncaused political chaos throughout the world.[291]In 1932 the payment of reparations was suspended by the international community, by which point Germany had paid only the equivalent of 20.598 billion gold marks.[292]With the rise ofAdolf Hitler, all bonds and loans that had been issued and taken out during the 1920s and early 1930s were cancelled.David Andelmannotes "Refusing to pay doesn't make an agreement null and void. The bonds, the agreement, still exist." Thus, following the Second World War, at theLondon Conferencein 1953, Germany agreed to resume payment on the money borrowed. On 3October 2010, Germany made the final payment on these bonds.[i]
The Australian prime minister,Billy Hughes, wrote to the British prime minister,David Lloyd George, "You have assured us that you cannot get better terms. I much regret it, and hope even now that some way may be found of securing agreement for demanding reparation commensurate with the tremendous sacrifices made by the British Empire and her Allies." Australia received £5,571,720 in war reparations, but the direct cost of the war to Australia had been £376,993,052, and, by the mid-1930s, repatriation pensions, war gratuities, interest and sinking fund charges were £831,280,947.[297]
In the Balkans,Yugoslav nationalistssuch as the leader,Ante Trumbić, strongly supported the war, desiring the freedom ofYugoslavsfrom Austria-Hungary and other foreign powers and the creation of an independent Yugoslavia. TheYugoslav Committee, led by Trumbić, was formed in Paris on 30 April 1915, but shortly moved its office to London.[298]In April 1918, the Rome Congress of Oppressed Nationalities met, includingCzechoslovak, Italian,Polish,Transylvanian, and Yugoslav representatives who urged the Allies to support nationalself-determinationfor the peoples residing within Austria-Hungary.[299]
In the Middle East,Arab nationalismsoared in Ottoman territories in response to the rise of Turkish nationalism during the war, with Arab nationalist leaders advocating the creation of apan-Arabstate. In 1916, the Arab Revolt began in Ottoman-controlled territories of the Middle East to achieve independence.[300]
In East Africa,Iyasu VofEthiopiawas supporting theDervish statewho were at war with the British in theSomaliland campaign.[301]Von Syburg, the German envoy inAddis Ababa, said, "now the time has come for Ethiopia to regain the coast of the Red Sea driving the Italians home, to restore the Empire to its ancient size." The Ethiopian Empire was on the verge of entering World WarI on the side of the Central Powers before Iyasu's overthrow at theBattle of Segaledue to Allied pressure on the Ethiopian aristocracy.[302]
Several socialist parties initially supported the war when it began in August 1914.[299]But European socialists split on national lines, with the concept ofclass conflictheld by radical socialists such as Marxists andsyndicalistsbeing overborne by their patriotic support for the war.[303]Once the war began, Austrian, British, French, German, and Russian socialists followed the rising nationalist current by supporting their countries' intervention in the war.[304]
Italian nationalismwas stirred by the outbreak of the war and was initially strongly supported by a variety of political factions. One of the most prominent and popular Italian nationalist supporters of the war wasGabriele D'Annunzio, who promotedItalian irredentismand helped sway the Italian public to support intervention in the war.[305]TheItalian Liberal Party, under the leadership ofPaolo Boselli, promoted intervention in the war on the side of the Allies and used the Dante Alighieri Society to promote Italian nationalism.[306]Italian socialists were divided on whether to support the war or oppose it; some were militant supporters of the war, includingBenito MussoliniandLeonida Bissolati.[307]However, theItalian Socialist Partydecided to oppose the war after anti-militarist protestors were killed, resulting in ageneral strikecalledRed Week.[308]The Italian Socialist Party purged itself of pro-war nationalist members, including Mussolini.[308]Mussolini formed the pro-interventionistIl Popolo d'Italiaand theFasci Rivoluzionario d'Azione Internazionalista("RevolutionaryFascifor International Action") in October 1914 that later developed into theFasci Italiani di Combattimentoin 1919, the origin of fascism.[309]Mussolini's nationalism enabled him to raise funds fromAnsaldo(an armaments firm) and other companies to createIl Popolo d'Italiato convince socialists and revolutionaries to support the war.[310]
On both sides, there was large-scale fundraising for soldiers' welfare, their dependents and those injured. TheNail Menwere a German example. Around the British Empire, there were many patriotic funds, including theRoyal Patriotic Fund Corporation,Canadian Patriotic Fund,Queensland Patriotic Fundand, by 1919, there were 983 funds in New Zealand.[311]At the start of the next world war the New Zealand funds were reformed, having been criticised as overlapping, wasteful and abused,[312]but 11 were still functioning in 2002.[313]
Many countries jailed those who spoke out against the conflict. These includedEugene Debsin the US andBertrand Russellin Britain. In the US, theEspionage Act of 1917andSedition Act of 1918made it a federal crime to oppose military recruitment or make any statements deemed "disloyal". Publications at all critical of the government were removed from circulation by postal censors,[314][page needed]and many served long prison sentences for statements of fact deemed unpatriotic.
Several nationalists opposed intervention, particularly within states that the nationalists were hostile to. Although the vast majority of Irish people consented to participate in the war in 1914 and 1915, a minority of advancedIrish nationalistshad staunchly opposed taking part.[315][page needed]The war began amid the Home Rule crisis in Ireland that had resurfaced in 1912, and by July 1914 there was a serious possibility of an outbreak of civil war in Ireland. Irish nationalists and Marxists attempted to pursue Irish independence, culminating in theEaster Risingof 1916, with Germany sending 20,000 rifles to Ireland to stir unrest in Britain.[316]The British government placed Ireland undermartial lawin response to the Easter Rising, though once the immediate threat of revolution had dissipated, the authorities did try to make concessions to nationalist feeling.[317][page needed]However, opposition to involvement in the war increased in Ireland, resulting in theConscription Crisis of 1918.
Other opposition came fromconscientious objectors—some socialist, some religious—who had refused to fight. In Britain, 16,000 people asked for conscientious objector status.[318]Some of them, most notably prominent peace activistStephen Hobhouse, refused both military andalternative service.[319]Many suffered years of prison, includingsolitary confinement. Even after the war, in Britain, many job advertisements were marked "No conscientious objectors need to apply".[320]
On 1–4 May 1917, about 100,000 workers and soldiers ofPetrograd, and after them, the workers and soldiers of other Russian cities, led by the Bolsheviks, demonstrated under banners reading "Down with the war!" and "all power to the Soviets!". The mass demonstrations resulted in a crisis for theRussian Provisional Government.[321]InMilan, in May 1917, Bolshevik revolutionaries organised and engaged in rioting calling for an end to the war, and managed to close down factories and stop public transportation.[322]The Italian army was forced to enter Milan with tanks and machine guns to face Bolsheviks andanarchists, who fought violently until 23 May when the army gained control of the city. Almost 50 people (including three Italian soldiers) were killed and over 800 people were arrested.[322]
World War I began as a clash of 20th-century technology and 19th-centurytactics, with the inevitably large ensuing casualties. By the end of 1917, however, the major armies had modernised and were making use of telephone,wireless communication,[324]armoured cars,tanks(especially with the advent of the prototype tank,Little Willie), and aircraft.[325]
Artillery also underwent a revolution. In 1914, cannons were positioned in the front line and fired directly at their targets. By 1917,indirect firewith guns (as well as mortars and even machine guns) was commonplace, using new techniques for spotting and ranging, notably, aircraft and thefield telephone.[326]
Fixed-wing aircraftwere initially used forreconnaissanceandground attack. To shoot down enemy planes,anti-aircraft gunsandfighter aircraftwere developed.Strategic bomberswere created, principally by the Germans and British, though the former usedZeppelinsas well.[327]Towards the end of the conflict,aircraft carrierswere used for the first time, with HMSFuriouslaunchingSopwith Camelsin a raid to destroy the Zeppelin hangars atTønderin 1918.[328]
The non-military diplomatic and propaganda interactions among the nations were designed to build support for the cause or to undermine support for the enemy. For the most part, wartime diplomacy focused on five issues:propaganda campaigns; defining and redefining the war goals, which became harsher as the war went on; luring neutral nations (Italy, Ottoman Empire, Bulgaria, Romania) into the coalition by offering slices of enemy territory; and encouragement by the Allies of nationalistic minority movements inside the Central Powers, especially among Czechs, Poles, and Arabs. In addition, multiple peace proposals were coming from neutrals, or one side or the other; none of them progressed very far.[329][330][page needed][331][page needed]
Memorials were built in thousands of villages and towns. Close to battlefields, those buried in improvised burial grounds were gradually moved to formal graveyards under the care of organisations such as theCommonwealth War Graves Commission, theAmerican Battle Monuments Commission, theGerman War Graves Commission, andLe Souvenir français. Many of these graveyards also have monuments to the missing orunidentifieddead, such as theMenin Gate Memorial to the Missingand theThiepval Memorial to the Missing of the Somme.[332][333]
In 1915,John McCrae, a Canadian army doctor, wrote the poemIn Flanders Fieldsas a salute to those who perished in the war. It is still recited today, especially onRemembrance DayandMemorial Day.[334][335]
National World War I Museum and MemorialinKansas City, Missouri, is a memorial dedicated to all Americans who served in World WarI. TheLiberty Memorialwas dedicated on 1 November 1921.[336]
The British government budgeted substantial resources tothe commemoration of the war during the period 2014 to 2018. The lead body is theImperial War Museum.[337]On 3August 2014, French PresidentFrançois Hollandeand German PresidentJoachim Gaucktogether markedthe centenary of Germany's declaration of war on Franceby laying the first stone of a memorial in Vieil Armand, known in German asHartmannswillerkopf, for French and German soldiers killed in the war.[338]As part of commemorations for thecentenary of the 1918 Armistice, French PresidentEmmanuel Macronand German ChancellorAngela Merkelvisited the site of the signing of the Armistice of Compiègne and unveiled a plaque to reconciliation.[339]
... "Strange, friend," I said, "Here is no cause to mourn.""None," said the other, "Save the undone years"...
The first efforts to comprehend the meaning and consequences ofmodern warfarebegan during the initial phases of the war and are still underway more than a century later. Teaching World War I has presented special challenges. When compared withWorld War II, the First World War is often thought to be "a wrong war fought for the wrong reasons"; it lacks themetanarrativeofgood versus evilthat characterises retellings of the Second World War. Lacking recognizable heroes and villains, it is often taught thematically, invoking simplifiedtropesthat obscure the complexity of the conflict.[341]
Historian Heather Jones argues that thehistoriographyhas been reinvigorated by a cultural turn in the 21st century. Scholars have raised entirely new questions regardingmilitary occupation,radicalisationof politics,race,medical science,genderandmental health. Among the major subjects that historians have long debated regarding the war include:Why the war began; why theAllieswon; whether generals were responsible forhigh casualty rates; how soldiers endured the poor conditions oftrench warfare; and to what extent the civilianhome frontaccepted and endorsed the war effort.[342][343]
As late as 2007,unexploded ordnanceat battlefield sites likeVerdunandSommecontinued to pose a danger. In France and Belgium, locals who discover caches of unexploded munitions are assisted by weapons disposal units. In some places, plant life has still not recovered from the effects of the war.[341]
Lawson, Eric; Lawson, Jane (2002).The First Air Campaign: August 1914 – November 1918. Da Capo Press.ISBN978-0-306-81213-2.
Lieberman, Benjamin (2013).The Holocaust and Genocides in Europe. New York: Continuum Publishing Corporation.ISBN978-1-4411-9478-7.
TheFrench Revolution(French:Révolution française[ʁevɔlysjɔ̃fʁɑ̃sɛːz]) was a period of political and societal change inFrancewhich began with theEstates General of 1789and ended with theCoup of 18 Brumaireon 9 November 1799. Many of the revolution's ideas are considered fundamental principles ofliberal democracy,[1]and its values remain central to modern French political discourse.[2]
The causes of the revolutionwere a combination of social, political, and economic factors which theancien régime("old regime") proved unable to manage. A financial crisis and widespread social distress led to the convocation of the Estates General in May 1789, its first meeting since 1614. The representatives of theThird Estatebroke away and re-constituted themselves as aNational Assemblyin June. TheStorming of the BastilleinParison 14 July was followed by radical measures by the Assembly, among them theabolition of feudalism, state control over theCatholic Church, anda declaration of rights. The next three years were dominated by a struggle for political control. KingLouis XVI's attemptedflight to Varennesin June 1791 further discredited the monarchy, and military defeats after the outbreak of theFrench Revolutionary Warsin April 1792 led toan armed insurrection on 10 August 1792. Themonarchywas replaced by theFrench First Republicin September, andLouis XVI was executedin January 1793.
After anotherrevolt in June 1793, the constitution was suspended, and political power passed from theNational Conventionto theCommittee of Public Safety, dominated by radicalJacobinsled byMaximilien Robespierre. About 16,000 people were sentenced by theRevolutionary Tribunaland executed in theReign of Terror, which ended in July 1794 with theThermidorian Reaction. Weakened by external threats and internal opposition, the Committee of Public Safety was replaced in November 1795 by theDirectory. Its instability ended in the coup of 18 Brumaire and the establishment of theConsulate, withNapoleon Bonaparteas First Consul.
The Revolution resulted from multiple long-term and short-term factors, culminating in a social, economic, financial and political crisis in the late 1780s.[3][4][5]Combined with resistance to reform by the ruling elite and indecisive policy byLouis XVIand his ministers, the result was a crisis the state was unable to manage.[6][7]
Between 1715 and 1789, the French population grew from 21 to 28 million, 20% of whom lived in towns or cities, Paris alone having over 600,000 inhabitants.[8]This was accompanied by a tripling in the size of the middle class, which comprised almost 10% of the population by 1789.[9]Despite increases in overall prosperity, its benefits were largely restricted to the rentier and mercantile classes, while the living standards fell for wage labourers and peasant farmers who rented their land.[10][11]Economic recession from 1785, combined with bad harvests in 1787 and 1788, led to high unemployment and food prices, causing a financial and political crisis.[3][12][13][14]
While the state also experienced a debt crisis, the level of debt itself was not high compared with Britain's.[15]A significant problem was that tax rates varied widely from one region to another, were often different from the official amounts, and were collected inconsistently. Its complexity meant uncertainty over the amount contributed by any authorised tax caused resentment among all taxpayers.[16][a]Attempts to simplify the system were blocked by the regionalParlementswhich approved financial policy. The resulting impasse led to the calling of theEstates General of 1789, which became radicalised by the struggle for control of public finances.[18]
Louis XVI was willing to consider reforms, but he often backed down when faced with opposition from conservative elements within the nobility.Enlightenmentcritiques of social institutions were widely discussed among the educated French elite. At the same time, theAmerican Revolutionand the European revolts of the 1780s inspired public debate on issues such as patriotism, liberty, equality, and democracy. These shaped the response of the educated public to the crisis,[19]while scandals such as theAffair of the Diamond Necklacefuelled widespread anger at the court, nobility, and church officials.[20]
France faced a series of budgetary crises during the 18th century as revenues failed to keep pace with expenditure.[21][22]Although the economy grew solidly, the increase was not reflected in a proportional growth in taxes,[21]their collection being contracted totax farmerswho kept much of it as personal profit. As the nobility and Church benefited from many exemptions, the tax burden fell mainly on peasants.[23]Reform was difficult because new tax laws had to be registered with regional judicial bodies orparlementsthat were able to block them. The king could impose laws by decree, but this risked open conflict with theparlements, the nobility, and those subject to new taxes.[24]
France primarily funded theAnglo-French Warof 1778–1783 through loans. Following the peace, the monarchy borrowed heavily, culminating in a debt crisis. By 1788, half of state revenue was required to service its debt.[25]In 1786, the French finance minister,Calonne, proposed a package of reforms including a universal land tax, the abolition of grain controls and internal tariffs, and new provincial assemblies appointed by the king. The new taxes were rejected, first by a hand-pickedAssembly of Notablesdominated by the nobility, then by theparlementswhen submitted by Calonne's successorBrienne. The notables andparlementsargued that the proposed taxes could only be approved by an Estates-General, a representative body that had last met in 1614.[26]
The conflict between the Crown and theparlementsbecame a national political crisis. Both sides issued a series of public statements, the government arguing that it was combating privilege and theparlementdefending the ancient rights of the nation. Public opinion was firmly on the side of theparlements, and riots broke out in several towns. Brienne's attempts to raise new loans failed, and on 8 August 1788, he announced that the king would summon an Estates-General to convene the following May. Brienne resigned and was replaced byJacques Necker.[27]
In September 1788, the Parlement of Paris ruled that the Estates-General should convene in the same form as in 1614, meaning that the three estates (the clergy, nobility, and Third Estate or "commons") would meet and vote separately, with votes counted by estate rather than by head. As a result, the clergy and nobility could combine to outvote the Third Estate despite representing less than 5% of the population.[28][29]
Following the relaxation of censorship and laws against political clubs, a group of liberal nobles and middle class activists known as the Society of Thirty launched a campaign for the doubling of Third Estate representation and individual voting. The public debate sparked an average of 25 new political pamphlets published each week from 25 September 1788.[30]TheAbbé Sieyèsissued influential pamphlets titledWhat Is the Third Estate?denouncing the privilege of the clergy and nobility, and arguing the Third Estate represented the nation and should sit alone as a National Assembly. Activists such asJean Joseph Mounier,Antoine BarnaveandMaximilien Robespierreorganised regional meetings, petitions and literature in support of these demands.[31]In December, the king agreed to double the representation of the Third Estate but left the question of counting votes for the Estates-General to decide.[32]
The Estates-General contained three separate bodies, theFirst Estaterepresenting 100,000 clergy, theSecondthe nobility, and theThirdthe "commons".[33]Since each met separately, and any proposals had to be approved by at least two, the First and Second Estates could outvote the Third despite representing less than 5% of the population.[28]
Although theCatholic Church in Franceowned nearly 10% of all land, as well as receiving annualtithespaid by peasants,[34]three-quarters of the 303 clergy elected were parish priests, many of whom earned less than unskilled labourers and had more in common with their poor parishioners than with the bishops of the first estate.[35][36]The Second Estate elected 322 deputies, representing about 400,000 men and women, who owned about 25% of the land and collectedseigneurialdues and rents from their tenants. Most delegates were town-dwelling members of thenoblesse d'épée, or traditional aristocracy. Courtiers and representatives of thenoblesse de robe(those who derived rank from judicial or administrative posts) were underrepresented.[37]Of the 610 deputies of the Third Estate, about two-thirds held legal qualifications and almost half were venal office holders. Less than 100 were in trade or industry, and none were peasants or artisans.[38]To assist delegates, each region completed a list of grievances, known asCahiers de doléances.[39]Tax inequality and seigneurial dues (feudal payments owed to landowners) headed the grievances in thecahiers de doleancesfor the estate.[40]
On 5 May 1789 the Estates-General convened atVersailles. Necker outlined the state budget and reiterated the king's decision that each estate should decide on which matters it would agree to meet and vote in common with the other estates. On the following day, each estate was to separately verify the credentials of their representatives. The Third Estate, however, voted to invite the other estates to join them in verifying all the representatives of the Estates-General in common and to agree that votes should be counted by head. Fruitless negotiations lasted to 12 June when the Third Estate began verifying its own members. On 17 June, the Third Estate declared itself to be the National Assembly of France and that all existing taxes were illegal.[41]Within two days, more than 100 members of the clergy had joined them.[42]
Shaken by this challenge to his authority, the king agreed to a reform package that he would announce at a royal session of the Estates-General. TheSalle des Étatswas closed to prepare for the joint session, but the members of the Estates-General were not informed in advance. On 20 June, when the members of the Third Estate found their meeting place closed, they moved to a nearbytennis court and sworenot to disperse until a new constitution had been agreed.[43]
At the royal session the king announced a series of tax and other reforms and stated that no new taxes or loans would be implemented without the consent of the Estates-General. However, he stated that the three estates were sacrosanct and it was up to each estate to agree to end their privileges and decide on which matters they would vote in common with the other estates. At the end of the session the Third Estate refused to leave the hall and reiterated their oath not to disperse until a constitution had been agreed. Over the next days more members of the clergy joined the National Assembly. On 27 June, faced with popular demonstrations and mutinies in hisFrench Guards, Louis XVI capitulated. He commanded the members of the first and second estates to join the third in the National Assembly.[44]
Even the limited reforms the king had announced went too far forMarie Antoinetteand Louis' younger brother theComte d'Artois. On their advice, Louis dismissed Necker again as chief minister on 11 July.[45]On 12 July, the Assembly went into a non-stop session following rumours that the king was planning to use theSwiss Guardsto force it to close. The news brought crowds of protestors into the streets, and soldiers of the eliteGardes Françaisesrefused to disperse them.[46]
On 14 July many of these soldiers joined a crowdattacking the Bastille, a royal fortress with large stores of arms and ammunition. Its governor,Bernard-René de Launay, surrendered after several hours of fighting that cost the lives of 83 attackers. Launay was taken to theHôtel de Ville, where he was killed and his head placed on a pike and paraded around the city. Although rumoured to hold many prisoners, the Bastille held only seven: four forgers, a lunatic, a failed assassin, and a deviant nobleman. Nevertheless, it was a potent symbol of theAncien Régimeand it was demolished in the following weeks.[47]Bastille Dayhas become the French national holiday.[48]
Alarmed by the prospect of losing control of the capital, Louis appointed theMarquis de Lafayettecommander of theNational Guard, withJean-Sylvain Baillyas head of a new administrative structure known as theCommune. On 17 July, Louis visited Paris accompanied by 100 deputies, where he was greeted by Bailly and accepted atricolorecockadeto loud cheers. However, it was clear power had shifted from his court; he was welcomed as 'Louis XVI, father of the French and king of a free people.'[49]
The short-lived unity enforced on the Assembly by a common threat quickly dissipated. Deputies argued over constitutional forms, while civil authority rapidly deteriorated. On 22 July, former Finance MinisterJoseph Foullonand his son were lynched by a Parisian mob, and neither Bailly nor Lafayette could prevent it. In rural areas, wild rumours and paranoia resulted in the formation of militia and an agrarian insurrection known as theGreat Fear.[50]The breakdown of law and order and frequent attacks on aristocratic property led much of the nobility to flee abroad. Theseémigrésfunded reactionary forces within France and urged foreign monarchs to back acounter-revolution.[51]
In response, the Assembly published theAugust Decreeswhichabolished feudalism. Over 25% of French farmland was subject tofeudal dues, providing the nobility with most of their income; these were now cancelled, along with church tithes. While their former tenants were supposed to pay them compensation, collecting it proved impossible, and the obligation was annulled in 1793.[52]Other decrees included equality before the law, opening public office to all, freedom of worship, and cancellation of special privileges held by provinces and towns.[53]
With the suspension of the 13 regionalparlementsin November, the key institutional pillars of the old regime had all been abolished in less than four months. From its early stages, the Revolution therefore displayed signs of its radical nature; what remained unclear was the constitutional mechanism for turning intentions into practical applications.[54]
On 9 July, the National Assembly declared itself theNational Constituent Assembly[55]and appointed a committee to draft a constitution and statement of rights.[56]Twenty drafts were submitted, which were used by a sub-committee to create aDeclaration of the Rights of Man and of the Citizen, withMirabeaubeing the most prominent member.[57]The declaration was approved by the Assembly and published on 26 August as a statement of principle.[58]
The Assembly now concentrated on the constitution.Mounierand his monarchist supporters advocated abicameralsystem, with anupper houseappointed by the king, who would also have the right to appoint ministers and veto legislation. On 10 September, the majority of the Assembly, led by Sieyès andTalleyrand, voted in favour of a single body, and the following day approved a "suspensive veto" for the king, meaning Louis could delay implementation of a law but not block it indefinitely. In October, the Assembly voted to restrict political rights, including voting rights, to "active citizens", defined as French males over the age of 25 who paid direct taxes equal to three days' labour. The remainder were designated "passive citizens", restricted to "civil rights", a distinction opposed by a significant minority, including theJacobin clubs.[59][60]By mid-1790, the main elements of a constitutional monarchy were in place, although the constitution was not accepted by Louis until 1791.[61]
Food shortages and the worsening economy caused frustration at the lack of progress and led to popular unrest in Paris. This came to a head in late September 1789, when the Flanders Regiment arrived in Versailles to reinforce the royal bodyguard and were welcomed with a formal banquet as was common practice. The radical press described this as a 'gluttonous orgy' and claimed the tricolour cockade had been abused, while the Assembly viewed their arrival as an attempt to intimidate them.[62]
On 5 October, crowds of women assembled outside theHôtel de Ville, agitating against high food prices and shortages.[63]These protests quickly turned political, and after seizing weapons stored at theHôtel de Ville,some 7,000 of themmarched on Versailles, where they entered the Assembly to present their demands. They were followed to Versailles by 15,000 members of the National Guard under Lafayette, who was virtually "a prisoner of his own troops".[64]
When the National Guard arrived later that evening, Lafayette persuaded Louis the safety of his family required their relocation to Paris. Next morning, some of the protestors broke into the royal apartments, searching for Marie Antoinette, who had escaped. They ransacked the palace, killing several guards. Order was eventually restored, and the royal family and Assembly left for Paris, escorted by the National Guard.[65]Louis had announced his acceptance of the August Decrees and the declaration, and his official title changed from 'King of France' to 'King of the French'.[66]
HistorianJohn McMannersargues "in eighteenth-century France, throne and altar were commonly spoken of as in close alliance; their simultaneous collapse ... would one day provide the final proof of their interdependence." One suggestion is that after a century of persecution, someFrench Protestantsactively supported an anti-Catholic regime, a resentment fuelled by Enlightenment thinkers such asVoltaire.[67]Jean-Jacques Rousseau, considered a philosophical founder of the revolution,[68][69][70]wrote it was "manifestly contrary to thelaw of nature... that a handful of people should gorge themselves with superfluities, while the hungry multitude goes in want of necessities."[71]
The Revolution caused a massive shift of power from the Catholic Church to the state; although the extent of religious belief has been questioned, elimination of tolerance for religious minorities meant by 1789 being French also meant being Catholic.[72]The church was the largest individual landowner in France, controlling nearly 10% of all estates and leviedtithes, effectively a 10% tax on income, collected from peasant farmers in the form of crops. In return, it provided a minimal level of social support.[73]
The August Decrees abolished tithes, and on 2 November the Assembly confiscated all church property, the value of which was used to back a new paper currency known asassignats. In return, the state assumed responsibilities such as paying the clergy and caring for the poor, the sick and the orphaned.[74]On 13 February 1790, religious orders andmonasterieswere dissolved, whilemonksandnunswere encouraged to return to private life.[75]
TheCivil Constitution of the Clergyof 12 July 1790 made them employees of the state, established rates of pay, and developed a system for electing priests and bishops.Pope Pius VIand many French Catholics objected to this since it denied the authority of thePopeover the French church. In October, 30bishopswrote a declaration denouncing the law, further fuelling opposition.[76]When clergy were required to swear loyalty to the Civil Constitution in November, it split the church between the 24% who complied and the majority who refused.[77]This stiffened popular resistance against state interference, especially in traditionally Catholic areas such asNormandy,Brittanyand theVendée, where only a few priests took the oath and the civilian population turned against the revolution.[76]The result was state-led persecution of "refractory clergy", many of whom were forced into exile, deported, or executed.[78]
The period from October 1789 to spring 1791 is usually seen as one of relative tranquility, when some of the most important legislative reforms were enacted. However, conflict over the source of legitimate authority was more apparent in the provinces, where officers of theAncien Régimehad been swept away but not yet replaced by new structures. This was less obvious in Paris, since the National Guard made it the best policed city in Europe, but disorder in the provinces inevitably affected members of the Assembly.[79]
Centrists led by Sieyès, Lafayette, Mirabeau and Bailly created a majority by forging consensus withmonarchienslike Mounier, and independents includingAdrien Duport,BarnaveandAlexandre Lameth. At one end of the political spectrum,reactionarieslikeCazalèsandMaurydenounced the Revolution in all its forms, with radicals likeMaximilien Robespierreat the other. He andJean-Paul Maratopposed the criteria for "active citizens", gaining them substantial support among the Parisian proletariat, many of whom had been disenfranchised by the measure.[80]
On 14 July 1790, celebrations were held throughout France commemorating the fall of the Bastille, with participants swearing an oath of fidelity to "the nation, the law and the king." TheFête de la Fédérationin Paris was attended by the royal family, with Talleyrand performing amass. Despite this show of unity, the Assembly was increasingly divided, while external players like the Paris Commune and National Guard competed for power. One of the most significant was the Jacobin club; originally a forum for general debate, by August 1790 it had over 150 members, split into different factions.[81]
The Assembly continued to develop new institutions; in September 1790, the regionalParlementswere abolished and their legal functions replaced by a new independent judiciary, withjury trialsfor criminal cases. However, moderate deputies were uneasy at popular demands for universal suffrage, labour unions and cheap bread, and over the winter of 1790 and 1791, they passed a series of measures intended to disarm popular radicalism. These included exclusion of poorer citizens from the National Guard, limits on use of petitions and posters, and the June 1791Le Chapelier Lawsuppressing trade guilds and any form of worker organisation.[82]
The traditional force for preserving law and order was the army, which was increasingly divided between officers, who largely came from the nobility, and ordinary soldiers. In August 1790, the loyalist GeneralBouillésuppressed a serious mutiny atNancy; although congratulated by the Assembly, he was criticised by Jacobin radicals for the severity of his actions. Growing disorder meant many professional officers either left or became émigrés, further destabilising the institution.[83]
Held in theTuileries Palaceunder virtual house arrest, Louis XVI was urged by his brother and wife to re-assert his independence by taking refuge with Bouillé, who was based atMontmédywith 10,000 soldiers considered loyal to the Crown.[84]The royal family left the palace in disguise on the night of 20 June 1791; late the next day, Louis was recognised as he passed throughVarennes, arrested and taken back to Paris. The attempted escape had a profound impact on public opinion; since it was clear Louis had been seeking refuge in Austria, the Assembly now demanded oaths of loyalty to the regime and began preparing for war, while fear of 'spies and traitors' became pervasive.[85]
Despite calls to replace the monarchy with a republic, Louis retained his position but was generally regarded with acute suspicion and forced to swear allegiance to the constitution. A new decree stated retracting this oath, making war upon the nation, or permitting anyone to do so in his name would be considered abdication. However, radicals led byJacques Pierre Brissotprepared a petition demanding his deposition, and on 17 July, an immense crowd gathered in theChamp de Marsto sign. Led by Lafayette, the National Guard was ordered to "preserve public order" and responded to a barrage of stones byfiring into the crowd, killing between 13 and 50 people.[86]
The massacre badly damaged Lafayette's reputation; the authorities responded by closing radical clubs and newspapers, while their leaders went into exile or hiding, including Marat.[87]On 27 August, EmperorLeopold IIand KingFrederick William II of Prussiaissued theDeclaration of Pillnitzdeclaring their support for Louis and hinting at an invasion of France on his behalf. In reality, the meeting between Leopold and Frederick was primarily to discuss thepartitions of Poland; the declaration was intended to satisfy Comte d'Artois and other French émigrés, but the threat rallied popular support behind the regime.[88]
Based on a motion proposed by Robespierre, existing deputies were barred fromelections held in Septemberfor theFrench Legislative Assembly. Although Robespierre was one of those excluded, his support in the clubs gave him a political power base not available to Lafayette and Bailly, who resigned respectively as head of the National Guard and the Paris Commune. The new laws were gathered together in the1791 Constitution, and submitted to Louis XVI, who pledged to defend it "from enemies at home and abroad". On 30 September, the Constituent Assembly was dissolved, and the Legislative Assembly convened the next day.[89]
The Legislative Assembly is often dismissed by historians as an ineffective body, compromised by divisions over the role of the monarchy, an issue exacerbated when Louis attempted to prevent or reverse limitations on his powers.[90]At the same time, restricting the vote to those who paid a minimal amount of tax disenfranchised a significant proportion of the 6 million Frenchmen over 25, while only 10% of those able to vote actually did so. Finally, poor harvests and rising food prices led to unrest among the urban class known assans-culottes, who saw the new regime as failing to meet their demands for bread and work.[91]
This meant the new constitution was opposed by significant elements inside and outside the Assembly, itself split into three main groups. 264 members were affiliated with Barnave'sFeuillants, constitutional monarchists who considered the Revolution had gone far enough, while another 136 were Jacobin leftists who supported a republic, led by Brissot and usually referred to asBrissotins.[92]The remaining 345 belonged toLa Plaine, a centrist faction who switched votes depending on the issue, but many of whom shared doubts as to whether Louis was committed to the Revolution.[92]After he officially accepted the new Constitution, one recorded response was "Vive le roi, s'il est de bon foi!", or "Long live the king – if he keeps his word".[93]
Although a minority in the Assembly, control of key committees allowed theBrissotinsto provoke Louis into using his veto. They first managed to pass decrees confiscating émigré property and threatening them with the death penalty.[94]This was followed by measures against non-juring priests, whose opposition to the Civil Constitution led to a state of near civil war in southern France, which Barnave tried to defuse by relaxing the more punitive provisions. On 29 November, the Assembly approved a decree giving refractory clergy eight days to comply, or face charges of 'conspiracy against the nation', an act opposed even by Robespierre.[95]When Louis vetoed both, his opponents were able to portray him as opposed to reform in general.[96]
Brissot accompanied this with a campaign for war against Austria and Prussia, often interpreted as a mixture of calculation and idealism. While exploiting popular anti-Austrianism, it reflected a genuine belief in exporting the values of political liberty and popular sovereignty.[97]Simultaneously, conservatives headed by Marie Antoinette also favoured war, seeing it as a way to regain control of the military, and restore royal authority. In December 1791, Louis made a speech in the Assembly giving foreign powers a month to disband the émigrés or face war, an act greeted with enthusiasm by supporters, but suspicion from opponents.[98]
Barnave's inability to build a consensus in the Assembly resulted in the appointment of a new government, chiefly composed ofBrissotins. On 20 April 1792, theFrench Revolutionary Warsbegan when French armies attacked Austrian and Prussian forces along their borders, before suffering a series ofdisastrous defeats. In an effort to mobilise popular support, the government ordered non-juring priests to swear the oath or be deported, dissolved theConstitutional Guardand replaced it with 20,000fédérés; Louis agreed to disband the Guard, but vetoed the other two proposals, while Lafayette called on the Assembly to suppress the clubs.[99]
Popular anger increased when details of theBrunswick Manifestoreached Paris on 1 August, threatening 'unforgettable vengeance' should any oppose the Allies in seeking to restore the power of the monarchy.On the morning of 10 August, a combined force of the Paris National Guard and provincial fédérés attacked the Tuileries Palace, killing many of the Swiss Guards protecting it.[100]Louis and his family took refuge with the Assembly and shortly after 11:00 am, the deputies present voted to 'temporarily relieve the king', effectively suspending the monarchy.[101]
In late August,elections were heldfor theNational Convention. Restrictions on the franchise meant the number of votes cast fell to 3.3 million, versus 4 million in 1791, while intimidation was widespread.[102]TheBrissotinssplit between moderateGirondinsled by Brissot, and radicalMontagnards, headed by Robespierre,Georges DantonandJean-Paul Marat. While loyalties constantly shifted, voting patterns suggest roughly 160 of the 749 deputies can generally be categorised asGirondists, with another 200Montagnards. The remainder were part of a centrist faction known asLa Plaine, headed byBertrand Barère,Pierre Joseph CambonandLazare Carnot.[103]
In theSeptember Massacres, between 1,100 and 1,600 prisoners held in Parisian jails weresummarily executed, the vast majority being common criminals.[104]A response to the capture ofLongwyandVerdunby Prussia, the perpetrators were largely National Guard members andfédéréson their way to the front. While responsibility is still disputed, even moderates expressed sympathy for the action, which soon spread to the provinces. One suggestion is that the killings stemmed from concern over growing lawlessness, rather than political ideology.[105]
On 20 September, the French defeated the Prussians at theBattle of Valmy, in what was the first major victory by the army of France during theRevolutionary Wars. Emboldened by this, on 22 September the Convention replaced the monarchy with theFrench First Republicand introduceda new calendar, with 1792 becoming "Year One".[106]The next few months were taken up with the trial ofCitoyen Louis Capet, formerly Louis XVI. While evenly divided on the question of his guilt, members of the convention were increasingly influenced by radicals based within the Jacobin clubs and Paris Commune. The Brunswick Manifesto made it easy to portray Louis as a threat to the Revolution, especially when extractsfrom his personal correspondenceshowed him conspiring with Royalist exiles.[107]
On 17 January 1793, Louis was sentenced to death for "conspiracy against public liberty and general safety". 361 deputies were in favour, 288 against, while another 72 voted to execute him, subject to delaying conditions. The sentence wascarried out on 21 Januaryon thePlace de la Révolution, now thePlace de la Concorde.[108]Conservatives across Europe called for the destruction of revolutionary France, and in February the Convention responded by declaring war onBritainand theDutch Republic. Together with Austria and Prussia, these two countries were later joined bySpain,Portugal,Naples, andTuscanyin theWar of the First Coalition.[109]
The Girondins hoped war would unite the people behind the government and provide an excuse for rising prices and food shortages, but they found themselves the target of popular anger. Many left for the provinces. The first conscription measure orlevée en masseon 24 February sparked riots in Paris and other regional centres. Already unsettled by changes imposed on the church, in March the traditionally conservative and royalistVendéerose in revolt. On 18th, GeneralCharles François Dumouriezwasdefeated at Neerwindenand defected to the Austrians. Uprisings followed inBordeaux,Lyon,Toulon,MarseilleandCaen. The Republic seemed on the verge of collapse.[110]
The crisis led to the creation on 6 April 1793 of theCommittee of Public Safety, an executive committee accountable to the convention.[111]The Girondins made a fatal political error by indicting Marat before theRevolutionary Tribunalfor allegedly directing the September massacres; he was quickly acquitted, further isolating the Girondins from thesans-culottes. WhenJacques Hébertcalled for a popular revolt against the "henchmen of Louis Capet" on 24 May, he was arrested by theCommission of Twelve, a Girondin-dominated tribunal set up to expose 'plots'. In response to protests by the Commune, the Commission warned "if by your incessant rebellions something befalls the representatives of the nation, Paris will be obliterated".[110]
Growing discontent allowed the clubs to mobilise against the Girondins. Backed by the Commune and elements of the National Guard, on 31 May theyattempted to seize power in a coup. Although the coup failed, on 2 June the convention was surrounded by a crowd of up to 80,000, demanding cheap bread, unemployment pay and political reforms, including restriction of the vote to thesans-culottes, and the right to remove deputies at will.[112]Ten members of the commission and another twenty-nine members of the Girondin faction were arrested, and on 10 June, the Montagnards took over the Committee of Public Safety.[113]
Meanwhile, a committee led by Robespierre's close allyLouis Antoine de Saint-Justwas tasked with preparinga new constitution. Completed in only eight days, it was ratified by the convention on 24 June and contained radical reforms, includinguniversal male suffrage. However, normal legal processes were suspended following the assassination of Marat on 13 July by the GirondistCharlotte Corday, which the Committee of Public Safety used as an excuse to take control. The 1793 Constitution was suspended indefinitely in October.[114]
Key areas of focus for the new government included creating a new state ideology, economic regulation and winning the war.[115]They were helped by divisions among their internal opponents; while areas like the Vendée andBrittanywanted to restore the monarchy, most supported the Republic but opposed the regime in Paris. On 17 August, the Convention voted a secondlevée en masse; despite initial problems in equipping and supplying such large numbers, by mid-October Republican forces had re-taken Lyon, Marseille and Bordeaux, while defeating Coalition armies atHondschooteandWattignies.[116]The new class of military leaders included a young colonel namedNapoleon Bonaparte, who was appointed commander of artillery at thesiege of Toulonthanks to his friendship withAugustin Robespierre. His success in that role resulted in promotion to theArmy of Italyin April 1794, and the beginning of his rise to military and political power.[117]
Although intended to bolster revolutionary fervour, theReign of Terrorrapidly degenerated into the settlement of personal grievances. At the end of July, the Convention setprice controlson a wide range of goods, with the death penalty for hoarders. On 9 September, 'revolutionary groups' were established to enforce these controls, while theLaw of Suspectson 17 September approved the arrest of suspected "enemies of freedom". This initiated what has become known as the "Terror". From September 1793 to July 1794, around 300,000 were arrested,[118]with some 16,600 people executed on charges of counter-revolutionary activity, while another 40,000 may have been summarily executed, or died awaiting trial.[119]
Price controls made farmers reluctant to sell their produce in Parisian markets, and by early September the city was suffering acute food shortages. At the same time, the war increased public debt, which the Assembly tried to finance by selling confiscated property. However, few would buy assets that might be repossessed by their former owners, a concern that could only be achieved by military victory. This meant the financial position worsened as threats to the Republic increased, while printingassignatsto deal with the deficit further increased inflation.[120]
On 10 October, the Convention recognised the Committee of Public Safety as the supremeRevolutionary Governmentand suspended the constitution until peace was achieved.[114]In mid-October, Marie Antoinette was convicted of a long list of crimes and guillotined; two weeks later, the Girondist leaders arrested in June were also executed, along withPhilippe Égalité. The "Terror" was not confined to Paris, with over 2,000 killed in Lyons after its recapture.[121]
AtCholeton 17 October, the Republican army won a decisive victory over theVendée rebels, and the survivors escaped into Brittany. Adefeat at Le Manson 23 December ended the rebellion as a major threat, although the insurgency continued until 1796. The extent of the repression that followed has been debated by French historians since the mid-19th century.[122]Between November 1793 and February 1794, over 4,000 weredrowned in the Loire at Nantesunder the supervision ofJean-Baptiste Carrier. Historian Reynald Secher claims that as many as 117,000 died between 1793 and 1796. Although those numbers have been challenged,François Furetconcludes it "not only revealed massacre and destruction on an unprecedented scale, but a zeal so violent that it has bestowed as its legacy much of the region's identity."[123][b]
At the height of the Terror, not even its supporters were immune from suspicion, leading to divisions within theMontagnardfaction between radicalHébertistsand moderates led by Danton.[c]Robespierre saw their dispute as de-stabilising the regime, and, as adeist, objected to theanti-religious policiesadvocated by theatheistHébert, who was arrested and executed on 24 March with 19 of his colleagues.[127]To retain the loyalty of the remaining Hébertists, Danton was arrested and executed on 5 April withCamille Desmoulins, after ashow trialthat arguably did more damage to Robespierre than any other act in this period.[128]
TheLaw of 22 Prairial(10 June) denied "enemies of the people" the right to defend themselves. Those arrested in the provinces were sent to Paris for judgment; from March to July, executions in Paris increased from 5 to 26 per day.[129]Many Jacobins ridiculed the festival of theCult of the Supreme Beingon 8 June, a lavish and expensive ceremony led by Robespierre, who was also accused of circulating claims he was a second Messiah. Relaxation of price controls and rampant inflation caused increasing unrest among thesans-culottes, but theimproved military situationreduced fears the Republic was in danger. Fearing their own survival depended on Robespierre's removal, on 29 June three members of the Committee of Public Safety openly accused him of being a dictator.[130]Robespierre responded by refusing to attend Committee meetings, allowing his opponents to build a coalition against him. In a speech made to the convention on 26 July, he claimed certain members were conspiring against the Republic, an almost certain death sentence if confirmed. When he refused to provide names, the session broke up in confusion. That evening he repeated these claims at the Jacobins club, where it was greeted with demands for execution of the 'traitors'. Fearing the consequences if they did not act first, hisopponents attacked Robespierreand his allies in the Convention next day. When Robespierre attempted to speak, his voice failed, one deputy crying "The blood of Danton chokes him!"[131]
After the Convention authorised his arrest, he and his supporters took refuge in theHotel de Ville, which was defended by elements of the National Guard. Other units loyal to the Convention stormed the building that evening and detained Robespierre, who severely injured himself attempting suicide. He was executed on 28 July with 19 colleagues, including Saint-Just andGeorges Couthon, followed by 83 members of the Commune.[132]The Law of 22 Prairial was repealed, any surviving Girondists reinstated as deputies, and the Jacobin Club was closed and banned.[133]
There are various interpretations of the Terror and the violence with which it was conducted. Furet argues that the intense ideological commitment of the revolutionaries and their utopian goals required the extermination of any opposition.[134]A middle position suggests violence was not inevitable but the product of a series of complex internal events, exacerbated by war.[135]
The bloodshed did not end with the death of Robespierre;southern Francesaw awave of revenge killings, directed against alleged Jacobins, Republican officials and Protestants. Although the victors of Thermidor asserted control over the Commune by executing their leaders, some of those closely involved in the "Terror" retained their positions. They includedPaul Barras, later chief executive of theFrench Directory, andJoseph Fouché, director of the killings in Lyon who served asMinister of Policeunder the Directory, the Consulate andEmpire.[136]Despite his links to Augustin Robespierre, military success in Italy meant Bonaparte escaped censure.[137]
The December 1794Treaty of La Jaunayeended theChouanneriein western France by allowing freedom of worship and the return of non-juring priests.[138]This was accompanied by military success; in January 1795, French forces helped theDutch Patriotsset up theBatavian Republic, securing their northern border.[139]The war with Prussia was concluded in favour of France by thePeace of Baselin April 1795, while Spain made peace shortly thereafter.[140]
However, the Republic still faced a crisis at home. Food shortages arising from a poor 1794 harvest were exacerbated in northern France by the need to supply the army inFlanders, while the winter was the worst since 1709.[141]By April 1795, people were starving, and theassignatwas worth only 8% of its face value; in desperation,the Parisian poor rose again.[142]They were quickly dispersed and the main impact was another round of arrests, while Jacobin prisoners in Lyon were summarily executed.[143]
A committee drafted theConstitution of the Year III, approved byplebisciteon 23 September 1795 and put into place on 27 September.[144]Largely designed byPierre DaunouandBoissy d'Anglas, it established abicameral legislature, intended to slow down the legislative process, ending the wild swings of policy under the previous unicameral systems. TheCouncil of 500was responsible for drafting legislation, which was reviewed and approved by theCouncil of Ancients, an upper house containing 250 men over the age of 40. Executive power was in the hands of five directors, selected by the Council of Ancients from a list provided by the lower house, with a five-year mandate.[145]
Deputies were chosen by indirect election, a total franchise of around 5 million voting in primaries for 30,000 electors, or 0.6% of the population. Since they were also subject to stringent property qualification, it guaranteed the return of conservative or moderate deputies. In addition, rather than dissolving the previous legislature as in 1791 and 1792, the so-called 'law of two-thirds' ruled only 150 new deputies would be elected each year. The remaining 600Conventionnelskept their seats, a move intended to ensure stability.[146]
Jacobin sympathisers viewed theFrench Directoryas a betrayal of the Revolution, whileBonapartistslater justified Napoleon's coup by emphasising its corruption.[147]The regime also faced internal unrest, a weak economy, and an expensive war, while the Council of 500 could block legislation at will. Since the directors had no power to call new elections, the only way to break a deadlock was rule by decree or use force. As a result, the directory was characterised by "chronic violence, ambivalent forms of justice, and repeated recourse to heavy-handed repression."[148]
Retention of theConventionnelsensured theThermidoriansheld a majority in the legislature and three of the five directors, but they were increasingly challenged by the right. On 5 October, Convention troops led by Napoleonput down a royalist risingin Paris; when thefirst legislative electionswere held two weeks later, over 100 of the 150 new deputies were royalists of some sort.[149]The power of the Parisiansans-culotteshad been broken by the suppression of the May 1795 revolt; relieved of pressure from below, the Jacobin clubs became supporters of the directory, largely to prevent restoration of the monarchy.[150]
Removal of price controls and a collapse in the value of theassignatled to inflation and soaring food prices. By April 1796, over 500,000 Parisians were unemployed, resulting in the May insurrection known as theConspiracy of the Equals. Led by the revolutionaryFrançois-Noël Babeuf, their demands included immediate implementation of the 1793 Constitution, and a more equitable distribution of wealth. Despite support from sections of the military, the revolt was easily crushed, while Babeuf and other leaders were executed.[151]Nevertheless, by 1799 the economy had been stabilised, and important reforms made allowing steady expansion of French industry. Many of these remained in place for much of the 19th century.[152]
Prior to 1797, three of the five directors were firmly Republican; Barras,Révellière-LépeauxandJean-François Rewbell, as were around 40% of the legislature. The same percentage were broadlycentristor unaffiliated, along with two directors,Étienne-François LetourneurandLazare Carnot. Although only 20% were committed Royalists, many centrists supported the restoration of the exiledLouis XVIIIin the belief this would bring peace.[153]The elections of May 1797 resulted in significant gains for the right, with RoyalistsJean-Charles Pichegruelected president of the Council of 500, andBarthélemyappointed a director.[154]
With Royalists apparently on the verge of power, Republicans attempted a pre-emptivecoup on 4 September. Using troops from Napoleon's Army of Italy underPierre Augereau, the Council of 500 was forced to approve the arrest of Barthélemy, Pichegru and Carnot. The elections were annulled, 63 leading Royalists deported toFrench Guiana, and laws were passed against émigrés, Royalists and ultra-Jacobins. The removal of his conservative opponents opened the way for direct conflict between Barras and those on the left.[155]
Fighting continued despite general war weariness, and the1798 electionsresulted in a resurgence in Jacobin strength. Napoleon'sinvasion of Egyptin July 1798 confirmed European fears of French expansionism, and theWar of the Second Coalitionbegan in November. Without a majority in the legislature, the directors relied on the army to enforce decrees and extract revenue from conquered territories. Generals like Napoleon andBarthélemy Catherine Joubertbecame central to the political process, while both the army and directory became notorious for their corruption.[156]
It has been suggested the directory collapsed because by 1799, many 'preferred the uncertainties of authoritarian rule to the continuing ambiguities of parliamentary politics'.[157]The architect of its end was Sieyès, who when asked what he had done during the Terror allegedly answered "I survived". Nominated to the directory, his first action was to remove Barras, with the help of allies including Talleyrand, and Napoleon's brotherLucien, president of the Council of 500.[158]On 9 November 1799, thecoup of 18 Brumairereplaced the five directors with theFrench Consulate, which consisted of three members, Napoleon, Sieyès, andRoger Ducos. Most historians consider this the end point of the French Revolution.[159]
The role of ideology in the Revolution is controversial withJonathan Israelstating that the "radical Enlightenment" was the primary driving force of the Revolution.[160]Cobban, however, argues "[t]he actions of the revolutionaries were most often prescribed by the need to find practical solutions to immediate problems, using the resources at hand, not by pre-conceived theories."[161]
The identification of ideologies is complicated by the profusion of revolutionary clubs, factions and publications, absence of formal political parties, and individual flexibility in the face of changing circumstances.[162]In addition, although the Declaration of the Rights of Man was a fundamental document for all revolutionary factions, its interpretation varied widely.[163]
While all revolutionaries professed their devotion to liberty in principle, "it appeared to mean whatever those in power wanted."[164]For example, the liberties specified in the Rights of Man were limited by law when they might "cause harm to others, or be abused". Prior to 1792, Jacobins and others frequently opposed press restrictions on the grounds these violated a basic right.[165]However, the radical National Convention passed laws in September 1793 and July 1794 imposing the death penalty for offences such as "disparaging the National Convention", and "misleading public opinion."[166]
While revolutionaries also endorsed the principle of equality, few advocated equality of wealth since property was also viewed as a right.[167]The National Assembly opposed equal political rights for women,[168]while the abolition of slavery in the colonies was delayed until February 1794 because it conflicted with the property rights of slave owners, and many feared it would disrupt trade.[169]Political equality for male citizens was another divisive issue, with the 1791 constitution limiting the right to vote and stand for office to males over 25 who met a property qualification, so-called "active citizens". This restriction was opposed by many activists, including Robespierre, the Jacobins, and Cordeliers.[170]
The principle that sovereignty resided in the nation was a key concept of the Revolution.[171]However, Israel argues this obscures ideological differences over whether the will of the nation was best expressed through representative assemblies and constitutions, or direct action by revolutionary crowds, and popular assemblies such as the sections of the Paris commune.[172]Many considered constitutional monarchy as incompatible with the principle of popular sovereignty,[173]but prior to 1792, there was a strong bloc with an ideological commitment to such a system, based on the writings ofThomas Hobbes,John Locke,Montesquieuand Voltaire.[174]
Israel argues the nationalisation of church property and the establishment of the Constitutional Church reflected an ideological commitment to secularism, and a determination to undermine a bastion of old regime privilege.[175]While Cobban agrees the Constitutional Church was motivated by ideology, he sees its origins in the anti-clericalism of Voltaire and other Enlightenment figures.[176]
Jacobins were hostile to formal political parties and factions which they saw as a threat to national unity and the general will, with "political virtue" and "love of country" key elements of their ideology.[177][178]They viewed the ideal revolutionary as selfless, sincere, free of political ambition, and devoted to the nation.[179]The disputes leading to the departure first of theFeuillants, then later theGirondists, were conducted in terms of the relative political virtue and patriotism of the disputants. In December 1793, all members of the Jacobin clubs were subject to a "purifying scrutiny", to determine whether they were "men of virtue".[180]
The Revolution initiated a series of conflicts that began in 1792 and ended with Napoleon'sdefeat at Waterlooin 1815. In its early stages, this seemed unlikely; the 1791 Constitution specifically disavowed "war for the purpose of conquest", and although traditional tensions between France and Austria re-emerged in the 1780s, EmperorJoseph IIcautiously welcomed the reforms. Austria wasat war with the Ottomans, as werethe Russians, while both were negotiating with Prussia over partitioning Poland. Most importantly, Britain preferred peace, and as EmperorLeopold IIstated after the Declaration of Pillnitz, "without England, there is no case".[181]
In late 1791, factions within the Assembly came to see war as a way to unite the country and secure the Revolution by eliminating hostile forces on its borders and establishing its "natural frontiers".[182]France declared war on Austria in April 1792 and issued the firstconscriptionorders, with recruits serving for twelve months. By the time peace finally came in 1815, the conflict had involved every major European power as well as the United States, redrawn the map of Europe and expanded into theAmericas, theMiddle East, and theIndian Ocean.[183]
From 1701 to 1801, the population of Europe grew from 118 to 187 million; combined with new mass production techniques, this allowed belligerents to support large armies, requiring the mobilisation of national resources. It was a different kind of war, fought by nations rather than kings, intended to destroy their opponents' ability to resist, but also to implement deep-ranging social change. While all wars are political to some degree, this period was remarkable for the emphasis placed on reshaping boundaries and the creation of entirely new European states.[184]
In April 1792, French armies invaded theAustrian Netherlandsbut suffered a series of setbacks before victory over an Austrian-Prussian army at Valmy in September. After defeating a second Austrian army atJemappes on 6 November, they occupied the Netherlands, areas of theRhineland,NiceandSavoy. Emboldened by this success, in February 1793 France declared war on theDutch Republic, Spain and Britain, beginning theWar of the First Coalition.[185]However, the expiration of the 12-month term for the 1792 recruits forced the French to relinquish their conquests. In August, new conscription measures were passed, and by May 1794 the French army had between 750,000 and 800,000 men.[186]Despite high rates of desertion, this was large enough to manage multiple internal and external threats; for comparison, the combined Prussian-Austrian army was less than 90,000.[187]
By February 1795, France had annexed the Austrian Netherlands, established their frontier on the left bank of the Rhine and replaced the Dutch Republic with theBatavian Republic, a satellite state. These victories led to the collapse of the anti-French coalition; Prussia made peace in April 1795, followed soon after by Spain, leaving Britain and Austria as the only major powers still in the war.[188]In October 1797, a series of defeats by Bonaparte in Italy led Austria to agree to theTreaty of Campo Formio, in which they formally ceded the Netherlands and recognised theCisalpine Republic.[189]
Fighting continued for two reasons; first, French state finances had come to rely on indemnities levied on their defeated opponents. Second, armies were primarily loyal to their generals, for whom the wealth achieved by victory and the status it conferred became objectives in themselves. Leading soldiers like Hoche, Pichegru and Carnot wielded significant political influence and often set policy; Campo Formio was approved by Bonaparte, not the Directory, which strongly objected to terms it considered too lenient.[189]
Despite these concerns, the Directory never developed a realistic peace programme, fearing the destabilising effects of peace and the consequent demobilisation of hundreds of thousands of young men. As long as the generals and their armies stayed away from Paris, they were happy to allow them to continue fighting, a key factor behind sanctioning Bonaparte's invasion of Egypt. This resulted in aggressive and opportunistic policies, leading to theWar of the Second Coalitionin November 1798.[190]
In 1789, the most populous French colonies were Saint-Domingue (today Haiti), Martinique, Guadeloupe, the Île Bourbon (Réunion) and the Île de la France. These colonies produced commodities such as sugar, coffee and cotton for exclusive export to France. There were about 700,000 slaves in the colonies, of which about 500,000 were in Saint-Domingue. Colonial products accounted for about a third of France's exports.[191]
In February 1788, theSociété des Amis des Noirs(Society of the Friends of Blacks) was formed in France with the aim of abolishing slavery in the empire. In August 1789, colonial slave owners and merchants formed the rivalClub de Massiacto represent their interests. When the Constituent Assembly adopted the Declaration of the Rights of Man and of the Citizen in August 1789, delegates representing the colonial landowners successfully argued that the principles should not apply in the colonies as they would bring economic ruin and disrupt trade. Colonial landowners also gained control of the Colonial Committee of the Assembly from where they exerted a powerful influence against abolition.[192][193]
People of colour also faced social and legal discrimination in mainland France and its colonies, including a bar on their access to professions such as law, medicine and pharmacy.[194]In 1789–90, a delegation of free coloureds, led byVincent OgéandJulien Raimond, unsuccessfully lobbied the Assembly to end discrimination against free coloureds. Ogé left for Saint-Domingue where an uprising against white landowners broke out in October 1790. The revolt failed, and Ogé was killed.[195][193]
In May 1791, the National Assembly granted full political rights to coloureds born of two free parents but left the rights of freed slaves to be determined by the colonial assemblies. The assemblies refused to implement the decree and fighting broke out between the coloured population of Saint-Domingue and white colonists, each side recruiting slaves to their forces.A major slave revoltfollowed in August.[196]
In March 1792, the Legislative Assembly responded to the revolt by granting citizenship to all free coloureds and sending two commissioners,Léger-Félicité SonthonaxandÉtienne Polverel, and 6,000 troops to Saint-Domingue to enforce the decree. On arrival in September, the commissioners announced that slavery would remain in force. Over 72,000 slaves were still in revolt, mostly in the north.[197]
Brissot and his supporters envisaged an eventual abolition of slavery but their immediate concern was securing trade and the support of merchants for the revolutionary wars. After Brissot's fall, the new constitution of June 1793 included a new Declaration of the Rights of Man and the Citizen but excluded the colonies from its provisions. In any event, the new constitution was suspended until France was at peace.[198]
In early 1793, royalist planters from Guadeloupe and Saint-Domingue formed an alliance with Britain. The Spanish supported insurgent slaves, led byJean-François PapillonandGeorges Biassou, in the north of Saint-Domingue. White planters loyal to the republic sent representatives to Paris to convince the Jacobin controlled Convention that those calling for the abolition of slavery were British agents and supporters of Brissot, hoping to disrupt trade.[199]
In June, the commissioners in Saint-Domingue freed 10,000 slaves fighting for the republic. As the royalists and their British and Spanish supporters were also offering freedom for slaves willing to fight for their cause, the commissioners outbid them by abolishing slavery in the north in August, and throughout the colony in October. Representatives were sent to Paris to gain the approval of the convention for the decision.[199][200]
The Convention voted for the abolition of slavery in the colonies on 4 February 1794 and decreed that all residents of the colonies had the full rights of French citizens irrespective of colour.[201]An army of 1,000 sans-culottes led byVictor Hugueswas sent to Guadeloupe to expel the British and enforce the decree. The army recruited former slaves and eventually numbered 11,000, capturing Guadeloupe and other smaller islands. Abolition was also proclaimed on Guyane.Martiniqueremained under British occupation, while colonial landowners inRéunionand theÎles Mascareignesrepulsed the republicans.[202]Black armies drove the Spanish out of Saint-Domingue in 1795, and the last of the British withdrew in 1798.[203]
In republican controlled areas from 1793 to 1799, freed slaves were required to work on their former plantations or for their former masters if they were in domestic service. They were paid a wage and gained property rights. Black and coloured generals were effectively in control of large areas of Guadeloupe and Saint-Domingue, includingToussaint Louverturein the north of Saint-Domingue, andAndré Rigaudin the south. Historian Fréderic Régent states that the restrictions on the freedom of employment and movement of former slaves meant that, "only whites, persons of color already freed before the decree, and former slaves in the army or on warships really benefited from general emancipation."[202]
Newspapers and pamphlets played a central role in stimulating and defining the Revolution. Prior to 1789, there have been a small number of heavily censored newspapers that needed a royal licence to operate, but the Estates General created an enormous demand for news, and over 130 newspapers appeared by the end of the year. Among the most significant were Marat'sL'Ami du peupleandElysée Loustallot'sRevolutions de Paris[fr].[204]Over the next decade, more than 2,000 newspapers were founded, 500 in Paris alone. Most lasted only a matter of weeks but they became the main communication medium, combined with the very large pamphlet literature.[205]
Newspapers were read aloud in taverns and clubs and circulated hand to hand. There was a widespread assumption that writing was a vocation, not a business, and the role of the press was the advancement of civic republicanism.[206]By 1793 the radicals were most active but initially the royalists flooded the country with their publication the "L'Ami du Roi[fr]" (Friends of the King) until they were suppressed.[207]
To illustrate the differences between the new Republic and the old regime, the leaders needed to implement a new set of symbols to be celebrated instead of the old religious and monarchical symbols. To this end, symbols were borrowed from historic cultures and redefined, while those of the old regime were either destroyed or reattributed acceptable characteristics. These revised symbols were used to instil in the public a new sense of tradition and reverence for the Enlightenment and the Republic.[208]
"La Marseillaise" (French pronunciation:[lamaʁsɛjɛːz]) became thenational anthemof France. The song was written and composed in 1792 byClaude Joseph Rouget de Lisle, and was originally titled "Chant de guerre pour l'Armée du Rhin". TheFrench National Conventionadopted it as theFirst Republic'santhem in 1795. It acquired its nickname after being sung in Paris byvolunteersfromMarseillemarching on the capital.
The song is the first example of the "European march" anthemic style, while the evocative melody and lyrics led to its widespread use as a song of revolution and incorporation into many pieces of classical and popular music. De Lisle was instructed to 'produce a hymn which conveys to the soul of the people the enthusiasm which it (the music) suggests.'[210]
Theguillotineremains "the principal symbol of the Terror in the French Revolution."[211]Invented by a physician during the Revolution as a quicker, more efficient and more distinctive form of execution, the guillotine became a part of popular culture and historic memory. It was celebrated on the left as the people's avenger, for example in therevolutionary songLa guillotine permanente,[212]and cursed as the symbol of the Terror by the right.[213]
Its operation became a popular entertainment that attracted great crowds of spectators. Vendors sold programmes listing the names of those scheduled to die. Many people came day after day and vied for the best locations from which to observe the proceedings; knitting women (tricoteuses) formed a cadre of hardcore regulars, inciting the crowd. Parents often brought their children. By the end of the Terror, the crowds had thinned drastically. Repetition had staled even this most grisly of entertainments, and audiences grew bored.[214]
Cockadeswere widely worn by revolutionaries beginning in 1789. They pinned the blue-and-red cockade of Paris onto the white cockade of theAncien Régime.Camille Desmoulinsasked his followers to wear green cockades on 12 July 1789. The Paris militia, formed on 13 July, adopted a blue and red cockade. Blue and red are the traditional colours of Paris, and they are used on the city's coat of arms. Cockades with various colour schemes were used during the storming of the Bastille on 14 July.[215]
The Liberty cap, also known as thePhrygian cap, orpileus, is a brimless, felt cap that is conical in shape with the tip pulled forward. It reflects Roman republicanism and liberty, alluding to the Roman ritual ofmanumission, in which a freed slave receives the bonnet as a symbol of his newfound liberty.[216]
Deprived of political rights by theAncien Régime, the Revolution initially allowed women to participate, although only to a limited degree. Activists included Girondists likeOlympe de Gouges, author of theDeclaration of the Rights of Woman and of the Female Citizen, and Charlotte Corday, killer of Marat. Others likeThéroigne de Méricourt,Pauline Léonand theSociety of Revolutionary Republican Womensupported the Jacobins, staged demonstrations in the National Assembly and took part in the October 1789 March to Versailles. Despite this, the 1791 and 1793 constitutions denied them political rights and democratic citizenship.[217]
In 1793, the Society of Revolutionary Republican Women campaigned for strict price controls on bread, and a law that would compel all women to wear the tricolour cockade. Although both demands were successful, in October the male-dominated Jacobins who then controlled the government denounced the Society as dangerous rabble-rousers and made all women's clubs and associations illegal. Organised women were permanently shut out of the French Revolution after 30 October 1793.[218]
At the same time, especially in the provinces, women played a prominent role in resisting social changes introduced by the Revolution. This was particularly so in terms of the reduced role of the Catholic Church; for those living in rural areas, closing of the churches meant a loss of normality.[219]This sparked a counter-revolutionary movement led by women; while supporting other political and social changes, they opposed the dissolution of the Catholic Church and revolutionary cults like theCult of the Supreme Being.[220]Olwen Huftonargues some wanted to protect the Church from heretical changes enforced by revolutionaries, viewing themselves as "defenders of faith".[221]
Olympe de Gougeswas an author whose publications emphasised that while women and men were different, this should not prevent equality under the law. In her Declaration of the Rights of Woman and of the Female Citizen she insisted women deserved rights, especially in areas concerning them directly, such as divorce and recognition of illegitimate children.[222][full citation needed]Along with other Girondists, she was executed in November 1793 during the Terror.
Madame Roland, also known as Manon or Marie Roland, was another important female activist whose political focus was not specifically women but other aspects of the government. A Girondist, her personal letters to leaders of the Revolution influenced policy; in addition, she often hosted political gatherings of the Brissotins, a political group which allowed women to join. She too was executed in November 1793.[223]
The Revolution abolished many economic constraints imposed by theAncien Régime, including church tithes and feudal dues although tenants often paid higher rents and taxes.[224]All church lands were nationalised, along with those owned by Royalist exiles, which were used to back paper currency known asassignats, and the feudalguildsystem eliminated.[225]It also abolished the highly inefficient system oftax farming, whereby private individuals would collect taxes for a hefty fee. The government seized the foundations that had been set up (starting in the 13th century) to provide an annual stream of revenue for hospitals, poor relief, and education. The state sold the lands but typically local authorities did not replace the funding and so most of the nation's charitable andschool systems were massively disrupted.[226]
Between 1790 and 1796, industrial and agricultural output dropped, foreign trade plunged, and prices soared, forcing the government to finance expenditure by issuing ever increasing quantitiesassignats. When this resulted in escalating inflation, the response was to impose price controls and persecute private speculators and traders, creating ablack market. Between 1789 and 1793, the annual deficit increased from 10% to 64% of gross national product, while annual inflation reached 3,500% after a poor harvest in 1794 and the removal of price controls. The assignats were withdrawn in 1796 but inflation continued until the introduction of the gold-basedFranc germinalin 1803.[227]
The French Revolution had a major impact on western history by ending feudalism in France and creating a path for advances in individual freedoms throughout Europe.[228][2]The revolution represented the most significant challenge to political absolutism up to that point in history and spread democratic ideals throughout Europe and ultimately the world.[229]Its impact onFrench nationalismwas profound, while also stimulating nationalist movements throughout Europe.[230]Some modern historians argue the concept of thenation statewas a direct consequence of the revolution.[231]As such, the revolution is often seen as marking the start ofmodernityand themodern period.[232]
The long-term impact on France was profound, shaping politics, society, religion and ideas, and polarising politics for more than a century. HistorianFrançois Aulardwrites:
"From the social point of view, the Revolution consisted in the suppression of what was called the feudal system, in the emancipation of the individual, in greater division of landed property, the abolition of the privileges of noble birth, the establishment of equality, the simplification of life.... The French Revolution differed from other revolutions in being not merely national, for it aimed at benefiting all humanity."[233][title missing]
The revolution permanently crippled the power of the aristocracy and drained the wealth of the Church, although the two institutions survived. Hanson suggests the French underwent a fundamental transformation in self-identity, evidenced by the elimination of privileges and their replacement by intrinsichuman rights.[234]After the collapse of theFirst French Empirein 1815, the French public lost many of the rights and privileges earned since the revolution, but remembered the participatory politics that characterised the period. According to Paul Hanson, "Revolution became a tradition, andrepublicanisman enduring option."[235]
The Revolution meant an end to arbitrary royal rule and held out the promise of rule by law under a constitutional order. Napoleon as emperor set up a constitutional system and the restored Bourbons were forced to retain one. After the abdication ofNapoleon IIIin 1871, theFrench Third Republicwas launched with a deep commitment to upholding the ideals of the Revolution.[236][237]TheVichy regime(1940–1944) tried to undo the revolutionary heritage but retained the republic. However, there were no efforts by the Bourbons, Vichy or any other government to restore the privileges that had been stripped away from the nobility in 1789. France permanently became a society of equals under the law.[235]
Agriculture was transformed by the Revolution. With the breakup of large estates controlled by the Church and the nobility and worked by hired hands, rural France became more a land of small independent farms. Harvest taxes were ended, such as the tithe and seigneurial dues.Primogeniturewas ended both for nobles and peasants, thereby weakening the family patriarch, and led to a fall in the birth rate since all children had a share in the family property.[238]Cobban argues the Revolution bequeathed to the nation "a ruling class of landowners."[239]
Economic historians are divided on the economic impact of the Revolution. One suggestion is the resulting fragmentation of agricultural holdings had a significant negative impact in the early years of 19th century, then became positive in the second half of the century because it facilitated the rise in human capital investments.[240]Others argue the redistribution of land had an immediate positive impact on agricultural productivity, before the scale of these gains gradually declined over the course of the 19th century.[241]
In the cities, entrepreneurship on a small scale flourished, as restrictive monopolies, privileges, barriers, rules, taxes and guilds gave way. However, the British blockade virtually ended overseas and colonial trade, hurting the cities and their supply chains. Overall, the Revolution did not greatly change the French business system, and probably helped freeze in place the horizons of the small business owner. The typical businessman owned a small store, mill or shop, with family help and a few paid employees; large-scale industry was less common than in other industrialising nations.[242]
Historians often see the impact of the Revolution as through the institutions and ideas exported by Napoleon. Economic historians Dan Bogart, Mauricio Drelichman, Oscar Gelderblom, and Jean-Laurent Rosenthal describe Napoleon'scodified lawas the French Revolution's "most significant export."[243]According toDaron Acemoglu, Davide Cantoni,Simon Johnson, andJames A. Robinsonthe French Revolution had long-term effects in Europe. They suggest that "areas that were occupied by the French and that underwent radical institutional reform experienced more rapidurbanizationand economic growth, especially after 1850. There is no evidence of a negative effect of French invasion."[244]
The Revolution sparked intense debate in Britain. TheRevolution Controversywas a "pamphlet war" set off by the publication ofA Discourse on the Love of Our Country, a speech given byRichard Priceto theRevolution Societyon 4 November 1789, supporting the French Revolution.Edmund Burkeresponded in November 1790 with his own pamphlet,Reflections on the Revolution in France, attacking the French Revolution as a threat to the aristocracy of all countries.[245][246]William Coxeopposed Price's premise that one's country is principles and people, not the State itself.[247]
Conversely, two seminal political pieces of political history were written in Price's favour, supporting the general right of the French people to replace their State. One of the first of these "pamphlets" into print wasA Vindication of the Rights of MenbyMary Wollstonecraft. Wollstonecraft's title was echoed byThomas Paine'sRights of Man, published a few months later. In 1792Christopher WyvillpublishedDefence of Dr. Price and the Reformers of England, a plea for reform and moderation.[248]This exchange of ideas has been described as "one of the great political debates in British history".[249]
In Ireland, the effect was to transform what had been an attempt by Protestant settlers to gain some autonomy into a mass movement led by theSociety of United Irishmeninvolving Catholics and Protestants. It stimulated the demand for further reform throughout Ireland, especially inUlster. The upshot was a revolt in 1798, led byWolfe Tone, that was crushed by Britain.[250]German reaction to the Revolution swung from favourable to antagonistic. At first it brought liberal and democratic ideas, the end of guilds, serfdom and the Jewish ghetto. It brought economic freedoms and agrarian and legal reform. Above all the antagonism helped stimulate and shapeGerman nationalism.[251]
France invaded Switzerland and turned it into the "Helvetic Republic" (1798–1803), a French puppet state. French interference with localism and traditions was deeply resented in Switzerland, although some reforms took hold and survived in the laterperiod of restoration.[252][253]France invaded and occupied the region now known as Belgium between 1794 and 1814. The new government enforced reforms, incorporating the region into France. Resistance was strong in every sector, as Belgian nationalism emerged to oppose French rule. The French legal system, however, was adopted, with its equal legal rights, and abolition of class distinctions.[254]
The Kingdom of Denmark adopted liberalising reforms in line with those of the French Revolution. Reform was gradual and the regime itself carried outagrarian reformsthat had the effect of weakening absolutism by creating a class of independent peasantfreeholders. Much of the initiative came from well-organised liberals who directed political change in the first half of the 19th century.[255]TheConstitution of Norwayof 1814 was inspired by the French Revolution[256]and was considered to be one of the most liberal and democratic constitutions at the time.[257]
Initially, most people in theProvince of Quebecwere favourable toward the revolutionaries' aims. The Revolution took place against the background of an ongoing campaign for constitutional reform in the colony byLoyalistemigrants from the United States.[258]Public opinion began to shift against the Revolution after the Flight to Varennes and further soured after the September Massacres and the subsequent execution of Louis XVI.[259]French migration tothe Canadasexperienced a substantial decline during and after the Revolution. Only a limited number of artisans, professionals, and religious emigres were allowed to settle in the region during this period.[260]Most emigres settled inMontrealorQuebec City.[260]The influx of religious emigres also revitalised the local Catholic Church, with exiled priests establishing a number of parishes across the Canadas.[260]
In the United States, the French Revolution deeply polarised American politics, and this polarisation led to the creation of theFirst Party System. In 1793, as war broke out in Europe, theDemocratic-Republican Partyled by formerAmerican minister to FranceThomas Jeffersonfavored revolutionary France and pointed to the 1778 treaty that was still in effect.George Washingtonand his unanimous cabinet, including Jefferson, decided that the treaty did not bind the United States to enter the war. Washingtonproclaimed neutralityinstead.[261]
The first writings on the French revolution were near contemporaneous with events and mainly divided along ideological lines. These includedEdmund Burke's conservative critiqueReflections on the Revolution in France(1790) and Thomas Paine's response Rights of Man (1791).[262]From 1815, narrative histories dominated, often based on first-hand experience of the revolutionary years. By the mid-nineteenth century, more scholarly histories appeared, written by specialists and based on original documents and a more critical assessment of contemporary accounts.[263]
Dupuy identifies three main strands in nineteenth century historiography of the Revolution. The first is represented by reactionary writers who rejected the revolutionary ideals of popular sovereignty, civil equality, and the promotion of rationality, progress and personal happiness over religious faith. The second stream is those writers who celebrated its democratic, and republican values. The third were liberals likeGermaine de StaëlandGuizot, who accepted the necessity of reforms establishing a constitution and the rights of man, but rejected state interference with private property and individual rights, even when supported by a democratic majority.[264]
Jules Micheletwas a leading 19th-century historian of the democratic republican strand, andThiers,MignetandTocquevillewere prominent in the liberal strand.[265]Hippolyte Taine'sOrigins of Contemporary France(1875–1894) was modern in its use of departmental archives, but Dupuy sees him as reactionary, given his contempt for the crowd, and Revolutionary values.[266]
The broad distinction between conservative, democratic-republican and liberal interpretations of the Revolution persisted in the 20th-century, although historiography became more nuanced, with greater attention to critical analysis of documentary evidence.[266][267]Alphonse Aulard(1849–1928) was the first professional historian of the Revolution; he promoted graduate studies, scholarly editions, and learned journals.[268][269]His major work,The French Revolution, a Political History, 1789–1804(1905), was a democratic and republican interpretation of the Revolution.[270]
Socio-economic analysis and a focus on the experiences of ordinary people dominated French studies of the Revolution from the 1930s.[271]Georges Lefebvreelaborated a Marxist socio-economic analysis of the revolution with detailed studies of peasants, the rural panic of 1789, and the behaviour of revolutionary crowds.[272][273]Albert Soboul, also writing in the Marxist-Republican tradition, published a major study of thesans-culottesin 1958.[274]
Alfred Cobbanchallenged Jacobin-Marxist social and economic explanations of the revolution in two important works,The Myth of the French Revolution(1955) andSocial Interpretation of the French Revolution(1964). He argued the Revolution was primarily a political conflict, which ended in a victory for conservative property owners, a result which retarded economic development.[275][276]
In their 1965 work,La Revolution française,François Furetand Denis Richet also argued for the primacy of political decisions, contrasting the reformist period of 1789 to 1790 with the following interventions of the urban masses which led to radicalisation and an ungovernable situation.[277]
From the 1990s, Western scholars largely abandoned Marxist interpretations of the revolution in terms of bourgeoisie-proletarian class struggle as anachronistic. However, no new explanatory model has gained widespread support.[232][278]The historiography of the Revolution has expanded into areas such as cultural and regional histories, visual representations, transnational interpretations, and decolonisation.[277]
TheAmerican Revolution(1765–1783) was an ideological and political movement in theThirteen Coloniesin what was thenBritish America. The revolution culminated in theAmerican Revolutionary War, which was launched on April 19, 1775, in theBattles of Lexington and Concord. Leaders of the American Revolution werecolonial separatist leaderswho, asBritish subjects, initiallysought incremental levels of autonomybut came to embrace the cause of full independence and the necessity of prevailing in the Revolutionary War to obtain it. TheSecond Continental Congress, which represented the colonies and convened in present-dayIndependence HallinPhiladelphia, formed theContinental Armyand appointedGeorge Washingtonas its commander-in-chief in June 1775, and unanimously adopted theDeclaration of Independencethe following year, which inspired, formalized, and escalated the war. For most of the eight-year war, its outcome appeared uncertain. But in 1781, a decisive victory by Washington and the Continental Army in theSiege of YorktowninspiredKing George IIIand the British to negotiate an end tocolonial rulein the colonies and acknowledge their independence, which was codified in theTreaty of Parisin 1783, leading to the establishment of the sovereignUnited States of America.
Discontent with colonial rule began shortly after the defeat ofFrancein theFrench and Indian Warin 1763. Even though the colonies had fought in and supported the war,British Parliamentimposed new taxes to compensate for wartime costs and turned control of the colonies' western lands over to the British officials inMontreal. Representatives from several colonies convened theStamp Act Congressin 1765; its "Declaration of Rights and Grievances" argued thattaxation without representationviolated theirrights as Englishmen. In 1767, tensions flared again following British Parliament's passage of theTownshend Acts. In an effort to quell the mounting rebellion, King George III deployedBritish troopstoBoston, where British troops killed protesters in theBoston Massacreon March 5, 1770. In 1772, anti-tax demonstratorsdestroyed the Royal Navy customs schoonerGaspeeoff present-dayWarwick, Rhode Island. On December 16, 1773, in a seminal event in the American Revolution's escalation,Sons of Libertyactivistswearing costumesofNative Americansinstigated theBoston Tea Party, during which they boarded and dumped chests of tea owned by the BritishEast India CompanyintoBoston Harbor. London responded by closing Boston Harbor and enacting aseries of punitive laws, which effectively ended self-government in Massachusetts but also served to expand and intensify the revolutionary cause.
In late 1774, 12 of the Thirteen Colonies sent delegates to theFirst Continental Congress, which met insideCarpenters' Hallin Philadelphia; theProvince of Georgiajoined in 1775. The First Continental Congress began coordinatingPatriotresistance through underground networks ofcommittees. Following the Battles of Lexington and Concord, Washington and the Continental Army responded bysurrounding Boston, forcing the British to withdraw by sea in March 1776, and leaving Patriots in control in every colony. In August 1775, King George IIIproclaimed Massachusetts to be in a state of open defiance and rebellion.
In 1776, the Second Continental Congress began debating and deliberating on theArticles of Confederation, an effort to establish a self-governing rule of law in the Thirteen Colonies. On July 2, they passed theLee Resolution, affirming their support for national independence, and on July 4, 1776, they unanimously adopted theDeclaration of Independence, authored primarily byThomas Jefferson, which embodied the political philosophies ofliberalismandrepublicanism, rejectedmonarchyandaristocracy, and famously proclaimed that "all men are created equal".
The Revolutionary War continued for another five years during whichFranceultimately entered the war, supporting the colonial cause of independence. On September 28, 1781, Washington, with support fromMarquis de Lafayette, theFrench Army, andFrench Navy, led the Continental Army's most decisive victory, capturing roughly 7,500 British troops led by British generalCharles Cornwallisduring theSiege of Yorktown, leading to the collapse of King George's control of Parliament and consensus in Parliament that the war should be ended on American terms. On September 3, 1783, the British signed theTreaty of Paris, granting their former colonies nearly all the territory east of theMississippi Riverand south of theGreat Lakes. About 60,000Loyalists migrated to other British territoriesin Canada and elsewhere, but the great majority remained in the United States. With its victory in the American Revolution, the United States became the first constitutional republic in world history founded on theconsent of the governedand therule of law.
Under the British policy ofsalutary neglect, Britain traded with the colonies but otherwise mostly left them alone over the first 150 years of the colonies’ existence. The colonists became accustomed to running their own affairs, and they liked it. This British policy changed significantly after theFrench and Indian War, prompting the Thirteen Colonies to seek greater autonomy from Britain. After the Revolution, one colonist, Capt. Levi Preston, ofDanvers, Massachusetts, was asked why the Americans rebelled against England, responded, "…we always had governed ourselves, and we always meant to. They didn’t mean we should."[1]
TheThirteen Colonieswere established in the 17th century as part of theEnglish Empire, and they formed part of theBritish Empireafter theunion of England and Scotlandin 1707.[2]The development of a unique American identity can be traced to theEnglish Civil War(1642–1651) and its aftermath. ThePuritan coloniesofNew Englandsupported theCommonwealthgovernment responsible for theexecution of King Charles I. After theStuart Restorationof 1660,Massachusettsdid not recognizeCharles IIas the legitimate king for more than a year after hiscoronation. InKing Philip's War(1675–1678), the New England colonies fought a handful ofNative Americantribes without military assistance from England, thereby contributing to the development of a uniquely American identity separate from that of theBritish people.[3]
In the 1680s, Charles and his brother,James II, attempted to bring New England under direct English control.[4]The colonists fiercely opposed this, andthe Crownnullified theircolonial chartersin response.[5]In 1686, James finalized these efforts by consolidating the separate New England colonies along withNew YorkandNew Jerseyinto theDominion of New England.Edmund Androswas appointed royal governor and tasked with governing the new Dominion under hisdirect rule. Colonial assemblies andtown meetingswere restricted, new taxes were levied, and rights were abridged. Dominion rule triggered bitter resentment throughout New England.[6]When James tried to rule withoutParliament, the English aristocracy removed him from power in theGlorious Revolutionof 1688.[7]This was followed by the1689 Boston revolt, which overthrew Dominion rule.[8][9]Colonial governments reasserted their control after the revolt. The new monarchs,WilliamandMary, granted new charters to the individual New England colonies, and local democratic self-government was restored.[10][11]
After theGlorious Revolutionin 1688, theBritish Empirewas aconstitutional monarchywith sovereignty in theKing-in-Parliament. Aristocrats inherited seats in theHouse of Lords, while thegentryand merchants controlled the electedHouse of Commons. The king ruled throughcabinet ministerswho depended on majority support in the Commons to govern effectively.[12]British subjects on both sides of the Atlantic proudly claimed the unwrittenBritish constitution, with its guarantees of therights of Englishmen, protected personal liberty better than any other government.[13]It served as the model forcolonial governments. The Crown appointed aroyal governorto exerciseexecutivepower.[14]Property ownerselected acolonial assemblywith powers to legislate and levy taxes, but the British government reserved the right tovetocolonial legislation.[12]Radical Whigideology profoundly influenced American political philosophy with its love of liberty and opposition to tyrannical government.[15]
With little industry except shipbuilding, the colonies exported agricultural products to Britain in return for manufactured goods. They also imported molasses, rum, and sugar from theBritish West Indies.[16]The British government pursued a policy ofmercantilismin order to grow its economic and political power. According to mercantilism, the colonies existed for the mother country's economic benefit, and the colonists' economic needs took second place.[17]In 1651, Parliament passed the first in a series ofNavigation Acts, which restricted colonial trade with foreign countries. The Thirteen Colonies could trade with the rest of the empire but only ship certain commodities like tobacco to Britain. Any European imports bound for British America had to first pass through an English port and pay customs duties.[18]Other laws regulated colonial industries, such as theWool Act 1698, theHat Act 1731, and theIron Act 1750.[19][20]
Colonial reactions to these policies were mixed. TheMolasses Act 1733placed a duty of sixpenceper gallon upon foreign molasses imported into the colonies. This act was particularly egregious to the New England colonists, who protested it as taxation without representation. The act increased the smuggling of foreign molasses, and the British government ceased enforcement efforts after the 1740s.[21]On the other hand, certain merchants and local industries benefitted from the restrictions on foreign competition. The limits on foreign-built ships greatly benefitted the colonial shipbuilding industry, particularly in New England.[22]Some argue that the economic impact was minimal on the colonists,[23][24]but the political friction that the acts triggered was more serious, as the merchants most directly affected were also the most politically active.[25]
The British government lacked the resources and information needed to control the colonies. Instead, British officials negotiated and compromised with colonial leaders to gain compliance with imperial policies. The colonies defended themselves withcolonial militias, and the British government rarely sent military forces to America before 1755.[26]According to historianRobert Middlekauff, "Americans had become almost completely self-governing" before the American Revolution, a practice that was consistent with the British monarchy's practice ofsalutary neglect.[27]
During theFrench and Indian War(1754–1763), the British government fielded 45,000 soldiers, halfBritish Regularsand half colonial volunteers. The colonies also contributed money to the war effort; however, two-fifths of this spending was reimbursed by the British government. Great Britain defeated France and acquiredthat nation's territoryeast of the Mississippi River.[28]
In early 1763, theBute ministrydecided to permanently garrison 10,000 soldiers in North America.[29][30]This would allow approximately 1,500 politically well-connectedBritish Armyofficers to remain on active duty with full pay (stationing a standing army inGreat Britainduring peacetime was politically unacceptable).[31]A standing army would provide defense against Native Americans in the west and foreign populations in newly acquired territories (the French inCanadaand the Spanish inFlorida). In addition, British soldiers could prevent white colonists from instigating conflict with Native Americans and help collect customs duties.[32]
Migration beyond theAppalachian Mountainsincreased after the French threat was removed, and Native Americans launchedPontiac's War(1763–1766) in response. TheGrenville ministryissued theRoyal Proclamation of 1763, designating the territory between the Appalachian Mountains and the Mississippi River as anIndian Reserveclosed to white settlement. The Proclamation failed to stop westward migration while angering settlers, fur traders, and land speculators in the Thirteen Colonies.[33]
George Grenvillebecameprime ministerin 1763, and "the need for money played a part in every important decision made by Grenville regarding the colonies—and for that matter by the ministries that followed up to 1776."[34]The national debt had grown to £133 million with annual debt payments of £5 million (out of an £8 million annual budget). Stationing troops in North America on a permanent basis would cost another £360,000 a year. On aper capitabasis, Americans only paid 1shillingin taxes to the empire compared to 26 shillings paid by the English.[30]Grenville believed that the colonies should help pay the troop costs.[35]
In 1764 Parliament passed theSugar Act, decreasing the existing customs duties on sugar and molasses but providing stricter measures of enforcement and collection. That same year, Grenville proposed direct taxes on the colonies to raise revenue, but he delayed action to see whether the colonies would propose some way to raise the revenue themselves.[36]
Parliament passed theStamp Actin March 1765, which imposeddirect taxeson the colonies for the first time. All official documents, newspapers, almanacs, and pamphlets were required to have the stamps—even decks of playing cards. The colonists did not object that the taxes were high; they were actually low.[a][37]They objected to their lack of representation in the Parliament, which gave them no voice concerning legislation that affected them, such as the tax, violatingthe unwritten English constitution. This grievance was summarized in the slogan "No taxation without representation". Shortly following adoption of the Stamp Act, theSons of Libertyformed, and began using public demonstrations, boycotts, and threats of violence to ensure that the British tax laws became unenforceable. InBoston, the Sons of Liberty burned the records of the vice admiralty court and looted the home of chief justiceThomas Hutchinson. Several legislatures called for united action, and nine colonies sent delegates to theStamp Act Congressin New York City in October. Moderates led byJohn Dickinsondrew up aDeclaration of Rights and Grievancesstating that the colonists were equal to all other British citizens and that taxes passed without representation violated theirrights as Englishmen, and Congress emphasized their determination by organizinga boycott on imports of all British merchandise.[38]American spokesmen such as Samuel Adams, James Otis, John Hancock, John Dickinson, Thomas Paine, and many others, rejected aristocracy and propounded "republicanism" as the political philosophy that was best suited to American conditions.[39][40]
The Parliament at Westminster saw itself as the supreme lawmaking authority throughoutthe Empireand thus entitled to levy any tax without colonial approval or even consultation.[41]They argued that the colonies were legallyBritish corporationssubordinate to the British Parliament.[42]Parliament insisted that the colonists effectively enjoyed a "virtual representation", as most British people did, since only a small minority of the British population were eligible to elect representatives to Parliament.[43]However, Americans such asJames Otismaintained that there was no one in Parliament responsible specifically to any colonial constituency, so they were not "virtually represented" by anyone in Parliament.[44]
TheRockingham governmentcame to power in July 1765, and Parliament debated whether to repeal the stamp tax or to send an army to enforce it. Benjamin Franklin appeared before them to make the case for repeal, explaining that the colonies had spent heavily in manpower, money, and blood defending the empire, and that further taxes to pay for those wars were unjust and might bring about a rebellion. Parliament agreed and repealed the tax on February 21, 1766, but they insisted in theDeclaratory Actof March 1766 that they retained full power to make laws for the colonies "in all cases whatsoever".[45][46]The repeal nonetheless caused widespread celebrations in the colonies.
In 1767, theBritish Parliamentpassed theTownshend Acts, which placed duties on several staple goods, including paper, glass, and tea, and established a Board of Customs inBostonto more rigorously execute trade regulations. Parliament's goal was not so much to collect revenue but to assert its authority over the colonies. The new taxes were enacted on the belief that Americans only objected to internal taxes and not to external taxes such as custom duties. However, in his widely read pamphlet,Letters from a Farmer in Pennsylvania,John Dickinsonargued against the constitutionality of the acts because their purpose was to raise revenue and not to regulate trade.[47]Colonists responded to the taxes by organizing new boycotts of British goods. These boycotts were less effective, however, as the goods taxed by the Townshend Acts were widely used.
In February 1768, the Assembly ofMassachusetts Bay Colonyissued a circular letterto the other colonies urging them to coordinate resistance. The governor dissolved the assembly when it refused to rescind the letter. Meanwhile, a riot broke out in Boston in June 1768 over the seizure of the sloopLiberty, owned byJohn Hancock, for alleged smuggling. Customs officials were forced to flee, prompting the British to deploy troops to Boston. A Boston town meeting declared that no obedience was due to parliamentary laws and called for the convening of a convention. A convention assembled but only issued a mild protest before dissolving itself. In January 1769, Parliament responded to the unrest by reactivating theTreason Act 1543which called for subjects outside the realm to face trials for treason in England. The governor of Massachusetts was instructed to collect evidence of said treason, and the threat caused widespread outrage, though it was not carried out.
On March 5, 1770, a large crowd gathered around a group of British soldiers on a Boston street. The crowd grew threatening, throwing snowballs, rocks, and debris at them. One soldier was clubbed and fell.[48]There was no order to fire, but the soldiers panicked and fired into the crowd. They hit 11 people; three civilians died of wounds at the scene of the shooting, and two died shortly after. The event quickly came to be called theBoston Massacre. The soldiers were tried and acquitted (defended byJohn Adams), but the widespread descriptions soon began to turn colonial sentiment against the British. This accelerated the downward spiral in the relationship between Britain and the province of Massachusetts.[48]
A new ministry underLord Northcame to power in 1770, and Parliament repealed most of the Townshend duties, except the tax on tea. This temporarily resolved the crisis, and the boycott of British goods largely ceased, with only the more radical patriots such asSamuel Adamscontinuing to agitate.[citation needed]
In June 1772, American patriots, includingJohn Brown, burned a British warship that had been vigorously enforcing unpopular trade regulations, in what became known as theGaspeeAffair. The affair was investigated for possible treason, but no action was taken.
In 1773,private letters were publishedin which Massachusetts Governor Thomas Hutchinson claimed that the colonists could not enjoy all English liberties, and in which Lieutenant GovernorAndrew Olivercalled for the direct payment of colonial officials, which had been paid by local authorities. This would have reduced the influence of colonial representatives over their government. The letters' contents were used as evidence of a systematic plot against American rights, and discredited Hutchinson in the eyes of the people; the colonial Assembly petitioned for his recall. Benjamin Franklin,postmaster generalfor the colonies, acknowledged that he leaked the letters, which led to him being removed from his position.
In Boston, Samuel Adams set about creating newCommittees of Correspondence, which linked Patriots in all 13 colonies and eventually provided the framework for a rebel government. Virginia, the largest colony, set up its Committee of Correspondence in early 1773, on which Patrick Henry and Thomas Jefferson served.[49]A total of about 7,000 to 8,000 Patriots served on these Committees; Loyalists were excluded. The committees became the leaders of the American resistance to British actions, and later largely determined the war effort at the state and local level. When the First Continental Congress decided to boycott British products, the colonial and local Committees took charge, examining merchant records and publishing the names of merchants who attempted to defy the boycott by importing British goods.[50]
Meanwhile, Parliament passed theTea Actlowering the price of taxed tea exported to the colonies, to help the BritishEast India Companyundersell smuggled untaxed Dutch tea. Special consignees were appointed to sell the tea to bypass colonial merchants. The act was opposed by those who resisted the taxes and also by smugglers who stood to lose business.[citation needed]In every colony demonstrators warned merchants not to bring in tea that included the hated new tax. In most instances, the consignees were forced by the Americans to resign and the tea was turned back, but Massachusetts governor Hutchinson refused to allow Boston merchants to give in to pressure.
A town meeting in Boston determined that the tea would not be landed, and ignored a demand from the governor to disperse. On December 16, 1773, a group of men, led by Samuel Adams and dressed to evoke the appearance of Indigenous people, boarded the ships of the East India Company and dumped £10,000 worth of tea from their holds (approximately £636,000 in 2008) intoBoston Harbor. Decades later, this event became known as theBoston Tea Partyand remains a significant part of American patriotic lore.[51][page needed]
The British government responded by passing four laws that came to be known as theIntolerable Acts, further darkening colonial opinion towards England.[52]The first was theMassachusetts Government Actwhich altered the Massachusetts charter and restricted town meetings. The second was theAdministration of Justice Actwhich ordered that all British soldiers to be tried were to be arraigned in Britain, not in the colonies. The third was theBoston Port Act, which closed the port of Boston until the British had been compensated for the tea lost in the Boston Tea Party. The fourth was theQuartering Act of 1774, which allowed royal governors to house British troops in the homes of citizens without permission of the owner.[53]
In response, Massachusetts patriots issued theSuffolk Resolvesand formed an alternative shadow government known as the Provincial Congress, which began training militia outside British-occupied Boston.[54]In September 1774, theFirst Continental Congressconvened, consisting of representatives from each colony, to serve as a vehicle for deliberation and collective action. During secret debates, conservativeJoseph Gallowayproposed the creation of a colonial Parliament that would be able to approve or disapprove acts of the British Parliament, but his idea was tabled in a vote of 6 to 5 and was subsequently removed from the record.[citation needed]Congress called for a boycottbeginning on December 1, 1774, of all British goods; it was enforced by new local committees authorized by the Congress.[55]It also began coordinatingPatriotresistance by militias which existed in every colony and which had gained military experience in the French and Indian War. For the first time, the Patriots were armed and unified against Parliament.
King Georgedeclared Massachusetts to be in a state of rebellion in February 1775[56]and the British garrison received orders to seize the rebels' weapons and arrest their leaders, leading to theBattles of Lexington and Concordon April 19, 1775. The Patriots assembled a militia 15,000 strong and laid siege to Boston, occupied by 6500 British soldiers. TheSecond Continental Congressconvened in Philadelphia on June 14, 1775. The congress was divided on the best course of action. They authorized formation of theContinental Armyand appointed George Washington as its commander-in-chief, and produced theOlive Branch Petitionin which they attempted to come to an accord with King George. The king, however, issued aProclamation of Rebellionwhich declared that the states were "in rebellion" and the members of Congress were traitors. TheBattle of Bunker Hillfollowed on June 17, 1775. It was a British victory—but at a great cost: about 1,000 British casualties from a garrison of about 6,000, as compared to 500 American casualties from a much larger force.[57][58]
AsBenjamin Franklinwrote toJoseph Priestleyin October 1775:
Britain, at the expense of three millions, has killed 150 Yankees this campaign, which is £20,000 a head ... During the same time, 60,000 children have been born in America. From these data his mathematical head will easily calculate the time and expense necessary to kill us all.[59]
In the winter of 1775, the Americansinvaded northeastern Quebecunder generalsBenedict ArnoldandRichard Montgomery, expecting to rally sympathetic colonists there. The attack was a failure; many Americans were killed, captured, or died of smallpox.
In March 1776, aided by thefortification of Dorchester Heightswith cannons recentlycaptured at Fort Ticonderoga, the Continental Army led by George Washington forced the British toevacuate Boston. The revolutionaries now fully controlled all thirteen colonies and were ready to declare independence. There still were many Loyalists, but they were no longer in control anywhere by July 1776, and all of the Royal officials had fled.[60]
Following theBattle of Bunker Hillin June 1775, the Patriots had control ofMassachusettsoutsideBoston's city limits, and the Loyalists suddenly found themselves on the defensive with no protection from the British army. In each of the Thirteen Colonies, American patriots overthrew their existing governments, closed courts, and drove out British colonial officials. They held elected conventions and establishedtheir own legislatures, which existed outside any legal parameters established by the British. New constitutions were drawn up in each state to supersede royal charters. They proclaimed that they were nowstates, no longercolonies.[61]
On January 5, 1776,New Hampshireratified the first state constitution. In May 1776, Congress voted to suppress all forms of crown authority, to be replaced by locally created authority.New Jersey,South Carolina, andVirginiacreated their constitutions before July 4.Rhode IslandandConnecticutsimply took their existingroyal chartersand deleted all references to the crown.[62]The new states were all committed to republicanism, with no inherited offices. On May 26, 1776,John Adamswrote James Sullivan fromPhiladelphiawarning against extendingthe franchisetoo far:
Depend upon it, sir, it is dangerous to open so fruitful a source of controversy and altercation, as would be opened by attempting to alter the qualifications of voters. There will be no end of it. New claims will arise. Women will demand a vote. Lads from twelve to twenty one will think their rights not enough attended to, and every man, who has not a farthing, will demand an equal voice with any other in all acts of state. It tends to confound and destroy all distinctions, and prostrate all ranks, to one common level[.][63][64]
The resulting constitutions in states, including those ofDelaware,Maryland,Massachusetts,New York, andVirginia[b]featured:
InPennsylvania,New Jersey, andNew Hampshire, the resulting constitutions embodied:
The radical provisions ofPennsylvania's constitution lasted 14 years. In 1790, conservatives gained power in the state legislature, called a new constitutional convention, and rewrote the constitution. The new constitution substantially reduced universal male suffrage, gave the governor veto power and patronage appointment authority, and added an upper house with substantial wealth qualifications to the unicameral legislature.Thomas Painecalled it a constitution unworthy of America.[65]
In April 1776, theNorth Carolina Provincial Congressissued theHalifax Resolvesexplicitly authorizing its delegates to vote for independence.[66]By June, nine Provincial Congresses supported independence from Britain, and Pennsylvania, Delaware, Maryland, and New York followed.Richard Henry Leewas instructed by the Virginia legislature to propose independence, and he did so on June 7, 1776.
Gathered at Pennsylvania State House, later renamedIndependence Hall, inPhiladelphia, 56 of the nation'sFounding Fathers, representingAmerica's Thirteen Colonies, unanimously adopted and issued to KingGeorge IIItheDeclaration of Independence, which was drafted largely byThomas Jeffersonand presented by theCommittee of Five, which was charged with authoring it. The Congress struck several provisions of Jefferson's draft, and then adopted it unanimously on July 4.[67]The Declaration embodied the political philosophies ofliberalismandrepublicanism, rejectedmonarchyandaristocracy, and famously proclaimed that "all men are created equal". With the issuance of the Declaration of Independence, each colony began operating as independent and sovereign states. The next step was to form aunionto facilitate international relations and alliances.[68][69]
On November 5, 1777, the Congress approved theArticles of Confederation and Perpetual Unionand sent it to each state for ratification. The Congress immediately began operating under the Articles' terms, providing a structure ofshared sovereigntyduring prosecution of theRevolutionary Warand facilitating international relations and alliances. The Articles were fully ratified on March 1, 1781. At that point, the Continental Congress was dissolved and a new government of theUnited States in Congress Assembledtook its place the following day, on March 2, 1782, withSamuel Huntingtonleading the Congress as presiding officer.[70][71]
According to British historianJeremy Black, the British had significant advantages, including a highly trained army, the world's largest navy, and an efficient system of public finance that could easily fund the war. However, they seriously misunderstood the depth of support for the American Patriot position, misinterpreting the situation as merely a large-scale riot. The British government believed that they could overawe the Americans by sending a large military and naval force:
Convinced that the Revolution was the work of a full few miscreants who had rallied an armed rabble to their cause, they expected that the revolutionaries would be intimidated .... Then the vast majority of Americans, who were loyal but cowed by the terroristic tactics ... would rise up, kick out the rebels, and restore loyal government in each colony.[72]
In theSiege of Boston, Washington forced the British out of the city in the spring of 1776, and neither the British nor the Loyalists controlled any significant areas. The British, however, were amassing forces at their naval base atHalifax, Nova Scotia. They returned in force in July 1776, landing in New York and defeating Washington's Continental Army in August at theBattle of Brooklyn. This gave the British control of New York City and its strategicharbor. Following that victory, they requested a meeting with representatives from Congress to negotiate an end to hostilities.[73][74]
A delegation includingJohn AdamsandBenjamin Franklinmet British admiralRichard HoweonStaten IslandinNew York Harboron September 11 in what became known as theStaten Island Peace Conference. Howe demanded that the Americans retract theDeclaration of Independence, which they refused to do, and negotiations ended. The British thenseized New York Cityand nearly captured Washington's army. They made the city their main political and military base of operations, holding it untilNovember 1783. The city became the destination for Loyalist refugees and a focal point of Washington'sintelligence network.[73][74]
The British also took New Jersey, pushing the Continental Army into Pennsylvania. Washingtoncrossed the Delaware Riverback into New Jersey in a surprise attack in late December 1776 and defeated theHessianand British armies atTrentonandPrinceton, thereby regaining control of most of New Jersey. The victories gave an important boost to Patriots at a time when morale was flagging, and they have become iconic events of the war.
In September 1777, in anticipation of acoordinated attackby the British Army on the revolutionary capital of Philadelphia, the Continental Congress was forced to depart Philadelphia temporarily forBaltimore, where they continued deliberations.
In 1777, the British sent Burgoyne's invasion force from Canada south to New York to seal off New England. Their aim was to isolate New England, which the British perceived as the primary source of agitation. Rather than move north to support Burgoyne, the British army in New York City went to Philadelphia in a major case of mis-coordination, capturing it from Washington. The invasion army underBurgoynewas much too slow and became trapped in northern New York state. It surrendered after theBattles of Saratogain October 1777. From early October 1777 until November 15, a siege distracted British troops atFort Mifflin, Philadelphia, Pennsylvania, and allowed Washington time to preserve the Continental Army by safely leading his troops to harsh winter quarters atValley Forge.
On August 23, 1775, George III declared Americans to be traitors to the Crown if they took up arms against royal authority. There were thousands of British and Hessian soldiers in American hands following their surrender at the Battles of Saratoga. Lord Germain took a hard line, but the British generals on American soil never held treason trials, and instead treated captured American soldiers as prisoners of war.[75]The dilemma was that tens of thousands of Loyalists were under American control and American retaliation would have been easy. The British built much of their strategy around using these Loyalists.[76]The British maltreated the prisoners whom they held, resulting in more deaths to American prisoners of war than from combat operations.[76]At the end of the war, both sides released their surviving prisoners.[77]
The capture of a British army at Saratogaencouraged the French to formally enter the war in support of Congress, and Benjamin Franklin negotiated a permanent military alliance in early 1778; France thus became the first foreign nation to officially recognize the Declaration of Independence. On February 6, 1778, the United States and France signed theTreaty of Amity and Commerceand theTreaty of Alliance.[78]William Pittspoke out in Parliament urging Britain to make peace in America and to unite with America against France, while British politicians who had sympathized with colonial grievances now turned against the Americans for allying with Britain's rival and enemy.[79]
The Spanish and the Dutch became allies of the French in 1779 and 1780 respectively, forcing the British to fight a global war without major allies, and requiring it to slip through a combined blockade of the Atlantic. Britain began to view the American war for independence as merely one front in a wider war,[80]and the British chose to withdraw troops from America to reinforce the British colonies in the Caribbean, which were under threat of Spanish or French invasion. British commander SirHenry Clintonevacuated Philadelphia and returned to New York City.  Washington intercepted him in theBattle of Monmouth Court House, the last major battle fought in the north. After an inconclusive engagement, the British retreated to New York City. The northern war subsequently became a stalemate, as the focus of attention shifted to the smaller southern theater.[81]
TheBritish Royal Navyblockaded ports and heldNew York Cityfor the duration of the war, and other cities for brief periods, but failed in their effort to destroy Washington's forces. The British strategy now concentrated on a campaign in thesouthern states. With fewer regular troops at their disposal, the British commanders saw the "southern strategy" as a more viable plan, as they perceived the south as strongly Loyalist with a large population of recent immigrants and large numbers of slaves who might be tempted to run away from their masters to join the British and gain their freedom.[82]
Beginning in late December 1778, the British capturedSavannahand controlled theGeorgiacoastline. In 1780, they launched a fresh invasion andtook Charleston. A significant victory at theBattle of Camdenmeant that royal forces soon controlled most of Georgia and South Carolina. The British set up a network of forts inland, hoping that the Loyalists would rally to the flag.[83]Not enough Loyalists turned out, however, and the British had to fight their way north into North Carolina and Virginia with a severely weakened army. Behind them, much of the territory that they had already captured dissolved into a chaoticguerrilla war, fought predominantly between bands of Loyalists and American militia, which negated many of the gains that the British had previously made.[83]
The British army under Cornwallis marched toYorktown, Virginia, where they expected to be rescued by a British fleet.[84]The fleet did arrive, but so did a larger French fleet. The French were victorious in theBattle of the Chesapeake, and the British fleet returned to New York for reinforcements, leaving Cornwallis trapped. In October 1781, the British surrendered their second invading army of the war under a siege by the combined French and Continental armies commanded by Washington.[85]
Washington did not know if or when the British might reopen hostilities after Yorktown. They still had 26,000 troops occupying New York City, Charleston, and Savannah, together with a powerful fleet. The French army and navy departed, so the Americans were on their own in 1782–83.[86]The American treasury was empty, and the unpaid soldiers were growing restive, almost to the point of mutiny or possiblecoup d'etat. Washington dispelled the unrest among officers of theNewburgh Conspiracyin 1783, and Congress subsequently created the promise of a five years bonus for all officers.[87]
Historians continue to debate whether the odds were long or short for American victory.John E. Ferlingsays that the odds were so long that the American victory was "almost a miracle".[88]On the other hand,Joseph Ellissays that the odds favored the Americans, and asks whether there ever was any realistic chance for the British to win. He argues that this opportunity came only once, in the summer of 1776, and Admiral Howe and his brother General Howe "missed several opportunities to destroy the Continental Army .... Chance, luck, and even the vagaries of the weather played crucial roles." Ellis's point is that the strategic and tactical decisions of the Howes were fatally flawed because they underestimated the challenges posed by the Patriots. Ellis concludes that, once the Howe brothers failed, the opportunity "would never come again" for a British victory.[89]
Support for the conflict had never been strong in Britain, where many sympathized with the Americans, but now it reached a new low.[90]King George wanted to fight on, but his supporters lost control of Parliament and they launched no further offensives in America on the eastern seaboard.[81][c]
During negotiations in Paris, the American delegation discovered that France supported American independence but no territorial gains, hoping to confine the new nation to the area east of the Appalachian Mountains. The Americans opened direct secret negotiations with London, cutting out the French. British Prime MinisterLord Shelburnewas in charge of the British negotiations, and he saw a chance to make the United States a valuable economic partner, facilitating trade and investment opportunities.[92]The US obtained all the land east of the Mississippi River, includingsouthern Canada, but Spain took control of Florida from the British. It gained fishing rights off Canadian coasts, and agreed to allow British merchants and Loyalists to recover their property. Prime Minister Shelburne foresaw highly profitable two-way trade between Britain and the rapidly growing United States, which did come to pass. The blockade was lifted and American merchants were free to trade with any nation anywhere in the world.[93]
The British largely abandoned their Indigenous allies, who were not a party to this treaty and did not recognize it until they were defeated militarily by the United States. However, the British did sell them munitions and maintain forts in American territory until theJay Treatyof 1795.[94]
Losing the war and the Thirteen Colonies was a shock to Britain. The war revealed the limitations of Britain'sfiscal-military statewhen they discovered that they suddenly faced powerful enemies with no allies, and they were dependent on extended and vulnerable transatlantic lines of communication. The defeat heightened dissension and escalated political antagonism to the King's ministers.  The King went so far as to draft letters of abdication, although they were never delivered.[95]Inside Parliament, the primary concern changed from fears of an over-mighty monarch to the issues of representation, parliamentary reform, and government retrenchment. Reformers sought to destroy what they saw as widespreadinstitutional corruption, and the result was a crisis from 1776 to 1783. The crisis ended after 1784 confidence in the British constitution was restored during the administration of Prime MinisterWilliam Pitt.[96][97][d]
Britain's war against the Americans, the French, and the Spanish cost about £100 million. The Treasury borrowed 40 percent of the money that it needed.[99]Britain had a sophisticated financial system based on the wealth of thousands of landowners who supported the government, together with banks and financiers in London. In London the British had relatively little difficulty financing their war, keeping their suppliers and soldiers paid, and hiring tens of thousands of German soldiers.[100]
In sharp contrast, Congress and the American states had no end of difficulty financing the war.[101]In 1775, there was at most 12 million dollars in gold in the colonies, not nearly enough to cover current transactions, let alone finance a major war. The British made the situation much worse by imposing a tight blockade on every American port, which cut off almost all trade. One partial solution was to rely on volunteer support from militiamen and donations from patriotic citizens.[102][103]Another was to delay actual payments, pay soldiers and suppliers in depreciated currency, and promise that it would be made good after the war. Indeed, the soldiers and officers were given land grants in 1783 to cover the wages that they had earned but had not been paid during the war. The national government did not have a strong leader in financial matters until 1781, whenRobert Morriswas namedSuperintendent of Finance of the United States.[102]Morris used a French loan in 1782 to set up the privateBank of North Americato finance the war. He reduced thecivil list, saved money by using competitive bidding for contracts, tightened accounting procedures, and demanded the national government's full share of money and supplies from the individual states.[102]
Congress used four main methods to cover the cost of the war, which cost about 66 million dollars in specie (gold and silver).[104]Congress made issues of paper money, known colloquially as "Continental Dollars", in 1775–1780 and in 1780–1781. The first issue amounted to 242 million dollars. This paper money would supposedly be redeemed for state taxes, but the holders were eventually paid off in 1791 at the rate of one cent on the dollar. By 1780, the paper money was so devalued that the phrase "not worth a Continental" became synonymous with worthlessness.[105]The skyrocketing inflation was a hardship on the few people who had fixed incomes, but 90 percent of the people were farmers and were not directly affected by it. Debtors benefited by paying off their debts with depreciated paper. The greatest burden was borne by the soldiers of the Continental Army whose wages were usually paid late and declined in value every month, weakening their morale and adding to the hardships of their families.[106]
Beginning in 1777, Congress repeatedly asked the states to provide money, but the states had no system of taxation and were of little help. By 1780, Congress was making requisitions for specific supplies of corn, beef, pork, and other necessities, an inefficient system which barely kept the army alive.[107][108]Starting in 1776, the Congress sought to raise money by loans from wealthy individuals, promising to redeem the bonds after the war. The bonds were redeemed in 1791 at face value, but the scheme raised little money because Americans had little specie, and many of the rich merchants were supporters of the Crown. The French secretly supplied the Americans with money, gunpowder, and munitions to weaken Great Britain; the subsidies continued when France entered the war in 1778, and the French government and Paris bankers lent large sums[quantify]to the American war effort. The Americans struggled to pay off the loans; they ceased making interest payments to France in 1785 and defaulted on installments due in 1787. In 1790, however, they resumed regular payments on their debts to the French,[109]and settled their accounts with the French government in 1795 when James Swan, an American banker, assumed responsibility for the balance of the debt in exchange for the right to refinance it at a profit.[110]
The war ended in 1783 and was followed by a period of prosperity. The national government was still operating under the Articles of Confederation andsettled the issue of the western territories, which the states ceded to Congress. American settlers moved rapidly into those areas, with Vermont, Kentucky, and Tennessee becoming states in the 1790s.[111]
However, the national government had no money either to pay the war debts owed to European nations and the private banks, or to pay Americans who had been given millions of dollars of promissory notes for supplies during the war. Nationalists led by Washington, Alexander Hamilton, and other veterans feared that the new nation was too fragile to withstand an international war, or even the repetition of internal revolts such as theShays's Rebellionof 1786 in Massachusetts. They convinced Congress to call thePhiladelphia Conventionin 1787.[112]The Convention adopted a newConstitutionwhich provided for arepublicwith a much stronger national government in afederalframework, including an effective executive in acheck-and-balancesystem with the judiciary and legislature.[113]The Constitution was ratified in 1788, after a fierce debate in the states over the proposed new government. Thenew administrationunder President George Washington took office in New York in March 1789.[114]James Madisonspearheaded Congressional legislation proposing amendments to the Constitution as assurances to those cautious about federal power, guaranteeing many of theinalienable rightsthat formed a foundation for the revolution. Rhode Island was the final state to ratify the Constitution in 1790, the first ten amendments were ratified in 1791 and became known as theUnited States Bill of Rights.
The national debt fell into three categories after the American Revolution. The first was the $12 million owed to foreigners, mostly money borrowed from France. There was general agreement to pay the foreign debts at full value. The national government owed $40 million and state governments owed $25 million to Americans who had sold food, horses, and supplies to the Patriot forces. There were also other debts which consisted ofpromissory notesissued during the war to soldiers, merchants, and farmers who accepted these payments on the premise that the new Constitution would create a government that would pay these debts eventually.
The war expenses of the individual states added up to $114 million, compared to $37 million by the central government.[115]In 1790, Congress combined the remaining state debts with the foreign and domestic debts into one national debt totaling $80 million at the recommendation of first Secretary of the TreasuryAlexander Hamilton. Everyone received face value for wartime certificates, so that the national honor would be sustained and the national credit established.[116]
The population of the Thirteen States was not homogeneous in political views and attitudes. Loyalties and allegiances varied widely within regions and communities and even within families, and sometimes shifted during the Revolution.
The American Enlightenment was a critical precursor of the American Revolution. Chief among the ideas of the American Enlightenment were the concepts of natural law, natural rights, consent of the governed, individualism, property rights, self-ownership, self-determination, liberalism, republicanism, and defense against corruption. A growing number of American colonists embraced these views and fostered an intellectual environment which led to a new sense of political and social identity.[117]
John Lockeis often referred to as "the philosopher of the American Revolution" due to his work in theSocial ContractandNatural Rightstheories that underpinned the Revolution's political ideology.[119]Locke'sTwo Treatises of Governmentpublished in 1689 was especially influential. He argued that all humans were created equally free, and governments therefore needed the "consent of the governed".[120]In late eighteenth-century America, belief was still widespread in "equality by creation" and "rights by creation".[121]Locke's ideas on liberty influenced the political thinking of English writers such asJohn Trenchard,Thomas Gordon, andBenjamin Hoadly, whose political ideas in turn also had a strong influence on the American Patriots.[122]His work also inspired symbols used in the American Revolution such as the "Appeal to Heaven" found on thePine Tree Flag, which alludes to Locke's concept of theright of revolution.[123]
The theory of the social contract influenced the belief among many of the Founders that theright of the people to overthrow their leaders, should those leaders betray the historicrights of Englishmen, was one of the "natural rights" of man.[124][125]The Americans heavily relied onMontesquieu's analysis of the wisdom of the "balanced" British Constitution (mixed government) in writing the state and national constitutions.
The American interpretation ofrepublicanismwas inspired by theWhig partyin Great Britain which openly criticized the corruption within the British government.[126]Americans were increasingly embracing republican values, seeing Britain as corrupt and hostile to American interests.[127]The colonists associated political corruption with ostentatious luxury and inherited aristocracy.[128]
The Founding Fathers were strong advocates of republican values, particularlySamuel Adams,Patrick Henry,John Adams,Benjamin Franklin,Thomas Jefferson,Thomas Paine,George Washington,James Madison, andAlexander Hamilton,[129]which required men to put civic duty ahead of their personal desires. Men werehonor boundby civic obligation to be prepared and willing to fight for the rights and liberties of their countrymen. John Adams wrote toMercy Otis Warrenin 1776, agreeing with some classical Greek and Roman thinkers: "Public Virtue cannot exist without private, and public Virtue is the only Foundation of Republics." He continued:
There must be a positive Passion for the public good, the public Interest, Honour, Power, and Glory, established in the Minds of the People, or there can be no Republican Government, nor any real Liberty. And this public Passion must be Superior to all private Passions. Men must be ready, they must pride themselves, and be happy to sacrifice their private Pleasures, Passions, and Interests, nay their private Friendships and dearest connections, when they Stand in Competition with the Rights of society.[130]
Protestant churches that had separated from theChurch of England, called "dissenters", were the "school of democracy", in the words of historian Patricia Bonomi.[131]Before the Revolution, theSouthern Coloniesand three of theNew England Colonieshad officialestablished churches:CongregationalinMassachusetts Bay,Connecticut, andNew Hampshire, and the Church of England inMaryland,Virginia,North-Carolina,South Carolina, andGeorgia. TheNew York,New Jersey,Pennsylvania,Delaware, and theColony of Rhode Island and Providence Plantationshad no officially established churches.[132]Church membership statistics from the period are unreliable and scarce,[133]but what little data exists indicates that the Church of England was not in the majority, not even in the colonies where it was the established church, and they probably did not comprise even 30 percent of the population in most localities (with the possible exception of Virginia).[132]
John Witherspoon, who was considered a "new light"Presbyterian, wrote widely circulated sermons linking the American Revolution to the teachings of theBible. Throughout the colonies, dissentingProtestantministers from the Congregational,Baptist, and Presbyterian churches preached Revolutionary themes in their sermons while mostChurch of Englandclergymen preached loyalty to the king, thetitular headof the Englishstate church.[134]Religious motivation for fighting tyranny transcended socioeconomic lines.[131]The Declaration of Independence also referred to the "Laws of Nature and of Nature's God" as justification for the Americans' separation from the British monarchy: the signers of the Declaration professed their "firm reliance on the Protection of divine Providence", and they appealed to "the Supreme Judge for the rectitude of our intentions".[135]
HistorianBernard Bailynargues that the evangelicalism of the era challenged traditional notions of natural hierarchy by preaching that the Bible teaches that all men are equal, so that the true value of a man lies in his moral behavior, not in his class.[136]Kidd argues that religiousdisestablishment, belief in God as the source of human rights, and shared convictions about sin, virtue, and divine providence worked together to unite rationalists and evangelicals and thus encouraged a large proportion of Americans to fight for independence from the Empire. Bailyn, on the other hand, denies that religion played such a critical role.[137]Alan Heimert argues that New Light anti-authoritarianism was essential to furthering democracy in colonial American society, and set the stage for a confrontation with British monarchical and aristocratic rule.[138]
The Revolution was effected before the war commenced. The Revolution was in the minds and hearts of the people .... This radical change in the principles, opinions, sentiments, and affections of the people was the real American Revolution.[139]
In the mid-20th century, historianLeonard Woods Labareeidentified eight characteristics of the Loyalists that made them essentially conservative, opposite to the characteristics of the Patriots.[140]Loyalists tended to feel that resistance to the Crown was morally wrong, while the Patriots thought that morality was on their side.[141][142]Loyalists were alienated when the Patriots resorted to violence, such as burning houses andtarring and feathering. Loyalists wanted to take a centrist position and resisted the Patriots' demand to declare their opposition to the Crown. Many Loyalists had maintained strong and long-standing relations with Britain, especially merchants in port cities such as New York and Boston.[141][142]Many Loyalists felt that independence was bound to come eventually, but they were fearful that revolution might lead to anarchy, tyranny, or mob rule. In contrast, the prevailing attitude among Patriots was a desire to seize the initiative.[141][142]Labaree also wrote that Loyalists were pessimists who lacked the confidence in the future displayed by the Patriots.[140]
Historians in the early 20th century such asJ. Franklin Jamesonexamined the class composition of the Patriot cause, looking for evidence of a class war inside the revolution.[143]More recent historians have largely abandoned that interpretation, emphasizing instead the high level of ideological unity.[144]Both Loyalists and Patriots were a "mixed lot",[145][146]but ideological demands always came first. The Patriots viewed independence as a means to gain freedom from British oppression and to reassert their basic rights. Most yeomen farmers, craftsmen, and small merchants joined the Patriot cause to demand more political equality. They were especially successful in Pennsylvania but less so in New England, where John Adams attacked Thomas Paine'sCommon Sensefor the "absurd democratical notions" that it proposed.[145][146]
The revolution became a personal issue forthe king, fueled by his growing belief that British leniency would be taken as weakness by the Americans. He also sincerely believed that he was defendingBritain's constitutionagainst usurpers, rather than opposing patriots fighting for their natural rights.[147]King George III is often accused of obstinately trying to keep Great Britain at war with the revolutionaries in America, despite the opinions of his own ministers.[148]In the words of the British historianGeorge Otto Trevelyan, the King was determined "never to acknowledge the independence of the Americans, and to punish their contumacy by the indefinite prolongation of a war which promised to be eternal."[149]The king wanted to "keep the rebels harassed, anxious, and poor, until the day when, by a natural and inevitable process, discontent and disappointment were converted into penitence and remorse".[150]Later historians defend George by saying in the context of the times no king would willingly surrender such a large territory,[151][152]and his conduct was far less ruthless than contemporary monarchs in Europe.[153]After the surrender of a British army at Saratoga, both Parliament and the British people were largely in favor of the war; recruitment ran at high levels and although political opponents were vocal, they remained a small minority.[151][154]
With the setbacks in America,Lord Northasked to transfer power toLord Chatham, whom he thought more capable, but George refused to do so; he suggested instead that Chatham serve as a subordinate minister in North's administration, but Chatham refused. He died later in the same year.[155]Lord North was allied to the "King's Friends" in Parliament and believed George III had the right to exercise powers.[156]In early 1778,Britain's chief rival Francesigned atreaty of alliancewith the United States, and the confrontation soon escalated from a "rebellion" to something that has been characterized as "world war".[157]The French fleet was able to outrun the British naval blockade of the Mediterranean and sailed to North America.[157]The conflict now affected North America, Europe andIndia.[157]The United States and France were joined bySpainin 1779 and theDutch Republic, while Britain had no major allies of its own, except for the Loyalist minority in America and German auxiliaries (i.e.Hessians).Lord GowerandLord Weymouthboth resigned from the government. Lord North again requested that he also be allowed to resign, but he stayed in office at George III's insistence.[158]Opposition to the costly war was increasing, and in June 1780 contributed to disturbances in London known as theGordon riots.[158]
As late as theSiege of Charlestonin 1780, Loyalists could still believe in their eventual victory, as British troops inflicted defeats on the Continental forces at theBattle of Camdenand theBattle of Guilford Court House.[159]In late 1781, the news of Cornwallis's surrender at the siege of Yorktown reached London; Lord North's parliamentary support ebbed away and he resigned the following year. The king drafted an abdication notice, which was never delivered,[152][160]finally accepted the defeat in North America, and authorized peace negotiations. TheTreaties of Paris, by which Britain recognized the independence of the United States andreturned Floridato Spain, were signed in 1782 and 1783 respectively.[161]In early 1783, George III privately conceded "America is lost!" He reflected that the Northern colonies had developed into Britain's "successful rivals" in commercial trade and fishing.[162]
WhenJohn Adamswas appointedAmerican Minister to Londonin 1785, George had become resigned to the new relationship between his country and the former colonies. He told Adams, "I was the last to consent to the separation; but the separation having been made and having become inevitable, I have always said, as I say now, that I would be the first to meet the friendship of the United States as an independent power."[163]
Those who fought for independence were called "Revolutionaries", "Continentals", "Rebels", "Patriots", "Whigs", "Congress-men", or "Americans" during and after the war. They included a full range of social and economic classes but were unanimous regarding the need to defend the rights of Americans and uphold the principles of republicanism in rejecting monarchy and aristocracy, while emphasizing civic virtue by citizens. The signers of the Declaration of Independence were mostly—with definite exceptions—well-educated, of British stock, and of the Protestant faith.[164][165]Newspapers were strongholds of patriotism(although there were a few Loyalist papers) and printed many pamphlets, announcements, patriotic letters, and pronouncements.[166]
According to historian Robert Calhoon, 40 to 45 percent of the white population in the Thirteen Colonies supported the Patriots' cause, 15 to 20 percent supported the Loyalists, and the remainder were neutral or kept a low profile.[167]Mark Lender concludes that ordinary people became insurgents against the British because they held a sense of rights which the British were violating, rights that stressed local autonomy, fair dealing, and government by consent. They were highly sensitive to the issue of tyranny, which they saw manifested in the British response to the Boston Tea Party. The arrival in Boston of the British Army heightened their sense of violated rights, leading to rage and demands for revenge. They had faith that God was on their side.[168]
Thomas Paine published his pamphletCommon Sensein January 1776, after the Revolution had started. It was widely distributed and often read aloud in taverns, contributing significantly to concurrently spreading the ideas of republicanism and liberalism, bolstering enthusiasm for separation from Great Britain and encouraging recruitment for the Continental Army.[169]Paine presented the Revolution as the solution for Americans alarmed by the threat of tyranny.[169]
The consensus of scholars is that about 15 to 20 percent of the white population remained loyal to the British Crown.[170]Those who actively supported the king were known at the time as "Loyalists", "Tories", or "King's men". The Loyalists never controlled territory unless the British Army occupied it. They were typically older, less willing to break with old loyalties, and often connected to the Church of England; they included many established merchants with strong business connections throughout the Empire, as well as royal officials such as Thomas Hutchinson of Boston.[171]
There were 500 to 1,000Black Loyalists, enslaved African Americans who escaped to British lines and supported Britain's cause via several means. Many of them died from disease, but the survivors were evacuated by the British totheir remaining colonies in North America.[172]
The revolution could divide families, such asWilliam Franklin, son of Benjamin Franklin and royal governor of theProvince of New Jerseywho remained loyal to the Crown throughout the war. He and his father never spoke again.[173]Recent immigrants who had not been fully Americanized were also inclined to support the King.[174]
After the war, the great majority of the half-million Loyalists remained in America and resumed normal lives. Some became prominent American leaders, such asSamuel Seabury. Approximately 46,000 Loyalists relocated to Canada; others moved to Britain (7,000), Florida, or the West Indies (9,000). The exiles represented approximately two percent of the total population of the colonies.[175]Nearly all Black Loyalists left for Nova Scotia, Florida, or England, where they could remain free.[176]Loyalists who left the South in 1783 took thousands of their slaves with them as they fled to theBritish West Indies.[175]
A minority of uncertain size tried to stay neutral in the war. Most kept a low profile, but the Quakers were the most important group to speak out for neutrality, especially in Pennsylvania. The Quakers continued to do business with the British even after the war began, and they were accused of supporting British rule, "contrivers and authors of seditious publications" critical of the revolutionary cause.[177][full citation needed]Most Quakers remained neutral, althougha sizeable numberparticipated to some degree.
Women contributed to the American Revolution in many ways and were involved on both sides. Formal politics did not include women, but ordinary domestic behaviors became charged with political significance as Patriot women confronted a war which permeated all aspects of political, civil, and domestic life. They participated by boycotting British goods, spying on the British, following armies as they marched, washing, cooking, and mending for soldiers, delivering secret messages, and even fighting disguised as men in a few cases, such asDeborah Samson.Mercy Otis Warrenheld meetings in her house and cleverly attacked Loyalists with her creative plays and histories.[178]Many women also acted as nurses and helpers, tending to the soldiers' wounds and buying and selling goods for them. Some of thesecamp followerseven participated in combat, such as Madam John Turchin who led her husband's regiment into battle.[179]Above all, women continued the agricultural work at home to feed their families and the armies. They maintained their families during their husbands' absences and sometimes after their deaths.[180]
American women were integral to the success of the boycott of British goods,[181]as the boycotted items were largely household articles such as tea and cloth. Women had to return to knitting goods and to spinning and weaving their own cloth—skills that had fallen into disuse. In 1769, the women of Boston produced 40,000 skeins of yarn, and 180 women inMiddletown, Massachusettswove 20,522 yards (18,765 m) of cloth.[180]Many women gathered food, money, clothes, and other supplies during the war to help the soldiers.[182]A woman's loyalty to her husband could become an open political act, especially for women in America committed to men who remained loyal to the King. Legal divorce, usually rare, was granted to Patriot women whose husbands supported the King.[183][184]
In early 1776, France set up a major program of aid to the Americans, and the Spanish secretly added funds. Each country spent one million "livres tournaises" to buy munitions. Adummy corporationrun byPierre Beaumarchaisconcealed their activities. American Patriots obtained some munitions from the Dutch Republic as well, through the French and Spanish ports in theWest Indies.[185]Heavy expenditures and a weak taxation system pushed France toward bankruptcy.[186]
In 1777,Charles François Adrien le Paulmier, Chevalier d'Annemours, acting as asecret agentfor France, made sure GeneralGeorge Washingtonwas privy to his mission. He followed Congress around for the next two years, reporting what he observed back to France.[187]TheTreaty of Alliancebetween the French and the Americans followed in 1778, which led to more French money,matérieland troops being sent to the United States.
Spain did not officially recognize the United States, but it was a French ally and it separately declared war on Britain on June 21, 1779.Bernardo de Gálvez, general of the Spanish forces inNew Spain, also served as governor of Louisiana. He led an expedition of colonial troops to capture Florida from the British and to keep open a vital conduit for supplies going to the Americans.[188]
Ethnic Germans served on both sides of the American Revolutionary War. As George III was also theElectorofHanover, many supported the Loyalist cause and served as allies of theKingdom of Great Britain; most notably rentedauxiliary troops[189]from German states such as theLandgraviate of Hessen-Kassel.
AmericanPatriotstended to represent such troops asmercenariesin propaganda against the British Crown. Even American historians followed suit, in spite of Colonial-era jurists drawing a distinction between auxiliaries and mercenaries, with auxiliaries serving their prince when sent to the aid of another prince, and mercenaries serving a foreign prince as individuals.[189]By this distinction the troops which served in the American Revolution were auxiliaries.
Other German individuals came to assist the American revolutionaries, most notablyFriedrich Wilhelm von Steuben, who served as a general in the Continental Army and is credited with professionalizing that force, but most Germans who served were already colonists. Von Steuben's native Prussia joined theLeague of Armed Neutrality,[190]and KingFrederick II of Prussiawas well appreciated in the United States for his support early in the war. He expressed interest in opening trade with the United States and bypassing English ports, and allowed an American agent to buy arms in Prussia.[191]Frederick predicted American success,[192]and promised to recognize the United States and American diplomats once France did the same.[193]Prussia also interfered in the recruiting efforts of Russia and neighboring German states when they raised armies to send to the Americas, and Frederick II forbade enlistment for the American war within Prussia.[194]All Prussian roads were denied to troops from Anhalt-Zerbst,[195]which delayed reinforcements that Howe had hoped to receive during the winter of 1777–1778.[196]
However, when theWar of the Bavarian Succession(1778–1779) erupted, Frederick II became much more cautious with Prussian/British relations. U.S. ships were denied access to Prussian ports, and Frederick refused to officially recognize the United States until they had signed theTreaty of Paris. Even after the war, Frederick II predicted that the United States was too large to operate as arepublic, and that it would soon rejoin the British Empire with representatives in Parliament.[197]
Most Indigenous people rejected pleas that they remain neutral and instead supported the British Crown. The great majority of the 200,000 Indigenous people east of the Mississippi distrusted the Americans and supported the British cause, hoping to forestall continued expansion of settlement into their territories.[199][200]Those tribes closely involved in trade tended to side with the Patriots, although political factors were important as well. Some tried to remain neutral, seeing little value in joining what they perceived to be a "white man's war", and fearing reprisals from whichever side they opposed.
The great majority of Indigenous people did not participate directly in the war, with the notable exceptions of warriors and bands associated with four of theIroquoistribes in New York and Pennsylvania which allied with the British,[200]and theOneidaandTuscaroratribes among the Iroquois of central and western New York who supported the American cause.[201]The British did have other allies, particularly in theregions of southwest Quebecon the Patriot's frontier. The British provided arms to Indigenous people who were led by Loyalists in war parties to raid frontier settlements from theCarolinasto New York. These war parties managed to kill many settlers on the frontier, especially in Pennsylvania and New York's Mohawk Valley.[202]
In 1776,Cherokeewar parties attacked American Colonists all along the southern Quebec frontier of the uplands throughout theWashington District, North Carolina(now Tennessee) and the Kentucky wilderness area.[203]TheChickamauga CherokeeunderDragging Canoeallied themselves closely with the British, and fought on for an additional decade after the Treaty of Paris was signed. They launched raids with roughly 200 warriors, as seen in theCherokee–American wars; they could not mobilize enough forces to invade settler areas without the help of allies, most often theCreek.
Joseph Brant(alsoThayendanegea) of the powerfulMohawktribe in New York was the most prominent Indigenous leader against the Patriot forces.[198]In 1778 and 1780, he led 300 Iroquois warriors and 100 white Loyalists in multiple attacks on small frontier settlements in New York and Pennsylvania, killing many settlers and destroying villages, crops, and stores.[204]
In 1779, theContinental Army forced the hostile Indigenous people out of upstate New Yorkwhen Washington sent an army underJohn Sullivanwhich destroyed 40 evacuated Iroquois villages in central and western New York. TheBattle of Newtownproved decisive, as the Patriots had an advantage of three-to-one, and it ended significant resistance; there was little combat otherwise.  Facing starvation and homeless for the winter, the Iroquois fled to Canada.[205]
At the peace conference following the war, the British ceded lands which they did not really control, without consultation with their Indigenous allies. They transferred control to the United States of all the land south of the Great Lakes east of the Mississippi and north of Florida. Calloway concludes:
Burned villages and crops, murdered chiefs, divided councils and civil wars, migrations, towns and forts choked with refugees, economic disruption, breaking of ancient traditions, losses in battle and to disease and hunger, betrayal to their enemies, all made the American Revolution one of the darkest periods in American Indian history.[206]
Free Blacks in theNew England ColoniesandMiddle Coloniesin the North as well asSouthern Coloniesfought on both sides of the War, but the majority fought for the Patriots. Gary Nash reports that there were about 9,000 Black veteran Patriots, counting the Continental Army and Navy, state militia units, privateers, wagoneers in the Army, servants to officers, and spies.[207]Ray Raphael notes that thousands did join the Loyalist cause, but "a far larger number, free as well as slave, tried to further their interests by siding with the patriots."[208]Crispus Attuckswas one of the five people killed in theBoston Massacrein 1770 and is considered the first American casualty for the cause of independence.
The effects of the war were more dramatic in the South. Tens of thousands of slaves escaped to British lines throughout the South, causing dramatic losses to slaveholders and disrupting cultivation and harvesting of crops. For instance,South Carolinawas estimated to have lost about 25,000 slaves to flight, migration, or death which amounted to a third of its slave population.[209]
During the war, the British commanders attempted to weaken the Patriots by issuing proclamations of freedom to their slaves.[210]In the November 1775 document known asDunmore's ProclamationVirginia royal governor,Lord Dunmorerecruited Black men into the British forces with the promise of freedom, protection for their families, and land grants. Some men responded and briefly formed the BritishEthiopian Regiment. HistorianDavid Brion Davisexplains the difficulties with a policy of wholesale arming of the slaves:
But England greatly feared the effects of any such move on its ownWest Indies, where Americans had already aroused alarm over a possible threat to incite slave insurrections. The British elites also understood that an all-out attack on one form of property could easily lead to an assault on all boundaries of privilege and social order, as envisioned by radical religious sects in Britain's seventeenth-century civil wars.[211]
Davis underscores the British dilemma: "Britain, when confronted by the rebellious American colonists, hoped to exploit their fear of slave revolts while also reassuring the large number of slave-holding Loyalists and wealthy Caribbean planters and merchants that their slave property would be secure".[212]The Americans, however, accused the British of encouraging slave revolts, with the issue becoming one of the27 colonial grievances.[213]
The existence ofslavery in the American colonieshad attracted criticism from both sides of the Atlantic as many could not reconcile the existence of the institution with the egalitarian ideals espoused by leaders of the Revolution. British writerSamuel Johnsonwrote "how is it we hear the loudest yelps for liberty among the drivers of the Negroes?" in a text opposing the grievances of the colonists.[214]Referring to this contradiction, English abolitionistThomas Daywrote in a 1776 letter that
if there be an object truly ridiculous in nature, it is an American patriot, signing resolutions of independency with the one hand, and with the other brandishing a whip over his affrighted slaves.[215]
Thomas Jefferson unsuccessfully attempted to include a section in the Declaration of Independence which asserted that King George III had "forced" theslave tradeonto the colonies.[216]Despite the turmoil of the period, African-Americans contributed to the foundation of an American national identity during the Revolution.Phyllis Wheatley, an African-American poet, popularized the image ofColumbiato represent America.[217][full citation needed]
The 1779Philipsburg Proclamationexpanded the promise of freedom for Black men who enlisted in the British military to all the colonies in rebellion. British forces gave transportation to 10,000 slaves when they evacuatedSavannahandCharleston, carrying through on their promise.[218]They evacuated and resettled more than 3,000Black Loyalistsfrom New York toNova Scotia,Upper Canada, andLower Canada. Others sailed with the British to England or were resettled as freedmen in theWest Indiesof the Caribbean. But slaves carried to the Caribbean under control of Loyalist masters generally remained slaves until British abolition of slavery in its colonies in 1833–1838. More than 1,200 of the Black Loyalists of Nova Scotia later resettled in the British colony ofSierra Leone, where they became leaders of theKrioethnic group ofFreetownand the later national government. Many of their descendants still live in Sierra Leone, as well as other African countries.[219][full citation needed]
After the Revolution, genuinely democratic politics became possible in the former American colonies.[220]The rights of the people were incorporated into state constitutions. Concepts of liberty, individual rights, equality among men and hostility toward corruption became incorporated as core values of liberal republicanism. The new United States government was empowered to undertake its own project ofterritorial expansionandsettler colonialism. The greatest challenge to the old order in Europe was the challenge to inherited political power and the democratic idea that government rests on theconsent of the governed. The example of the first successful revolution against a European empire, and the first successful establishment of a republican form of democratically elected government, provided a model for many other colonial peoples who realized that they too could break away and become self-governing nations with directly elected representative government.[221][page needed]
Interpretations vary concerning the effect of the Revolution. Historians such asBernard Bailyn,Gordon Wood, andEdmund Morganview it as a unique and radical event which produced deep changes and had a profound effect on world affairs, such as an increasing belief in the principles of the Enlightenment. These were demonstrated by a leadership and government that espoused protection of natural rights, and a system of laws chosen by the people.[226]John Murrin, by contrast, argues that the definition of "the people" at that time was mostly restricted to free men who passed a property qualification.[227][228]
Edmund Morgan has argued that, in terms of long-term impact on American society and values:
The first shot of the American Revolution at the Battle of Lexington and Concord is referred to as the"shot heard 'round the world". The Revolutionary War victory not only established the United States as the first modern constitutional republic, but marked the transition from an age of monarchy to a new age of freedom by inspiring similar movements worldwide.[231]The American Revolution was the first of the "Atlantic Revolutions": followed most notably by theFrench Revolution, theHaitian Revolution, and theLatin American wars of independence. Aftershocks contributed torebellions in Ireland, thePolish–Lithuanian Commonwealth, and the Netherlands.[232][233][231]
TheU.S. Constitution, drafted shortly after independence, remains the world's oldest written constitution, and has been emulated by other countries, in some cases verbatim.[234]Some historians and scholars argue that the subsequent wave of independence and revolutionary movements has contributed to the continued expansion of democratic government; 144 countries, representing two-third of the world's population, are full or partially democracies of same form.[235][224][236][237][225][222]
The Dutch Republic, also at war with Britain, was the next country after France to sign a treaty with the United States, on October 8, 1782.[78]On April 3, 1783, Ambassador ExtraordinaryGustaf Philip Creutz, representing KingGustav III of Sweden, and Benjamin Franklin, signed aTreaty of Amity and Commercewith the U.S.[78]
The Revolution had a strong, immediate influence in Great Britain, Ireland, the Netherlands, and France. Many British and IrishWhigsin Parliament spoke glowingly in favor of the American cause. In Ireland, the Protestant minoritywho controlled Irelanddemandedself-rule. Under the leadership ofHenry Grattan, theIrish Patriot Partyforced the reversal of mercantilist prohibitions against trade with other British colonies. The King and his cabinet in London could not risk another rebellion, and so made a series of concessions to the Patriot faction in Dublin. Armed volunteer units of theProtestant Ascendancywere set up ostensibly to protect against an invasion from France. As had been in colonial America, so too in Ireland now the King no longer had amonopoly of lethal force.[238][231][239]
For many Europeans, such as theMarquis de Lafayette, who later were active during the era of theFrench Revolution, the American case along with theDutch Revolt(end of the 16th century) and the 17th centuryEnglish Civil War, was among the examples of overthrowing an old regime. The American Declaration of Independence influenced the FrenchDeclaration of the Rights of Man and of the Citizenof 1789.[240][241]The spirit of the Declaration of Independence led to laws ending slavery in all the Northern states and the Northwest Territory, with New Jersey the last in 1804. States such as New Jersey and New York adopted gradual emancipation, which kept some people as slaves for more than two decades longer.[242][231][243]
During the revolution, the contradiction between the Patriots' professed ideals of liberty and the institution of slavery generated increased scrutiny of the latter.[245]: 235[246]: 105–106[247]: 186As early as 1764, the Boston Patriot leaderJames Otis, Jr.declared that all men, "white or black", were "by the law of nature" born free.[245]: 237Anti-slavery calls became more common in the early 1770s. In 1773,Benjamin Rush, the future signer of the Declaration of Independence, called on "advocates for American liberty" to oppose slavery.[245]: 239Slavery became an issue that had to be addressed.  As historian Christopher L. Brown put it, slavery "had never been on the agenda in a serious way before," but the Revolution "forced it to be a public question from there forward."[248][249]
In the late 1760s and early 1770s, several colonies, including Massachusetts and Virginia, attempted to restrict the slave trade, but were prevented from doing so by royally appointed governors.[245]: 245In 1774, as part of a broader non-importation movement aimed at Britain, the Continental Congress called on all the colonies to ban the importation of slaves, and the colonies passed acts doing so.[245]: 245
In the first two decades after the American Revolution, state legislatures and individuals took actions to free slaves, in part based on revolutionary ideals. Northern states passed new constitutions that contained language about equal rights or specifically abolished slavery; some states, such as New York and New Jersey, where slavery was more widespread, passed laws by the end of the 18th century to abolish slavery by a gradual method. By 1804, all the northern states had passed laws outlawing slavery, either immediately or over time.[250]
No southern state abolished slavery. However, individual owners could free their slaves by personal decision. Numerous slaveholders who freed their slaves cited revolutionary ideals in their documents; others freed slaves as a reward for service. Records also suggest that some slaveholders were freeing their own mixed-race children, born into slavery to slave mothers. The number of free Blacks as a proportion of the Black population in the upper South increased from less than 1 percent to nearly 10 percent between 1790 and 1810 as a result of these actions.[251][252][253][254][255][256][257][258][259][260][excessive citations]Nevertheless, slavery continued in the South, where it became a "peculiar institution", setting the stage for future sectional conflict between North and South over the issue.[247]: 186–187
Thousands of free Blacks in the northern states fought in the state militias and Continental Army. In the south, both sides offered freedom to slaves who would perform military service. Roughly 20,000 slaves fought in the American Revolution.[261]
The status of women during the Revolutionary War can be illustrated by the interchange of gender, sexuality, citizenship, and class. While women were entering a period in which they found themselves gaining more identity within society, it was clear that they were still very much considered under men as their role in society remained being a good wife and mother. Their clothes, the way they responded to their husband, and listened to their husband, was incredibly important in the social sphere. Having a woman who was dressed well for her role as a good wife and mother as well as fitting the social role, was a symbol of not only status, but a family devoted to the republic. As they continued to nurture social and political partnerships, their role in enabling the success of the revolution emphasized their changing role in society – leading to the post-revolutionary reconstruction of gender ideology.
The democratic ideals of the Revolution inspired changes in the roles of women.[262]Patriot women married to Loyalists who left the state could get a divorce and obtain control of the ex-husband's property.[263]Abigail Adamsexpressed to her husband, the president, the desire of women to have a place in the new republic:
I desire you would remember the Ladies, and be more generous and favourable to them than your ancestors. Do not put such unlimited power into the hands of the Husbands.[264]
The Revolution sparked a discussion on the rights of woman and an environment favorable to women's participation in politics. Briefly the possibilities for women's rights were highly favorable, but a backlash led to a greater rigidity that excluded women from politics.[265]
Soon after the Revolutionary War, Mary Wallostonecraft would publish:Vindication of the Rights of Woman(1792) – challenging the idea that rights should only be granted to men. Her radical ideas would give ground to the conversation in allowing women to be bearers of rights alongside men – that while the rights of man were taking on a new meaning post-revolutionary America, it was time for the rights of women too.
However, this new sense of independence and dignity did not come with ease, as a gender hierarchy would continue to bound what it meant for women to have rights during the Post-Revolutionary era.
Women's rights were founded on the Scottish theory that treated rights simply as benefits . The emphasis was on duty and obligation, instead of liberty and choice – confining women to the traditional role of wife and mother. On the other hand, men's rights were heavily inspired by Locke, as it emphasized equality, individual autonomy, and the expansion of personal freedoms. So while women were becoming bearers of rights, the foundation and philosophy of those given rights differed vastly.
The early national period of America would continue to struggle with the concept of rights and equality, as women faced the notion that women should be under the dominance of men – carried by Christian beliefs. Women were blamed for the “Fall of Man”, in reference to Eve and Adam in the Bible. So while women were beginning to bear rights, the type of language that was being used when talking about the rights of women was done with care and hesitance.
For more than thirty years, however, the 1776New Jersey State Constitutiongave the vote to "all inhabitants" who had a certain level of wealth, including unmarried women and blacks (not married women because they could not own property separately from their husbands), until in 1807, when that state legislature passed a bill interpreting the constitution to mean universalwhite malesuffrage, excluding paupers.[266]
Tens of thousands of Loyalists left the United States following thewar; Philip Ranlet estimates 20,000, whileMaya Jasanoffestimates as many as 70,000.[267]Some migrated to Britain, but the great majority received land and subsidies for resettlement in British colonies in North America, especiallyQuebec(concentrating in theEastern Townships),Prince Edward Island, andNova Scotia.[268]Britain created the colonies of Upper Canada (Ontario) andNew Brunswickexpressly for their benefit, and the Crown awarded land to Loyalists as compensation for losses in the United States. Nevertheless, approximately eighty-five percent of the Loyalists stayed in the United States as American citizens, and some of the exiles later returned to the U.S.[269]Patrick Henry spoke of the issue of allowing Loyalists to return as such: "Shall we, who have laid the proud British lion at our feet, be frightened of its whelps?" His actions helped secure return of the Loyalists to American soil.[270]
The American Revolution has a central place in the American memory[271]as the story of the nation's founding. It is covered in the schools, memorialized by two national holidays,Washington's Birthdayin February andIndependence Dayin July, and commemorated in innumerable monuments. George Washington's estate atMount Vernonwas one of the first national pilgrimages for tourists and attracted 10,000 visitors a year by the 1850s.[272]
The Revolution became a matter of contention in the 1850s in the debates leading to theAmerican Civil War(1861–1865), as spokesmen of both theNorthern United Statesand theSouthern United Statesclaimed that their region was the true custodian of the legacy of 1776.[273]TheUnited States Bicentennialin 1976 came a year after the American withdrawal from theVietnam War, and speakers stressed the themes of renewal and rebirth based on a restoration of traditional values.[274]
Today, more than 100battlefields and historic sites of the American Revolutionare protected and maintained by the government. TheNational Park Servicealone manages and maintains more than 50 battlefield parks and many other sites such asIndependence Hallthat are related to the Revolution.[275]The privateAmerican Battlefield Trustuses government grants and other funds to preserve almost 700 acres of battlefield land in six states, and the ambitious private recreation/restoration/preservation/interpretation of over 300 acres of pre-1790Colonial Williamsburgwas created in the first half of the 20th century for public visitation.[276]
TheCold Warwas a period of globalgeopoliticalrivalry between theUnited States(US) and theSoviet Union(USSR) and their respective allies, the capitalistWestern Blocand communistEastern Bloc, which lasted from 1947 until thedissolution of the Soviet Unionin 1991. The termcold waris used because there was no direct fighting between the twosuperpowers, though each supported opposing sides in regional conflicts known asproxy wars. In addition to the struggle for ideological and economic influence and anarms racein both conventional andnuclear weapons, the Cold War was expressed through technological rivalries such as theSpace Race,espionage,propaganda campaigns,embargoes, andsports diplomacy.
After the end ofWorld War IIin 1945, during which the US and USSR had been allies, the USSR installedsatellite governmentsin its occupied territories in Eastern Europe andNorth Koreaby 1949, resulting in the political division of Europe (and Germany) by an "Iron Curtain". The USSR testedits first nuclear weaponin 1949, four years after their use by the USat Hiroshima and Nagasaki, and allied with thePeople's Republic of China, founded in 1949. The US declared theTruman Doctrineof "containment" of communism in 1947, launched theMarshall Planin 1948 to assist Western Europe's economic recovery, and founded theNATOmilitary alliance in 1949 (matched by the Soviet-ledWarsaw Pactin 1955). TheBerlin Blockadeof 1948 to 1949 was an early confrontation, as was theKorean Warof 1950 to 1953, which ended in a stalemate.
US involvement in regime change during the Cold Warincluded support foranti-communistandright-wing dictatorshipsand uprisings, whileSoviet involvementincluded the funding ofleft-wing parties,wars of independence, and dictatorships. As nearly all the colonial states underwentdecolonization, many becameThird Worldbattlefields of the Cold War. Both powers used economic aid in an attempt to win the loyalty ofnon-aligned countries. TheCuban Revolutionof 1959 installed the first communist regime in the Western Hemisphere, and in 1962, theCuban Missile Crisisbegan after deployments of US missiles in Europe and Soviet missiles in Cuba; it is widely consideredthe closestthe Cold War came to escalating intonuclear war. Another major proxy conflict was theVietnam Warof 1955 to 1975, which ended in defeat for the US.
The USSR solidified its domination of Eastern Europe with its crushing of theHungarian Revolutionin 1956 and theWarsaw Pact invasion of Czechoslovakiain 1968. Relations between the USSR and China broke down by 1961, with theSino-Soviet splitbringing the two states to the brink of war amida border conflictin 1969. In 1972,the US initiated diplomatic contacts with Chinaand the US and USSR signed a series of treaties limiting their nuclear arsenals during a period known asdétente. In 1979, the toppling of US-allied governments inIranandNicaraguaand the outbreak of theSoviet–Afghan Waragain raised tensions. In 1985,Mikhail Gorbachevbecame leader of the USSR and expanded political freedoms, which contributed to therevolutions of 1989in the Eastern Bloc and thecollapse of the USSRin 1991, ending the Cold War.
WriterGeorge Orwellusedcold war, as a general term, in his essay "You and the Atomic Bomb", published 19 October 1945. Contemplating a world living in the shadow of the threat ofnuclear warfare, Orwell looked atJames Burnham's predictions of a polarized world, writing:
Looking at the world as a whole, the drift for many decades has been not towards anarchy but towards the reimposition of slavery... James Burnham's theory has been much discussed, but few people have yet considered its ideological implications—that is, the kind of world-view, the kind of beliefs, and the social structure that would probably prevail in a state which was at once unconquerable and in a permanent state of "cold war" with its neighbours.[1]
InThe Observerof 10 March 1946, Orwell wrote, "after the Moscow conference last December, Russia began to make a 'cold war' on Britain and the British Empire."[2]
The first use of the term to describe the specificpost-wargeopolitical confrontation between the Soviet Union and the United States came in a speech byBernard Baruch, an influential advisor to Democratic presidents,[3]on 16 April 1947. The speech, written by journalistHerbert Bayard Swope,[4]proclaimed, "we are today in the midst of a cold war."[5]Newspaper columnistWalter Lippmanngave the term wide currency with his bookThe Cold War. When asked in 1947 about the source of the term, Lippmann traced it to a French term from the 1930s,la guerre froide.[6][B]
The roots of the Cold War can be traced to diplomatic and military tensions preceding World War II. The 1917Russian Revolutionand the subsequentTreaty of Brest-Litovsk, where Soviet Russia ceded vast territories to Germany, deepened distrust among the Western Allies. Allied intervention in the Russian Civil War further complicated relations, and although the Soviet Union later allied with Western powers to defeatNazi Germany, this cooperation was strained by mutual suspicions.
In the immediate aftermath of World War II, disagreements about the future of Europe, particularlyEastern Europe, became central. The Soviet Union's establishment of communist regimes in the countries it had liberated from Nazi control—enforced by the presence of theRed Army—alarmed the US and UK. Western leaders saw this as Soviet expansionism, clashing with their vision of a democratic Europe. Economically, the divide was sharpened with the introduction of theMarshall Planin 1947, a US initiative to provide financial aid to rebuild Europe and prevent the spread of communism by stabilizing capitalist economies. The Soviet Union rejected the Marshall Plan, seeing it as an effort by the US to impose its influence on Europe. In response, the Soviet Union establishedComecon(Council for Mutual Economic Assistance) to foster economic cooperation among communist states.
The United States and itsWestern Europeanallies sought to strengthen their bonds and used the policy ofcontainmentagainst Soviet influence; they accomplished this most notably through the formation ofNATO, which was essentially a defensive agreement in 1949. The Soviet Union countered with theWarsaw Pactin 1955, which had similar results with the Eastern Bloc. As by that time the Soviet Union already had an armed presence and political domination all over its eastern satellite states, the pact has been long considered superfluous.[7]Although nominally a defensive alliance, the Warsaw Pact's primary function was to safeguardSoviet hegemonyover itsEastern Europeansatellites, with the pact's only direct military actions having been the invasions of its own member states to keep them from breaking away;[8]in the 1960s, the pact evolved into a multilateral alliance, in which the non-Soviet Warsaw Pact members gained significant scope to pursue their own interests. In 1961, Soviet-alliedEast Germanyconstructed theBerlin Wallto prevent the citizens ofEast Berlinfrom fleeing toWest Berlin, at the time part of United States-alliedWest Germany.[9]Major crises of this phase included theBerlin Blockadeof 1948–1949, theChinese Communist Revolutionof 1945–1949, theKorean Warof 1950–1953, theHungarian Revolution of 1956and theSuez Crisisof that same year, theBerlin Crisis of 1961, theCuban Missile Crisisof 1962, and theVietnam Warof 1955–1975. Both superpowers competed for influence inLatin Americaand theMiddle East, and the decolonising states ofAfrica,Asia, andOceania.
Following the Cuban Missile Crisis, this phase of the Cold War saw theSino-Soviet split. Between China and the Soviet Union's complicated relations within the Communist sphere, leading to theSino-Soviet border conflict, while France, a Western Bloc state, began to demand greater autonomy of action. TheWarsaw Pact invasion of Czechoslovakiaoccurred to suppress thePrague Springof 1968, while the United States experienced internal turmoil from thecivil rights movementandopposition to United States involvement in the Vietnam War. In the 1960s–1970s, an internationalpeace movementtook root among citizens around the world. Movements againstnuclear weapons testingand fornuclear disarmamenttook place, with largeanti-war protests. By the 1970s, both sides had started making allowances for peace and security, ushering in a period ofdétentethat saw theStrategic Arms Limitation Talksand the1972 visit by Richard Nixon to Chinathat opened relations with China as a strategic counterweight to the Soviet Union. A number of self-proclaimedMarxist–Leninistgovernments were formed in the second half of the 1970s indeveloping countries, includingAngola,Mozambique,Ethiopia,Cambodia,Afghanistan, andNicaragua.
Détente collapsed at the end of the decade with the beginning of theSoviet–Afghan Warin 1979. Beginning in the 1980s, this phase was another period of elevated tension. TheReagan Doctrineled to increased diplomatic, military, and economic pressures on the Soviet Union, which at the time was undergoing theEra of Stagnation. This phase saw the new Soviet leaderMikhail Gorbachevintroducing the liberalizing reforms ofglasnost("openness") andperestroika("reorganization") and ending Soviet involvement in Afghanistan in 1989. Pressures for national sovereignty grew stronger in Eastern Europe, and Gorbachev refused to further support the Communist governments militarily.
The fall of theIron Curtainafter thePan-European Picnicand theRevolutions of 1989, which represented a peaceful revolutionary wave with the exception of theRomanian revolutionand theAfghan Civil War (1989–1992), overthrew almost all of the Marxist–Leninist regimes of the Eastern Bloc. TheCommunist Party of the Soviet Unionitself lost control in the country and was banned following the1991 Soviet coup attemptthat August. This in turn led to the formaldissolution of the Soviet Unionin December 1991 and the collapse of Communist governments across much of Africa and Asia. TheRussian Federationbecame the Soviet Union's successor state, while many of the other republics emerged as fully independentpost-Soviet states.[10]The United States was left as the world's sole superpower.
In February 1946,George F. Kennan's "Long Telegram" from Moscow to Washington helped to articulate the US government's increasingly hard line against the Soviets, which would become the basis for US strategy toward the Soviet Union. The telegram galvanized a policy debate that would eventually shape theTruman administration's Soviet policy.[11]Washington's opposition to the Soviets accumulated after broken promises by Stalin andMolotovconcerning Europe and Iran.[12]Following the World War IIAnglo-Soviet invasion of Iran, the country was occupied by the Red Army in the far north and the British in the south.[13]Iran was used by the United States and British to supply the Soviet Union, and the Allies agreed to withdraw from Iran within six months after the cessation of hostilities.[13]However, when this deadline came, the Soviets remained in Iran under the guise of theAzerbaijan People's GovernmentandKurdishRepublic of Mahabad.[14]On 5 March, former British prime minister Winston Churchill delivered his famous "Iron Curtain" speech calling for an Anglo-American alliance against the Soviets, whom he accused of establishing an "iron curtain" dividing Europe.[15][16]
A week later, on 13 March, Stalin responded vigorously to the speech, saying Churchill could be compared toAdolf Hitlerinsofar as he advocated the racial superiority ofEnglish-speaking nationsso that they could satisfy their hunger for world domination, and that such a declaration was "a call for war on the USSR." The Soviet leader also dismissed the accusation that the USSR was exerting increasing control over the countries lying in its sphere. He argued that there was nothing surprising in "the fact that the Soviet Union, anxious for its future safety, [was] trying to see to it that governments loyal in their attitude to the Soviet Union should exist in these countries."[17][18]
Soviet territorial demands to Turkey regarding the Dardanelles in theTurkish Straits crisisand Black Seaborder disputeswere also a major factor in increasing tensions.[12][19]In September, the Soviet side produced theNovikovtelegram, sent by the Soviet ambassador to the US but commissioned and "co-authored" byVyacheslav Molotov; it portrayed the US as being in the grip of monopoly capitalists who were building up military capability "to prepare the conditions for winning world supremacy in a new war".[20]On 6 September 1946,James F. Byrnesdelivered aspeechin Germany repudiating theMorgenthau Plan(a proposal to partition and de-industrialize post-war Germany) and warning the Soviets that the US intended to maintain a military presence in Europe indefinitely.[21][22]As Byrnes stated a month later, "The nub of our program was to win the German people ... it was a battle between us and Russia over minds ..." In December, the Soviets agreed to withdraw from Iran after persistent US pressure, an early success of containment policy.
By 1947, US presidentHarry S. Trumanwas outraged by the perceived resistance of the Soviet Union to American demands in Iran, Turkey, and Greece, as well as Soviet rejection of theBaruch Planon nuclear weapons.[23]In February 1947, the British government announced that it could no longer afford to finance theKingdom of Greeceinits civil waragainst Communist-led insurgents.[24]In the same month, Stalin conducted the rigged1947 Polish legislative electionwhich constituted an open breach of theYalta Agreement. TheUS governmentresponded by adopting a policy ofcontainment,[25]with the goal of stopping the spread ofcommunism. Truman delivered a speech calling for the allocation of $400 million to intervene in the war and unveiled theTruman Doctrine, which framed the conflict as a contest between free peoples andtotalitarianregimes.[25]American policymakers accused the Soviet Union of conspiring against the Greek royalists in an effort toexpand Soviet influenceeven though Stalin had told the Communist Party to cooperate with the British-backed government.[26][27][28]
Enunciation of the Truman Doctrine marked the beginning of a US bipartisan defense and foreign policy consensus betweenRepublicansandDemocratsfocused on containment anddeterrencethat weakened during and after theVietnam War, but ultimately persisted thereafter.[29]Moderate and conservative parties in Europe, as well as social democrats, gave virtually unconditional support to the Western alliance,[30]whileEuropeanandAmerican Communists, financed by theKGBand involved in its intelligence operations,[31]adhered to Moscow's line, although dissent began to appear after 1956. Other critiques of the consensus policy came fromanti-Vietnam War activists, theCampaign for Nuclear Disarmament, and theanti-nuclear movement.[32]
In early 1947, France, Britain and the United States unsuccessfully attempted to reach an agreement with the Soviet Union for a plan envisioning an economically self-sufficient Germany, including a detailed accounting of the industrial plants, goods and infrastructure already taken by the Soviets.[33]In June 1947, in accordance with theTruman Doctrine, the United States enacted theMarshall Plan, a pledge of economic assistance for all European countries willing to participate.[33]Under the plan, which President Harry S. Truman signed on 3 April 1948, the US government gave to Western European countries over $13 billion (equivalent to $189 billion in 2016). Later, the program led to the creation of theOECD.
The plan's aim was to rebuild the democratic and economic systems of Europe and to counter perceived threats to theEuropean balance of power, such as communist parties seizing control.[34]The plan also stated that European prosperity was contingent upon German economic recovery.[35]One month later, Truman signed theNational Security Act of 1947, creating a unifiedDepartment of Defense, theCentral Intelligence Agency(CIA), and theNational Security Council(NSC). These would become the main bureaucracies for US defense policy in the Cold War.[36]
Stalin believed economic integration with the West would allowEastern Bloccountries to escape Soviet control, and that the US was trying to buy a pro-US re-alignment of Europe.[37]Stalin therefore prevented Eastern Bloc nations from receiving Marshall Plan aid.[37]The Soviet Union's alternative to the Marshall Plan, which was purported to involve Soviet subsidies and trade with central and eastern Europe, became known as theMolotov Plan(later institutionalized in January 1949 as theCouncil for Mutual Economic Assistance).[27]Stalin was also fearful of a reconstituted Germany; his vision of a post-war Germany did not include the ability to rearm or pose any kind of threat to the Soviet Union.[38]
In early 1948, Czech Communists executed acoup d'étatinCzechoslovakia(resulting in the formation of theCzechoslovak Socialist Republic), the only Eastern Bloc state that the Soviets had permitted to retain democratic structures.[39]The public brutality of the coup shocked Western powers more than any event up to that point and swept away the last vestiges of opposition to the Marshall Plan in the United States Congress.[40][41]
In an immediate aftermath of the crisis, theLondon Six-Power Conferencewas held, resulting in theSovietboycott of the Allied Control Council and its incapacitation, an event marking the beginning of the full-blown Cold War, as well as ending any hopes at the time for a single German government and leading to formation in 1949 of theFederal Republic of GermanyandGerman Democratic Republic.[42]
The twin policies of the Truman Doctrine and the Marshall Plan led to billions in economic and military aid for Western Europe, Greece, and Turkey. With the US assistance, the Greek militarywon its civil war.[36]Under the leadership ofAlcide De Gasperithe ItalianChristian Democratsdefeated the powerfulCommunist–Socialistalliance in theelections of 1948.[43]
Outside of Europe, the United States also began to express interest in the development of many other countries, so that they would not fall under the sway of Eastern Bloc communism.  In his January 1949 inaugural address, Truman declared for the first time in U.S. history thatinternational developmentwould be a key part of U.S. foreign policy.  The resulting program later became known as thePoint Four Programbecause it was the fourth point raised in his address.[44]
All major powers engaged in espionage, using a great variety of spies,double agents,moles, and new technologies such as the tapping of telephone cables.[45]The SovietKGB("Committee for State Security"), the bureau responsible for foreign espionage and internal surveillance, was famous for its effectiveness. The most famous Soviet operation involved itsatomic spiesthat delivered crucial information from the United States'Manhattan Project, leading the USSR to detonate its first nuclear weapon in 1949, four years after the American detonation and much sooner than expected.[46][47]A massive network of informants throughout the Soviet Union was used to monitor dissent from official Soviet politics and morals.[45][48]Although to an extentdisinformationhad always existed, the term itself was invented, and the strategy formalized by ablack propagandadepartment of the Soviet KGB.[49][C]
Based on the amount of top-secret Cold War archival information that has been released, historianRaymond L. Garthoffconcludes there probably was parity in the quantity and quality of secret information obtained by each side. However, the Soviets probably had an advantage in terms ofHUMINT(human intelligence or interpersonal espionage) and "sometimes in its reach into high policy circles." In terms of decisive impact, however, he concludes:[50]
We also can now have high confidence in the judgment that there were no successful "moles" at the political decision-making level on either side. Similarly, there is no evidence, on either side, of any major political or military decision that was prematurely discovered through espionage and thwarted by the other side. There also is no evidence of any major political or military decision that was crucially influenced (much less generated) by an agent of the other side.
According to historian Robert L. Benson, "Washington's forte was'signals' intelligence– the procurement and analysis of coded foreign messages," leading to theVenona projector Venona intercepts, which monitored the communications of Soviet intelligence agents.[51]Moynihanwrote that the Venona project contained "overwhelming proof of the activities of Soviet spy networks in America, complete with names, dates, places, and deeds."[52]The Venona project was kept highly secret even from policymakers until theMoynihan Commissionin 1995.[52]Despite this, the decryption project had already been betrayed and dispatched to the USSR byKim PhilbyandBill Weisbandin 1946,[52][53]as was discovered by the US by 1950.[54]Nonetheless, the Soviets had to keep their discovery of the program secret, too, and continued leaking their own information, some of which was still useful to the American program.[53]According to Moynihan, even President Truman may not have been fully informed of Venona, which may have left him unaware of the extent of Soviet espionage.[55][56]
Clandestineatomic spiesfrom the Soviet Union, who infiltrated theManhattan Projectduring WWII, played a major role in increasing tensions that led to the Cold War.[51]
In addition to usual espionage, the Western agencies paid special attention to debriefingEastern Bloc defectors.[57]Edward Jay Epsteindescribes that the CIA understood that the KGB used "provocations", or fake defections, as a trick to embarrass Western intelligence and establish Soviet double agents. As a result, from 1959 to 1973, the CIA required that East Bloc defectors went through a counterintelligence investigation before being recruited as a source of intelligence.[58]
During the late 1970s and 1980s, the KGB perfected its use of espionage to sway and distort diplomacy.[59]Active measureswere "clandestine operations designed to further Soviet foreign policy goals," consisting of disinformation, forgeries, leaks to foreign media, and the channeling of aid to militant groups.[60]Retired KGB Major GeneralOleg Kalugindescribed active measures as "the heart and soul ofSoviet intelligence."[61]
During theSino-Soviet split, "spy wars" also occurred between the USSR and PRC.[62]
In September 1947, the Soviets createdCominformto impose orthodoxy within the international communist movement and tighten political control over Sovietsatellitesthrough coordination of communist parties in theEastern Bloc.[37]Cominform faced an embarrassing setback the following June, when theTito–Stalin splitobliged its members to expelYugoslavia, which remained communist but adopted anon-alignedposition and began accepting financial aid from the US.[63]
Besides Berlin, the status of the city ofTriestewas at issue. Until the break between Tito and Stalin, the Western powers and the Eastern bloc faced each other uncompromisingly. In addition to capitalism and communism, Italians and Slovenes, monarchists and republicans as well as war winners and losers often faced each other irreconcilably. The neutral buffer stateFree Territory of Trieste, founded in 1947 with the United Nations, was split up and dissolved in 1954 and 1975, also because of the détente between the West and Tito.[64][65]
The US and Britain merged their western German occupation zones into "Bizone" (1 January 1947, later "Trizone" with the addition of France's zone, April 1949).[66]As part of the economic rebuilding of Germany, in early 1948, representatives of a number of Western European governments and the United States announced an agreement for a merger of western German areas into a federal governmental system.[67]In addition, in accordance with theMarshall Plan, they began to re-industrialize and rebuild the West German economy, including the introduction of a newDeutsche Markcurrency to replace the oldReichsmarkcurrency that the Soviets had debased.[68]The US had secretly decided that a unified and neutral Germany was undesirable, withWalter Bedell Smithtelling General Eisenhower "in spite of our announced position, we really do not want nor intend to accept German unification on any terms that the Russians might agree to, even though they seem to meet most of our requirements."[69]
Shortly thereafter, Stalin instituted the Berlin Blockade (June 1948 – May 1949), one of the first major crises of the Cold War, preventing Western supplies from reaching West Germany's exclave ofWest Berlin.[70]The United States (primarily), Britain, France, Canada, Australia, New Zealand, and several other countries began the massive "Berlin airlift", supplying West Berlin with provisions despite Soviet threats.[71]
The Soviets mounted a public relations campaign against the policy change. Once again, the East Berlin communists attempted to disrupt theBerlin municipal elections,[66]which were held on 5 December 1948 and produced a turnout of 86% and an overwhelming victory for the non-communist parties.[72]The results effectively divided the city into East and West, the latter comprising US, British and French sectors. 300,000 Berliners demonstrated and urged the international airlift to continue,[73]and US Air Force pilotGail Halvorsencreated "Operation Vittles", which supplied candy to German children.[74]The Airlift was as much a logistical as a political and psychological success for the West; it firmly linked West Berlin to the United States.[75]In May 1949, Stalin lifted the blockade.[76][77]
In 1952, Stalin repeatedlyproposed a planto unify East and West Germany under a single government chosen in elections supervised by the United Nations, if the new Germany were to stay out of Western military alliances, but this proposal was turned down by the Western powers. Some sources dispute the sincerity of the proposal.[78]
Britain, France, the United States, Canada and eight other western European countries signed theNorth Atlantic Treatyof April 1949, establishing theNorth Atlantic Treaty Organization(NATO).[76]That August, thefirst Soviet atomic devicewas detonated inSemipalatinsk,Kazakh SSR.[27]Following Soviet refusals to participate in a German rebuilding effort set forth by western European countries in 1948,[67][79]the US, Britain and France spearheaded the establishment of theFederal Republic of Germanyfrom thethree Western zones of occupationin April 1949.[80]The Soviet Union proclaimedits zone of occupationin Germany theGerman Democratic Republicthat October.[81]
Media in theEastern Blocwas anorgan of the state, completely reliant on and subservient to the communist party. Radio and television organizations were state-owned, while print media was usually owned by political organizations, mostly by the local communist party.[82]Soviet radio broadcasts used Marxist rhetoric to attack capitalism, emphasizing themes of labor exploitation, imperialism and war-mongering.[83]
Along with the broadcasts of theBBCand theVoice of Americato Central and Eastern Europe,[84]a major propaganda effort began in 1949 wasRadio Free Europe/Radio Liberty, dedicated to bringing about the peaceful demise of the communist system in the Eastern Bloc.[85]Radio Free Europe attempted to achieve these goals by serving as a surrogate home radio station, an alternative to the controlled and party-dominated domestic press in the Soviet Bloc.[85]Radio Free Europe was a product of some of the most prominent architects of America's early Cold War strategy, especially those who believed that the Cold War would eventually be fought by political rather than military means, such as George F. Kennan.[86]Soviet and Eastern Bloc authorities used various methods to suppress Western broadcasts, includingradio jamming.[87][88]
American policymakers, including Kennan andJohn Foster Dulles, acknowledged that the Cold War was in its essence a war of ideas.[86]The United States, acting through the CIA, funded a long list of projects to counter the communist appeal among intellectuals in Europe and the developing world.[89]The CIA alsocovertlysponsored a domestic propaganda campaign calledCrusade for Freedom.[90]
The rearmament of West Germany was achieved in the early 1950s. Its main promoter wasKonrad Adenauer, the chancellor of West Germany, with France the main opponent. Washington had the decisive voice. It was strongly supported by the Pentagon (the US military leadership), and weakly opposed by President Truman; the State Department was ambivalent. The outbreak of the Korean War in June 1950 changed the calculations and Washington now gave full support. That also involved namingDwight D. Eisenhowerin charge of NATO forces and sending more American troops to West Germany. There was a strong promise that West Germany would not develop nuclear weapons.[91]
Widespread fears of another rise ofGerman militarismnecessitated the new military to operate within an alliance framework underNATOcommand.[92]In 1955, Washington secured full German membership of NATO.[81]In May 1953,Lavrentiy Beria, by then in a government post, had made an unsuccessful proposal to allow the reunification of a neutral Germany to prevent West Germany's incorporation into NATO, but his attempts were cut short after he wasexecuted several months laterduring a Soviet power struggle.[93]The events led to the establishment of theBundeswehr, the West German military, in 1955.[94][95]
In 1949,Mao Zedong'sPeople's Liberation ArmydefeatedChiang Kai-shek's United States-backedKuomintang(KMT) Nationalist Government in China. The KMT-controlled territory was nowrestrictedto the island ofTaiwan, the nationalist government of which exists to this day. The Kremlin promptly created an alliance with the newly formed People's Republic of China.[96]According to Norwegian historianOdd Arne Westad, the communists won the Civil War because they made fewer military mistakes than Chiang Kai-Shek made, and because in his search for a powerful centralized government, Chiang antagonized too many interest groups in China. Moreover, his party was weakened during thewar against Japan. Meanwhile, the communists told different groups, such as the peasants, exactly what they wanted to hear, and they cloaked themselves under the cover ofChinese nationalism.[97]
Confronted with thecommunist revolution in Chinaandthe end of the American atomic monopolyin 1949, the Truman administration quickly moved to escalate and expand itscontainmentdoctrine.[27]InNSC 68, a secret 1950 document, the National Security Council proposed reinforcing pro-Western alliance systems and quadrupling spending on defense.[27]Truman, under the influence of advisorPaul Nitze, saw containment as implying completerollbackof Soviet influence in all its forms.[98]
United States officials moved to expand this version of containment intoAsia,Africa, andLatin America, in order to counter revolutionary nationalist movements, often led by communist parties financed by the USSR.[99]In this way, this US would exercise "preponderant power," oppose neutrality, andestablish globalhegemony.[98]In the early 1950s (a period sometimes known as the "Pactomania"), the US formalized a series of alliances withJapan(a former WWII enemy),South Korea,Taiwan,Australia,New Zealand,Thailandand thePhilippines(notablyANZUSin 1951 andSEATOin 1954), thereby guaranteeing the United States a number of long-term military bases.[81]
One of the more significant examples of the implementation of containment was the United Nations US-led intervention in theKorean War. In June 1950, after years of mutual hostilities,[D][100][101]Kim Il Sung'sNorth Korean People's ArmyinvadedSouth Korea. Stalin had been reluctant to support the invasion[E]but ultimately sent advisers.[102]To Stalin's surprise,[27]theUnited Nations Security Councilbacked the defense of South Korea, although the Soviets were then boycotting meetings in protest of the fact thatTaiwan(Republic of China), not thePeople's Republic of China, held a permanent seat on the council.[103]AUN forceof sixteen countries faced North Korea,[104]although 40 percent of troops were South Korean, and about 50 percent were from the United States.[105]
The US initially seemed to follow containment, only pushing back North Korea across the38th Paralleland restoring South Korea's sovereignty while allowing North Korea's survival as a state. However, the success of theInchon landinginspired the US/UN forces to pursue arollbackstrategy instead and to overthrow communist North Korea, thereby allowing nationwide elections under U.N. auspices.[106]GeneralDouglas MacArthurthen advanced into North Korea. The Chinese, fearful of a possible US invasion, sent in a large army and pushed the U.N. forces back below the 38th parallel.[107]The episode was used to support the wisdom of thecontainmentdoctrine as opposed to rollback. The Communists were later pushed to roughly around the original border, with minimal changes. Among other effects, the Korean War galvanisedNATOto develop a military structure.[108]TheKorean Armistice Agreementwas approved in July 1953.[109][110]
In 1953, changes in political leadership on both sides shifted the dynamic of the Cold War.[36]Dwight D. Eisenhowerwas inaugurated president that January. During the last 18 months of the Truman administration, the American defense budget had quadrupled, and Eisenhower moved to reduce military spending by a third while continuing to fight the Cold War effectively.[27]
Joseph Stalindied in 1953.Nikita Khrushcheveventually won the ensuing power struggle by the mid-1950s. In 1956, hedenounced Joseph Stalinand proceeded to ease controls over the party and society (de-Stalinization).[36]
On 18 November 1956, while addressing Western dignitaries at a reception in Moscow's Polish embassy, Khrushchev infamously declared, "Whether you like it or not, history is on our side.We will bury you", shocking everyone present.[111]He would later claim he had not been referring to nuclear war, but the "historically fated victory of communism over capitalism."[112]
Eisenhower's secretary of state, John Foster Dulles, initiated a "New Look" for thecontainmentstrategy, calling for a greater reliance on nuclear weapons against US enemies in wartime.[36]Dulles also enunciated the doctrine of "massive retaliation", threatening a severe US response to any Soviet aggression. Possessing nuclear superiority, for example, allowed Eisenhower to face down Soviet threats to intervene in the Middle East during the 1956Suez Crisis.[27]The declassified US plans for retaliatory nuclear strikes in the late 1950s included the "systematic destruction" of 1,200 major urban centers in the Soviet Bloc and China, including Moscow, East Berlin and Beijing.[113][114]
In spite of these events, there were substantial hopes for détente whenan upswing in diplomacy took place in 1959, including a two-week visit by Khrushchev to the US, and plans for a two-power summit for May 1960. The latter was disturbed by theU-2 spy plane scandal, however, in which Eisenhower was caught lying about the intrusion of American surveillance aircraft into Soviet territory.[115][116]
WhileStalin's death in 1953 slightly relaxed tensions, the situation in Europe remained an uneasy armed truce.[117]The Soviets, who had already created a network of mutual assistance treaties in theEastern Blocby 1949, established a formal alliance therein, theWarsaw Pact, in 1955. It stood opposed to NATO.[81]
TheHungarian Revolution of 1956occurred shortly after Khrushchev arranged the removal of Hungary's Stalinist leaderMátyás Rákosi.[118]In response to a popular anti-communist uprising,[F]the new regime formally disbanded thesecret police, declared its intention to withdraw from the Warsaw Pact and pledged to re-establish free elections. TheSoviet Armyinvaded.[119]Thousands of Hungarians were killed and arrested, imprisoned and deported to the Soviet Union,[120]and approximately 200,000 Hungarians fled Hungary.[121]Hungarian leaderImre Nagyand others were executed following secret trials.[122]
From 1957 through 1961, Khrushchev openly and repeatedly threatened the West with nuclear annihilation. He claimed that Soviet missile capabilities were far superior to those of the United States, capable of wiping out any American or European city. According toJohn Lewis Gaddis, Khrushchev rejected Stalin's "belief in the inevitability of war," however. The new leader declared his ultimate goal was "peaceful coexistence".[123]In Khrushchev's formulation, peace would allow capitalism to collapse on its own,[124]as well as giving the Soviets time to boost their military capabilities,[125]which remained for decades until Gorbachev's later "new thinking" envisioning peaceful coexistence as an end in itself rather than a form of class struggle.[126]
The events in Hungary produced ideological fractures within the communist parties of the world, particularly in Western Europe, with great decline in membership, as many in both western and socialist countries felt disillusioned by the brutal Soviet response.[127]The communist parties in the West would never recover.[127]
In 1957, Polish foreign ministerAdam Rapackiproposed theRapacki Planfor a nuclear free zone in central Europe. Public opinion tended to be favourable in the West, but it was rejected by leaders of West Germany, Britain, France and the United States. They feared it would leave the powerful conventional armies of the Warsaw Pact dominant over the weaker NATO armies.[128]
During November 1958, Khrushchev made an unsuccessful attempt to turn all of Berlin into an independent, demilitarized "free city". He gave the United States, Great Britain and France a six-month ultimatum to withdraw their troops from the sectors of West Berlin, or he would transfer control of Western access rights to the East Germans. Khrushchev earlier explained toMao Zedongthat "Berlin is the testicles of the West. Every time I want to make the West scream, I squeeze on Berlin."[129]NATO formally rejected the ultimatum in mid-December and Khrushchev withdrew it in return for a Geneva conference on the German question.[130]
Like Truman and Eisenhower,John F. Kennedysupported containment. President Eisenhower'sNew Lookpolicy had emphasized the use of less expensive nuclear weapons todeterSoviet aggression by threatening massive nuclear attacks on all of the Soviet Union. Nuclear weapons were much cheaper than maintaining a large standing army, so Eisenhower cut conventional forces to save money. Kennedy implemented a new strategy known asflexible response. This strategy relied on conventional arms to achieve limited goals. As part of this policy, Kennedy expanded theUnited States special operations forces, elite military units that could fight unconventionally in various conflicts. Kennedy hoped that the flexible response strategy would allow the US to counter Soviet influence without resorting to nuclear war.[131]
To support his new strategy, Kennedy ordered a massive increase in defense spending and a rapid build-up of the nuclear arsenal to restore the lost superiority over the Soviet Union. In his inaugural address, Kennedy promised "to bear any burden" in the defense of liberty, and he repeatedly asked for increases in military spending and authorization of new weapons systems. From 1961 to 1964, the number of nuclear weapons increased by 50 percent, as did the number of B-52 bombers to deliver them. The new ICBM force grew from 63 intercontinental ballistic missiles to 424. He authorized 23 new Polaris submarines, each of which carried 16 nuclear missiles. Kennedy also called on cities to construct fallout shelters.[132][133]
Nationalist movements in some countries and regions, notablyGuatemala, Indonesia andIndochina, were often allied with communist groups or otherwise perceived to be unfriendly to Western interests.[36]In this context, the United States and the Soviet Union increasingly competed for influence by proxy in the Third World asdecolonizationgained momentum in the 1950s and early 1960s.[134]Both sides were selling armaments to gain influence.[135]The Kremlin saw continuing territorial losses by imperial powers as presaging the eventual victory of their ideology.[136]
The United States used theCentral Intelligence Agency(CIA) to undermine neutral or hostile Third World governments and to support allied ones.[137]In 1953, President Eisenhower implementedOperation Ajax, a covert coup operation to overthrow the Iranian prime minister,Mohammad Mosaddegh. The popularly elected Mosaddegh had been a Middle Eastern nemesis of Britain since nationalizing the British-ownedAnglo-Iranian Oil Companyin 1951.Winston Churchilltold the United States that Mosaddegh was "increasingly turning towards Communist influence."[138][139]The pro-Westernshah,Mohammad Reza Pahlavi, assumed control as anautocraticmonarch.[140]The shah's policies included banning the communistTudeh Party of Iran, and general suppression of political dissent bySAVAK, the shah's domestic security and intelligence agency.
In Guatemala, abanana republic, the1954 Guatemalan coup d'étatousted the left-wing PresidentJacobo Árbenzwith material CIA support.[141]The post-Arbenz government—amilitary juntaheaded byCarlos Castillo Armas—repealed aprogressive land reform law, returned nationalized property belonging to theUnited Fruit Company, set up aNational Committee of Defense Against Communism, and decreed aPreventive Penal Law Against Communismat the request of the United States.[142]
The non-aligned Indonesian government ofSukarnowas faced with a major threat to its legitimacy beginning in 1956 when several regional commanders began to demand autonomy fromJakarta. After mediation failed, Sukarno took action to remove the dissident commanders. In February 1958, dissident military commanders in Central Sumatra (ColonelAhmad Husein) and North Sulawesi (Colonel Ventje Sumual) declared theRevolutionary Government of the Republic of Indonesia-PermestaMovement aimed at overthrowing the Sukarno regime. They were joined by many civilian politicians from theMasyumi Party, such asSjafruddin Prawiranegara, who were opposed to the growing influence of the communistPartai Komunis Indonesia. Due to their anti-communist rhetoric, the rebels received arms, funding, and other covert aid from the CIA untilAllen Lawrence Pope, an American pilot, was shot down after a bombing raid on government-heldAmbonin April 1958. The central government responded by launching airborne and seaborne military invasions of rebel strongholds atPadangandManado. By the end of 1958, the rebels were militarily defeated, and the last remaining rebel guerilla bands surrendered by August 1961.[143]
In theRepublic of the Congo, also known as Congo-Léopoldville, newly independent fromBelgiumsince June 1960, theCongo Crisiserupted on 5 July leading to the secession of the regionsKatangaandSouth Kasai. CIA-backed PresidentJoseph Kasa-Vubuordered the dismissal of the democratically elected Prime MinisterPatrice Lumumbaand the Lumumba cabinet in September over massacres by the armed forces during theinvasion of South Kasaiand for involving Soviets in the country.[144][145]Later the CIA-backed ColonelMobutu Sese Sekoquickly mobilized his forces to seize power through a military coup d'état,[145]and worked with Western intelligence agencies to imprison Lumumba and hand him over to Katangan authorities who executed him by firing squad.[146][147]
InBritish Guiana, the leftistPeople's Progressive Party(PPP) candidateCheddi Jaganwon the position of chief minister in a colonially administered election in 1953 but was quickly forced to resign from power after Britain's suspension of the still-dependent nation's constitution.[148]Embarrassed by the landslide electoral victory of Jagan's allegedly Marxist party, the British imprisoned the PPP's leadership and maneuvered the organization into a divisive rupture in 1955.[149]Jagan again won the colonial elections in 1957 and 1961, despite Britain's shift to a reconsideration of its view of the left-wing Jagan as a Soviet-style communist at this time. The United States pressured the British to withholdGuyana's independence until an alternative to Jagan could be identified, supported, and brought into office.[150]InMalaya, the British colonialistssuppressedthe communist anti-colonial rebellion.
Worn down by the communistguerrilla warfor Vietnamese full independence and handed a watershed defeat by communistViet Minhrebels at theBattle of Dien Bien Phu, the French accepted a negotiated abandonment of their neo-colonial stake in Vietnam right in 1954. On June 4, France granted full sovereignty to the anti-communistState of Vietnam, an independent country within theFrench Union.[151]In theGeneva Conferencein July, peace accords were signed, leaving Vietnam divided between a pro-Soviet administration inNorth Vietnamand a pro-Western administration inSouth Vietnamat the17th parallel north. Between 1954 and 1961, Eisenhower's United States sent economic aid and military advisers to strengthen South Vietnam's pro-Western government against communist efforts to destabilize it.[27]
Many emerging nations of Asia, Africa, and Latin America rejected the pressure to choose sides in the East–West competition. In 1955, at theBandung Conferencein Indonesia, dozens of Third World governments resolved to stay out of the Cold War.[152]The consensus reached at Bandung culminated with the creation of theBelgrade-headquarteredNon-Aligned Movementin 1961.[36]Meanwhile, Khrushchev broadened Moscow's policy to establish ties withIndiaand other key neutral states. Independence movements in the Third World transformed the post-war order into a more pluralistic world of decolonized African and Middle Eastern nations and of rising nationalism in Asia and Latin America.[27]
After 1956, the Sino-Soviet alliance began to break down. Mao had defended Stalin when Khrushchev criticized him in 1956 and treated the new Soviet leader as a superficial upstart, accusing him of having lost his revolutionary edge.[153]For his part, Khrushchev, disturbed by Mao's glib attitude toward nuclear war, referred to the Chinese leader as a "lunatic on a throne".[154]
After this, Khrushchev made many desperate attempts to reconstitute the Sino-Soviet alliance, but Mao considered it useless and denied any proposal.[153]The Chinese-Soviet animosity spilled out in an intra-communist propaganda war.[155]Further on, the Soviets focused on a bitter rivalry with Mao's China for leadership of the global communist movement.[156]Historian Lorenz M. Lüthi argues:
On thenuclear weaponsfront, the United States and the Soviet Union pursued nuclear rearmament and developed long-range weapons with which they could strike the territory of the other.[81]In August 1957, the Soviets successfully launched the world's firstintercontinental ballistic missile(ICBM),[158]and in October they launched the first Earth satellite,Sputnik 1.[159]This led to what became known as theSputnik crisis. TheCentral Intelligence Agencydescribed the orbit of Sputnik 1 as a "stupendous scientific achievement" and concluded that the USSR had likely perfected an intercontinental ballistic missile (ICBM) capable of reaching 'any desired target with accuracy'.[160]
The launch of Sputnik inaugurated theSpace Race. This led to a series of historic space exploration milestones, and most notably theApolloMoon landingsfrom 1969 by the United States, which astronautFrank Bormanlater described as "just a battle in the Cold War."[161]The public's reaction in the Soviet Union was mixed. The Soviet government limited the release of information about the lunar landing, which affected the reaction. A portion of the populace did not give it any attention, and another portion was angered by it.[162]A major Cold War element of the Space Race wassatellite reconnaissance, as well assignals intelligenceto gauge which aspects of the space programs had military capabilities.[163]The SovietSalyut programme, conducted in the 1970s and 80s, put a manned space station in long term orbit; two of the successful installations to the station were covers for secret militaryAlmazreconnaissance stations:Salyut 3, andSalyut 5.[164][165][166][167]
During the whole duration of the cold war, the US and the USSR represented the largest and dominant space powers of the world.[168]Despite their fierce competition, both nations signed international space treaties in the 1960s which would limit the militarization of space.[169]
The first research ofanti-satellite weapontechnology also came about during this period.[170]
Later, the US and USSR pursued some cooperation in space as part ofdétente, notably theApollo–Soyuzorbital rendezvous and docking.[171]
InCuba, the26th of July Movement, led by young revolutionariesFidel CastroandChe Guevara, seized power in theCuban Revolutionon 1 January 1959.[172]Although Fidel Castro's first refused to categorize his new government as socialist and repeatedly denying being a communist, Castro appointed Marxists to senior government and military positions.[173][174][175]
Diplomatic relations between Cuba and the United Statescontinued for some time after Batista's fall, but President Eisenhower deliberately left the capital to avoid meeting Castro during the latter's trip toWashington, D.C.in April, leaving Vice PresidentRichard Nixonto conduct the meeting in his place.[176]Cuba began negotiating for arms purchases from the Eastern Bloc in March 1960.[177]The same month, Eisenhower gave approval toCIAplans and funding to overthrow Castro.[178]
In January 1961, just prior to leaving office, Eisenhower formally severed relations with the Cuban government. That April, the administration of newly elected American PresidentJohn F. Kennedymounted the unsuccessful CIA-organizedship-borne invasionof the island byCuban exilesat Playa Girón and Playa Larga inSanta Clara Province—a failure that publicly humiliated the United States.[179]Castro responded by publicly embracingMarxism–Leninism, and the Soviet Union pledged toprovide further support.[179]In December, the US governmentbegan a violent campaignofterroristattacks against civilians in Cuba, andcovert operationsand sabotage against the administration, in an attempt to overthrow the Cuban government.[184]
TheBerlin Crisis of 1961was the last major incident in the Cold War regarding the status of Berlin andpost–World War II Germany. By the early 1950s, theSoviet approach to restricting emigration movementwas emulated by most of the rest of theEastern Bloc.[185]However, hundreds of thousands ofEast Germansannually emigrated to free and prosperousWest Germanythrough a "loophole" in the system that existed betweenEast BerlinandWest Berlin.[186][187]
The emigration resulted in a massive "brain drain" from East Germany to West Germany of younger educated professionals, such that nearly 20% of East Germany's population had migrated to West Germany by 1961.[188]That June, theSoviet Unionissued a newultimatumdemanding the withdrawal ofAllied forcesfrom West Berlin.[189]The request was rebuffed, but the United States now limited its security guarantees to West Berlin.[190]On 13 August, East Germany erected a barbed-wire barrier that would eventually be expanded through construction into theBerlin Wall, effectively closing the loophole and preventing its citizens from fleeing to the West.[191]
The Kennedy administration continued seeking ways to oust Castro following the Bay of Pigs invasion, experimenting with various ways of covertly facilitating the overthrow of the Cuban government. Significant hopes were pinned on the program of terrorist attacks and other destabilization operations known asOperation Mongoose, that was devised under the Kennedy administration in 1961. Khrushchev learned of the project in February 1962,[192]and preparations to install Soviet nuclear missiles in Cuba were undertaken in response.[192]
Alarmed, Kennedy considered various reactions. He ultimately responded to the installation of nuclear missiles in Cuba with anaval blockade, and he presented an ultimatum to the Soviets. Khrushchev backed down from a confrontation, and the Soviet Union removed the missiles in return for a public American pledge not to invade Cuba again as well as a covert deal to remove US missiles from Turkey.[193]
TheCuban Missile Crisis(October–November 1962) brought the world closer tonuclear warthan ever before.[194]The aftermath led to efforts in thenuclear arms raceat nuclear disarmament and improving relations, although the Cold War's first arms control agreement, theAntarctic Treaty, had come into force in 1961.[J]
The compromise embarrassed Khrushchev and the Soviet Union because the withdrawal of US missiles from Italy and Turkey was a secret deal between Kennedy and Khrushchev, and the Soviets were seen as retreating from circumstances that they had started. In 1964, Khrushchev's Kremlin colleagues managed toousthim, but allowed him a peaceful retirement.[195]He was accused of rudeness and incompetence, and John Lewis Gaddis argues that he was also blamed with ruining Soviet agriculture, bringing the world to the brink of nuclear war, and becoming an "international embarrassment" when he authorized construction of the Berlin Wall.[196]According to Dobrynin, the top Soviet leadership took the Cuban outcome as "a blow to its prestige bordering on humiliation".[197][198]
In the course of the 1960s and 1970s, Cold War participants struggled to adjust to a new, more complicated pattern of international relations in which the world was no longer divided into two clearly opposed blocs.[36]From the beginning of the post-war period, with American help Western Europe and Japan rapidly recovered from the destruction of World War II and sustained strong economic growth through the 1950s and 1960s, with per capita GDPs approaching those of the United States, whileEastern Bloc economies stagnated.[36][199]
TheVietnam Wardescended into a quagmire for the United States, leading to a decline in international prestige and economic stability, derailing arms agreements, and provoking domestic unrest. America's withdrawal from the war led it to embrace a policy ofdétentewith both China and the Soviet Union.[200]
In the1973 oil crisis, Organization of Petroleum Exporting Countries (OPEC) cut their petroleum output. This raised oil prices and hurt Western economies, but helped the Soviet Union by generating a huge flow of money from its oil sales.[201]
As a result of the oil crisis, combined with the growing influence of Third World alignments such as OPEC and theNon-Aligned Movement, less powerful countries had more room to assert their independence and often showed themselves resistant to pressure from either superpower.[99]Meanwhile, Moscow was forced to turn its attention inward to deal with the Soviet Union's deep-seated domestic economic problems.[36]During this period, Soviet leaders such asLeonid BrezhnevandAlexei Kosyginembraced the notion of détente.[36]
Under PresidentJohn F. Kennedy, US troop levels in Vietnam grew from just under a thousand in 1959 to 16,000 in 1963.[202][203]South Vietnamese President Ngo Dinh Diem's heavy-handedcrackdown on Buddhist monksin 1963 led the US to endorse a deadlymilitary coup against Diem.[204]The war escalated further in 1964 following the controversialGulf of Tonkin incident, in which a US destroyer was alleged to have clashed with North Vietnamese fast attack craft. TheGulf of Tonkin Resolutiongave PresidentLyndon B. Johnsonbroad authorization to increase US military presence, deploying groundcombat unitsfor the first time and increasing troop levels to 184,000.[205]Soviet leader Leonid Brezhnev responded by reversing Khrushchev's policy of disengagement and increasing aid to the North Vietnamese, hoping to entice the North from its pro-Chinese position. The USSR discouraged further escalation of the war, however, providing just enough military assistance to tie up American forces.[206]From this point, thePeople's Army of Vietnam(PAVN) engaged in moreconventional warfarewith US and South Vietnamese forces.[207]
TheTet Offensiveof 1968 proved to be the turning point of the war. Despite years of American tutelage and aid, the South Vietnamese forces were unable to withstand the communist offensive and the task fell to US forces instead.[208]At the same time, in 1963–1965, American domestic politics saw the triumph ofliberalism. According to historian Joseph Crespino:
ThePartial Nuclear Test Ban Treatywas signed on August 5, 1963, by the United States, the Soviet Union, and over 100 other nations. This treaty banned nuclear weapons tests in the atmosphere, outer space, and underwater, restricting such tests to underground environments.[210][211][212][213]The treaty followed heightened concerns over the militarization of space, amplified by the United States' Starfish Prime test in 1962, which involved the detonation of a nuclear device in the upper atmosphere.[214][215]
To further delineate the peaceful use of outer space, the United Nations facilitated the drafting of theTreaty on Principles Governing the Activities of States in the Exploration and Use of Outer Space, including the Moon and Other Celestial Bodies, commonly known as the Outer Space Treaty. Signed on January 27, 1967, by the United States, the Soviet Union, and the United Kingdom, it entered into force on October 10, 1967. The treaty established space as a domain to be used exclusively for peaceful purposes, prohibiting the placement of nuclear weapons or any other weapons of mass destruction in orbit or on celestial bodies.[216][217][218]
In 1968, a period of political liberalization took place inCzechoslovakiacalled thePrague Spring. An "Action Program" of reforms included increasingfreedom of the press,freedom of speechandfreedom of movement, along with an economic emphasis onconsumer goods, the possibility of a multiparty government, limitations on the power of the secret police,[219][220]and potential withdrawal from the Warsaw Pact.[221]
In answer to the Prague Spring, on 20 August 1968, theSoviet Army, together with most of their Warsaw Pact allies,invaded Czechoslovakia.[222]The invasion was followed by a wave of emigration, including an estimated 70,000 Czechs and Slovaks initially fleeing, with the total eventually reaching 300,000.[223][224]The invasion sparked intense protests from Yugoslavia, Romania, China, and from Western European countries.[225]
As a result of theSino-Soviet split, tensions along the Chinese–Soviet borderreached their peakin 1969, when the Soviet planned to launch alarge-scale nuclear strike against China.[226]United States PresidentRichard Nixonintervened,[226]and decided to use the conflict to shift the balance of power towards the West in the Cold War through a policy of rapproachment with China, which began with his1972 visit to Chinaand culminated in 1979 with the signing of theJoint Communiqué on the Establishment of Diplomatic RelationsbyPresident Carter and Chinese Communist Party leader Deng Xiaoping.[227][228]
Although indirect conflict between Cold War powers continued through the late 1960s and early 1970s, tensions were beginning to ease.[229]Following the ousting of Khrushchev, another period ofcollective leadershipensued, consisting of Leonid Brezhnev as general secretary,Alexei Kosyginas Premier andNikolai Podgornyas Chairman of the Presidium, lasting until Brezhnev established himself in the early 1970s as the preeminent Soviet leader.
Following his visit to China, Nixon met with Soviet leaders in Moscow.[230]TheseStrategic Arms Limitation Talksresulted in landmark arms control treaties. These aimed to limit the development of costly anti-ballistic missiles and nuclear missiles.[36]
Nixon and Brezhnev proclaimed a new era of "peaceful coexistence" and established the groundbreaking new policy ofdétente(or cooperation) between the superpowers. Meanwhile, Brezhnev attempted to revive the Soviet economy, which was declining in part because of heavy military expenditures. The Soviet Union'smilitary budgetin the 1970s was massive, 40–60% of the federal budget and 15% of GDP.[231]Between 1972 and 1974, the two sides also agreed to strengthen their economic ties,[27]including agreements for increased trade. As a result of their meetings,détentewould replace the hostility of the Cold War and the two countries would live mutually.[232]These developments coincided withBonn's "Ostpolitik" policy formulated by the West German ChancellorWilly Brandt,[225]an effort to normalize relations between West Germany and Eastern Europe. Other agreements were concluded to stabilize the situation in Europe, culminating in theHelsinki Accordssigned at theConference on Security and Co-operation in Europein 1975.[233]
The Helsinki Accords, in which the Soviets promised to grant free elections in Europe, has been called a major concession to ensure peace by the Soviets. In practice, the Soviet government significantly curbed therule of law,civil liberties,protection of lawandguarantees of property,[234][235]which were considered examples of "bourgeois morality" by Soviet legal theorists such asAndrey Vyshinsky.[236]The Soviet Union signed legally-binding human rights documents, such as theInternational Covenant on Civil and Political Rightsin 1973 and the Helsinki Accords in 1975, but they were neither widely known or accessible to people living under Communist rule, nor were they taken seriously by the Communist authorities.[237]Human rights activists in the Soviet Union were regularly subjected to harassment, repressions and arrests.
The pro-Soviet American business magnateArmand HammerofOccidental Petroleumoften mediated trade relations. AuthorDaniel Yergin, in his bookThe Prize, writes that Hammer "ended up as a go-between for five Soviet General Secretaries and seven U.S. Presidents."[238]Hammer had extensive business relationship in the Soviet Union stretching back to the 1920s with Lenin's approval.[239][240]According toChristian Science Monitorin 1980, "although his business dealings with the Soviet Union were cut short when Stalin came to power, he had more or less single-handedly laid the groundwork for the [1980] state of Western trade with the Soviet Union."[239]
Kissinger and Nixon were "realists" who deemphasized idealistic goals like anti-communism or promotion of democracy worldwide because those goals were too expensive in terms of America's economic capabilities.[241]They rejected "idealism" as impractical and too expensive, and neither man showed much sensitivity to the plight of people living under Communism. Kissinger's realism fell out of fashion as idealism returned to American foreign policy with Carter's moralism emphasizing human rights, and Reagan's rollback strategy aimed at destroying Communism.[242]
In the 1970s, the KGB, led byYuri Andropov, continued to persecute distinguishedSoviet dissidents, such asAleksandr SolzhenitsynandAndrei Sakharov, who were criticising the Soviet leadership in harsh terms.[243]Indirect conflict between the superpowers continued through this period of détente in the Third World, particularly during political crises in the Middle East, Chile, Ethiopia, and Angola.[244]
In 1973, Nixon announced his administration was committed to seekingmost favored nationtrade status with the USSR,[245]which was challenged by Congress in theJackson-Vanik Amendment.[246]The United States had long linked trade with the Soviet Union to its foreign policy toward the Soviet Union and, especially since the early 1980s, toSoviet human rights policies. TheJackson-Vanik Amendment, which was attached to the1974 Trade Act, linked the granting ofmost-favored-nationto the USSR to the right of persecutedSoviet Jewsto emigrate. Because the Soviet Union refused the right of emigration to Jewishrefuseniks, the ability of the President to apply most-favored nation trade status to the Soviet Union was restricted.[247]
Although PresidentJimmy Cartertried to place another limit on the arms race with aSALT IIagreement in 1979,[248]his efforts were undermined by the other events that year, including theIranian Revolutionand theNicaraguan Revolution, which both ousted pro-US governments, and his retaliation against theSoviet coup in Afghanistanin December.[27]
The period in the late 1970s and early 1980s showed an intensive reawakening of Cold War tensions and conflicts. Tensions greatly increased between the major powers with both sides becoming more militant.[249]Digginssays, "Reagan went all out to fight the second cold war, by supporting counterinsurgencies in the third world."[250]Coxsays, "The intensity of this 'second' Cold War was as great as its duration was short."[251]
In April 1978, the communistPeople's Democratic Party of Afghanistan(PDPA) seized power inAfghanistanin theSaur Revolution. Within months, opponents of the communist regime launched an uprising in eastern Afghanistan that quickly expanded into acivil warwaged by guerrillamujahideenagainst government forces countrywide.[252]TheIslamic Unity of Afghanistan Mujahideeninsurgents received military training and weapons in neighboringPakistanandChina,[253][254]while the Soviet Union sent thousands of military advisers to support the PDPA government.[252]Meanwhile, increasing friction between the competing factions of the PDPA—the dominantKhalqand the more moderateParcham—resulted in the dismissal of Parchami cabinet members and the arrest of Parchami military officers under the pretext of a Parchami coup. By mid-1979, the United States had started a covert program to assist the mujahideen.[255][256]
In September 1979, Khalqist PresidentNur Muhammad Tarakiwas assassinated in a coup within the PDPA orchestrated by fellow Khalq memberHafizullah Amin, who assumed the presidency. Distrusted by the Soviets, Amin was assassinated by Soviet special forces duringOperation Storm-333in December 1979. Afghan forces suffered losses during the Soviet operation; 30 Afghan palace guards and over 300 army guards were killed while another 150 were captured.[257]In the aftermath of the operation, a total of 1,700 Afghan soldiers who surrendered to Soviet forces were taken as prisoners,[258]and the Soviets installedBabrak Karmal, the leader of the PDPA's Parcham faction, as Amin's successor. Veterans of the Soviet Union'sAlpha Grouphave stated that Operation Storm-333 was one of the most successful in the unit's history. Documents released following thedissolution of the Soviet Unionin the 1990s revealed that the Soviet leadership believed Amin had secret contacts within theAmerican embassy in Kabuland "was capable of reaching an agreement with the United States";[259]however, allegations of Amin colluding with the Americans have been widely discredited.[260][K][L]The PDBA was tasked to fill the vacuum and carried out a purge of Amin supporters. Soviet troops were deployed to put Afghanistan under Soviet control with Karmal in more substantial numbers, although the Soviet government did not expect to do most of the fighting in Afghanistan. As a result, however, the Soviets were now directly involved in what had been a domestic war in Afghanistan.[261]
Carter responded to the Soviet invasion by withdrawing theSALT IItreaty from ratification, imposing embargoes on grain and technology shipments to the USSR, and demanding a significant increase in military spending, and further announced theboycottof the1980 Summer Olympicsin Moscow, which was joined by 65 other nations.[262][263][264]He described the Soviet incursion as "the most serious threat to the peace since the Second World War".[265]
In January 1977, four years prior to becoming president,Ronald Reaganbluntly stated, in a conversation withRichard V. Allen, his basic expectation in relation to the Cold War. "My idea of American policy toward the Soviet Union is simple, and some would say simplistic," he said. "It is this: We win and they lose."[266]In 1980, Ronald Reagan won the1980 presidential election, vowing to increase military spending and confront the Soviets everywhere.[267]Both Reagan and new British Prime MinisterMargaret Thatcherdenounced the Soviet Union and its ideology. Reagan labeled the Soviet Union an "evil empire" and predicted that Communism would be left on the "ash heap of history," while Thatcher inculpated the Soviets as "bent on world dominance."[268]In 1982, Reagan tried to cut off Moscow's access to hard currency by impeding its proposed gas line to Western Europe. It hurt the Soviet economy, but it also caused ill will among American allies in Europe who counted on that revenue. Reagan retreated on this issue.[269][270]
By early 1985, Reagan's anti-communist position had developed into a stance known as the newReagan Doctrine—which, in addition to containment, formulated an additional right to subvert existing communist governments.[271]Besides continuing Carter's policy of supporting the Islamic opponents of the Soviet Union and the Soviet-backedPDPAgovernment in Afghanistan, the CIA also sought to weaken the Soviet Union itself by promotingIslamismin the majority-MuslimCentral Asian Soviet Union.[272]Additionally, the CIA encouraged anti-communist Pakistan's ISI to train Muslims from around the world to participate in thejihadagainst the Soviet Union.[272]
Pope John Paul IIprovided a moral focus foranti-communism; a visit to his native Poland in 1979 stimulated a religious and nationalist resurgence centered on theSolidarity movementtrade union that galvanized opposition, and may have led to hisattempted assassinationtwo years later.[273][274][275]In December 1981, Poland'sWojciech Jaruzelskireacted to the crisis by imposinga period of martial law. Reagan imposed economic sanctions on Poland in response.[276]Mikhail Suslov, the Kremlin's top ideologist, advised Soviet leaders not to intervene if Poland fell under the control of Solidarity, for fear it might lead to heavy economic sanctions, resulting in a catastrophe for the Soviet economy.[276]
The Soviet Union had built up a military that consumed as much as 25 percent of its gross national product at the expense ofconsumer goodsand investment in civilian sectors.[277]Soviet spending on thearms raceand other Cold War commitments both caused and exacerbated deep-seated structural problems in the Soviet system,[278]which experienced at leasta decade of economic stagnationduring the late Brezhnev years.
Soviet investment in the defense sector was not driven by military necessity but in large part by the interests of thenomenklatura, which was dependent on the sector for their own power and privileges.[279]TheSoviet Armed Forcesbecame the largest in the world in terms of the numbers and types of weapons they possessed, in the number of troops in their ranks, and in the sheer size of theirmilitary–industrial base.[280]However, the quantitative advantages held by the Soviet military often concealed areas where the Eastern Bloc dramatically lagged behind the West.[281]For example, thePersian Gulf Wardemonstrated how thearmor,fire control systems, and firing range of the Soviet Union's most common main battle tank, theT-72, were drastically inferior to the AmericanM1 Abrams, yet the USSR fielded almost three times as many T-72s as the US deployed M1s.[282]
By the early 1980s, the USSR had built up a military arsenal and army surpassing that of the United States. Soon after the Soviet invasion of Afghanistan, President Carter began massively building up the United States military. This buildup was accelerated by the Reagan administration, which increased the military spending from 5.3 percent of GNP in 1981 to 6.5 percent in 1986,[283]the largest peacetime defense buildup in United States history.[284]The American-Soviet tensions present during 1983 was defined by some as the start of "Cold War II". While in retrospective this phase of the Cold War was generally defined as a "war of words",[285]the Soviet's "peace offensive" was largely rejected by the West.[286]
Tensions continued to intensify as Reagan revived theB-1 Lancerprogram, which had been canceled by the Carter administration,[287]producedLGM-118 Peacekeepermissiles,[288]installed US cruise missiles in Europe, and announced the experimentalStrategic Defense Initiative, dubbed "Star Wars" by the media, a defense program to shoot down missiles in mid-flight.[289]The Soviets deployedRSD-10 Pioneerballistic missilestargeting Western Europe, and NATO decided, under the impetus of the Carter presidency, to deployMGM-31 Pershingand cruise missiles in Europe, primarily West Germany.[290]This deployment placed missiles just 10 minutes' striking distance from Moscow.[291]
After Reagan's military buildup, the Soviet Union did not respond by further building its military,[292]because the enormous military expenses, along with inefficientplanned manufacturingandcollectivized agriculture, were already a heavy burden for theSoviet economy.[293]At the same time,Saudi Arabiaincreased oil production,[294]even as other non-OPEC nations were increasing production.[M]These developments contributed to the1980s oil glut, which affected the Soviet Union as oil was the main source of Soviet export revenues.[277]Issues withcommand economics,[295]oil price decreases and large military expenditures gradually brought the Soviet economy to stagnation.[294]
On 1 September 1983, the Soviet Union shot downKorean Air Lines Flight 007, aBoeing 747with 269 people aboard, including sitting CongressmanLarry McDonald, an action which Reagan characterized as a massacre. The airliner was en route from Anchorage to Seoul but owing to a navigational mistake made by the crew, it flew through Russianprohibited airspace. TheSoviet Air Forcetreated the unidentified aircraft as an intruding U.S.spy planeand destroyed it withair-to-air missiles.[296]The incident increased support for military deployment, overseen by Reagan, which stood in place until the later accords between Reagan and Mikhail Gorbachev.[297]During the early hours of 26 September 1983, the1983 Soviet nuclear false alarm incidentoccurred; systems inSerpukhov-15underwent a glitch that claimed severalintercontinental ballistic missileswere heading towards Russia, but officerStanislav Petrovcorrectly suspected it was afalse alarm, ensuring the Soviets did not respond to the non-existent attack.[298]As such, he has been credited as "the man who saved the world".[299]TheAble Archer 83exercise in November 1983, a realistic simulation of a coordinated NATO nuclear release, was perhaps the most dangerous moment since the Cuban Missile Crisis, as the Soviet leadership feared that a nuclear attack might be imminent.[300]
American domestic public concerns about intervening in foreign conflicts persisted from the end of the Vietnam War.[301]The Reagan administration emphasized the use of quick, low-costcounterinsurgencytactics to intervene in foreign conflicts.[301]In 1983, the Reagan administration intervened in the multisidedLebanese Civil War,invaded Grenada,bombed Libyaand backed the Central AmericanContras, anti-communist paramilitaries seeking to overthrow the Soviet-alignedSandinistagovernment in Nicaragua.[99]While Reagan's interventions against Grenada and Libya were popular in the United States, his backing of the Contra rebels wasmired in controversy.[302]The Reagan administration's backing of the military government ofGuatemaladuring theGuatemalan Civil War, in particular the regime ofEfraín Ríos Montt, was also controversial.[303]
Meanwhile, the Soviets incurred high costs for their own foreign interventions. Although Brezhnev was convinced in 1979 that theSoviet war in Afghanistanwould be brief, Muslim guerrillas, aided by the US, China, Britain, Saudi Arabia and Pakistan,[254]waged a fierce resistance against the invasion.[304]The Kremlin sent nearly 100,000 troops to support its puppet regime in Afghanistan, leading many outside observers to dub the war "the Soviets' Vietnam".[304]However, Moscow's quagmire in Afghanistan was far more disastrous for the Soviets than Vietnam had been for the Americans because the conflict coincided with a period of internal decay and domestic crisis in the Soviet system.
A seniorUS State Departmentofficial predicted such an outcome as early as 1980, positing that the invasion resulted in part from a:
...domestic crisis within the Sovietsystem. ... Itmay be that the thermodynamic law ofentropyhas ... caughtup with the Soviet system, which now seems to expend more energy on simply maintaining its equilibrium than on improving itself. We could be seeing a period of foreign movement at a time of internal decay.[305]
By the time the comparatively youthfulMikhail GorbachevbecameGeneral Secretaryin 1985,[268]the Soviet economy was stagnant and faced a sharp fall in foreign currency earnings as a result of the downward slide in oil prices in the 1980s.[306]These issues prompted Gorbachev to investigate measures to revive the ailing state.[306]
An ineffectual start led to the conclusion that deeper structural changes were necessary, and in June 1987 Gorbachev announced an agenda of economic reform calledperestroika, or restructuring.[307]Perestroika relaxed theproduction quotasystem, allowed cooperative ownership of small businesses and paved the way for foreign investment. These measures were intended to redirect the country's resources from costly Cold War military commitments to more productive areas in the civilian sector.[307]
Despite initial skepticism in the West, the new Soviet leader proved to be committed to reversing the Soviet Union's deteriorating economic condition instead of continuing the arms race with the West.[308]Partly as a way to fight off internal opposition from party cliques to his reforms, Gorbachev simultaneously introducedglasnost, or openness, which increased freedom of the press and the transparency of state institutions.[309]Glasnostwas intended to reduce the corruption at the top of theCommunist Partyand moderate theabuse of powerin theCentral Committee.[310]Glasnost also enabled increased contact between Soviet citizens and the Western world, particularly with the United States, contributing to the acceleratingdétentebetween the two nations.[311]
In response to the Kremlin's military andpolitical concessions, Reagan agreed to renew talks on economic issues and the scaling-back of the arms race.[312]The firstsummitwas held in November 1985 inGeneva,Switzerland.[312]Asecond summitwas held in October 1986 inReykjavík,Iceland. Talks went well until the focus shifted to Reagan's proposedStrategic Defense Initiative(SDI), which Gorbachev wanted to be eliminated. Reagan refused.[313]The negotiations failed, but the third summit (Washington Summit (1987), 8–10 December 1987) led to a breakthrough with the signing of theIntermediate-Range Nuclear Forces Treaty(INF). The INF treaty eliminated all nuclear-armed, ground-launched ballistic and cruise missiles with ranges between 500 and 5,500 kilometers (310 and 3,420 mi) and their infrastructure.[314]
During 1988, it became apparent to the Soviets that oil and gas subsidies, along with the cost of maintaining massive troops levels, represented a substantial economic drain.[315]In addition, the security advantage ofa buffer zonewas recognised as irrelevant and the Sovietsofficially declaredthat they would no longer intervene in the affairs ofsatellite statesin Central and Eastern Europe.[316]George H. W. Bushand Gorbachev met at theMoscow Summitin May 1988 and theGovernors Island Summitin December 1988.
In 1989,Soviet forces withdrew from Afghanistanwithout achieving their objectives.[317]Later that year, theBerlin Wall, theInner German borderand theIron Curtainfell. On 3 December 1989, Gorbachev and Bush declared the Cold War over at theMalta Summit. In February 1990, Gorbachev agreed with the US-proposedTreaty on the Final Settlement with Respect to Germanyand signed it on 12 September 1990, paving the way for theGerman reunification.[315]When the Berlin Wall came down, Gorbachev's "Common European Home" concept began to take shape.[318][319]The two former adversaries were partners in theGulf WaragainstIraq(August 1990 – February 1991).[320]During the final summit in Moscow in July 1991, Gorbachev and Bush signed theSTART Iarms control treaty.[321]
Two developments dominated the decade that followed: the increasingly apparent crumbling of the Soviet Union's economic and political structures, and the patchwork attempts at reforms to reverse that process. Kenneth S. Deffeyes argued inBeyond Oilthat theReagan administrationencouragedSaudi Arabiatolower the price of oilto the point where the Soviets could not make a profit selling their oil, and resulted in the depletion of the country'shard currencyreserves.[322]
Brezhnev's next two successors, transitional figures with deep roots in his tradition, did not last long.Yuri Andropovwas 68 years old andKonstantin Chernenko72 when they assumed power; both died in less than two years. In an attempt to avoid a third short-lived leader, in 1985, the Soviets turned to the next generation and selectedMikhail Gorbachev. He made significant changes in the economy and party leadership, calledperestroika. His policy ofglasnostfreed publicaccess to informationafter decades of heavy government censorship. Gorbachev also moved to end the Cold War. In 1988, the USSR abandoned itswar in Afghanistanand began towithdraw its forces. In the following year,Gorbachev refused to interfere in the internal affairs of the Soviet satellite states, which paved the way for theRevolutions of 1989. In particular, the standstill of the Soviet Union at thePan-European Picnicin August 1989 then set a peaceful chain reaction in motion, at the end of which the Eastern Bloc collapsed. With the tearing down of theBerlin Walland with East and West Germany pursuing re-unification, theIron Curtainbetweenthe Westand Soviet-occupied regions came down.[323][324][325]
By 1989, the Soviet alliance system was on the brink of collapse, and, deprived of Soviet military support, the communist leaders of the Warsaw Pact states were losing power.[317]Grassroots organizations, such as Poland'sSolidaritymovement, rapidly gained ground with strong popular bases.
The Pan-European Picnic in August 1989 in Hungary finally started a peaceful movement that the rulers in the Eastern Bloc could not stop. It was the largest movement of refugees from East Germany since the Berlin Wall was built in 1961 and ultimately brought about the fall of the Iron Curtain. The patrons of the picnic,Otto von Habsburgand the Hungarian Minister of StateImre Pozsgay, saw the planned event as an opportunity to test Mikhail Gorbachev's reaction. The Austrian branch of thePaneuropean Union, which was then headed by Otto von Habsburg, distributed thousands of brochures inviting the GDR holidaymakers in Hungary to a picnic near the border at Sopron. But with the mass exodus at the Pan-European Picnic the subsequent hesitant behavior of the ruling Socialist Unity Party of East Germany and the non-interference of the Soviet Union broke the dams. Now tens of thousands of media-informed East Germans made their way to Hungary, which was no longer willing to keep its borders completely closed or to oblige its border troops to use armed force. On the one hand, this caused disagreement among the Eastern European states and, on the other hand, it was clear to the Eastern European population that the governments no longer had absolute power.[323][324][325][326]
In 1989, the communist governments in Poland and Hungary became the first to negotiate the organization of competitive elections. In Czechoslovakia and East Germany, mass protests unseated entrenched communist leaders. The communist regimes in Bulgaria and Romania also crumbled, in the latter case as the result of aviolent uprising. Attitudes had changed enough that US Secretary of StateJames Bakersuggested that the American government would not be opposed to Soviet intervention in Romania, on behalf of the opposition, to prevent bloodshed.[327]
The tidal wave of change culminated with thefall of the Berlin Wallin November 1989, which symbolized the collapse of European communist governments and graphically ended the Iron Curtain divide of Europe. The1989 revolutionary waveswept across Central and Eastern Europe and peacefully overthrew all of the Soviet-styleMarxist–Leninist states: East Germany, Poland, Hungary, Czechoslovakia and Bulgaria;[328]Romania was the only Eastern-bloc country to topple its communist regime violently and execute its head of state.[329]
At the same time, the Soviet republics started legal moves towards potentially declaringsovereigntyover their territories, citing the freedom to secede in Article 72 of the USSR constitution.[330]On 7 April 1990, a law was passed allowing a republic to secede if more than two-thirds of its residents voted for it in a referendum.[331]Many held their first free elections in the Soviet era for their own national legislatures in 1990. Many of these legislatures proceeded to produce legislation contradicting the Union laws in what was known as the 'War of Laws'. In 1989, theRussian SFSRconvened a newly elected Congress of People's Deputies.Boris Yeltsinwas elected its chairman. On 12 June 1990, the Congressdeclared Russia's sovereignty over its territoryand proceeded to pass laws that attempted to supersede some of the Soviet laws. After a landslide victory ofSąjūdisin Lithuania, that country declared its independence restored on 11 March 1990, citing the illegality of theSoviet occupation of the Baltic states. Soviet forces attempted to halt the secession by crushing popular demonstrations in Lithuania (Bloody Sunday) and Latvia (The Barricades), as a result, numerous civilians were killed or wounded. However, these actions only bolstered international support for the secessionists.[332]
Areferendum for the preservation of the USSRwas held on 17 March 1991 in nine republics (the remainder having boycotted the vote), with the majority of the population in those republics voting for preservation of the Union in the form of a new federation. The referendum gave Gorbachev a minor boost. In the summer of 1991, theNew Union Treaty, which would have turned the country into a much looser Union, was agreed upon by eight republics. The signing of the treaty, however, was interrupted by theAugust Coup—an attempted coup d'état by hardline members of the government and the KGB who sought to reverse Gorbachev's reforms and reassert the central government's control over the republics. After the coup collapsed, Russian president Yeltsin was seen as a hero for his decisive actions, while Gorbachev's power was effectively ended. The balance of power tipped significantly towards the republics. In August 1991, Latvia and Estonia immediately declared the restoration of their full independence (following Lithuania's 1990 example). Gorbachev resigned as general secretary in late August, and soon afterwards, the party's activities were indefinitely suspended—effectively ending its rule. By the fall, Gorbachev could no longer influence events outside Moscow, and he was being challenged even there by Yeltsin, who had been electedPresident of Russiain July 1991.
Later in August, Gorbachev resigned asgeneral secretary of the Communist party, andRussianPresident Boris Yeltsin ordered the seizure of Soviet property. Gorbachev clung to power as the President of the Soviet Union until 25 December 1991, when theUSSR dissolved.[333]Fifteen statesemerged from the Soviet Union, with by far the largest and most populous one (which also was the founder of the Soviet state with theOctober Revolutionin Petrograd), theRussian Federation, taking full responsibility for all the rights and obligations of the USSR under the Charter of the United Nations, including the financial obligations. As such, Russia assumed the Soviet Union'sUN membership and permanent membership on the Security Council, nuclear stockpile and the control over the armed forces.[10]
In his1992 State of the Union Address, US President George H. W. Bush expressed his emotions: "The biggest thing that has happened in the world in my life, in our lives, is this: By the grace of God, America won the Cold War."[334]Bush and Yeltsin met in February 1992, declaring a new era of "friendship and partnership".[335]In January 1993, Bush and Yeltsin agreed toSTART II, which provided for further nuclear arms reductions on top of the original START treaty.[336]
In summing up the international ramifications of these events,Vladislav Zubokstated: 'The collapse of theSoviet empirewas an event of epochal geopolitical, military, ideological, and economic significance.'[337]After thedissolution of the Soviet Union, Russia drastically cutmilitary spending, and restructuring the economy left millions unemployed.[338]According to Western analysis, the neoliberal reforms in Russia culminated in arecessionin the early 1990s more severe than theGreat Depressionas experienced by the United States and Germany.[339]Western analysts suggest that in the 25 years following the end of the Cold War, only five or six of the post-communist states are on a path to joining the rich and capitalist world while most are falling behind, some to such an extent that it will take several decades to catch up to where they were before the collapse of communism.[340][341]
Stephen Holmesof theUniversity of Chicagoargued in 1996 that decommunization, after a brief active period, quickly ended in near-universal failure. After the introduction oflustration, demand for scapegoats has become relatively low, and former communists have been elected for high governmental and other administrative positions. Holmes notes that the only real exception was formerEast Germany, where thousands of formerStasiinformers have been fired from public positions.[342]
Holmes suggests the following reasons for the failure of decommunization:[342]
Compared with thedecommunizationefforts of the other former constituents of theEastern Blocand theSoviet Union, decommunization in Russia has been restricted to half-measures, if conducted at all.[343]Notable anti-communist measures in the Russian Federation include the banning of theCommunist Party of the Soviet Union(and the creation of theCommunist Party of the Russian Federation) as well as changing the names of some Russian cities back to what they were before the 1917October Revolution(Leningrad toSaint Petersburg, Sverdlovsk toYekaterinburgand Gorky toNizhny Novgorod),[344]though others were maintained, withUlyanovsk(former Simbirsk),Tolyatti(former Stavropol) andKirov(former Vyatka) being examples. Even though Leningrad and Sverdlovsk were renamed, regions that were named after them are still officially called Leningrad and Sverdlovsk oblasts.[345]
Nostalgia for the Soviet Unionis gradually on the rise in Russia.[346]Communist symbols continue to form an important part of the rhetoric used instate-controlled media, as banning on them in other countries is seen by theRussian foreign ministryas "sacrilege" and "a perverse idea of good and evil".[344]The process ofdecommunization in Ukraine, a neighbouringpost-Soviet state, was met with fierce criticism by Russia.[344]TheState Anthem of the Russian Federation, adopted in 2000 (the same yearVladimir Putinbegan his first term as president of Russia), uses the exact same music as theState Anthem of the Soviet Union, but with new lyrics written bySergey Mikhalkov.[347][348]
Conversely, decommunization in Ukraine started during and after thedissolution of the Soviet Unionin 1991[349]With the success of theRevolution of Dignityin 2014, theUkrainian governmentapprovedlawsthat outlawedcommunist symbols.[350]In July 2015, President of UkrainePetro Poroshenkosigned a set of laws that started a six-month period for the removal of communist monuments (excludingWorld War IImonuments) and renaming of public places named after communist-related themes.[344][351][352]At the time, this meant that 22 cities and 44 villages were set to get new names.[353]In 2016, 51,493 streets and 987 cities and villages were renamed, and 1,320Lenin monumentsand 1,069 monuments to other communist figures removed.[354]Violation of the law carries a penalty of a potential media ban and prison sentences of up to five years.[355][356]TheMinistry of the Interiorstripped theCommunist Party of Ukraine, theCommunist Party of Ukraine (renewed), and theCommunist Party of Workers and Peasantsof their right to participate in elections and stated it was continuing the court actions that started in July 2014 to end the registration ofcommunist parties in Ukraine.[357]By 16 December 2015, these three parties had been banned in Ukraine; the Communist Party of Ukraine appealed the ban to theEuropean Court of Human Rights.[358][359][360]
The Cold War had provided external stabilizing pressures. Both the United States and the Soviet Union had a vested interest in Yugoslavia’s stability, ensuring it remained a buffer state in the East-West divide. This resulted in financial and political support for its regime. When the Cold War ended, this external support evaporated, leaving Yugoslavia more vulnerable to internal divisions.[362][363]
As Yugoslavia fragmented,the warsbegan afterSloveniaandCroatiadeclared independence in 1991.Serbia, underSlobodan Milošević, opposed these moves.[364]The Bosnian War(1992–1995) was the most brutal of the Yugoslav Wars, characterized by ethnic cleansing and genocide. International organizations, including the United Nations, struggled to manage the violence. NATO eventually intervened with airstrikes in Bosnia (1995) as part ofOperation Deliberate Forceand later in Kosovo (1999) as part ofOperation allied force. These interventions marked the transition of NATO as a deterrent to the Soviet Union, to also functioning at the time as an active peacekeeping and conflict-resolution force.[365]
The post-Cold War world is considered to beunipolar, with the United States the sole remainingsuperpower.[366][367]The Cold War defined the political role of the United States after World War II—by 1989 the United States had military alliances with 50 countries, with 526,000 troops stationed abroad,[368]with 326,000 in Europe (two-thirds of which were inWest Germany)[369]and 130,000 in Asia (mainlyJapanandSouth Korea).[368]The Cold War also marked the zenith of peacetimemilitary–industrial complexesand large-scalemilitary funding of science.[370]
Cumulative US military expenditures throughout the entire Cold War amounted to an estimated $8 trillion. Nearly 100,000 Americans died in theKoreanandVietnam Wars.[371]Although Soviet casualties are difficult to estimate, as a share of gross national product the financial cost for the Soviet Union was much higher than that incurred by the United States.[372]
Millions died in the superpowers'proxy warsaround the globe, most notably in eastern Asia.[373][N]Most of the proxy wars and subsidies for local conflicts ended along with the Cold War; interstate wars, ethnic wars, revolutionary wars, as well as refugee and displaced persons crises have declined sharply in the post-Cold War years.[374]
However, the aftermath of the Cold War is not considered to be concluded. Many of the economic and social tensions that were exploited to fuel Cold War competition in parts of the Third World remain acute. The breakdown of state control in a number of areas formerly ruled by communist governments produced new civil and ethnic conflicts, particularly in the formerYugoslavia. In Central and Eastern Europe, the end of the Cold War has ushered in an era ofeconomic growthand an increase in the number ofliberal democracies, while in other parts of the world, such as Afghanistan, independence was accompanied bystate failure.[249]
The Cold War endures as a popular topic reflected in entertainment media, and continuing to the present with post-1991 Cold War-themed feature films, novels, television and web series, and other media. In 2013, a KGB-sleeper-agents-living-next-door action drama series,The Americans, set in the early 1980s, was ranked No. 6 on theMetacriticannual Best New TV Shows list; its six-season run concluded in May 2018.[375][376]
Interpreting the course and origins of the conflict has been a source of heated controversy among historians, political scientists, and journalists.[377]In particular, historians have sharply disagreed as to who was responsible for the breakdown of Soviet–US relations after the Second World War; and whether the conflict between the two superpowers was inevitable or could have been avoided.[378]Historians have also disagreed on what exactly the Cold War was, what the sources of the conflict were, and how to disentangle patterns of action and reaction between the two sides.[249]
Although explanations of the origins of the conflict in academic discussions are complex and diverse, several general schools of thought on the subject can be identified. Historians commonly speak of three different approaches to the study of the Cold War: "orthodox" accounts, "revisionism", and "post-revisionism".[370]
"Orthodox" accounts place responsibility for the Cold War on the Soviet Union and its expansion further into Europe.[370]"Revisionist" writers place more responsibility for the breakdown of post-war peace on the United States, citing a range of US efforts to isolate and confront the Soviet Union well before the end of World War II.[370]"Post-revisionists" see the events of the Cold War as more nuanced and attempt to be more balanced in determining what occurred during the Cold War.[370]Much of the historiography on the Cold War weaves together two or even all three of these broad categories.[81]
TheRoman Empireruled the Mediterranean and much of Europe, Western Asia and North Africa. TheRomansconquered most of this during theRepublic, and it was ruled by emperors followingOctavian's  assumption of effective sole rule in 27 BC. Thewestern empirecollapsed in 476 AD, but theeastern empirelasted until thefall of Constantinoplein 1453.
By 100 BC, thecity of Romehad expanded its rule from theItalian peninsulato most of theMediterraneanand beyond. However, it was severely destabilised bycivil wars and political conflicts, which culminated in thevictory of OctavianoverMark AntonyandCleopatraat theBattle of Actiumin 31 BC, and the subsequent conquest of thePtolemaic Kingdomin Egypt. In 27 BC, theRoman Senategranted Octavian overarching military power (imperium) and the new title ofAugustus, marking hisaccession as the first Roman emperor. The vast Roman territories were organized intosenatorialprovinces, governed by proconsuls who were appointed by lot annually, andimperialprovinces, which belonged to the emperor but were governed bylegates.[21]
Thefirst two centuries of the Empiresaw a period of unprecedented stability and prosperity known as thePax Romana(lit.'Roman Peace'). Rome reached itsgreatest territorial extentunderTrajan(r.98–117 AD), but a period of increasing trouble and decline began underCommodus(r.180–192). In the 3rd century, the Empire underwent a49-year crisisthat threatened its existence due to civil war,plaguesandbarbarian invasions. TheGallicandPalmyreneempires broke away from the state and a series ofshort-lived emperorsled the Empire, which was later reunified underAurelian(r.270–275). The civil wars ended with the victory ofDiocletian(r.284–305), who set up two different imperial courts in theGreek East and Latin West.Constantine the Great(r.306–337), the firstChristian emperor, moved the imperial seat from Rome toByzantiumin 330, and renamed itConstantinople. TheMigration Period, involvinglarge invasions by Germanic peoplesand by theHunsofAttila, led to the decline of theWestern Roman Empire. With thefall of Ravennato theGermanic Heruliansand thedeposition of Romulus Augustusin 476 byOdoacer, the Western Empire finally collapsed. TheByzantine (Eastern Roman) Empiresurvived for another millennium with Constantinople as its sole capital, untilthe city's fallin 1453.[f]
Due to the Empire's extent and endurance, its institutions and culture hada lasting influenceon the development oflanguage,religion,art,architecture,literature,philosophy,law, andforms of governmentacross its territories.Latinevolved into theRomance languageswhileMedieval Greekbecame the language of the East. TheEmpire's adoptionofChristianityresulted in the formation of medievalChristendom. Roman andGreek arthad a profound impact on theItalian Renaissance. Rome's architectural tradition served as the basis forRomanesque,Renaissance, andNeoclassical architecture, influencingIslamic architecture. The rediscovery ofclassical scienceandtechnology(which formed the basis forIslamic science) in medieval Europe contributed to theScientific RenaissanceandScientific Revolution. Many modern legal systems, such as theNapoleonic Code, descend from Roman law. Rome's republican institutions have influenced theItalian city-state republicsof the medieval period, the earlyUnited States, and modern democraticrepublics.
Rome had begun expanding shortly after the founding of theRoman Republicin the 6th century BC, though not outside theItalian Peninsulauntil the 3rd century BC. Thus, it was an "empire" (a great power) long before it had an emperor.[23]The Republic was not a nation-state in the modern sense, but a network of self-ruled towns (with varying degrees of independence from theSenate) and provinces administered by military commanders. It was governed by annually electedmagistrates(Roman consulsabove all) in conjunction with the Senate.[24]The 1st century BC was a time of political and military upheaval, which ultimately led to rule by emperors.[25][26][27]The consuls' military power rested in the Roman legal concept ofimperium, meaning "command" (typically in a military sense).[28]Occasionally, successful consuls or generals were given the honorary titleimperator(commander); this is the origin of the wordemperor, since this title was always bestowed to the early emperors.[29][g]
Rome suffered a long series of internal conflicts, conspiracies, andcivil warsfrom the late second century BC (seeCrisis of the Roman Republic) while greatly extending its power beyond Italy. In 44 BCJulius Caesarwas brieflyperpetual dictatorbefore beingassassinatedby a faction that opposed his concentration of power. This faction was driven from Rome and defeated at theBattle of Philippiin 42 BC byMark Antonyand Caesar's adopted sonOctavian. Antony and Octaviandivided the Roman worldbetween them, but this did not last long. Octavian's forces defeated those of Mark Antony andCleopatraat theBattle of Actiumin 31 BC. In 27 BC theSenategave him the titleAugustus("venerated") and made himprinceps("foremost") withproconsularimperium, thus beginning thePrincipate, the first epoch of Roman imperial history. Although the republic stood in name, Augustus had all meaningful authority.[31]During his 40-year rule, a new constitutional order emerged so that, upon his death,Tiberiuswould succeed him as the newde factomonarch.[32]
The 200 years that began with Augustus's rule is traditionally regarded as thePax Romana("Roman Peace"). The cohesion of the empire was furthered by a degree of social stability and economic prosperity that Rome had never before experienced. Uprisings in the provinces were infrequent and put down "mercilessly and swiftly".[33]The success of Augustus in establishing principles of dynastic succession was limited by his outliving a number of talented potential heirs. TheJulio-Claudian dynastylasted for four more emperors—Tiberius,Caligula,Claudius, andNero—before it yielded in 69 AD to the strife-tornYear of the Four Emperors, from whichVespasianemerged as the victor. Vespasian became the founder of the briefFlavian dynasty, followed by theNerva–Antonine dynastywhich produced the "Five Good Emperors":Nerva,Trajan,Hadrian,Antoninus Pius, andMarcus Aurelius.[34]
In the view of contemporary Greek historianCassius Dio, the accession ofCommodusin 180 marked the descent "from a kingdom of gold to one of rust and iron",[35]a comment which has led some historians, notablyEdward Gibbon, to take Commodus' reign as the beginning of theEmpire's decline.[36][37]
In 212, during the reign ofCaracalla,Roman citizenshipwas granted to all freeborn inhabitants of the empire. TheSeveran dynastywas tumultuous; an emperor's reign was ended routinely by his murder or execution and, following its collapse, the Empire was engulfed by theCrisis of the Third Century, a period ofinvasions,civil strife,economic disorder, andplague.[38]In defininghistorical epochs, this crisis sometimes marks the transition fromClassicaltoLate Antiquity.Aurelian(r.270–275) stabilised the empire militarily andDiocletianreorganised and restored much of it in 285.[39]Diocletian's reign brought the empire's most concerted effort against the perceived threat ofChristianity, the "Great Persecution".[40]
Diocletian divided the empire into four regions, each ruled by a separatetetrarch.[41]Confident that he fixed the disorder plaguing Rome, he abdicated along with his co-emperor, but the Tetrarchycollapsed shortly after. Order was eventually restored byConstantine the Great, who became the first emperor toconvert to Christianity, and who establishedConstantinopleas the new capital of the Eastern Empire. During the decades of theConstantinianandValentiniandynasties, the empire was divided along an east–west axis, with dual power centres in Constantinople and Rome.Julian, who under the influence of his adviserMardoniusattempted to restoreClassical RomanandHellenistic religion, only briefly interrupted the succession of Christian emperors.Theodosius I, the last emperor to rule over both East and West, died in 395 after making Christianity thestate religion.[42]
TheWestern Roman Empirebegan todisintegratein the early 5th century. The Romans fought off all invaders, most famouslyAttila,[43]but the empire hadassimilated so many Germanic peoplesof dubious loyalty to Rome that the empire started to dismember itself.[44]Most chronologiesplace the end of the Western Roman Empire in 476, whenRomulus Augustuluswasforced to abdicateto theGermanicwarlordOdoacer.[45][46][47]
Odoacer ended the Western Empire by declaringZenosole emperor and placing himself as Zeno's nominal subordinate. In reality, Italy was ruled by Odoacer alone.[45][46][48]The Eastern Roman Empire, called theByzantine Empireby later historians, continued until the reign ofConstantine XI Palaiologos, the last Roman emperor. He died in battle in 1453 againstMehmed IIand hisOttomanforces during thesiege of Constantinople. Mehmed II adopted the title ofcaesarin an attempt to claim a connection to the former Empire.[49][50]His claim was soon recognized by thePatriarchate of Constantinople, but not by European monarchs.
The Roman Empire wasone of the largestin history, with contiguous territories throughout Europe, North Africa, and the Middle East.[51]The Latin phraseimperium sine fine("empire without end"[52]) expressed the ideology that neither time nor space limited the Empire. InVirgil'sAeneid, limitless empire is said to be granted to the Romans byJupiter.[53]This claim of universal dominion was renewed when the Empire came under Christian rule in the 4th century.[h]In addition to annexing large regions, the Romans directly altered their geography, for examplecutting down entire forests.[55]
Roman expansionwas mostly accomplished under theRepublic, though parts of northern Europe were conquered in the 1st century, when Roman control in Europe, Africa, and Asia was strengthened. UnderAugustus, a "global map of the known world" was displayed for the first time in public at Rome, coinciding with the creation of the most comprehensivepolitical geographythat survives from antiquity, theGeographyofStrabo.[56]When Augustus died, the account of his achievements (Res Gestae) prominently featured the geographical cataloguing of the Empire.[57]Geography alongside meticulous written records were central concerns ofRoman Imperial administration.[58]
The Empire reached its largest expanse underTrajan(r.98–117),[59]encompassing 5 million km2.[17][18]The traditional population estimate of55–60 millioninhabitants[60]accounted for between one-sixth and one-fourth of the world's total population[61]and made it the most populous unified political entity in the West until the mid-19th century.[62]Recentdemographic studieshave argued for a population peak from70 millionto more than100 million.[63]Each of the three largest cities in the Empire—Rome,Alexandria, andAntioch—was almost twice the size of any European city at the beginning of the 17th century.[64]
Then the empire stretched fromHadrian's Wallin drizzle-soakednorthern Englandto the sun-baked banks of theEuphratesin Syria; from the greatRhine–Danuberiver system, which snaked across the fertile, flat lands of Europe from theLow Countriesto theBlack Sea, to the rich plains of the North African coast and the luxuriant gash of theNile Valleyin Egypt. The empire completely circled theMediterranean... referred to by its conquerors asmare nostrum—'our sea'.[60]
Trajan's successorHadrianadopted a policy of maintaining rather than expanding the empire. Borders (fines) were marked, and the frontiers (limites) patrolled.[59]The most heavily fortified borders were the most unstable.[26]Hadrian's Wall, which separated the Roman world from what was perceived as an ever-presentbarbarianthreat, is the primary surviving monument of this effort.[66]
Latin and Greek were the main languages of the Empire,[i]but the Empire was deliberately multilingual.[71]Andrew Wallace-Hadrillsays "The main desire of the Roman government was to make itself understood".[72]At the start of the Empire, knowledge of Greek was useful to pass as educated nobility and knowledge of Latin was useful for a career in the military, government, or law.[73]Bilingual inscriptions indicate the everyday interpenetration of the two languages.[74]
Latin and Greek's mutual linguistic and cultural influence is a complex topic.[75]Latin words incorporated into Greek were very common by the early imperial era, especially for military, administration, and trade and commerce matters.[76]Greek grammar, literature, poetry and philosophy shaped Latin language and culture.[77][78]
There was never a legal requirement for Latin in the Empire, but it represented a certain status.[80]High standards of Latin,Latinitas, started with the advent of Latin literature.[81]Due to the flexible language policy of the Empire, a natural competition of language emerged that spurredLatinitas, to defend Latin against the stronger cultural influence of Greek.[82]Over time Latin usage was used to project power and a higher social class.[83][84]Most of the emperors were bilingual but had a preference for Latin in the public sphere for political reasons, a "rule" that first started during thePunic Wars.[85]Different emperors up until Justinian would attempt to require the use of Latin in various sections of the administration but there is no evidence that a linguistic imperialism existed during the early Empire.[86]
After all freeborn inhabitants were universallyenfranchisedin212, many Roman citizens would have lacked a knowledge of Latin.[87]The wide use ofKoine Greekwas what enabled the spread of Christianity and reflects its role as thelingua francaof the Mediterranean during the time of the Empire.[88]Following Diocletian's reforms in the 3rd century CE, there was a decline in the knowledge of Greek in the west.[89]Spoken Latin later fragmented into the incipientromance languagesin the 7th century CE following the collapse of the Empire's west.[90]
The dominance of Latin and Greek among the literate elite obscure the continuity of other spoken languages within the Empire.[91]Latin, referred to in its spoken form asVulgar Latin, gradually replacedCelticandItalic languages.[92][93]References to interpreters indicate the continuing use of local languages, particularly in Egypt withCoptic, and in military settings along the Rhine and Danube. Romanjuristsalso show a concern for local languages such asPunic,Gaulish, andAramaicin assuring the correct understanding of laws and oaths.[94]InAfrica, Libyco-Berber and Punic were used in inscriptions into the 2nd century.[91]InSyria,Palmyrenesoldiers used theirdialect of Aramaicfor inscriptions, an exception to the rule that Latin was the language of the military.[95]The last reference to Gaulish was between 560 and 575.[96][97]The emergentGallo-Romance languageswould then be shaped by Gaulish.[98]Proto-BasqueorAquitanianevolved with Latin loan words to modernBasque.[99]TheThracian language, as were several now-extinct languages in Anatolia, are attested in Imperial-era inscriptions.[88][91]
The Empire was remarkably multicultural, with "astonishing cohesive capacity" to create shared identity while encompassing diverse peoples.[101]Public monuments and communal spaces open to all—such asforums,amphitheatres,racetracksandbaths—helped foster a sense of "Romanness".[102]
Roman society had multiple, overlappingsocial hierarchies.[103]The civil war preceding Augustus caused upheaval,[104]but did not effect an immediateredistribution of wealthand social power. From the perspective of the lower classes, a peak was merely added to the social pyramid.[105]Personal relationships—patronage, friendship (amicitia),family,marriage—continued to influence politics.[106]By the time ofNero, however, it was not unusual to find a former slave who was richer than a freeborn citizen, or anequestrianwho exercised greater power than a senator.[107]
The blurring of the Republic's more rigid hierarchies led to increasedsocial mobility,[108]both upward and downward, to a greater extent than all other well-documented ancient societies.[109]Women, freedmen, and slaves had opportunities to profit and exercise influence in ways previously less available to them.[110]Social life, particularly for those whose personal resources were limited, was further fostered by a proliferation ofvoluntary associationsandconfraternities(collegiaandsodalitates): professional and trade guilds, veterans' groups, religious sodalities, drinking and dining clubs,[111]performing troupes,[112]andburial societies.[113]
According to thejurist Gaius, the essential distinction in the Roman "law of persons" was that all humans were either free (liberi) or slaves (servi).[114]The legal status of free persons was further defined by their citizenship. Most citizens held limited rights (such as theius Latinum, "Latin right"), but were entitled to legal protections and privileges not enjoyed by non-citizens. Free people not considered citizens, but living within the Roman world, wereperegrini, non-Romans.[115]In 212, theConstitutio Antoninianaextended citizenship to all freeborn inhabitants of the empire. This legal egalitarianism required a far-reaching revision of existing laws that distinguished between citizens and non-citizens.[116]
Freeborn Roman women were considered citizens, but did not vote, hold political office, or serve in the military. A mother's citizen status determined that of her children, as indicated by the phraseex duobus civibus Romanis natos("children born of two Roman citizens").[j]A Roman woman kept her ownfamily name(nomen) for life. Children most often took the father's name, with some exceptions.[119]Women could own property, enter contracts, and engage in business.[120]Inscriptions throughout the Empire honour women as benefactors in funding public works, an indication they could hold considerable fortunes.[121]
The archaicmanusmarriagein which the woman was subject to her husband's authority was largely abandoned by the Imperial era, and a married woman retained ownership of any property she brought into the marriage. Technically she remained under her father's legal authority, even though she moved into her husband's home, but when her father died she became legally emancipated.[122]This arrangement was a factor in the degree of independence Roman women enjoyed compared to many other cultures up to the modern period:[123]although she had to answer to her father in legal matters, she was free of his direct scrutiny in daily life,[124]and her husband had no legal power over her.[125]Although it was a point of pride to be a "one-man woman" (univira) who had married only once, there was little stigma attached todivorce, nor to speedy remarriage after being widowed or divorced.[126]Girls had equal inheritance rights with boys if their father died without leaving a will.[127]A mother's right to own and dispose of property, including setting the terms of her will, gave her enormous influence over her sons into adulthood.[128]
As part of the Augustan programme to restore traditional morality and social order,moral legislationattempted to regulate conduct as a means of promoting "family values".Adulterywas criminalized,[129]and defined broadly as an illicit sex act (stuprum) between a male citizen and a married woman, or between a married woman and any man other than her husband. That is, adouble standardwas in place: a married woman could have sex only with her husband, but a married man did not commit adultery if he had sex with a prostitute or person of marginalized status.[130]Childbearing was encouraged: a woman who had given birth to three children was granted symbolic honours and greater legal freedom (theius trium liberorum).[131]
At the time of Augustus, as many as 35% of the people inRoman Italywere slaves,[132]making Rome one of five historical "slave societies" in which slaves constituted at least a fifth of the population and played a major role in the economy.[k][132]Slavery was a complex institution that supported traditional Roman social structures as well as contributing economic utility.[133]In urban settings, slaves might be professionals such as teachers, physicians, chefs, and accountants; the majority of slaves provided trained or unskilled labour.Agricultureand industry, such as milling and mining, relied on the exploitation of slaves. Outside Italy, slaves were on average an estimated 10 to 20% of the population, sparse inRoman Egyptbut more concentrated in some Greek areas. Expanding Roman ownership of arable land and industries affected preexisting practices of slavery in the provinces.[134]Although slavery has often been regarded as waning in the 3rd and 4th centuries, it remained an integral part of Roman society until gradually ceasing in the 6th and 7th centuries with the disintegration of the complex Imperial economy.[135]
Laws pertaining to slavery were "extremely intricate".[136]Slaves were considered property and had nolegal personhood. They could be subjected to forms of corporal punishment not normally exercised on citizens,sexual exploitation, torture, andsummary execution. A slave could not as a matter of law be raped; a slave's rapist had to be prosecuted by the owner for property damage under theAquilian Law.[137]Slaves had no right to the form of legal marriage calledconubium, but their unions were sometimes recognized.[138]Technically, a slave could not own property,[139]but a slave who conducted business might be given access to an individual fund (peculium) that he could use, depending on the degree of trust and co-operation between owner and slave.[140]Within a household or workplace, a hierarchy of slaves might exist, with one slave acting as the master of others.[141]Talented slaves might accumulate a large enoughpeculiumto justify their freedom, or bemanumittedfor services rendered. Manumission had become frequent enough that in 2 BC a law (Lex Fufia Caninia) limited the number of slaves an owner was allowed to free in his will.[142]
Following theServile Warsof the Republic, legislation under Augustus and his successors shows a driving concern for controlling the threat of rebellions through limiting the size of work groups, and for hunting down fugitive slaves.[143]Over time slaves gained increased legal protection, including the right to file complaints against their masters. A bill of sale might contain a clause stipulating that the slave could not be employed for prostitution, asprostitutes in ancient Romewere often slaves.[144]The burgeoning trade ineunuchsin the late 1st century prompted legislation that prohibited thecastrationof a slave against his will "for lust or gain".[145]
Roman slavery was not based onrace.[146]Generally, slaves in Italy were indigenous Italians,[147]with a minority of foreigners (including both slaves and freedmen) estimated at 5% of the total in the capital at its peak, where their number was largest. Foreign slaves had higher mortality and lower birth rates than natives and were sometimes even subjected to mass expulsions.[148]The average recorded age at death for the slaves of the city of Rome was seventeen and a half years (17.2 for males; 17.9 for females).[149]
During the period of republican expansionism when slavery had become pervasive, war captives were a main source of slaves. The range of ethnicities among slaves to some extent reflected that of the armies Rome defeated in war, and theconquest of Greecebrought a number of highly skilled and educated slaves. Slaves were also traded in markets and sometimes sold bypirates.Infant abandonmentand self-enslavement among the poor were other sources.[150]Vernae, by contrast, were "homegrown" slaves born to female slaves within the household, estate or farm. Although they had no special legal status, an owner who mistreated or failed to care for hisvernaefaced social disapproval, as they were considered part of the family household and in some cases might actually be the children of free males in the family.[151]
Rome differed fromGreek city-statesin allowing freed slaves to become citizens; any future children of a freedman were born free, with full rights of citizenship. After manumission, a slave who had belonged to a Roman citizen enjoyed active political freedom (libertas), including the right to vote.[152]His former master became his patron (patronus): the two continued to have customary and legal obligations to each other.[153][154]A freedman was not entitled to hold public office or the highest state priesthoods, but could play apriestly role. He could not marry a woman from a senatorial family, nor achieve legitimate senatorial rank himself, but during the early Empire, freedmen held key positions in the government bureaucracy, so much so thatHadrianlimited their participation by law.[154]The rise of successful freedmen—through political influence or wealth—is a characteristic of early Imperial society. The prosperity of a high-achieving group of freedmen is attested byinscriptions throughout the Empire.
The Latin wordordo(pluralordines) is translated variously and inexactly into English as "class, order, rank". One purpose of theRoman censuswas to determine theordoto which an individual belonged.[155]Two of the highestordinesin Rome were the senatorial and equestrian. Outside Rome, cities or colonies were led bydecurions, also known ascuriales.[156]
"Senator" was not itself an elected office in ancient Rome; an individual gained admission to the Senate after he had been elected to and served at least one term as anexecutive magistrate. A senator also had to meet a minimum property requirement of 1 millionsestertii.[157]Not all men who qualified for theordo senatoriuschose to take a Senate seat, which requiredlegal domicileat Rome. Emperors often filled vacancies in the 600-member body by appointment.[158]A senator's son belonged to theordo senatorius, but he had to qualify on his own merits for admission to the Senate. A senator could be removed for violating moral standards.[159]
In the time of Nero, senators were still primarily fromItaly, with some from the Iberian peninsula and southern France; men from the Greek-speaking provinces of the East began to be added under Vespasian.[160]The first senator from the easternmost province,Cappadocia, was admitted under Marcus Aurelius.[l]By theSeveran dynasty(193–235), Italians made up less than half the Senate.[162]During the 3rd century, domicile at Rome became impractical, and inscriptions attest to senators who were active in politics and munificence in their homeland (patria).[159]
Senators were the traditional governing class who rose through thecursus honorum, the political career track, but equestrians often possessed greater wealth and political power. Membership in the equestrian order was based on property; in Rome's early days,equitesor knights had been distinguished by their ability to serve as mounted warriors, but cavalry service was a separate function in the Empire.[m]A census valuation of 400,000 sesterces and three generations of free birth qualified a man as an equestrian.[164]The census of 28 BC uncovered large numbers of men who qualified, and in 14 AD, a thousand equestrians were registered atCádizandPaduaalone.[n][166]Equestrians rose through a military career track (tres militiae) to become highly placedprefectsandprocuratorswithin the Imperial administration.[167]
The rise of provincial men to the senatorial and equestrian orders is an aspect of social mobility in the early Empire. Roman aristocracy was based on competition, and unlike laterEuropean nobility, a Roman family could not maintain its position merely through hereditary succession or having title to lands.[168]Admission to the higherordinesbrought distinction and privileges, but also responsibilities. In antiquity, a city depended on its leading citizens to fund public works, events, and services (munera). Maintaining one's rank required massive personal expenditures.[169]Decurions were so vital for the functioning of cities that in the later Empire, as the ranks of the town councils became depleted, those who had risen to the Senate were encouraged to return to their hometowns, in an effort to sustain civic life.[170]
In the later Empire, thedignitas("worth, esteem") that attended on senatorial or equestrian rank was refined further with titles such asvir illustris("illustrious man").[171]The appellationclarissimus(Greeklamprotatos) was used to designate thedignitasof certain senators and their immediate family, including women.[172]"Grades" of equestrian status proliferated.[173]
As the republican principle of citizens' equality under the law faded, the symbolic and social privileges of the upper classes led to an informal division of Roman society into those who had acquired greater honours (honestiores) and humbler folk (humiliores). In general,honestioreswere the members of the three higher "orders", along with certain military officers.[174]The granting of universal citizenship in 212 seems to have increased the competitive urge among the upper classes to have their superiority affirmed, particularly within the justice system.[175]Sentencing depended on the judgment of the presiding official as to the relative "worth" (dignitas) of the defendant: anhonestiorcould pay a fine for a crime for which anhumiliormight receive ascourging.[176]
Execution, which was an infrequent legal penalty for free men under the Republic,[177]could be quick and relatively painless forhonestiores, whilehumilioresmight suffer the kinds of torturous death previously reserved for slaves, such ascrucifixionandcondemnation to the beasts.[178]In the early Empire, those who converted to Christianity could lose their standing ashonestiores, especially if they declined to fulfil religious responsibilities, and thus became subject to punishments that created the conditions ofmartyrdom.[179]
The three major elements of the Imperial state were the central government, the military, and the provincial government.[180]The military established control of a territory through war, but after a city or people was brought under treaty, the mission turned to policing: protecting Roman citizens, agricultural fields, and religious sites.[181]The Romans lacked sufficient manpower or resources to rule through force alone.Cooperation with local eliteswas necessary to maintain order, collect information, and extract revenue. The Romans often exploited internal political divisions.[182]
Communities with demonstrated loyalty to Rome retained their own laws, could collect their own taxes locally, and in exceptional cases were exempt from Roman taxation. Legal privileges and relative independence incentivized compliance.[183]Roman government was thuslimited, but efficient in its use of available resources.[184]
TheImperial cult of ancient Romeidentifiedemperorsand some members of their families withdivinely sanctionedauthority (auctoritas). The rite ofapotheosis(also calledconsecratio) signified the deceased emperor's deification.[185]The dominance of the emperor was based on the consolidation of powers from several republican offices.[186]The emperor made himself the central religious authority aspontifex maximus, and centralized the right to declare war, ratify treaties, and negotiate with foreign leaders.[187]While these functions were clearly defined during thePrincipate, the emperor's powers over time became less constitutional and more monarchical, culminating in theDominate.[188]
The emperor was the ultimate authority in policy- and decision-making, but in the early Principate, he was expected to be accessible and deal personally with official business and petitions. A bureaucracy formed around him only gradually.[189]The Julio-Claudian emperors relied on an informal body of advisors that included not only senators and equestrians, but trusted slaves and freedmen.[190]After Nero, the influence of the latter was regarded with suspicion, and the emperor's council (consilium) became subject to official appointment for greatertransparency.[191]Though the Senate took a lead in policy discussions until the end of theAntonine dynasty, equestrians played an increasingly important role in theconsilium.[192]The women of the emperor's family often intervened directly in his decisions.[193]
Access to the emperor might be gained at the daily reception (salutatio), a development of the traditional homage a client paid to his patron; public banquets hosted at the palace; and religious ceremonies. The common people who lacked this access could manifest their approval or displeasure as a group atgames.[194]By the 4th century, the Christian emperors became remote figureheads who issued general rulings, no longer responding to individual petitions.[195]Although the Senate could do little short of assassination and open rebellion to contravene the will of the emperor, it retained its symbolic political centrality.[196]The Senate legitimated the emperor's rule, and the emperor employed senators as legates (legati): generals, diplomats, and administrators.[197]
The practical source of an emperor's power and authority was the military. Thelegionarieswere paid by the Imperial treasury, and swore an annualoath of loyaltyto the emperor.[198]Most emperors chose a successor, usually a close family member oradoptedheir. The new emperor had to seek a swift acknowledgement of his status and authority to stabilize the political landscape. No emperor could hope to survive without the allegiance of thePraetorian Guardand the legions. To secure their loyalty, several emperors paid thedonativum, a monetary reward. In theory, the Senate was entitled to choose the new emperor, but did so mindful of acclamation by the army or Praetorians.[199]
After thePunic Wars, the Roman army comprised professional soldiers who volunteered for 20 years of active duty and five as reserves. The transition to a professional military began during the late Republic and was one of the many profound shifts away from republicanism, under which an army ofconscript citizensdefended the homeland against a specific threat. The Romans expanded their war machine by "organizing the communities that they conquered in Italy into a system that generated huge reservoirs of manpower for their army".[200]By Imperial times, military service was a full-time career.[201]The pervasiveness of military garrisons throughout the Empire was a major influence in the process ofRomanization.[202]
The primary mission of the military of the early empire was to preserve thePax Romana.[203]The three major divisions of the military were:
Through his military reforms, which included consolidating or disbanding units of questionable loyalty, Augustus regularized the legion. A legion was organized into tencohorts, each of which comprised sixcenturies, with a century further made up of ten squads (contubernia); the exact size of the Imperial legion, which was likely determined bylogistics, has been estimated to range from 4,800 to 5,280.[204]After Germanic tribes wiped out three legions in theBattle of the Teutoburg Forestin 9 AD, the number of legions was increased from 25 to around 30.[205]The army had about 300,000 soldiers in the 1st century, and under 400,000 in the 2nd, "significantly smaller" than the collective armed forces of the conquered territories. No more than 2% of adult males living in the Empire served in the Imperial army.[206]Augustus also created thePraetorian Guard: nine cohorts, ostensibly to maintain the public peace, which were garrisoned in Italy. Better paid than the legionaries, the Praetorians served only sixteen years.[207]
Theauxiliawere recruited from among the non-citizens. Organized in smaller units of roughly cohort strength, they were paid less than the legionaries, and after 25 years of service were rewarded withRoman citizenship, also extended to their sons. According toTacitus[208]there were roughly as many auxiliaries as there were legionaries—thus, around 125,000 men, implying approximately 250 auxiliary regiments.[209]TheRoman cavalryof the earliest Empire were primarily from Celtic, Hispanic or Germanic areas. Several aspects of training and equipment derived from the Celts.[210]
TheRoman navynot only aided in the supply and transport of the legions but also in the protection of thefrontiersalong the riversRhineandDanube. Another duty was protecting maritime trade against pirates. It patrolled the Mediterranean, parts of theNorth Atlanticcoasts, and theBlack Sea. Nevertheless, the army was considered the senior and more prestigious branch.[211]
An annexed territory became aRoman provincein three steps: making a register of cities, taking a census, and surveying the land.[212]Further government recordkeeping included births and deaths, real estate transactions, taxes, and juridical proceedings.[213]In the 1st and 2nd centuries, the central government sent out around 160 officials annually to govern outside Italy.[24]Among these officials were theRoman governors:magistrates elected at Romewho in the name of theRoman peoplegovernedsenatorial provinces; or governors, usually of equestrian rank, who held theirimperiumon behalf of the emperor inimperial provinces, most notablyRoman Egypt.[214]A governor had to make himself accessible to the people he governed, but he could delegate various duties.[215]His staff, however, was minimal: his official attendants (apparitores), includinglictors, heralds, messengers,scribes, and bodyguards;legates, both civil and military, usually of equestrian rank; and friends who accompanied him unofficially.[215]
Other officials were appointed as supervisors of government finances.[24]Separating fiscal responsibility from justice and administration was a reform of the Imperial era, to avoid provincial governors andtax farmersexploiting local populations for personal gain.[216]Equestrianprocurators, whose authority was originally "extra-judicial and extra-constitutional", managed both state-owned property and the personal property of the emperor (res privata).[215]Because Roman government officials were few, a provincial who needed help with a legal dispute or criminal case might seek out any Roman perceived to have some official capacity.[217]
In the High Empire, Italy was legally distinguished from the provinces, and along with some favored provincial communities, enjoyed immunity from theproperty taxandpoll tax. However, under the EmperorDiocletian, Italy lost these privileges and was subdivided intoprovinces.[218]
Roman courts heldoriginal jurisdictionover cases involving Roman citizens throughout the empire, but there were too few judicial functionaries to impose Roman law uniformly in the provinces. Most parts of the Eastern Empire already had well-established law codes and juridical procedures.[104]Generally, it was Roman policy to respect themos regionis("regional tradition" or "law of the land") and to regard local laws as a source of legal precedent and social stability.[104][219]The compatibility of Roman and local law was thought to reflect an underlyingius gentium, the "law of nations" orinternational lawregarded as common and customary.[220]If provincial law conflicted with Roman law or custom, Roman courts heardappeals, and the emperor held final decision-making authority.[104][219][o]
In the West, law had been administered on a highly localized or tribal basis, andprivate property rightsmay have been a novelty of the Roman era, particularly amongCelts. Roman law facilitated the acquisition of wealth by a pro-Roman elite.[104]The extension of universal citizenship to all free inhabitants of the Empire in 212 required the uniform application of Roman law, replacing local law codes that had applied to non-citizens. Diocletian's efforts to stabilize the Empire after theCrisis of the Third Centuryincluded two major compilations of law in four years, theCodex Gregorianusand theCodex Hermogenianus, to guide provincial administrators in setting consistent legal standards.[221]
The pervasiveness of Roman law throughout Western Europe enormously influenced the Western legal tradition, reflected by continued use ofLatin legal terminologyin modern law.
Taxation under the Empire amounted to about 5% of itsgross product.[222]The typical tax rate for individuals ranged from 2 to 5%.[223]The tax code was "bewildering" in its complicated system ofdirectandindirect taxes, some paid in cash and somein kind. Taxes might be specific to a province, or kinds of properties such asfisheries; they might be temporary.[224]Tax collection was justified by the need to maintain the military,[225]and taxpayers sometimes got a refund if the army captured a surplus of booty.[226]In-kind taxes were accepted from less-monetizedareas, particularly those who could supply grain or goods to army camps.[227]
The primary source of direct tax revenue was individuals, who paid apoll taxand a tax on their land, construed as a tax on its produce or productive capacity.[223]Tax obligations were determined by the census: each head of household provided a headcount of his household, as well as an accounting of his property.[228]A major source of indirect-tax revenue was theportoria, customs and tolls on trade, including among provinces.[223]Towards the end of his reign, Augustus instituted a 4% tax on the sale of slaves,[229]which Nero shifted from the purchaser to the dealers, who responded by raising their prices.[230]An owner who manumitted a slave paid a "freedom tax", calculated at 5% of value.[p]Aninheritance taxof 5% was assessed when Roman citizens above a certain net worth left property to anyone outside their immediate family. Revenues from the estate tax and from an auction tax went towards the veterans' pension fund (aerarium militare).[223]
Low taxes helped the Roman aristocracy increase their wealth, which equalled or exceeded the revenues of the central government. An emperor sometimes replenished his treasury by confiscating the estates of the "super-rich", but in the later period, theresistanceof the wealthy to paying taxes was one of the factors contributing to the collapse of the Empire.[61]
The Empire is best thought of as a network of regional economies, based on a form of "political capitalism" in which the state regulated commerce to assure its own revenues.[231]Economic growth, though not comparable to modern economies, was greater than that of most other societies prior toindustrialization.[232]Territorial conquests permitted a large-scale reorganization ofland usethat resulted in agricultural surplus and specialization, particularly in north Africa.[233]Some cities were known for particular industries. The scale of urban building indicates a significant construction industry.[233]Papyri preserve complex accounting methods that suggest elements ofeconomic rationalism,[233]and the Empire was highly monetized.[234]Although the means of communication and transport were limited in antiquity, transportation in the 1st and 2nd centuries expanded greatly, and trade routes connected regional economies.[235]Thesupply contracts for the armydrew on local suppliers near the base (castrum), throughout the province, and across provincial borders.[236]Economic historiansvary in their calculations of the gross domestic product during the Principate.[237]In the sample years of 14, 100, and 150 AD, estimates of per capita GDP range from 166 to 380HS. The GDP per capita ofItalyis estimated as 40[238]to 66%[239]higher than in the rest of the Empire, due to tax transfers from the provinces and the concentration of elite income.
Economic dynamism resulted in social mobility. Although aristocratic values permeated traditional elite society, wealth requirements forrankindicate a strong tendency towardsplutocracy. Prestige could be obtained through investing one's wealth in grand estates or townhouses, luxury items,public entertainments, funerary monuments, andreligious dedications. Guilds (collegia) and corporations (corpora) provided support for individuals to succeed through networking.[174]"There can be little doubt that the lower classes of ... provincial towns of the Roman Empire enjoyed a highstandard of livingnot equaled again in Western Europe until the 19th century".[240]Households in the top 1.5% ofincome distributioncaptured about 20% of income. The "vast majority" produced more than half of the total income, but lived nearsubsistence.[241]
The early Empire was monetized to a near-universal extent, using money as a way to expresspricesanddebts.[243]Thesestertius(English "sesterces", symbolized asHS) was the basic unit of reckoning value into the 4th century,[244]though the silverdenarius, worth four sesterces, was also used beginning in theSeveran dynasty.[245]The smallest coin commonly circulated was the bronzeas, one-tenthdenarius.[246]Bullionandingotsseem not to have counted aspecunia("money") and were used only on the frontiers. Romans in the first and second centuries counted coins, rather than weighing them—an indication that the coin was valued on its face. This tendency towardsfiat moneyled to thedebasementof Roman coinage in the later Empire.[247]The standardization of money throughout the Empire promoted trade and market integration.[243]The high amount of metal coinage in circulation increased themoney supplyfor trading or saving.[248]Rome had nocentral bank, and regulation of the banking system was minimal. Banks of classical antiquity typically keptless in reservesthan the full total of customers' deposits. A typical bank had fairly limitedcapital, and often only one principal.Senecaassumes that anyone involved inRoman commerceneeds access tocredit.[247]A professionaldepositbanker received and held deposits for a fixed or indefinite term, and lent money to third parties. The senatorial elite were involved heavily in private lending, both as creditors and borrowers.[249]The holder of a debt could use it as a means of payment by transferring it to another party, without cash changing hands. Although it has sometimes been thought that ancient Rome lackeddocumentary transactions, the system of banks throughout the Empire permitted the exchange of large sums without physically transferring coins, in part because of the risks of moving large amounts of cash. Only one serious credit shortage is known to have occurred in the early Empire, in 33 AD;[250]generally, available capital exceeded the amount needed by borrowers.[247]The central government itself did not borrow money, and withoutpublic debthad to funddeficitsfrom cash reserves.[251]
Emperors of theAntonineandSeverandynasties debased the currency, particularly thedenarius, under the pressures of meeting military payrolls.[244]Sudden inflation underCommodusdamaged the credit market.[247]In the mid-200s, the supply ofspeciecontracted sharply.[244]Conditions during theCrisis of the Third Century—such as reductions in long-distance trade, disruption of mining operations, and the physical transfer of gold coinage outside the empire by invading enemies—greatly diminished the money supply and the banking sector.[244][247]Although Roman coinage had long been fiat money orfiduciary currency, general economic anxieties came to a head underAurelian, and bankers lost confidence in coins. DespiteDiocletian's introduction of the goldsolidusand monetary reforms, the credit market of the Empire never recovered its former robustness.[247]
The main mining regions of the Empire were the Iberian Peninsula (silver, copper, lead, iron and gold);[4]Gaul (gold, silver, iron);[252]Britain (mainly iron, lead, tin),[253]theDanubian provinces(gold, iron);[254]MacedoniaandThrace(gold, silver); and Asia Minor (gold, silver, iron, tin). Intensive large-scale mining—of alluvial deposits, and by means ofopen-cast miningandunderground mining—took place from the reign of Augustus up to the early 3rd century, when the instability of the Empire disrupted production.[citation needed]
Hydraulic miningallowedbaseandprecious metalsto be extracted on a proto-industrial scale.[255]The total annual iron output is estimated at 82,500tonnes.[256]Copper and lead production levels were unmatched until theIndustrial Revolution.[257][258][259][260]At its peak around the mid-2nd century, the Roman silver stock is estimated at 10,000 t, five to ten times larger than the combined silver mass ofmedieval Europeand theCaliphatearound 800 AD.[259][261]As an indication of the scale of Roman metal production, lead pollution in theGreenland ice sheetquadrupled over prehistoric levels during the Imperial era and dropped thereafter.[262]
The Empire completely encircled the Mediterranean, which they called "our sea" (Mare Nostrum).[263]Roman sailing vessels navigated the Mediterranean as well as major rivers.[64]Transport by water was preferred where possible, as moving commodities by land was more difficult.[264]Vehicles, wheels, and ships indicate the existence of a great number of skilled woodworkers.[265]
Land transport utilized the advanced system ofRoman roads, called "viae". These roads were primarily built for military purposes,[266]but also served commercial ends. The in-kind taxes paid by communities included the provision of personnel, animals, or vehicles for thecursus publicus, the state mail and transport service established by Augustus.[227]Relay stations were located along the roads every seven to twelveRoman miles, and tended to grow into villages or trading posts.[267]Amansio(pluralmansiones) was a privately run service station franchised by the imperial bureaucracy for thecursus publicus. The distance betweenmansioneswas determined by how far a wagon could travel in a day.[267]Carts were usually pulled by mules, travelling about 4 mph.[268]
Roman provinces traded among themselves, but trade extended outside the frontiers to regions as far away asChinaandIndia.[269]Chinese trade was mostly conducted overland through middle men along theSilk Road; Indian trade also occurred by sea fromEgyptianports. The maincommoditywas grain.[270]Also traded were olive oil, foodstuffs,garum(fish sauce), slaves, ore and manufactured metal objects, fibres and textiles, timber,pottery,glassware, marble,papyrus, spices andmateria medica, ivory, pearls, and gemstones.[271]Though most provinces could produce wine,regional varietalswere desirable and wine was a central trade good.[272]
Inscriptions record 268 different occupations in Rome and 85 in Pompeii.[206]Professional associations or trade guilds (collegia) are attested for a wide range of occupations, some quite specialized.[174]
Work performed by slaves falls into five general categories: domestic, with epitaphs recording at least 55 different household jobs;imperial or public service; urban crafts and services; agriculture; and mining. Convicts provided much of the labour in the mines or quarries, where conditions were notoriously brutal.[273]In practice, there was little division of labour between slave and free,[104]and most workers were illiterate and without special skills.[274]The greatest number of common labourers were employed in agriculture: in Italian industrial farming (latifundia), these may have been mostly slaves, but elsewhere slave farm labour was probably less important.[104]
Textile and clothing production was a major source of employment. Both textiles and finished garments were traded and products were often named for peoples or towns, like afashion "label".[275]Better ready-to-wear was exported by local businessmen (negotiatoresormercatores).[276]Finished garments might be retailed by their sales agents, byvestiarii(clothing dealers), or peddled by itinerant merchants.[276]Thefullers(fullones) and dye workers (coloratores) had their own guilds.[277]Centonariiwere guild workers who specialized in textile production and the recycling of old clothes intopieced goods.[q]
The chiefRoman contributions to architecturewere thearch,vault, anddome. Some Roman structures still stand today, due in part to sophisticated methods of making cements andconcrete.[280]Roman templesdevelopedEtruscanand Greek forms, with some distinctive elements.Roman roadsare considered the most advanced built until the early 19th century.[citation needed]
Roman bridgeswere among the first large and lasting bridges, built from stone (and in most cases concrete) with the arch as the basic structure. The largest Roman bridge wasTrajan's bridgeover the lower Danube, constructed byApollodorus of Damascus, which remained for over a millennium the longest bridge to have been built.[281]The Romans built manydams and reservoirsfor water collection, such as theSubiaco Dams, two of which fed theAnio Novus, one of the largest aqueducts of Rome.[282]
The Romans constructed numerousaqueducts.De aquaeductu, a treatise byFrontinus, who served aswater commissioner, reflects the administrative importance placed on the water supply. Masonry channels carried water along a precisegradient, usinggravityalone. It was then collected in tanks and fed through pipes to public fountains, baths,toilets, or industrial sites.[283]The main aqueducts in Rome were theAqua Claudiaand theAqua Marcia.[284]The complex system built to supply Constantinople had its most distant supply drawn from over 120 km away along a route of more than 336 km.[285]Roman aqueducts were built to remarkably finetolerance, and to a technological standard not equalled until modern times.[286]The Romans also used aqueducts in their extensive mining operations across the empire.[287]
Insulated glazing(or "double glazing") was used in the construction ofpublic baths. Elite housing in cooler climates might havehypocausts, a form of central heating. The Romans were the first culture to assemble all essential components of the much latersteam engine: the crank and connecting rod system,Hero'saeolipile(generating steam power), thecylinderandpiston(in metal force pumps), non-returnvalves(in water pumps), andgearing(in water mills and clocks).[288]
The city was viewed as fostering civilization by being "properly designed, ordered, and adorned".[289]Augustus undertook a vast building programme in Rome, supported public displays of art that expressed imperial ideology, andreorganized the cityinto neighbourhoods(vici)administered at the local level with police and firefighting services.[290]A focus of Augustan monumental architecture was theCampus Martius, an open area outside the city centre: the Altar of Augustan Peace (Ara Pacis Augustae) was located there, as wasan obeliskimported from Egypt that formed the pointer (gnomon) of ahorologium. With its public gardens, the Campus was among the most attractive places in Rome to visit.[290]
City planning and urban lifestyles was influenced by the Greeks early on,[291]and in the Eastern Empire, Roman rule shaped the development of cities that already had a strong Hellenistic character. Cities such asAthens,Aphrodisias,EphesusandGerasatailored city planning and architecture to imperial ideals, while expressing their individual identity and regional preeminence.[292]In areas inhabited by Celtic-speaking peoples, Rome encouraged the development of urban centres with stone temples, forums, monumental fountains, and amphitheatres, often on or near the sites of preexisting walled settlements known asoppida.[293][294][r]Urbanization in Roman Africa expanded on Greek and Punic coastal cities.[267]
The network of cities (coloniae,municipia,civitatesor in Greek termspoleis) was a primary cohesive force during the Pax Romana.[195]Romans of the 1st and 2nd centuries were encouraged to "inculcate the habits of peacetime".[296]As the classicistClifford Andonoted:
Most of the culturalappurtenancespopularly associated with imperial culture—public cultand itsgamesandcivic banquets, competitions for artists, speakers, and athletes, as well as the funding of the great majority of public buildings and public display of art—were financed by private individuals, whose expenditures in this regard helped to justify their economic power and legal and provincial privileges.[297]
In the city of Rome, most people lived in multistory apartment buildings (insulae) that were often squalid firetraps. Public facilities—such as baths (thermae), toilets with running water (latrinae), basins or elaborate fountains (nymphea) delivering fresh water,[294]and large-scale entertainments such aschariot racesandgladiator combat—were aimed primarily at the common people.[298]
The public baths served hygienic, social and cultural functions.[299]Bathing was the focus of daily socializing.[300]Roman baths were distinguished by a series of rooms that offered communal bathing in three temperatures, with amenities that might include anexercise room,sauna,exfoliationspa,ball court, or outdoor swimming pool. Baths hadhypocaustheating: the floors were suspended over hot-air channels.[301]Public baths were part of urban culturethroughout the provinces, but in the late 4th century, individual tubs began to replace communal bathing. Christians were advised to go to the baths only for hygiene.[302]
Rich families from Rome usually had two or more houses: a townhouse (domus) and at least one luxury home (villa) outside the city. Thedomuswas a privately owned single-family house, and might be furnished with a private bath (balneum),[301]but it was not a place to retreat from public life.[303]Although some neighbourhoods show a higher concentration of such houses, they were not segregated enclaves. Thedomuswas meant to be visible and accessible. The atrium served as a reception hall in which thepaterfamilias(head of household) met withclientsevery morning.[290]It was a centre of family religious rites, containing ashrineandimages of family ancestors.[304]The houses were located on busy public roads, and ground-level spaces were often rented out as shops (tabernae).[305]In addition to a kitchen garden—windowboxes might substitute in theinsulae—townhouses typically enclosed aperistylegarden.[306]
The villa by contrast was an escape from the city, and in literature represents a lifestyle that balances intellectual and artistic interests (otium) with an appreciation of nature and agriculture.[307]Ideally a villa commanded a view or vista, carefully framed by the architectural design.[308]
Augustus' programme of urban renewal, and the growth of Rome's population to as many as one million, was accompanied by nostalgia for rural life. Poetry idealized the lives of farmers and shepherds. Interior decorating often featured painted gardens, fountains, landscapes, vegetative ornament,[308]and animals, rendered accurately enough to be identified by species.[309]On a more practical level, the central government took an active interest in supportingagriculture.[310]Producing food was the priority of land use.[311]Larger farms (latifundia) achieved aneconomy of scalethat sustained urban life.[310]Small farmers benefited from the development of local markets in towns and trade centres. Agricultural techniques such ascrop rotationandselective breedingwere disseminated throughout the Empire, and new crops were introduced from one province to another.[312]
Maintaining an affordable food supply to the city of Rome had become a major political issue in the late Republic, when the state began to provide a grain dole (Cura Annonae) to citizens who registered for it[310](about 200,000–250,000 adult males in Rome).[313]The dole cost at least 15% of state revenues,[310]but improved living conditions among the lower classes,[314]and subsidized the rich by allowing workers to spend more of their earnings on the wine and olive oil produced on estates.[310]The grain dole also had symbolic value: it affirmed the emperor's position as universal benefactor, and the right of citizens to share in "the fruits of conquest".[310]Theannona, public facilities, and spectacular entertainments mitigated the otherwise dreary living conditions of lower-class Romans, and kept social unrest in check. The satiristJuvenal, however, saw "bread and circuses" (panem et circenses) as emblematic of the loss of republican political liberty:[315]
The public has long since cast off its cares: the people that once bestowed commands, consulships, legions and all else, now meddles no more and longs eagerly for just two things: bread and circuses.[316]
Epidemicswere common in the ancient world, and occasionalpandemicsin the Empire killed millions. The Roman population was unhealthy. About 20 percent—a large percentage by ancient standards—lived in cities, Rome being the largest. The cities were a "demographic sink": the death rate exceeded the birth rate and constant immigration was necessary to maintain the population. Average lifespan is estimated at the mid-twenties, and perhaps more than half of children died before reaching adulthood. Dense urban populations andpoor sanitationcontributed to disease. Land and sea connections facilitated and sped the transfer of infectious diseases across the empire's territories. The rich were not immune; only two of emperor Marcus Aurelius's fourteen children are known to have reached adulthood.[317]
The importance of a good diet to health was recognized by medical writers such asGalen(2nd century). Views on nutrition were influenced by beliefs likehumoral theory.[318]A good indicator of nutrition and disease burden is average height: the average Roman was shorter in stature than the population of pre-Roman Italian societies and medieval Europe.[319]
Most apartments in Rome lacked kitchens, though a charcoalbraziercould be used for rudimentary cookery.[320]Prepared food was sold at pubs and bars, inns, and food stalls (tabernae,cauponae,popinae,thermopolia).[321]Carryoutand restaurants were for the lower classes;fine diningappeared only at dinner parties in wealthy homes with achef(archimagirus) and kitchen staff,[322]or banquets hosted by social clubs (collegia).[323]
Most Romans consumed at least 70% of their dailycaloriesin the form of cereals andlegumes.[324]Puls(pottage) was considered the food of the Romans,[325]and could be elaborated to produce dishes similar topolentaorrisotto.[326]Urban populations and the military preferred bread.[324]By the reign ofAurelian, the state had begun to distribute theannonaas a daily ration of bread baked in state factories, and addedolive oil, wine, and pork to the dole.[327]
Roman literature focuses on the dining habits of the upper classes,[328]for whom the evening meal (cena) had important social functions.[329]Guests were entertained in a finely decorated dining room (triclinium) furnished with couches. By the late Republic, women dined, reclined, and drank wine along with men.[330]The poet Martial describes a dinner, beginning with thegustatio("tasting" or "appetizer") salad. The main course waskid, beans, greens, a chicken, and leftover ham, followed by a dessert of fruit and wine.[331]Roman "foodies" indulged inwild game,fowlsuch aspeacockandflamingo, large fish (mulletwas especially prized), andshellfish. Luxury ingredients were imported from the far reaches of empire.[332]A book-length collection of Roman recipes is attributed toApicius, a name for several figures in antiquity that became synonymous with "gourmet".[333]
Refined cuisine could be moralized as a sign of either civilized progress or decadent decline.[334]Most often, because of the importance of landowning in Roman culture, produce—cereals, legumes, vegetables, and fruit—were considered more civilized foods than meat. TheMediterranean staplesofbread,wine, andoilweresacralizedby Roman Christianity, while Germanic meat consumption became a mark ofpaganism.[335]Some philosophers and Christians resisted the demands of the body and the pleasures of food, and adoptedfastingas an ideal.[336]Food became simpler in general as urban life in the West diminished and trade routes were disrupted;[337]the Church formally discouragedgluttony,[338]and hunting andpastoralismwere seen as simple and virtuous.[337]
WhenJuvenalcomplained that the Roman people had exchanged their political liberty for "bread and circuses", he was referring to the state-provided grain dole and thecircenses, events held in the entertainment venue called acircus. The largest such venue in Rome was theCircus Maximus, the setting ofhorse races,chariot races, the equestrianTroy Game, staged beast hunts (venationes), athletic contests,gladiator combat, andhistorical re-enactments. From earliest times, severalreligious festivalshad featured games (ludi), primarily horse and chariot races (ludi circenses).[339]The races retained religious significance in connection with agriculture,initiation, and the cycle of birth and death.[s]
Under Augustus, public entertainments were presented on 77 days of the year; by the reign of Marcus Aurelius, this had expanded to 135.[341]Circus games were preceded by an elaborate parade (pompa circensis) that ended at the venue.[342]Competitive events were held also in smaller venues such as theamphitheatre, which became the characteristic Roman spectacle venue, and stadium. Greek-style athletics includedfootraces,boxing,wrestling, and thepancratium.[343]Aquatic displays, such as the mock sea battle (naumachia) and a form of "water ballet", were presented in engineered pools.[344]State-supportedtheatrical events(ludi scaenici) took place on temple steps or in grand stone theatres, or in the smaller enclosed theatre called anodeon.[345]
Circuses were the largest structure regularly built in the Roman world.[346]The Flavian Amphitheatre, better known as theColosseum, became the regular arena for blood sports in Rome.[347]ManyRoman amphitheatres,circusesandtheatresbuilt in cities outside Italy are visible as ruins today.[347]The local ruling elite were responsible for sponsoring spectacles and arena events, which both enhanced their status and drained their resources.[178]The physical arrangement of the amphitheatre represented the order of Roman society: the emperor in his opulent box; senators and equestrians in reserved advantageous seats; women seated at a remove from the action; slaves given the worst places, and everybody else in-between.[348]The crowd could call for an outcome by booing or cheering, but the emperor had the final say. Spectacles could quickly become sites of social and political protest, and emperors sometimes had to deploy force to put down crowd unrest, most notoriously at theNika riotsin 532.[349]
The chariot teams were known by thecolours they wore. Fan loyalty was fierce and at times erupted intosports riots.[351]Racing was perilous, but charioteers were among the most celebrated and well-compensated athletes.[352]Circuses were designed to ensure that no team had an unfair advantage and to minimize collisions (naufragia),[353]which were nonetheless frequent and satisfying to the crowd.[354]The races retained a magical aura through their early association withchthonicrituals: circus images were considered protective or lucky,curse tabletshave been found buried at the site of racetracks, and charioteers were often suspected of sorcery.[355]Chariot racing continued into the Byzantine period under imperial sponsorship, but the decline of cities in the 6th and 7th centuries led to its eventual demise.[346]
The Romans thought gladiator contests had originated withfuneral gamesandsacrifices. Some of the earlieststyles of gladiator fightinghad ethnic designations such as "Thracian" or "Gallic".[356]The staged combats were consideredmunera, "services, offerings, benefactions", initially distinct from the festival games (ludi).[357]To mark the opening of the Colosseum,Tituspresented100 days of arena events, with 3,000 gladiators competing on a single day.[358]Roman fascination with gladiators is indicated by how widely they are depicted on mosaics, wall paintings, lamps, and in graffiti.[359]Gladiators were trained combatants who might be slaves, convicts, or free volunteers.[360]Death was not a necessary or even desirable outcome in matches between these highly skilled fighters, whose training was costly and time-consuming.[361]By contrast,noxiiwere convicts sentenced to the arena with little or no training, often unarmed, and with no expectation of survival; physical suffering and humiliation were considered appropriateretributive justice.[178]These executions were sometimes staged or ritualized as re-enactments ofmyths, and amphitheatres were equipped with elaboratestage machineryto create special effects.[178][362]
Modern scholars have found the pleasure Romans took in the "theatre of life and death"[363]difficult to understand.[364]Pliny the Youngerrationalized gladiator spectacles as good for the people, "to inspire them to face honourable wounds and despise death, by exhibiting love of glory and desire for victory".[365]Some Romans such asSenecawere critical of the brutal spectacles, but found virtue in the courage and dignity of the defeated fighter[366]—an attitude that finds its fullest expression with theChristians martyredin the arena. Tertullian considered deaths in the arena to be nothing more than a dressed-up form ofhuman sacrifice.[367]Evenmartyr literature, however, offers "detailed, indeed luxuriant, descriptions of bodily suffering",[368]and became a popular genre at times indistinguishable from fiction.[369]
The singularludus, "play, game, sport, training", had a wide range of meanings such as "word play", "theatrical performance", "board game", "primary school", and even "gladiator training school" (as inLudus Magnus).[370]Activities for children and young people in the Empire includedhoop rollingandknucklebones(astragalior "jacks"). Girls haddollsmade of wood,terracotta, and especiallybone and ivory.[371]Ball games includetrigonandharpastum.[372]People of all ages playedboard games, includinglatrunculi("Raiders") andXII scripta("Twelve Marks").[373]A game referred to asalea(dice) ortabula(the board) may have been similar tobackgammon.[374]Dicingas a form of gambling was disapproved of, but was a popular pastime during the festival of theSaturnalia.[375]
After adolescence, most physical training for males was of a military nature. TheCampus Martiusoriginally was an exercise field where young men learned horsemanship and warfare. Hunting was also considered an appropriate pastime. According toPlutarch, conservative Romans disapproved of Greek-style athletics that promoted a fine body for its own sake, and condemnedNero's efforts to encourage Greek-style athletic games.[376]Some women trained as gymnasts and dancers, and a rare few asfemale gladiators. The "Bikini Girls" mosaic shows young women engaging in routines comparable torhythmic gymnastics.[t][378]Women were encouraged to maintain health through activities such as playing ball, swimming, walking, or reading aloud (as a breathing exercise).[379]
In a status-conscious society like that of the Romans, clothing and personal adornment indicated the etiquette of interacting with the wearer.[380]Wearing the correct clothing reflected a society in good order.[381]There is little direct evidence of how Romans dressed in daily life, since portraiture may show the subject in clothing with symbolic value, and surviving textiles are rare.[382][383]
Thetogawas the distinctive national garment of the male citizen, but it was heavy and impractical, worn mainly for conducting political or court business and religious rites.[384][382]It was a "vast expanse" of semi-circular white wool that could not be put on and draped correctly without assistance.[384]The drapery became more intricate and structured over time.[385]Thetoga praetexta, with apurple or purplish-redstripe representing inviolability, was worn by children who had not come of age,curule magistrates, and state priests. Only the emperor could wear an all-purple toga (toga picta).[386]
Ordinary clothing was dark or colourful. The basic garment for all Romans, regardless of gender or wealth, was the simple sleevedtunic, with length differing by wearer.[387]The tunics of poor people and labouring slaves were made from coarse wool in natural, dull shades; finer tunics were made of lightweight wool or linen. A man of the senatorial or equestrian order wore a tunic with two purple stripes (clavi) woven vertically: the wider the stripe, the higher the wearer's status.[387]Other garments could be layered over the tunic. Common male attire also included cloaks, and in some regions,trousers.[388]In the 2nd century, emperors and elite men are often portrayed wearing thepallium, an originally Greek mantle; women are also portrayed in the pallium.Tertullianconsidered the pallium an appropriate garment both for Christians, in contrast to the toga, and for educated people.[381][382][389]
Roman clothing styles changed over time.[390]In theDominate, clothing worn by both soldiers and bureaucrats became highly decorated with geometrical patterns, stylized plant motifs, and in more elaborate examples, human or animal figures.[391]Courtiers of the later Empire wore elaborate silk robes. The militarization of Roman society, and the waning of urban life, affected fashion: heavy military-style belts were worn by bureaucrats as well as soldiers, and the toga was abandoned,[392]replaced by the pallium as a garment embodying social unity.[393]
Greek arthad a profound influence on Roman art.[394]Public art—includingsculpture, monuments such asvictory columnsortriumphal arches, and the iconography oncoins—is often analysed for historical or ideological significance.[395]In the private sphere, artistic objects were made forreligious dedications,funerary commemoration, domestic use, and commerce.[396]The wealthy advertised their appreciation of culture through artwork anddecorative artsin their homes.[397]Despite the value placed on art, even famous artists were of low social status, partly as they worked with their hands.[398]
Portraiture, which survives mainly in sculpture, was the most copious form of imperial art. Portraits during the Augustan period utilizeclassical proportions, evolving later into a mixture of realism and idealism.[399]Republican portraits were characterized byverism, but as early as the 2nd century BC, Greekheroic nuditywas adopted for conquering generals.[400]Imperial portrait sculptures may model a mature head atop a youthful nude or semi-nude body with perfect musculature.[401]Clothed in the toga or military regalia, the body communicates rank or role, not individual characteristics.[402]
Portraiture in painting is represented primarily by theFayum mummy portraits, which evoke Egyptian and Roman traditions of commemorating the dead with realistic painting. Marble portrait sculpture were painted, but traces have rarely survived.[403]
Examples of Roman sculpture survive abundantly, though often in damaged or fragmentary condition, including freestanding statuary in marble, bronze andterracotta, andreliefsfrom public buildings and monuments. Niches in amphitheatres were originally filled with statues,[404][405]as wereformal gardens.[406]Temples housed cult images of deities, often by famed sculptors.[407]
Elaborately carved marble and limestonesarcophagiare characteristic of the 2nd to 4th centuries.[408]Sarcophagus relief has been called the "richest single source of Roman iconography",[409]depictingmythological scenes[410]or Jewish/Christian imagery[411]as well as the deceased's life.
Initial Roman painting drew fromEtruscanandGreekmodels and techniques. Examples of Roman paintings can be found inpalaces,catacombsandvillas. Much of what is known of Roman painting is from the interior decoration of private homes, particularly as preserved by theeruption of Vesuvius. In addition to decorative borders and panels with geometric or vegetative motifs, wall painting depicts scenes from mythology and theatre, landscapes and gardens,spectacles, everyday life, anderotic art.
Mosaicsare among the most enduring of Romandecorative arts, and are found on floors and other architectural features. The most common is thetessellated mosaic, formed from uniform pieces(tesserae)of materials such as stone and glass.[413]Opus sectileis a related technique in which flat stone, usually coloured marble, is cut precisely into shapes from which geometric or figurative patterns are formed. This more difficult technique became especially popular for luxury surfaces in the 4th century (e.g. theBasilica of Junius Bassus).[414]
Figurativemosaics share many themes with painting, and in some cases use almost identicalcompositions. Geometric patterns and mythological scenes occur throughout the Empire. In North Africa, a particularly rich source of mosaics, homeowners often chose scenes of life on their estates, hunting, agriculture, and local wildlife.[412]Plentiful and major examples of Roman mosaics come also from present-day Turkey (particularly the (Antioch mosaics[415]), Italy, southern France, Spain, and Portugal.
Decorative artsfor luxury consumers included fine pottery, silver and bronze vessels and implements, and glassware. Pottery manufacturing was economically important, as were the glass and metalworking industries. Imports stimulated new regional centres of production. Southern Gaul became a leading producer of the finer red-gloss pottery (terra sigillata) that was a major trade good in 1st-century Europe.[416]Glassblowingwas regarded by the Romans as originating in Syria in the 1st century BC, and by the 3rd century, Egypt and theRhinelandhad become noted for fine glass.[417]
In Roman tradition, borrowed from the Greeks, literary theatre was performed by all-male troupes that used face masks with exaggerated facial expressions to portray emotion. Female roles were played by men indrag(travesti).[418]Roman literary theatre tradition is represented inLatin literatureby the tragedies ofSeneca, for example.
More popular than literary theatre was the genre-defyingmimustheatre, which featured scripted scenarios with free improvisation, risqué language and sex scenes, action sequences, and political satire, along with dance, juggling, acrobatics, tightrope walking, striptease, anddancing bears.[419]Unlike literary theatre,mimuswas played without masks, and encouraged stylistic realism. Female roles were performed by women.[420]Mimuswas related topantomimus, an early form ofstory balletthat contained no spoken dialogue but rather a sunglibretto, often mythological, either tragic or comic.[421]
Although sometimes regarded as foreign,musicand dance existed in Rome from earliest times.[422]Music was customary at funerals, and thetibia, a woodwind instrument, was played at sacrifices.[423]Song(carmen)was integral to almost every social occasion. Music was thought to reflect the orderliness of the cosmos.[424]Various woodwinds and"brass" instrumentswere played, as werestringed instrumentssuch as thecithara, and percussion.[423]Thecornu, a long tubular metal wind instrument, was used for military signals and on parade.[423]These instruments spread throughout the provinces and are widely depicted in Roman art.[425]The hydraulic pipe organ(hydraulis)was "one of the most significant technical and musical achievements of antiquity", and accompanied gladiator games and events in the amphitheatre.[423]Although certain dances were seen at times as non-Roman or unmanly, dancing was embedded in religious rituals of archaic Rome.[426]Ecstatic dancing was a feature of themystery religions, particularly the cults ofCybele[427]andIsis. In the secular realm, dancing girls fromSyriaandCadizwere extremely popular.[428]
Likegladiators, entertainers were legallyinfames, technically free but little better than slaves. "Stars", however, could enjoy considerable wealth and celebrity, and mingled socially and often sexually with the elite.[429]Performers supported each other by forming guilds, and several memorials for theatre members survive.[430]Theatre and dance were often condemned byChristian polemicistsin the later Empire.[422][431]
Estimates of the averageliteracy raterange from 5 to over 30%.[432][433][434]The Roman obsession with documents and inscriptions indicates the value placed on the written word.[435][436][u]Laws and edicts were posted as well as read out. Illiterate Roman subjects could have a government scribe (scriba) read or write their official documents for them.[433][438]The military produced extensive written records.[439]TheBabylonian Talmuddeclared "if all seas were ink, all reeds were pen, all skies parchment, and all men scribes, they would be unable to set down the full scope of the Roman government's concerns".[440]
Numeracywas necessary for commerce.[436][441]Slaves were numerate and literate in significant numbers; some were highly educated.[442]Graffiti and low-quality inscriptions with misspellings andsolecismsindicate casual literacy among non-elites.[443][v][93]
The Romans had an extensivepriestly archive, and inscriptions appear throughout the Empire in connection withvotivesdedicated by ordinary people, as well as "magic spells" (e.g. theGreek Magical Papyri).[444]
Books were expensive, since each copy had to be written out on a papyrus roll (volumen) by scribes.[445]Thecodex—pages bound to a spine—was still a novelty in the 1st century,[446]but by the end of the 3rd century was replacing thevolumen.[447]Commercial book production was established by the late Republic,[448]and by the 1st century certain neighbourhoods of Rome and Western provincial cities were known for their bookshops.[449]The quality of editing varied wildly,[450]andplagiarismorforgerywere common, since there was nocopyright law.[448]
Collectors amassed personal libraries,[451]and a fine library was part of the cultivated leisure (otium) associated with the villa lifestyle.[452]Significant collections might attract "in-house" scholars,[453]and an individual benefactor might endow a community with a library (asPliny the Youngerdid inComum).[454]Imperial libraries were open to users on a limited basis, and represented aliterary canon.[455]Books considered subversive might be publicly burned,[456]andDomitiancrucified copyists for reproducing works deemed treasonous.[457]
Literary texts were often shared aloud at meals or with reading groups.[458]Public readings (recitationes) expanded from the 1st through the 3rd century, giving rise to "consumer literature" for entertainment.[459]Illustrated books, including erotica, were popular, but are poorly represented by extant fragments.[460]
Literacy began to decline during theCrisis of the Third Century.[461]The emperor Julian banned Christians from teaching the classical curriculum,[462]but theChurch Fathersand other Christians adopted Latin and Greek literature, philosophy and science in biblical interpretation.[463]As the Western Roman Empire declined, reading became rarer even for those within the Church hierarchy,[464]although it continued in theByzantine Empire.[465]
Traditional Roman education was moral and practical. Stories were meant to instil Roman values (mores maiorum). Parents were expected to act as role models, and working parents passed their skills to their children, who might also enter apprenticeships.[467]Young children were attended by apedagogue, usually a Greek slave or former slave,[468]who kept the child safe, taught self-discipline and public behaviour, attended class and helped with tutoring.[469]
Formal education was available only to families who could pay for it; lack of state support contributed to low literacy.[470]Primary education in reading, writing, and arithmetic might take place at home if parents hired or bought a teacher.[471]Other children attended "public" schools organized by a schoolmaster (ludimagister) paid by parents.[472]Vernae(homeborn slave children) might share in-home or public schooling.[473]Boys and girls received primary education generally from ages 7 to 12, but classes were not segregated by grade or age.[474]Most schools employedcorporal punishment.[475]For the socially ambitious, education in Greek as well as Latin was necessary.[476]Schools became more numerous during the Empire, increasing educational opportunities.[476]
At the age of 14, upperclass males made theirrite of passageinto adulthood, and began to learn leadership roles through mentoring from a senior family member or family friend.[477]Higher education was provided bygrammaticiorrhetores.[478]Thegrammaticusor "grammarian" taught mainly Greek and Latin literature, with history, geography, philosophy or mathematics treated as explications of the text.[479]With the rise of Augustus, contemporary Latin authors such as Virgil and Livy also became part of the curriculum.[480]Therhetorwas a teacher of oratory or public speaking. The art of speaking (ars dicendi) was highly prized, andeloquentia("speaking ability, eloquence") was considered the "glue" of civilized society.[481]Rhetoric was not so much a body of knowledge (though it required a command of theliterary canon[482]) as it was a mode of expression that distinguished those who held social power.[483]The ancient model of rhetorical training—"restraint, coolness under pressure, modesty, and good humour"[484]—endured into the 18th century as a Western educational ideal.[485]
In Latin,illiteratuscould mean both "unable to read and write" and "lacking in cultural awareness or sophistication".[486]Higher education promoted career advancement.[487]Urban elites throughout the Empire shared a literary culture imbued with Greek educational ideals (paideia).[488]Hellenistic cities sponsored schools of higher learning to express cultural achievement.[489]Young Roman men often went abroad to study rhetoric and philosophy, mostly to Athens. The curriculum in the East was more likely to include music and physical training.[490]On the Hellenistic model, Vespasianendowed chairsof grammar, Latin and Greek rhetoric, and philosophy at Rome, and gave secondary teachers special exemptions from taxes and legal penalties.[491]In the Eastern Empire,Berytus(present-dayBeirut) was unusual in offering a Latin education, and became famous for itsschool of Roman law.[492]The cultural movement known as theSecond Sophistic(1st–3rd century AD) promoted the assimilation of Greek and Roman social, educational, and esthetic values.[493]
Literate women ranged from cultured aristocrats to girls trained to becalligraphersandscribes.[494][495]The ideal woman in Augustan love poetry was educated and well-versed in the arts.[496]Education seems to have been standard for daughters of the senatorial and equestrian orders.[473]An educated wife was an asset for the socially ambitious household.[494]
Literature under Augustus, along with that of the Republic, has been viewed as the "Golden Age" of Latin literature, embodyingclassical ideals.[497]The three most influential Classical Latin poets—Virgil,Horace, andOvid—belong to this period. Virgil'sAeneidwas a national epic in the manner of theHomeric epicsof Greece. Horace perfected the use ofGreek lyricmetresin Latin verse. Ovid's erotic poetry was enormously popular, but ran afoul of Augustan morality, contributing to his exile. Ovid'sMetamorphoseswove togetherGreco-Roman mythology; his versions ofGreek mythsbecame a primary source of laterclassical mythology, and his work was hugely influential onmedieval literature.[498]The earlyPrincipateproducedsatiristssuch asPersiusandJuvenal.
The mid-1st through mid-2nd century has conventionally been called the "Silver Age" of Latin literature. The three leading writers—Seneca,Lucan, andPetronius—committed suicide after incurringNero's displeasure.Epigrammatistand social observerMartialand the epic poetStatius, whose poetry collectionSilvaeinfluencedRenaissance literature,[499]wrote during the reign ofDomitian. Other authors of the Silver Age includedPliny the Elder, author of the encyclopedicNatural History; his nephew,Pliny the Younger; and the historianTacitus.
The principal Latin prose author of theAugustan ageis thehistorianLivy, whose account ofRome's foundingbecame the most familiar version in modern-era literature.The Twelve CaesarsbySuetoniusis a primary source for imperial biography. Among Imperial historians who wrote in Greek areDionysius of Halicarnassus,Josephus, andCassius Dio. Other major Greek authors of the Empire include the biographerPlutarch, the geographerStrabo, and the rhetorician and satiristLucian.
From the 2nd to the 4th centuries, Christian authors were in active dialogue with theclassical tradition.Tertullianwas one of the earliest prose authors with a distinctly Christian voice. After theconversion of Constantine, Latin literature is dominated by the Christian perspective.[500]In the late 4th century,Jeromeproduced the Latin translation of the Bible that became authoritative as theVulgate. Around that same time,AugustinewroteThe City of God against the Pagans, considered "a masterpiece of Western culture".[501]
In contrast to the unity of Classical Latin, the literary esthetic of late antiquity has atessellatedquality.[502]A continuing interest in the religious traditions of Rome prior to Christian dominion is found into the 5th century, with theSaturnaliaofMacrobiusandThe Marriage of Philology and MercuryofMartianus Capella. Latin poets of late antiquity includeAusonius,Prudentius,Claudian, andSidonius Apollinaris.
The Romans thought of themselves as highly religious, and attributed their success to their collective piety (pietas) and good relations with the gods (pax deorum). The archaic religion believed to have come from the earliestkings of Romewas the foundation of themos maiorum, "the way of the ancestors", central to Roman identity.[503]
Roman religion was practical and contractual, based on the principle ofdo ut des, "I give that you might give". Religion depended on knowledge and thecorrect practiceof prayer, ritual, and sacrifice, not on faith or dogma, although Latin literature preserves learned speculation on the nature of the divine. For ordinary Romans, religion was a part of daily life.[504]Each home had a household shrine to offer prayers andlibationsto the family's domestic deities. Neighbourhood shrines and sacred places such as springs and groves dotted the city. TheRoman calendarwas structured around religious observances; as many as 135 days were devoted toreligious festivalsand games (ludi).[505]
In the wake of theRepublic's collapse, state religion adapted to support the new regime. Augustus justified one-man rule with a vast programme of religious revivalism and reform.Public vowsnow were directed at the wellbeing of the emperor. So-called "emperor worship" expanded on a grand scale the traditionalveneration of the ancestral deadand of theGenius, the divinetutelaryof every individual. Upon death, an emperor could be made a state divinity (divus) by vote of the Senate. TheRoman imperial cult, influenced byHellenistic ruler cult, became one of the major ways Rome advertised its presence in the provinces and cultivated shared cultural identity. Cultural precedent in the Eastern provinces facilitated a rapid dissemination of Imperial cult, extending as far asNajran, in present-daySaudi Arabia.[w]Rejection of the state religion became tantamount to treason.
The Romans are known for thegreat number of deitiesthey honoured. As the Romans extended their territories, their general policy was to promote stability among diverse peoples by absorbing local deities and cults rather than eradicating them,[x]building temples that framed local theology within Roman religion. Inscriptions throughout the Empire record the side-by-side worship of local and Roman deities, including dedications made by Romans to local gods.[507]By the height of the Empire, numeroussyncretic or reinterpreted godswere cultivated, among them cults ofCybele,Isis,Epona, and of solar gods such asMithrasandSol Invictus, found as far north asRoman Britain. Because Romans had never been obligated to cultivate one god or cult only,religious tolerancewas not an issue.[508]
Mystery religions, which offered initiates salvation in the afterlife, were a matter of personal choice, practiced in addition to one'sfamily ritesand public religion. The mysteries, however, involved exclusive oaths and secrecy, which conservative Romans viewed with suspicion as characteristic of "magic", conspiracy (coniuratio), and subversive activity. Thus, sporadic and sometimes brutal attempts were made to suppress religionists. In Gaul, the power of thedruidswas checked, first by forbidding Roman citizens to belong to the order, and then by banning druidism altogether. However, Celtic traditions were reinterpreted within the context of Imperial theology, and a newGallo-Roman religioncoalesced; its capital at theSanctuary of the Three Gaulsestablished precedent for Western cult as a form of Roman-provincial identity.[509]The monotheistic rigour ofJudaismposed difficulties for Roman policy that led at times to compromise and granting of special exemptions. Tertullian noted that Judaism, unlike Christianity, was considered areligio licita, "legitimate religion". TheJewish–Roman warsresulted from political as well as religious conflicts; thesiege of Jerusalemin 70 AD led to the sacking of theSecond Templeand the dispersal of Jewish political power (seeJewish diaspora).
Christianity emerged inRoman Judaeaas aJewish religious sectin the 1st century and graduallyspreadout ofJerusalemthroughout the Empire and beyond. Imperially authorized persecutions were limited and sporadic, with martyrdoms occurring most often under the authority of local officials.[510]Tacitusreports that after theGreat Fire of Romein AD 64, the emperor attempted to deflect blame from himself onto the Christians.[511]A major persecution occurred under the emperorDomitian[512]and apersecution in 177took place at Lugdunum, the Gallo-Roman religious capital. A letter fromPliny the Younger, governor ofBithynia, describes his persecution and executions of Christians.[513]TheDecian persecutionof 246–251 seriously threatened theChristian Church, but ultimately strengthened Christian defiance.[514]Diocletianundertook themost severe persecution of Christians, from 303 to 311.[15]
From the 2nd century onward, theChurch Fatherscondemned the diverse religions practiced throughout the Empire as "pagan".[515]In the early 4th century,Constantine Ibecame the first emperor toconvert to Christianity. He supported the Church financially and made laws that favored it, but the new religion was already successful, having moved from less than 50,000 to over a million adherents between 150 and 250.[516]Constantine and his successors banned public sacrifice while tolerating other traditional practices. Constantine never engaged in apurge,[517]there were no "pagan martyrs" during his reign,[518]and people who had not converted to Christianity remained in important positions at court.[517]: 302Julianattempted to revive traditional public sacrifice andHellenistic religion, but met Christian resistance and lack of popular support.[519]
Christians of the 4th century believed the conversion of Constantine showed that Christianity had triumphed over paganism (in Heaven) and little further action besides such rhetoric was necessary.[521]Thus, their focus shifted towardsheresy.[522][523]According toPeter Brown, "In most areas, polytheists were not molested, and apart from a few ugly incidents of local violence, Jewish communities also enjoyed a century of stable, even privileged, existence".[523]: 641–643[524]There were anti-pagan laws, but they were not generally enforced; through the 6th century, centers of paganism existed in Athens, Gaza, Alexandria, and elsewhere.[525]
According to recent Jewish scholarship, toleration of the Jews was maintained under Christian emperors.[526]This did not extend toheretics:[526]Theodosius Imade multiple laws and acted against alternate forms of Christianity,[527]and heretics were persecuted and killed by both the government and the church throughout Late Antiquity. Non-Christians were not persecuted until the 6th century. Rome's original religious hierarchy and ritual influenced Christian forms,[528][529]and many pre-Christian practices survived in Christian festivals and local traditions.
Several states claimed to be theRoman Empire's successor. TheHoly Roman Empirewas established in 800 whenPope Leo IIIcrownedCharlemagneasRoman emperor. TheRussian Tsardom, as inheritor of the Byzantine Empire'sOrthodox Christiantradition, counted itself theThird Rome(Constantinople having been the second), in accordance with the concept oftranslatio imperii.[530]The last Eastern Roman titular,Andreas Palaiologos, sold the title of Emperor of Constantinople toCharles VIII of France; upon Charles' death, Palaiologos reclaimed the title and on his death granted it toFerdinand and Isabellaand their successors, who never used it. When theOttomans, who based their state on the Byzantine model, took Constantinople in 1453,Mehmed IIestablished his capital there and claimed to sit on the throne of the Roman Empire.[531]He even launched aninvasion of Otrantowith the purpose of re-uniting the Empire, which was aborted by his death. In the medieval West, "Roman" came to mean the church and the Catholic Pope. The Greek formRomaioiremained attached to the Greek-speaking Christian population of the Byzantine Empire and is still used byGreeks.[532]
The Roman Empire's control of the Italian Peninsula influencedItalian nationalismand theunification of Italy(Risorgimento) in 1861.[533]
In the United States, thefounderswere educated in theclassical tradition,[534]and used classical models forlandmarks in Washington, D.C..[535][536][537][538]The founders sawAthenian democracyandRoman republicanismas models for themixed constitution, but regarded the emperor as a figure of tyranny.[539]
In thehistory of Europe, theMiddle Agesormedieval periodlasted approximately from the 5th to the late 15th centuries, similarly to thepost-classicalperiod ofglobal history. It began with thefall of the Western Roman Empireand transitioned into theRenaissanceand theAge of Discovery. The Middle Ages is the middle period of the three traditional divisions of Western history:classical antiquity, the medieval period, and themodern period. The medieval period is itself subdivided into theEarly,High, andLate Middle Ages.
Population decline,counterurbanisation, the collapse of centralised authority, invasions, and mass migrations oftribes, which had begun inlate antiquity, continued into the Early Middle Ages. The large-scale movements of theMigration Period, including variousGermanic peoples, formed new kingdoms in what remained of the Western Roman Empire. In the 7th century,North Africaand the Middle East—once part of theByzantine Empire—came under the rule of theUmayyad Caliphate, an Islamic empire, after conquest byMuhammad's successors. Although there were substantial changes in society and political structures, the break withclassical antiquitywas incomplete. The still-sizeable Byzantine Empire, Rome's direct continuation, survived in the Eastern Mediterranean and remained a major power. The empire's law code, theCorpus Juris Civilisor "Code of Justinian", was rediscovered inNorthern Italyin the 11th century. In the West, most kingdoms incorporated the few extant Roman institutions. Monasteries were founded as campaigns toChristianisetheremaining pagans across Europecontinued. TheFranks, under theCarolingian dynasty, briefly established theCarolingian Empireduring the later 8th and early 9th centuries. It covered much of Western Europe but later succumbed to the pressures of internal civil wars combined with external invasions:Vikingsfrom the north,Magyarsfrom the east, andSaracensfrom the south.
During the High Middle Ages, which began after 1000, the population of Europe increased significantly as technological andagricultural innovationsallowed trade to flourish and theMedieval Warm Periodclimate change allowed crop yields to increase.Manorialism, the organisation ofpeasantsinto villages that owed rent and labour services to thenobles, andfeudalism, the political structure wherebyknightsand lower-status nobles owed military service to theiroverlordsin return for the right to rent from lands andmanors, were two of the ways society was organised in the High Middle Ages. This period also saw the collapse of the unified Christian church with theEast–West Schismof 1054. TheCrusades, first preached in 1095, were military attempts by Western European Christians to regain control of theHoly LandfromMuslims. Kings became the heads of centralisednation-states, reducing crime and violence but making the ideal of a unifiedChristendommore distant. Intellectual life was marked byscholasticism, a philosophy that emphasised joining faith to reason, and by the founding ofuniversities. The theology ofThomas Aquinas, the paintings ofGiotto, the poetry ofDanteandChaucer, the travels ofMarco Polo, and theGothic architectureof cathedrals such asChartresare among the outstanding achievements toward the end of this period and into the Late Middle Ages.
The Late Middle Ages was marked by difficulties and calamities, including famine, plague, and war, which significantly diminished the population of Europe; between 1347 and 1350, theBlack Deathkilled about a third of Europeans. Controversy,heresy, and theWestern Schismwithin theCatholic Churchparalleled the interstate conflict, civil strife, andpeasant revoltsthat occurred in the kingdoms. Cultural and technological developments transformed European society, concluding the Late Middle Ages and beginning theearly modern period.
The Middle Ages is one of the three major periods in the most enduring scheme for analysingEuropean history:classical civilisationorantiquity, the Middle Ages and themodern period.[1]The "Middle Ages" first appears in Latin in 1469 asmedia tempestasor "middle season".[2]In early usage, there were many variants, includingmedium aevum, or "middle age", first recorded in 1604,[3]andmedia saecula, or "middle centuries", first recorded in 1625.[4]The adjective "medieval" (or sometimes "mediaeval"[5]or "mediæval"),[6]meaning pertaining to the Middle Ages, derives frommedium aevum.[5]
Medieval writers divided history into periods such as the "Six Ages" or the "Four Empires" and considered their time to be the last before the end of the world.[7]When referring to their own times, they spoke of them as being "modern".[8]In the 1330s, the Italian humanist and poetPetrarchreferred to pre-Christian times asantiqua('ancient') and to the Christian period asnova('new').[9]Petrarch regarded the post-Roman centuries as "dark" compared to the "light" ofclassical antiquity.[10]Leonardo Bruniwas the first historian to usetripartite periodisationin hisHistory of the Florentine People(1442), with a middle period "between the fall of the Roman Empire and the revival of city life sometime in late eleventh and twelfth centuries".[11]Tripartiteperiodisationbecame standard after the 17th-century German historianChristoph Cellariusdivided history into three periods: ancient, medieval, and modern.[4]
The most commonly given starting point for the Middle Ages is around 500,[12]with the date of 476 first used by Bruni.[11][A]Later starting dates are sometimes used in the outer parts of Europe.[14]For Europe as a whole, 1500 is often considered to be the end of the Middle Ages,[15]but there is no universally agreed upon end date. Depending on the context, events such as theconquest of Constantinopleby the Turks in 1453,Christopher Columbus's first voyage to theAmericasin 1492, or theReformationin 1517 are sometimes used.[16]English historians often use theBattle of Bosworth Fieldin 1485 to mark the end of the period.[17]For Spain, dates commonly used are the death of KingFerdinand IIin 1516, the death of QueenIsabella I of Castilein 1504, or theconquest of Granadain 1492.[18]
Historians fromRomance-speakingcountries tend to divide the Middle Ages into two parts: an earlier "High" and later "Low" period. English-speaking historians, following their German counterparts, generally subdivide the Middle Ages into three intervals: "Early", "High", and "Late".[1]In the 19th century, the entire Middle Ages were often referred to as the "Dark Ages",[19]but with the adoption of these subdivisions, use of this term was restricted to the Early Middle Ages, at least among historians.[7]
TheRoman Empirereached its greatest territorial extent during the 2nd century AD; the following two centuries witnessed the slow decline of Roman control over its outlying territories.[21]Economic issues, including inflation, and external pressure on the frontiers combined to create theCrisis of the Third Century, with emperors coming to the throne only to be rapidly replaced by new usurpers.[22]Military expenses increased steadily during the 3rd century, mainly in response to thewarwith theSasanian Empire, which revived in the middle of the 3rd century.[23]The army doubled in size, and cavalry and smaller units replaced theRoman legionas the main tactical unit.[24]The need for revenue led to increased taxes and a decline in numbers of thecurial, or landowning, class, and decreasing numbers of them willing to shoulder the burdens of holding office in their native towns.[23]More bureaucrats were needed in the central administration to deal with the needs of the army, which led to complaints from civilians that there were more tax-collectors in the empire than tax-payers.[24]
The EmperorDiocletian(r. 284–305) split the empire into separately administeredeasternandwesternhalves in 286; the empire was not considered divided by its inhabitants or rulers, as legal and administrativepromulgationsin one division were considered valid in the other.[25][B]In 330, after a period of civil war,Constantine the Great(r. 306–337) refounded the city ofByzantiumas the newly renamed eastern capital,Constantinople.[26]Diocletian's reforms strengthened the governmental bureaucracy, reformed taxation, and strengthened the army, which bought the empire time but did not resolve the problems it was facing: excessive taxation, a declining birthrate, and pressures on its frontiers, among others.[27]Civil war between rival emperors became common in the middle of the 4th century, diverting soldiers from the empire's frontier forces and allowinginvadersto encroach.[28]For much of the 4th century, Roman society stabilised in a new form that differed from the earlierclassical period, with a widening gulf between the rich and poor, and a decline in the vitality of the smaller towns.[29]Another change was theChristianisation, or conversion of the empire toChristianity, a gradual process that lasted from the 2nd to the 5th centuries.[30][31]
In 376, theGoths, fleeing from theHuns, received permission from EmperorValens(r. 364–378) to settle in the Roman province ofThraciain theBalkans. The settlement did not go smoothly, and the Goths began to raid and plunder when Roman officials mishandled the situation.[C]Valens, attempting to put down the disorder, was killed fighting the Goths at theBattle of Adrianopleon 9 August 378.[33]In addition to the threat from such tribal confederacies in the north, internal divisions within the empire, especially within the Christian Church, caused problems.[34]In 400, theVisigothsinvaded the Western Roman Empire and, although briefly forced back from Italy, in 410sacked the city of Rome.[35]In 406 theAlans,Vandals, andSuevicrossed intoGaul; over the next three years they spread across Gaul and in 409 crossed thePyrenees Mountainsinto modern-day Spain.[36]TheMigration Periodbegan, when various peoples, initially largelyGermanic peoples, moved across Europe. TheFranks,Alemanni, and theBurgundiansall ended up in northern Gaul while theAngles,Saxons, andJutessettled in Britain,[37]and the Vandals went on to cross the strait of Gibraltar after which they conquered the province ofAfrica.[38]In the 430s the Huns began invading the empire; their kingAttila(r. 434–453) led invasions into the Balkans in 442 and 447, Gaul in 451, and Italy in 452.[39]The Hunnic threat remained until Attila's death in 453, when theHunnic confederationhe led fell apart.[40]These invasions by the tribes completely changed the political and demographic nature of what had been the Western Roman Empire.[37]
By the end of the 5th century, the western section of the empire was divided into smaller political units ruled by the tribes that had invaded in the early part of the century.[41]The deposition of the last emperor of the west,Romulus Augustulus, in 476 has traditionally marked the end of the Western Roman Empire.[13][D]By 493 the Italian peninsula was conquered by theOstrogoths.[42]The Eastern Roman Empire, often referred to as the Byzantine Empire after the fall of its western counterpart, had little ability to assert control over the lost western territories. TheByzantine emperorsmaintained a claim over the territory, but while none of the new kings in the west dared to elevate himself to the position of emperor of the west, Byzantine control of most of the Western Empire could not be sustained; the reconquest of the Mediterranean periphery and theItalian Peninsula(Gothic War) in the reign ofJustinian(r. 527–565) was the sole, and temporary, exception.[43]
The political structure of Western Europe changed with the end of the united Roman Empire. Although the movements of peoples during this period are usually described as "invasions", they were not just military expeditions but migrations of entire peoples into the empire. Such movements were aided by the refusal of the Western Roman elites to support the army or pay the taxes that would have allowed the military to suppress the migration.[44]The emperors of the 5th century were often controlled by military strongmen such asStilicho(d. 408),Aetius(d. 454),Aspar(d. 471),Ricimer(d. 472), orGundobad(d. 516), who were partly or fully of non-Roman background. When the line of Western emperors ceased, many of the kings who replaced them were from the same background. Intermarriage between the new kings and the Roman elites was common.[45]This led to a fusion of Roman culture with the customs of the invading tribes, including the popular assemblies that allowed free male tribal members more say in political matters than was common in the Roman state.[46]Material artefacts left by the Romans and the invaders are often similar, and tribal items were often modelled on Roman objects.[47]Much of the scholarly and written culture of the new kingdoms was also based on Roman intellectual traditions.[48]An important difference was the new polities' gradual loss of tax revenue. Many new political entities no longer supported their armies through taxes; instead, they relied on granting them land or rents. This meant there was less need for large tax revenues, so thetaxation systemsdecayed.[49]Warfare was common between and within the kingdoms. Slavery declined as the supply weakened, and society became more rural.[50][E]
Between the 5th and 8th centuries, new peoples and individuals filled the political void left by the centralised Roman government.[48]TheOstrogoths, a Gothic tribe, settled inRoman Italyin the late fifth century underTheoderic the Great(d. 526) and set up akingdommarked by its co-operation between the Italians and the Ostrogoths, at least until the last years of Theodoric's reign.[52]The Burgundians settled in Gaul, and after an earlier realm was destroyed by the Huns in 436, formed a new kingdom in the 440s. Between today'sGenevaandLyon, it grew to become the realm ofBurgundyin the late 5th and early 6th centuries.[53]Elsewhere in Gaul, the Franks andCeltic Britonsset up small polities.Franciawas centred in northern Gaul, and the first king of whom much is known isChilderic I(d. 481). His grave was discovered in 1653 and is remarkable for itsgrave goods, which included weapons and a large quantity of gold.[54]
Under Childeric's sonClovis I(r. 509–511), the founder of theMerovingian dynasty, the Frankish kingdom expanded and converted to Christianity. The Britons, related to the natives ofBritannia– modern-day Great Britain – settled in what is nowBrittany.[55][F]Other monarchies were established by theVisigothic Kingdomin theIberian Peninsula, theSuebiin northwestern Iberia, and theVandal KingdominNorth Africa.[53]In the sixth century, theLombardssettled inNorthern Italy, replacing the Ostrogothic kingdom with a grouping ofduchiesthat occasionally selected a king to rule over them all. By the late sixth century, this arrangement had been replaced by a permanent monarchy, theKingdom of the Lombards.[56]
The invasions brought new ethnic groups to Europe, although some regions received a larger influx of new peoples than others. In Gaul, for instance, the invaders settled much more extensively in the north-east than in the south-west.Slavssettled inCentralandEastern Europeand the Balkan Peninsula. Changes in languages accompanied the settlement of peoples.Latin, the literary language of the Western Roman Empire, was gradually replaced byvernacular languages, which evolved from Latin but were distinct from it, collectively known asRomance languages. These changes from Latin to the new languages took many centuries. Greek remained the language of the Byzantine Empire, but the migrations of the Slavs addedSlavic languagesto Eastern Europe.[57]
As Western Europe witnessed the formation of new kingdoms, the Eastern Roman Empire remained intact and experienced an economic revival that lasted into the early 7th century. There were fewer invasions of the eastern section of the empire; most occurred in the Balkans. Peace with theSasanian Empire, Rome's traditional enemy, lasted most of the 5th century. The Eastern Empire was marked by closer relations between the political state and the Christian Church, with doctrinal matters assuming an importance in Eastern politics that they did not have in Western Europe. Legal developments included the codification ofRoman law; the first effort—theCodex Theodosianus—was completed in 438.[59]Under Emperor Justinian (r. 527–565), another compilation took place—theCorpus Juris Civilis.[60]Justinian also oversaw the construction of theHagia Sophiain Constantinople and the reconquest of North Africa from the Vandals and Italy from the Ostrogoths,[61]underBelisarius(d. 565).[62]The conquest of Italy was not complete, as a deadly outbreak ofplague in 542led to the rest of Justinian's reign concentrating on defensive measures rather than further conquests.[61]
At the Emperor's death, the Byzantines had control ofmost of Italy, North Africa, and a small foothold in southern Spain. Historians have criticised Justinian's reconquests for overextending his realm and setting the stage for theearly Muslim conquests, but many of the difficulties faced by Justinian's successors were due not just to over-taxation to pay for his wars but to the essentially civilian nature of the empire, which made raising troops difficult.[63]
In the Eastern Empire, the Slavs' slow infiltration of the Balkans added further difficulty for Justinian's successors. It began gradually, but by the late 540s, Slavic tribes were inThraceandIllyriumand had defeated an imperial army nearAdrianoplein 551. In the 560s, theAvarsbegan to expand from their base on the north bank of theDanube; by the end of the 6th century, they were the dominant power in Central Europe and routinely able to force the Eastern emperors to pay tribute. They remained a strong power until 796.[64]
An additional problem to face the empire came as a result of the involvement of EmperorMaurice(r. 582–602) in Persian politics when he intervened in asuccession dispute. This led to a period of peace, but when Maurice was overthrown,the Persians invadedand during the reign of EmperorHeraclius(r. 610–641) controlled large chunks of the empire, including Egypt, Syria, andAnatoliauntil Heraclius' successful counterattack. In 628, the empire secured a peace treaty and recovered its lost territories.[65]
In Western Europe, some older Roman elite families died out while others became more involved with ecclesiastical than secular affairs. Values attached toLatin scholarshipandeducationmostly disappeared, and while literacy remained important, it became a practical skill rather than a sign of elite status. In the 4th century,Jerome(d. 420) dreamed that God rebuked him for spending more time readingCicerothan theBible. By the 6th century,Gregory of Tours(d. 594) had a similar dream, but instead of being chastised for reading Cicero, he was chastised for learningshorthand.[66]By the late 6th century, the principal means of religious instruction in the Church had become music and art rather than the book.[67]Most intellectual efforts went towards imitating classical scholarship, but someoriginal workswere created, along with now-lost oral compositions. The writings ofSidonius Apollinaris(d. 489),Cassiodorus(d.c.585), andBoethius(d. c. 525) were typical of the age.[68]
Changes also occurred among laypeople, as aristocratic culture focused on great feasts held in halls rather than on literary pursuits. Clothing for the elites was richly embellished with jewels and gold. Lords and kings supported the entourages of fighters who formed the backbone of the military forces.[G]Family ties within the elites were important, as were the virtues of loyalty, courage, and honour. These ties led to the prevalence of feuds in aristocratic society, including those related by Gregory of Tours inMerovingianGaul. Most feuds seem to have ended quickly with the payment of somecompensation.[71]Women took part in aristocratic society mainly in their roles as wives and mothers of men, with the role of mother of a ruler being especially prominent in Merovingian Gaul. InAnglo-Saxonsociety, the lack of many child rulers meant a lesser role for women as queen mothers, but this was compensated for by the increased role played byabbessesof monasteries. Only in Italy does it appear that women were always considered under the protection and control of a male relative.[72]
Peasant society is much less documented than the nobility. Most of the surviving information available to historians comes fromarchaeology; few detailed written records documenting peasant life remain from before the 9th century. Most of the descriptions of the lower classes come from eitherlaw codesor writers from the upper classes.[73]Landholdingpatterns in the West were not uniform; some areas had greatly fragmented landholding patterns, but in other areas, large contiguous blocks of land were the norm. These differences allowed for a wide variety of peasant societies, some dominated by aristocratic landholders and others having great autonomy.[74]Land settlement also varied greatly. Some peasants lived in large settlements that numbered as many as 700 inhabitants. Others lived in small groups of a few families and lived on isolated farms spread over the countryside. There were also areas where the pattern was a mix of two or more systems.[75]Unlike in the late Roman period, there was no sharp break between the legal status of the free peasant and the aristocrat, and a free peasant's family could rise into the aristocracy over several generations through military service to a powerful lord.[76]
Roman city life and culture changed greatly in the early Middle Ages. Although Italian cities remained inhabited, they contracted significantly in size. For instance, Rome shrank from hundreds of thousands to around 30,000 by the end of the 6th century.Roman templeswere converted intoChristian churchesand city walls remained in use.[77]In Northern Europe, cities also shrank, while civic monuments and other public buildings were raided for building materials. The establishment of new kingdoms often meant some growth for the towns chosen as capitals.[78]Although there had beenJewish communities in many Roman cities, theJewssuffered periods of persecution after the conversion of the empire to Christianity. Officially, they were tolerated, if subject to conversion efforts, and were sometimes encouraged to settle in new areas.[79]
Religious beliefs in the Eastern Roman Empire and Iran were in flux during the late sixth and early seventh centuries.Judaismwas an active proselytising faith, and at least oneArabpolitical leader converted to it.[H]In addition Jewish theologians wrote polemics defending their religion against Christian and Islamic influences.[81]
Christianity had active missions competing with the Persians'Zoroastrianismin seeking converts, especially among residents of theArabian Peninsula. All these strands came together with the emergence ofIslamin Arabia during the lifetime ofMuhammad(d. 632).[82]After his death, Islamic forces conquered much of the Eastern Roman Empire and Persia, starting withSyriain 634–635, continuing withPersiabetween 637 and 642, reachingEgyptin 640–641,North Africain the later seventh century, and theIberian Peninsulain 711.[83]By 714, Islamic forces controlled much of the peninsula in a region they calledAl-Andalus.[84]
The Islamic conquests reached their peak in the mid-eighth century. The defeat of Muslim forces at theBattle of Toursin 732 led to the reconquest of southern France by the Franks, but the main reason for the halt of Islamic growth in Europe was the overthrow of theUmayyad Caliphateand its replacement by theAbbasid Caliphate. The Abbasids moved their capital toBaghdadand were more concerned with the Middle East than Europe, losing control of sections of the Muslim lands. Umayyad descendants took over the Iberian Peninsula, theAghlabidscontrolled North Africa, and theTulunidsbecame rulers of Egypt.[85]By the middle of the 8th century, new trading patterns were emerging in the Mediterranean; trade between the Franks and the Arabs replaced the oldRoman economy. Franks traded timber, furs, swords, and enslaved people in return for silks and other fabrics, spices, and precious metals from the Arabs.[86]
The migrations and invasions of the 4th and 5th centuries disrupted trade networks around the Mediterranean. African goods stopped being imported into Europe, first disappearing from the interior and, by the 7th century, found only in a few cities such as Rome orNaples. By the end of the 7th century, under the impact of theMuslim conquests, African products were no longer found in Western Europe. Replacing goods from long-range trade with local products was a trend throughout the old Roman lands in the Early Middle Ages. This was especially marked in the lands that did not lie on the Mediterranean, such as northern Gaul or Britain. Non-local goods appearing in the archaeological record are usually luxury goods. In northern Europe, not only were the trade networks local, but the goods carried were simple, with little pottery or other complex products. Around the Mediterranean, pottery remained prevalent and appears to have been traded over medium-range networks, not just produced locally.[87]
The various Germanic states in the west all hadcoinagesthat imitated existing Roman and Byzantine forms. Gold continued to be minted until the end of the 7th century in 693–694, when it was replaced by silver in the Merovingian kingdom. The basic Frankish silver coin was thedenariusordenier, while the Anglo-Saxon version was called apenny. From these areas, the denier or penny spread throughout Europe from 700 to 1000. Copper or bronze coins were not struck, nor were gold, except in Southern Europe. No silver coins denominated in multiple units were minted.[88]
Christianity was a major unifying factor between Eastern and Western Europe before the Arab conquests, but the conquest of North Africa sundered maritime connections between those areas. Increasingly, the Byzantine Church differed in language, practices, andliturgyfrom the Western Church. The Eastern Church used Greek instead of Western Latin. Theological and political differences emerged, and by the early and middle 8th century, issues such asiconoclasm,clerical marriage, andstate control of the Churchhad widened to the extent that the cultural and religious differences were more significant than the similarities.[89]A formal break known as theEast–West Schismcame in 1054, when thepapacyand thepatriarchy of Constantinopleclashed overpapal supremacyandexcommunicatedeach other, which led to the division of Christianity into two Churches—the Western branch became theRoman Catholic Churchand the Eastern branch theEastern Orthodox Church.[90]
Theecclesiastical structureof the Roman Empire survived the movements and invasions in the West mostly intact. Still, the papacy was little regarded, and few of the Westernbishopslooked to the bishop of Rome for religious or political leadership.Many of the popesbefore 750 were more concerned with Byzantine affairs and Eastern theological controversies. The register, or archived copies of the letters, of PopeGregory the Great(pope 590–604) survived. Of those 850 letters, most were concerned with affairs in Italy or Constantinople. The only part of Western Europe where the papacy had influence was Britain, where Gregory had sent theGregorian missionin 597 to convert the Anglo-Saxons to Christianity.[91]Irish missionarieswere most active in Western Europe between the 5th and the 7th centuries, going first to England and Scotland and then on to the continent. Under suchmonksasColumba(d. 597) andColumbanus(d. 615), they founded monasteries, taught in Latin and Greek, and authored secular and religious works.[92]
The Early Middle Ages witnessed the rise ofmonasticismin the West. The shape of European monasticism was determined by traditions and ideas that originated with theDesert FathersofEgyptandSyria. Most European monasteries were of the type that focuses on the community experience of the spiritual life, calledcenobitism, which was pioneered byPachomius(d. 348) in the 4th century. Monastic ideals spread from Egypt to Western Europe in the 5th and 6th centuries throughhagiographical literaturesuch as theLife of Anthony.[93]Benedict of Nursia(d. 547) wrote theBenedictine Rulefor Western monasticism during the 6th century, detailing the administrative and spiritual responsibilities of a community of monks led by anabbot.[94]Monks and monasteries had a profound effect on the religious and political life of the Early Middle Ages, in various cases acting asland trustsfor powerful families, centres of propaganda and royal support in newly conquered regions, and bases for missions and proselytisation.[95]They were the main and sometimes only outposts of education and literacy in a region. Many of the surviving manuscripts of the Latinclassicswere copied in monasteries in the Early Middle Ages.[96]Monks were also the authors of new works, including history, theology, and other subjects, written by authors such asBede(d. 735), a native of northern England who wrote in the late 7th and early 8th centuries.[97]
The Frankish kingdom in northern Gaul split into kingdoms calledAustrasia,Neustria, andBurgundyduring the 6th and 7th centuries, all of them ruled by the Merovingian dynasty, who were descended from Clovis. The 7th century was a tumultuous period of wars between Austrasia and Neustria.[98]Such warfare was exploited byPippin I(d. 640), theMayor of the Palacefor Austrasia who became the power behind the Austrasian throne. Later, his family inherited the office and acted as advisers and regents. One of his descendants,Charles Martel(d. 741), won theBattle of Poitiersin 732, halting the advance of Muslim armies across the Pyrenees.[99][I]Great Britain was divided into small states dominated by the kingdoms ofNorthumbria,Mercia,Wessex, andEast Angliawhich descended from the Anglo-Saxon invaders. Smaller kingdoms in present-day Wales and Scotland were still under the control of the native Britons andPicts.[101]Ireland was divided into even smaller political units, usually known as tribal kingdoms, under the control of kings. There were perhaps as many as150 local kingsin Ireland of varying importance.[102]
TheCarolingian dynasty, as the successors to Charles Martel are known, officially took control of the kingdoms of Austrasia and Neustria in a coup of 753 led byPippin III(r. 752–768). A contemporary chronicle claims that Pippin sought and gained authority for this coup from PopeStephen II(pope 752–757). Pippin's takeover was reinforced with propaganda that portrayed the Merovingians as inept or cruel rulers, exalted the accomplishments of Charles Martel, and circulated stories of the family's great piety. At the time of his death in 768, Pippin left his kingdom in the hands of his two sons, Charles (r. 768–814) andCarloman(r. 768–771). When Carloman died of natural causes, Charles blocked the succession of Carloman's young son and installed himself as the king of the united Austrasia and Neustria. Charles, more often known as Charles the Great orCharlemagne, embarked upon a programme of systematic expansion in 774 that unified a large portion of Europe, eventually controlling modern-day France, northern Italy, andSaxony. In the wars that lasted beyond 800, he rewarded allies with war booty and command over parcels of land.[103]In 774, Charlemagne conquered the Lombards, which freed the papacy from the fear of Lombard conquest and marked the beginnings of thePapal States.[104][J]
The coronation of Charlemagne as emperor on Christmas Day 800 is regarded as a turning point in medieval history, marking a return of the Western Roman Empire since the new emperor ruled over much of the area previously controlled by the Western emperors.[107]It also marks a change in Charlemagne's relationship with the Byzantine Empire, as the assumption of the imperial title by the Carolingians asserted their equivalence to the Byzantine state.[108]There were several differences between the newly established Carolingian Empire and both the older Western Roman Empire and the concurrent Byzantine Empire. The Frankish lands were rural, with only a few small cities. Most of the people were peasants who settled on small farms. Little trade existed, and much of that was with the British Isles and Scandinavia, in contrast to the older Roman Empire with its trading networks centred on the Mediterranean.[107]The empire was administered by an itinerant court that travelled with the emperor, as well as approximately 300 imperial officials calledcounts, who administered thecountiesthe empire had been divided into. Clergy and local bishops served as officials, as well as the imperial officials, calledmissi dominici, who served as roving inspectors and troubleshooters.[109]
Charlemagne's court inAachenwas the centre of the cultural revival, sometimes referred to as the "Carolingian Renaissance". Literacy increased, as did development in the arts, architecture, jurisprudence, and liturgical and scriptural studies. The English monkAlcuin(d. 804) was invited to Aachen and brought theeducationavailable in the monasteries of Northumbria. Charlemagne'schancery—or writing office—made use of a newscripttoday known asCarolingian minuscule,[K]allowing a standard writing style that advanced communication across much of Europe. Charlemagne sponsored changes inchurch liturgy, imposing the Roman form of church service on his domains, as well as theGregorian chantin liturgical music for the churches. An important activity for scholars during this period was copying, correcting, and disseminating basic works on religious and secular topics to encourage learning. New works on religious topics and schoolbooks were also produced.[111]Grammarians of the period modified the Latin language, changing it from theClassical Latinof the Roman Empire into a more flexible form to fit the needs of the Church and government. By the reign of Charlemagne, the language had so diverged from the classical Latin that it was later calledMedieval Latin.[112]
Charlemagne planned to continue the Frankish tradition of dividing his kingdom between all his heirs but was unable to do so as only one son,Louis the Pious(r. 814–840), was still alive by 813. Just before Charlemagne died in 814, he crowned Louis as his successor. Numerous divisions of the empire marked Louis's reign of 26 years among his sons and, after 829, civil wars between various alliances of father and sons over the control of various parts of the empire. Eventually, Louis recognised his eldest sonLothair I(d. 855) as emperor and gave him Italy.[L]Louis divided the rest of the empire between Lothair andCharles the Bald(d. 877), his youngest son. Lothair tookEast Francia, comprising both banks of the Rhine and eastwards, leaving CharlesWest Franciawith the empire to the west of the Rhineland and the Alps.Louis the German(d. 876), the middle child, who had been rebellious to the last, was allowed to keep Bavaria under thesuzeraintyof his elder brother. The division was disputed.Pepin IIof Aquitaine(d. after 864), the emperor's grandson, rebelled in a contest forAquitaine, while Louis the German tried to annexe all of East Francia. Louis the Pious died in 840, with the empire still in chaos.[114]
A three-year civil war followed his death. By theTreaty of Verdun(843), a kingdom between theRhineandRhonerivers was created for Lothair to go with his lands in Italy, and his imperial title was recognised. Louis the German controlled Bavaria and the eastern lands in modern-day Germany. Charles the Bald received the western Frankish lands, comprising most of modern-day France.[114]Charlemagne's grandsons and great-grandsons divided their kingdoms between their descendants, eventually causing all internal cohesion to be lost.[115][M]In 987, the Carolingian dynasty was replaced in the western lands, with the crowning ofHugh Capet(r. 987–996) as king.[N][O]In the eastern lands, the dynasty had died out earlier, in 911, with the death ofLouis the Child,[118]and the selection of the unrelatedConrad I(r. 911–918) as king.[119]
Invasions, migrations, and raids by external foes accompanied the break-up of the Carolingian Empire. The Atlantic and northern shores were harassed by theVikings, who also raided the British Isles and settled there and in Iceland. In 911, the Viking chieftainRollo(d. c. 931) received permission from the Frankish KingCharles the Simple(r. 898–922) to settle in what becameNormandy.[120][P]The eastern parts of the Frankish kingdoms, especially Germany and Italy, were under continualMagyarassault until the invader's defeat at theBattle of Lechfeldin 955.[122]The break-up of the Abbasid dynasty meant that the Islamic world fragmented into smaller political states, some of which began expanding into Italy and Sicily, as well as over the Pyrenees into the southern parts of the Frankish kingdoms.[123]
Efforts by local kings to fight the invaders led to the formation of new political entities. InAnglo-Saxon England, KingAlfred the Great(r. 871–899) came to an agreement with the Viking invaders in the late 9th century, resulting inDanish settlementsin Northumbria, Mercia, and parts of East Anglia.[124]By the middle of the 10th century, Alfred's successors had conquered Northumbria and restored English control over most of the southern part of Great Britain.[125]In northern Britain,Kenneth MacAlpin(d. c. 860) united the Picts and theScotsinto theKingdom of Alba.[126]In the early 10th century, theOttonian dynastyhad established itself inGermany, and was engaged in driving back the Magyars. Its efforts culminated in the coronation in 962 ofOtto I(r. 936–973) asHoly Roman Emperor.[127]In 972, he secured recognition of his title by the Byzantine Empire, which he sealed with the marriage of his sonOtto II(r. 967–983) toTheophanu(d. 991), daughter of an earlier Byzantine EmperorRomanos II(r. 959–963).[128]By the late 10th centuryItalyhad been drawn into the Ottonian sphere after a period of instability;[129]Otto III(r. 996–1002) spent much of his later reign in the kingdom.[130]The western Frankish kingdom was more fragmented, and although kings remained nominally in charge, much of the political power devolved to the local lords.[131]
Missionary efforts to Scandinaviaduring the 9th and 10th centuries helped strengthen the growth of kingdoms such asSweden,Denmark, andNorway, which gained power and territory. Some kings converted to Christianity, although not all by 1000. Scandinavians also expanded and colonised throughout Europe. Besides the settlements in Ireland, England, and Normandy, further settlement took place in what becameRussiaandIceland. Swedish traders and raiders ranged down the rivers of the Russian steppe and even attempted to seize Constantinople in860and907.[132]Christian Spain, initially driven into a small section of the peninsula in the north, expanded slowly south during the 9th and 10th centuries, establishing the kingdoms ofAsturiasandLeón.[133]
In Eastern Europe, Byzantium revived its fortunes under EmperorBasil I(r. 867–886) and his successorsLeo VI(r. 886–912) andConstantine VII(r. 913–959), members of theMacedonian dynasty. Commerce revived, and the emperors oversaw the extension of a uniform administration to all the provinces. The military was reorganised, which allowed the emperorsJohn I(r. 969–976) andBasil II(r. 976–1025) to expand the frontiers of the empire on all fronts. The imperial court was the centre of a revival of classical learning, a process known as theMacedonian Renaissance. Writers such asJohn Geometres(fl.early 10th century) composed new hymns, poems, and other works.[134]Missionary efforts by both Eastern and Western clergy resulted in the conversion of theMoravians,Bulgars,Bohemians,Poles, Magyars, and Slavic inhabitants of theKievan Rus'. These conversions contributed to the founding of political states in the lands of those peoples—the states ofMoravia,Bulgaria,Bohemia,Poland, Hungary, and the Kievan Rus'.[135]Bulgaria, which was founded around 680, at its height reached from Budapest to the Black Sea and from the Dnieper River in modern Ukraine to the Adriatic Sea.[136]By 1018, the last Bulgarian nobles had surrendered to the Byzantine Empire.[137]
Few large stone buildings were constructed between the Constantinianbasilicasof the 4th and 8th centuries, although many smaller ones were built during the 6th and 7th centuries. By the beginning of the 8th century, the Carolingian Empire revived the basilica form of architecture.[139]One feature of the basilica is the use of atransept,[140]or the "arms" of a cross-shaped building that are perpendicular to the longnave.[141]Other new features of religious architecture include thecrossing towerand a monumentalentrance to the church, usually at the west end of the building.[142]
Carolingian artwas produced for a small group of figures around the court and the monasteries and churches they supported. It was dominated by efforts to regain the dignity and classicism of imperial Roman andByzantine artbut was also influenced by theInsular artof the British Isles. Insular art integrated the energy ofIrish CelticandAnglo-Saxon Germanicstyles of ornament with Mediterranean forms such as the book, and established many characteristics of art for the rest of the medieval period. Surviving religious works from the Early Middle Ages are mostlyilluminated manuscriptsand carvedivories, originally made for metalwork that has since been melted down.[143][144]Objects in precious metals were the most prestigious form of art, but almost all are lost except for a few crosses such as theCross of Lothair, severalreliquaries, and finds such as the Anglo-Saxon burial atSutton Hooand thehoardsofGourdonfrom Merovingian France,Guarrazarfrom Visigothic Spain andNagyszentmiklósnear Byzantine territory. There are survivals from the largebroochesinfibulaorpenannularform that were key pieces of personal adornment for elites, including the IrishTara Brooch.[145]Highly decorated books were mostlyGospel Booksand these have survived inlarger numbers, including the InsularBook of Kells, theBook of Lindisfarne, and the imperialCodex Aureus of St. Emmeram, which is one of the few to retain its "treasure binding" of gold encrusted with jewels.[146]Charlemagne's court seems to have been responsible for the acceptance of figurativemonumental sculptureinChristian art,[147]and by the end of the period near life-sized figures such as theGero Crosswere common in important churches.[148]
During the later Roman Empire, the principal military developments were attempts to create an effective cavalry force and the continued development of highly specialised types of troops. The creation of heavily armouredcataphract-type soldiers as cavalry was an important feature of the 5th-century Roman military. The various invading tribes had differing emphases on types of soldiers—ranging from the primarily infantry Anglo-Saxon invaders of Britain to the Vandals and Visigoths, who had a high proportion of cavalry in their armies.[149]During the early invasion period, thestirruphad not been introduced into warfare, which limited the usefulness of cavalry asshock troopsbecause it was not possible to put the full force of the horse and rider behind blows struck by the rider.[150]The greatest change in military affairs during the invasion period was the adoption of the Hunniccomposite bowin place of the earlier, and weaker,Scythiancomposite bow.[151]Another development was the increasing use oflongswords[152]and the progressive replacement ofscale armourbymail armourandlamellar armour.[153]
The importance of infantry and light cavalry declined during the early Carolingian period, with a growing dominance of elite heavy cavalry. The use ofmilitia-type leviesof the free population declined over the Carolingian period.[154]Although much of the Carolingian armies were mounted, a large proportion during the early period appear to have beenmounted infantry, rather than true cavalry.[155]One exception was Anglo-Saxon England, where the armies were still composed of regional levies, known as thefyrd, which were led by the local elites.[156]In military technology, one of the main changes was the return of thecrossbow, which had been known in Roman times and reappeared as a military weapon during the last part of the Early Middle Ages.[157]Another change was the introduction of the stirrup, which increased the effectiveness of cavalry as shock troops. A technological advance that had implications beyond the military was thehorseshoe, which allowed horses to be used in rocky terrain.[158]
The High Middle Ages was a period of tremendouspopulation expansion. The estimated population of Europe grew from 35 to 80 million between 1000 and 1347, although the exact causes remain unclear: improved agricultural techniques, the decline of slaveholding, amore clement climateand the lack of invasion have all been suggested.[161][162]As much as 90 per cent of the European population remained rural peasants. Many were no longer settled in isolated farms but had gathered into small communities, usually known asmanorsor villages.[162]These peasants were often subject to noble overlords and owed them rents and other services in a system known asmanorialism. There remained a few free peasants throughout this period and beyond,[163]with more of them in the regions of Southern Europe than in the north. The practice ofassarting, or bringing new lands into production by offering incentives to the peasants who settled them, also contributed to population expansion.[164]
Theopen-field systemof agriculture was commonly practised in most of Europe, especially in "northwestern and central Europe".[165]Such agricultural communities had three essential characteristics: individual peasant holdings in the form of strips of land were scattered among the different fields belonging to the manor; crops were rotated from year to year to preserve soil fertility; and common land was used for grazing livestock and other purposes. Some regions used a three-field system of crop rotation; others retained the older two-field system.[166]
Other sections of society included the nobility, clergy, and townspeople. Nobles, both the titlednobilityand simpleknights, exploited the manors and the peasants. However, they did not own lands outright but were granted rights to the income from a manor or other lands by an overlord through the system offeudalism. During the 11th and 12th centuries, these lands, orfiefs, came to be considered hereditary. In most areas, they were no longer divisible between all the heirs, as had been the case in the early medieval period. Instead, most fiefs and lands went to the eldest son.[167][Q]The dominance of the nobility was built upon its control of the land, its military service asheavy cavalry, control ofcastles, and various immunities from taxes or other impositions.[R]Castles, initially in wood but later in stone, began to be constructed in the 9th and 10th centuries in response to the disorder of the time, and protected from invaders and allowing lords defence from rivals. Control of castles allowed the nobles to defy kings or other overlords.[169]Nobles were stratified; kings and the highest-ranking nobility controlled large numbers of commoners and large tracts of land, as well as other nobles. Beneath them, lesser nobles had authority over smaller land areas and fewer people. Knights were the lowest level of nobility; they controlled but did not own land and had to serve other nobles.[170][S]
The clergy was divided into two types: thesecular clergy, who lived out in the world, and theregular clergy, who lived isolated under a religious rule and usually consisted of monks.[172]Throughout the period, monks remained a tiny proportion of the population, usually less than one percent.[173]Most of the regular clergy were drawn from the nobility, the same social class that served as the recruiting ground for the upper levels of the secular clergy. The localparishpriests were often drawn from the peasant class.[174]Townspeople were somewhat unusual, as they did not fit into the traditional three-fold division of society into nobles, clergy, and peasants. During the 12th and 13th centuries, the ranks of the townspeople expanded greatly as existing towns grew and new population centres were founded.[175]But throughout the Middle Ages, the population of the towns probably never exceeded 10 percent of the total population.[176]
Jews alsospread across Europeduring the period. Communities were established inGermanyandEnglandin the 11th and 12th centuries, butSpanish Jews, long settled in Spain under the Muslims, came under Christian rule and increasing pressure to convert to Christianity.[79]Most Jews were confined to the cities, as they were not allowed to own land or be peasants.[177][T]Besides the Jews, there were other non-Christians on the edges of Europe—pagan Slavs in Eastern Europe and Muslims in Southern Europe.[178]
Women in the Middle Ageswere officially required to be subordinate to some male, whether their father, husband, or other kinsman. Widows were often allowed much control over their lives, but they were still restricted legally. Women's work generally consisted of household or other domestically inclined tasks. Peasant women were usually responsible for caring for the household, child care, gardening, and animal husbandry near the house. They could supplement their household income by spinning or brewing at home. At harvest time, they were also expected to help with fieldwork.[179]Townswomen, like peasant women, were responsible for the household and could also engage in trade. The trades that were open to women varied by country and period.[180]Noblewomen were responsible for running a household and could occasionally be expected to handle estates in the absence of male relatives, but they were usually restricted from participation in military or government affairs. The only role open to women in the Church was that ofnuns, as they could not become priests.[179]
Incentralandnorthern Italyand inFlanders, the rise of towns that were, to a degree, self-governing stimulated economic growth and created an environment for new types of trade associations. Commercial cities on the shores of the Baltic entered into agreements known as theHanseatic League. The ItalianMaritime republicssuch asVenice,Genoa, andPisaexpanded their trade throughout the Mediterranean.[U]Greattrading fairswere established and flourished innorthern Franceduring the period, allowing Italian and German merchants to trade with each other as well as local merchants.[182]In the late 13th century new land and sea routes to the Far East were pioneered, famously described inThe Travels of Marco Polowritten by one of the traders,Marco Polo(d. 1324).[183]Besides new trading opportunities, agricultural and technological improvements increased crop yields, which allowed the trade networks to expand.[184]Rising trade brought new methods of dealing with money, and gold coinage was again minted in Europe, first in Italy and later in France and other countries. New forms of commercial contracts emerged, sharing risk among merchants. Accounting methods improved, partly through the use ofdouble-entry bookkeeping;letters of creditalso appeared, allowing easy transmission of money.[185]
The High Middle Ages was the formative period in the history of the modern Western state. Kings in France, England, and Spain consolidated their power and established lasting governing institutions.[186]New kingdoms such asHungaryandPoland, after their conversion to Christianity, became Central European powers.[187]The Magyars settled Hungary around 900 under KingÁrpád(d. c. 907) after a series of invasions in the 9th century.[188]The papacy, long attached to an ideology of independence from secular kings, first asserted its claim to temporal authority over the entire Christian world; thePapal Monarchyreached its apogee in the early 13th century under the pontificate ofInnocent III(pope 1198–1216).[189]Northern Crusadesand the advance of Christian kingdoms and military orders into previouslypaganregions in the Baltic andFinnicnorth-east brought the forced assimilation of numerous native peoples into European culture.[190]
During the early High Middle Ages, Germany was ruled by theOttonian dynasty, which struggled to control the powerful dukes ruling overterritorial duchies, tracing back to the Migration period. In 1024, they were replaced by theSalian dynasty, who famously clashed with the papacy under EmperorHenry IV(r. 1084–1105) over Church appointments as part of theInvestiture Controversy.[191]His successors continued to struggle against the papacy as well as the German nobility. A period of instability followed the death of EmperorHenry V(r. 1111–1125), who died without heirs, untilFrederick IBarbarossa(r. 1155–1190) took the imperial throne.[192]Although he ruled effectively, the basic problems remained, and his successors struggled into the 13th century.[193]Barbarossa's grandsonFrederick II(r. 1220–1250), who was also heir to the throne of Sicily through his mother, clashed repeatedly with the papacy. His court was famous for its scholars, and he was often accused ofheresy.[194]He and his successors faced many difficulties, including the invasion of theMongolsinto Europe in the mid-13th century. Mongols first shattered the Kyivan Rus' principalities and theninvaded Eastern Europein 1241, 1259, and 1287.[195]
Under theCapetian dynastythe French monarchy slowly began to expand its authority over the nobility, growing out of theÎle-de-Franceto exert control over more of the country in the 11th and 12th centuries.[196]They faced a powerful rival in theDukes of Normandy, who in 1066 underWilliam the Conqueror(duke 1035–1087), conquered England (r. 1066–1087) and created a cross-channel empire that lasted, in various forms, throughout the rest of the Middle Ages.[197][198]Normans also settled in Sicily and southern Italy, whenRobert Guiscard(d. 1085) landed there in 1059 and established a duchy that later became theKingdom of Sicily.[199]Under theAngevin dynastyofHenry II(r. 1154–1189) and his sonRichard I(r. 1189–1199), the kings of England ruled over England and large areas of France,[200][V]brought to the family by Henry II's marriage toEleanor of Aquitaine(d. 1204), heiress to much of southern France.[202][W]Richard's younger brotherJohn(r. 1199–1216) lost Normandy and the rest of the northern French possessions in 1204 to the French KingPhilip II Augustus(r. 1180–1223). This led to dissension among the English nobility. John's financial exactions to pay for his unsuccessful attempts to regain Normandy led in 1215 toMagna Carta, a charter that confirmed the rights and privileges of free men in England. UnderHenry III(r. 1216–1272), John's son, further concessions were made to the nobility, and royal power was diminished.[203]The French monarchy continued to make gains against the nobility during the late 12th and 13th centuries, bringing more territories within the kingdom under the king's personal rule and centralising the royal administration.[204]UnderLouis IX(r. 1226–1270), royal prestige rose to new heights as Louis served as a mediator for most of Europe.[205][X]
In Iberia, the Christian states, which had been confined to the north-western part of the peninsula, began to push back against the Islamic states in the south, a period known as theReconquista.[207]By about 1150, the Christian north had coalesced into the five major kingdoms ofLeón,Castile,Aragon,Navarre, andPortugal.[208]Southern Iberia remained under control of Islamic states, initially under theCaliphate of Córdoba, which broke up in 1031 into a shifting number of petty states known astaifas,[207]who fought with the Christians until theAlmohad Caliphatere-established centralised rule over Southern Iberia in the 1170s.[209]Christian forces advanced again in the early 13th century, culminating in the capture ofSevillein 1248.[210]
In the 11th century, theSeljuk Turkstook over much of the Middle East, occupying Persia during the 1040s, Armenia in the 1060s, and Jerusalem in 1070. In 1071, the Turkish army defeated the Byzantine army at theBattle of Manzikertand captured the Byzantine EmperorRomanus IV(r. 1068–1071). The Turks were then free to invade Asia Minor, which dealt a dangerous blow to the Byzantine Empire by seizing a large part of its population and its economic heartland. Although the Byzantines regrouped and recovered somewhat, they never fully regained Asia Minor and were often on the defensive. The Turks also had difficulties, losing control of Jerusalem to theFatimidsof Egypt and suffering from a series of internal civil wars.[212]The Byzantines also faced a revivedBulgaria, which in the late 12th and 13th centuries spread throughout the Balkans.[213]
The Crusades were intended to seizeJerusalemfrom Muslim control. TheFirst Crusadewas proclaimed by PopeUrban II(pope 1088–1099) at theCouncil of Clermontin 1095 in response to a request from the Byzantine EmperorAlexios I Komnenos(r. 1081–1118) for aid against further Muslim advances. Urban promisedindulgenceto anyone who took part. Tens of thousands of people from all levels of society mobilised across Europe and captured Jerusalem in 1099.[214]One feature of the crusades was thepogromsagainst local Jews that often took place as the crusaders left their countries for the East. These were especially brutal during the First Crusade,[79]when the Jewish communities inCologne,Mainz, andWormswere destroyed, as well as other communities in cities between the riversSeineand the Rhine.[215]Another outgrowth of the crusades was the foundation of a new type of monastic order, themilitary ordersof theTemplarsandHospitallers, which fused monastic life with military service.[216]
The Crusaders consolidated their conquests intoCrusader states. During the 12th and 13th centuries, there were a series of conflicts between them and the surrounding Islamic states. Appeals from the crusader states to the papacy led to further crusades,[214]such as theThird Crusade, called to try to regain Jerusalem, which had been captured bySaladin(d. 1193) in 1187.[217][Y]In 1203, theFourth Crusadewas diverted from the Holy Land to Constantinople, and captured the city in 1204, setting up aLatin Empire of Constantinople[219]and greatly weakening the Byzantine Empire. The Byzantines recaptured the city in 1261 but never regained their former strength.[220]By 1291, all the crusader states had been captured or forced from the mainland. However, a titularKingdom of Jerusalemsurvived on the island ofCyprusfor several years afterwards.[221]
Popes called for crusades elsewhere besides the Holy Land: in Spain, southern France, and along the Baltic.[214]The Spanish crusades became fused with theReconquistaof Spain from the Muslims. Although the Templars and Hospitallers took part in the Spanish crusades, similar Spanish military religious orders were founded, most of which had become part of the two main orders ofCalatravaandSantiagoby the beginning of the 12th century.[222]Northern Europe also remained outside Christian influence until the 11th century or later and became a crusading venue as part of the Northern Crusades of the 12th to 14th centuries. These crusades also spawned a military order, theOrder of the Sword Brothers. Another order, theTeutonic Knights, although founded in the crusader states, focused much of its activity in the Baltic after 1225 and, in 1309, moved its headquarters toMarienburginPrussia.[223]
During the 11th century, developments in philosophy and theology led to increased intellectual activity. There was a debate between therealistsand thenominalistsover the concept of "universals". Philosophical discourse was stimulated by the rediscovery ofAristotleand his emphasis onempiricismandrationalism. Scholars such asPeter Abelard(d. 1142) andPeter Lombard(d. 1164) introducedAristotelian logicinto theology. In the late 11th and early 12th centuriescathedral schoolsspread throughout Western Europe, signalling the shift of learning from monasteries to cathedrals and towns.[224]Cathedral schools were in turn replaced by theuniversitiesestablished in major European cities.[225]Philosophy and theology fused inscholasticism, an attempt by 12th- and 13th-century scholars to reconcile authoritative texts, most notably Aristotle and the Bible. This movement tried to employ a systemic approach to truth and reason[226]and culminated in the thought ofThomas Aquinas(d. 1274), who wrote theSumma Theologica, orSummary of Theology.[227]
Chivalryand the ethos ofcourtly lovedeveloped in royal and noble courts. This culture was expressed in thevernacular languagesrather than Latin and comprised poems, stories, legends, and popular songs spread bytroubadoursorMinnesängers, or wandering minstrels. Often the stories were written down in thechansons de geste, or "songs of great deeds", such asThe Song of RolandorThe Song of Hildebrand.[228]Secular and religious histories were also produced.[229]Geoffrey of Monmouth(d. c. 1155) composed hisHistoria Regum Britanniae, a collection of stories and legends aboutArthur.[230]Other works were more clearly history, such asOtto von Freising's (d. 1158)Gesta Friderici Imperatorisdetailing the deeds of Emperor Frederick Barbarossa, orWilliam of Malmesbury's (d. c. 1143)Gesta Regumon the kings of England.[229]
Legal studies advanced during the 12th century. Both secular law andcanon law, or ecclesiastical law, were studied in the High Middle Ages. Secular law, or Roman law, was significantly advanced by the discovery of theCorpus Juris Civilisin the 11th century, and by 1100, Roman law was being taught atBologna. This led to the recording and standardisation of legal codes throughout Western Europe. Canon law was also studied, and around 1140, a monk namedGratian(fl. 12th century), a teacher at Bologna, wrote what became the standard text of canon law—theDecretum.[231]
Among the results of the Greek and Islamic influence on this period in European history was the replacement ofRoman numeralswith thedecimalpositional number systemand the invention ofalgebra, which allowed more advanced mathematics. Astronomy advanced following the translation ofPtolemy'sAlmagestfrom Greek into Latin in the late 12th century. Medicine was also studied, especially in southern Italy, where Islamic medicine influenced theschool at Salerno.[232]
In the 12th and 13th centuries, Europe experienced economic growth and innovations in methods of production. Significant technological advances included the invention of thewindmill, the first mechanical clocks, the manufacture ofdistilled spirits, and the use of theastrolabe.[234]Concave spectacles were invented around 1286 by an unknown Italian artisan, probably working in or near Pisa.[235]
The development of a three-fieldrotation systemfor planting crops[162][Z]increased the usage of land from one-half in use each year under the old two-field system to two-thirds under the new system, with a consequent increase in production.[236]The development of theheavy ploughallowed heavier soils to be farmed more efficiently, aided by the spread of thehorse collar, which led to the use ofdraught horsesin place of oxen. Horses are faster than oxen and require less pasture, factors that aided the implementation of the three-field system.[237]Legumes – such as peas, beans, or lentils – were grown more widely as crops, in addition to the usual cereal crops of wheat, oats, barley, and rye.[238]
The construction ofcathedralsand castles advanced building technology, developing large stone buildings. Ancillary structures included new town halls, houses, bridges, andtithe barns.[239]Shipbuilding improved with the use of therib and plankmethod rather than the old Roman system ofmortise and tenon. Other improvements to ships included the use oflateensails and thestern-post rudder, both of which increased the speed at which ships could be sailed.[240]
In military affairs, the use of infantry with specialised roles increased. Along with the still-dominant heavy cavalry, armies often included mounted and infantrycrossbowmen, as well assappersand engineers.[241]Crossbows, which had been known in Late Antiquity, increased in use partly because of the increase insiegewarfare in the 10th and 11th centuries.[157][AA]The increasing use of crossbows during the 12th and 13th centuries led to the use of closed-facehelmets, heavy body armour, as well ashorse armour.[243]Gunpowderwas known in Europe by the mid-13th century with a recorded use in European warfare by the English against the Scots in 1304. However, it was merely used as an explosive and not as a weapon.Cannonwere being used for sieges in the 1320s, and hand-held guns were in use by the 1360s.[244]
In the 10th century, the establishment of churches and monasteries led to the development of stone architecture that elaborated vernacular Roman forms, from which the term "Romanesque" was derived. Where available, Romanbrickand stone buildings were recycled for their materials. From the tentative beginnings known as theFirst Romanesque, the style flourished and spread across Europe in a remarkably homogeneous form. Just before 1000, a great wave of stone churches were being built all over Europe.[245]Romanesquebuildings have massive stone walls, openings topped by semi-circular arches, small windows, and, particularly in France, arched stone vaults.[246]The largeportalwith coloured sculpture inhigh reliefbecame a central feature of façades, especially in France, and thecapitalsof columns were often carved with narrative scenes of imaginative monsters and animals.[247]According to art historianC. R. Dodwell, "virtually all the churches in the West were decorated with wall-paintings", of which few survive.[248]Simultaneous with the development in church architecture, the distinctive European form of the castle was developed and became crucial to politics and warfare.[249]
Romanesque art, especially metalwork, was at its most sophisticated inMosan art, in which distinct artistic personalities, includingNicholas of Verdun(d. 1205), become apparent. An almostclassical styleis seen in works such as afont at Liège,[250]contrasting with the writhing animals of the exactly contemporaryGloucester Candlestick. Large illuminated bibles andpsalterswere the typical forms of luxury manuscripts, and wall-painting flourished in churches, often following a scheme with aLast Judgementon the west wall, aChrist in Majestyat the east end, and narrative biblical scenes down the nave, or in the best surviving example, atSaint-Savin-sur-Gartempe, on thebarrel-vaultedroof.[251]
From the early 12th century, French builders developed theGothicstyle, marked by the use ofrib vaults,pointed arches,flying buttresses, and largestained glasswindows. It was used mainly in churches and cathedrals and continued until the 16th century in much of Europe. Classic examples of Gothic architecture includeChartres CathedralandReims Cathedralin France, as well asSalisbury Cathedralin England.[252]Stained glass became a crucial element in the design of churches, which continued to use extensive wall-paintings, now almost all lost.[253]
During this period, the practice of manuscript illumination gradually passed from monasteries to lay workshops, so that according toJanetta Benton"by 1300 most monks bought their books in shops",[254]and thebook of hoursdeveloped as a form of devotional book for lay-people. Metalwork remained the most prestigious art form, withLimoges enamela popular and relatively affordable option for objects such as reliquaries and crosses.[255]In Italy the innovations ofCimabueandDuccio, followed by theTrecentomasterGiotto(d. 1337), greatly increased the sophistication and status ofpanel paintingandfresco.[256]Increasing prosperity during the 12th century resulted in greater production of secular art; manycarved ivoryobjects such as gaming-pieces, combs, and small religious figures have survived.[257]
Monastic reform became an important issue during the 11th century, as elites began to worry that monks were not adhering to the rules binding them to a strictly religious life.Cluny Abbey, founded in theMâconregion of France in 909, was established as part of theCluniac Reforms, a larger movement of monastic reform in response to this fear.[259]Cluny quickly established a reputation for austerity and rigour. It sought to maintain a high quality of spiritual life by placing itself under the protection of the papacy and by electing its own abbot without interference from laymen, thus maintaining economic and political independence from local lords.[260]
Monastic reform inspired change in the secular Church. The ideals upon which it was based were brought to the papacy by PopeLeo IX(pope 1049–1054) and provided the ideology of clerical independence that led to the Investiture Controversy in the late 11th century. This involved PopeGregory VII(pope 1073–1085) and Emperor Henry IV, who initially clashed over episcopal appointments, a dispute that turned into a battle over the ideas ofinvestiture, clerical marriage, andsimony. The emperor saw the protection of the Church as one of his responsibilities and wanted to preserve the right to appoint his own choices as bishops within his lands. Still, the papacy insisted on the Church's independence from secular lords. These issues remained unresolved after the compromise of 1122, known as theConcordat of Worms. The dispute represents a significant stage in creating a papal monarchy separate from and equal tolayauthorities. It also had the permanent consequence of empowering German princes at the expense of the German emperors.[259]
The High Middle Ages was a period of great religious movements. Besides the Crusades and monastic reforms, people sought to participate in new forms of religious life. New monastic orders were founded, including theCarthusiansand theCistercians. The latter, in particular, expanded rapidly in their early years under the guidance ofBernard of Clairvaux(d. 1153). These new orders were formed in response to the feeling of the laity that Benedictine monasticism no longer met the needs of the laymen, who, along with those wishing to enter the religious life, wanted a return to the simplerhermeticalmonasticism of early Christianity, or to live anApostolic life.[216]Religious pilgrimageswere also encouraged. Old pilgrimage sites such as Rome, Jerusalem, andCompostelareceived increasing numbers of visitors, and new sites such asMonte GarganoandBarirose to prominence.[261]
In the 13th centurymendicant orders—theFranciscansand theDominicans—who swore vows of poverty and earned their living by begging, were approved by the papacy.[262]Religious groups such as theWaldensiansand theHumiliatialso attempted to return to the life of early Christianity in the middle 12th and early 13th centuries, another heretical movement condemned by the papacy. Others joined theCathars, another movement condemned as heretical by the papacy. In 1209, a crusade was preached against the Cathars, theAlbigensian Crusade, which, in combination with themedieval Inquisition, eliminated them.[263]
The first years of the 14th century were marked by famines, culminating in theGreat Famine of 1315–1317.[264]The causes of the Great Famine included the slow transition from theMedieval Warm Periodto theLittle Ice Age, which left the population vulnerable when bad weather caused crop failures.[265]The years 1313–1314 and 1317–1321 were excessively rainy throughout Europe, resulting in widespread crop failures.[266]The climate change—which resulted in a declining average annual temperature for Europe during the 14th century—was accompanied by an economic downturn.[267]
These troubles were followed in 1347 by theBlack Death, apandemicthat spread throughout Europe during the following three years.[268][AB]The death toll was probably about 35 million people in Europe, about one-third of the population. Towns were especially hard-hit because of their crowded conditions.[AC]Large areas of land were left sparsely inhabited, and in some places fields were left unworked. Wages rose as landlords sought to entice fewer available workers to their fields. Further problems were lower rents and lower demand for food, which cut into agricultural income. Urban workers also felt they had a right to greater earnings, andpopular uprisingsbroke out across Europe.[271]Among the uprisings were thejacqueriein France, thePeasants' Revoltin England, and revolts in the cities ofFlorencein Italy andGhentandBrugesin Flanders. The trauma of the plague led to an increased piety throughout Europe, manifested by the foundation of new charities, the self-mortification of theflagellants, and thescapegoating of Jews.[272]Conditions were further unsettled by the return of the plague throughout the rest of the 14th century; it continued to strike Europe periodically during the rest of the Middle Ages.[268]
Society throughout Europe was disturbed by the dislocations caused by the Black Death. Lands that had been marginally productive were abandoned as the survivors could acquire more fertile areas.[273]Althoughserfdomdeclined in Western Europe, it became more common in Eastern Europe, as landlords imposed it on those of their tenants who had previously been free.[274]Most peasants in Western Europe changed the work they had previously owed to their landlords into cash rents.[275]The percentage of serfs among the peasantry declined from a high of 90 to closer to 50 percent by the end of the period.[171]Landlords also became more conscious of common interests with other landholders and joined to extort their governments' privileges. Partly at the urging of landlords, governments attempted to legislate a return to the economic conditions that existed before the Black Death.[275]Non-clergy became increasingly literate, and urban populations began to imitate the nobility's interest in chivalry.[276]
Jewish communities wereexpelled from Englandin 1290 and fromFrance in 1306. Although some were allowed back into France, most were not. Many Jews emigrated eastwards,settling in Polandand Hungary.[277]The Jews were expelled fromSpain in 1492, and dispersed to Turkey, France, Italy, and Holland.[79]Therise of bankingin Italy during the 13th century continued throughout the 14th century, fuelled partly by the increasing warfare of the period and the needs of the papacy to move money between kingdoms. Many banking firms loaned money to royalty at great risk, as some were bankrupted when kings defaulted on their loans.[278][AD]
Strong, royalty-basednation statesrose throughout Europe in the Late Middle Ages, particularly inEngland,France, and the Christian kingdoms of the Iberian Peninsula:Aragon,Castile, andPortugal. The long conflicts of the period strengthened royal control over their kingdoms and were extremely hard on the peasantry. Kings profited from warfare that extended royal legislation and increased the lands they directly controlled.[279]Paying for the wars required that methods of taxation become more effective and efficient, and the rate of taxation often increased.[280]The requirement to obtain the consent of taxpayers allowed representative bodies such as theEnglish Parliamentand theFrench Estates Generalto gain power and authority.[281]
Throughout the 14th century, French kings sought to expand their influence at the expense of the territorial holdings of the nobility.[282]They ran into difficulties when attempting to confiscate the holdings of the English kings in southern France, leading to theHundred Years' War,[283]waged from 1337 to 1453.[284]Early in the war the English underEdward III(r. 1327–1377) and his sonEdward, the Black Prince(d. 1376),[AE]won the battles ofCrécyandPoitiers, captured the city ofCalais, and won control of much of France.[AF]The resulting stresses almost caused the disintegration of the French kingdom during the early years of the war.[287]In the early 15th century, France again came close to dissolving, but in the late 1420s, the military successes ofJoan of Arc(d. 1431) led to the victory of the French and the capture of the last English possessions in southern France in 1453.[288]The price was high, as the population of France at the end of the Wars was likely half what it had been at the start of the conflict. Conversely, the Wars positively affectedEnglish national identity, doing much to fuse the various local identities into a national English ideal. The conflict with France also helped create a national culture in England separate from French culture, which had previously been the dominant influence.[289]The dominance of the Englishlongbowbegan during early stages of the Hundred Years' War,[290]and cannon appeared on the battlefield at Crécy in 1346.[244]
In modern-day Germany, theHoly Roman Empirecontinued to rule, but the elective nature of the imperial crown meant there was no enduring dynasty around which a strong state could form.[291]Further east, the kingdoms ofPoland,Hungary, andBohemiagrew powerful.[292]In Iberia, the Christian kingdoms continued to gain land from the Muslim kingdoms of the peninsula;[293]Portugal concentrated on expanding overseas during the 15th century, while the other kingdoms were riven by difficulties over royal succession and other concerns.[294][295]After losing the Hundred Years' War, England went on to suffer a long civil war known as theWars of the Roses, which lasted into the 1490s[295]and only ended whenHenry Tudor(r. 1485–1509 as Henry VII) became king and consolidated power with his victory overRichard III(r. 1483–1485) atBosworthin 1485.[296]In Scandinavia,Margaret I of Denmark(r. in Denmark 1387–1412) consolidated Norway, Denmark, and Sweden in theUnion of Kalmar, which continued until 1523. The major power around the Baltic Sea was the Hanseatic League, a commercial confederation of city-states that traded from Western Europe to Russia.[297]Scotland emerged from English domination underRobert the Bruce(r. 1306–1329), who secured papal recognition of his kingship in 1328.[298]
Although thePalaeologiemperors recaptured Constantinople from the Western Europeans in 1261, they could never regain control of much of the former imperial lands. They usually controlled only a small section of the Balkan Peninsula near Constantinople, the city itself, and some coastal lands on theBlack Seaand around theAegean Sea. The former Byzantine lands in the Balkans were divided between the newKingdom of Serbia, theSecond Bulgarian Empire, and the city-state ofVenice. A new Turkish tribe threatened the power of the Byzantine emperors, theOttomans, who established themselves in Anatolia in the 13th century andsteadily expandedthroughout the 14th century. The Ottomans expanded into Europe, reducing Bulgaria to a vassal state by 1366 and taking over Serbia after its defeat at theBattle of Kosovoin 1389. Western Europeans rallied to the plight of the Christians in the Balkans and declared a new crusade in 1396; a great army was sent to the Balkans, where it was defeated at theBattle of Nicopolis.[299]Constantinople was finallycapturedby the Ottomans in 1453.[300]
During the tumultuous 14th century, disputes within the leadership of the Church led to theAvignon Papacyof 1309–1376,[301]also called the "Babylonian Captivity of the Papacy" (a reference to theBabylonian captivityof the Jews),[302]and then to theGreat Schism, lasting from 1378 to 1418, when there were two and later three rival popes, each supported by several states.[303]Ecclesiastical officials convened at theCouncil of Constancein 1414, and in the following year the council deposed one of the rival popes leaving only two claimants. Further depositions followed, and in November 1417, the council electedMartin V(pope 1417–1431) as pope.[304]
Besides the schism, the Western Church was riven by theological controversies, some of which became heresies.John Wycliffe(d. 1384), an English theologian, was condemned as a heretic in 1415 for teaching that the laity should have access to the text of the Bible as well as for holding views on theEucharistthat were contrary to Church doctrine.[305]Wycliffe's teachings influenced two of the major heretical movements of the later Middle Ages:Lollardyin England andHussitismin Bohemia.[306]The Bohemian movement initiated with the teaching ofJan Hus, who was burned at the stake in 1415 after being condemned as a heretic by the Council of Constance. The Hussite Church, although the target of a crusade, survived beyond the Middle Ages.[307]Other heresies were manufactured, such as the accusations against the Knights Templar that resulted in their suppression in 1312, and the division of their great wealth between the French KingPhilip IV(r. 1285–1314) and the Hospitallers.[308]
The papacy further refined the practice in theMassin the Late Middle Ages, holding that the clergy alone was allowed to partake of the wine in the Eucharist. This further distanced the secular laity from the clergy. The laity continued the practices of pilgrimages, veneration of relics, and belief in the power of the Devil. Mystics such asMeister Eckhart(d. 1327) andThomas à Kempis(d. 1471) wrote works that taught the laity to focus on their inner spiritual life, which laid the groundwork for the Protestant Reformation. Besides mysticism, belief in witches and witchcraft became widespread. By the late 15th century, the Church had begun to lend credence to populist fears of witchcraft with its condemnation of witches in 1484 and the publication in 1486 of theMalleus Maleficarum, the most popular handbook for witch-hunters.[309]
During the Later Middle Ages, theologians such asJohn Duns Scotus(d. 1308) andWilliam of Ockham(d. c. 1348)[226]led a reaction against intellectualist scholasticism, objecting to the application of reason to faith. Their efforts undermined the prevailingPlatonicidea of universals. Ockham's insistence that reason operates independently of faith allowed science to be separated from theology and philosophy.[310]Legal studies were marked by the steady advance of Roman law into areas of jurisprudence previously governed bycustomary law. England was the lone exception to this trend, where thecommon lawremained pre-eminent. Other countries codified their laws; legal codes were promulgated in Castile, Poland, andLithuania.[311]
Education remained mostly focused on the training of future clergy. The basic learning of the letters and numbers remained the province of the family or a village priest, but the secondary subjects of thetrivium—grammar, rhetoric, logic—were studied in cathedral schools or schools provided by cities. Commercial secondary schools spread, and some Italian towns had more than one such enterprise. Universities also spread throughout Europe in the 14th and 15th centuries. Lay literacy rates rose but were still low; one estimate gave a literacy rate of 10 percent of males and 1 percent of females in 1500.[312]
The publication of vernacular literature increased, withDante(d. 1321),Petrarch(d. 1374) andGiovanni Boccaccio(d. 1375) in 14th-century Italy,Geoffrey Chaucer(d. 1400) andWilliam Langland(d. c. 1386) in England, andFrançois Villon(d. 1464) andChristine de Pizan(d. c. 1430) in France. Much literature remained religious, and although a great deal of it continued to be written in Latin, a new demand developed for saints' lives and other devotional tracts in the vernacular languages.[311]This was fed by the growth of theDevotio Modernamovement, most prominently in the formation of theBrethren of the Common Life, but also in the works ofGerman mysticssuch as Meister Eckhart andJohannes Tauler(d. 1361).[313]Theatre also developed in the guise ofmiracle playsput on by the Church.[311]At the end of the period, the development of theprinting pressbyJohannes Gutenbergin about 1450 led to the establishment of publishing houses throughout Europe by 1500.[314]
In the early 15th century, the countries of theIberian Peninsulabegan to sponsor exploration beyond the boundaries of Europe. PrinceHenry the Navigatorof Portugal (d. 1460) sent expeditions that discovered theCanary Islands, theAzores, andCape Verdeduring his lifetime. After his death, exploration continued;Bartolomeu Dias(d. 1500) went around theCape of Good Hopein 1486, andVasco da Gama(d. 1524) sailed around Africa to India in 1498.[315]The combined Spanish monarchies of Castile and Aragon sponsored the voyage of exploration byChristopher Columbus(d. 1506) in 1492 thatdiscovered the Americas.[316]The English crown underHenry VIIsponsored the voyage ofJohn Cabot(d. 1498) in 1497, which landed onCape Breton Island.[317]
One of the major developments in the military sphere during the Late Middle Ages was the increased use of infantry and light cavalry.[318]The English also employed longbowmen, but other countries were unable to create similar forces with the same success.[319]Armour continued to advance, spurred by the increasing power of crossbows, andplate armourwas developed to protect soldiers from crossbows as well as the hand-held guns that were developed.[320]Pole armsreached new prominence with the development of the Flemish and Swiss infantry armed with pikes and other long spears.[321]
In agriculture, the increased usage of sheep with long-fibred wool allowed a stronger thread to be spun. In addition, thespinning wheelreplaced the traditionaldistafffor spinning wool, tripling production.[322][AG]A less technological refinement that still greatly affected daily life was the use of buttons as closures for garments, which allowed for better fitting without having to lace clothing on the wearer.[324]Windmills were refined with the creation of thetower mill, allowing the upper part of the windmill to be spun around to face the direction from which the wind was blowing.[325]Theblast furnaceappeared around 1350 in Sweden, increasing the quantity of iron produced and improving its quality.[326]The firstpatent lawin 1447 in Venice protected the rights of inventors to their inventions.[327]
The Late Middle Ages in Europe correspond to Italy's Trecento andEarly Renaissancecultural periods. Northern Europe and Spain continued to use Gothic styles, which became increasingly elaborate in the 15th century until almost the end.International Gothicwas a courtly style that reached much of Europe in the decades around 1400, producing masterpieces such as theTrès Riches Heures du Duc de Berry.[328]All over Europe secular art continued to increase in quantity and quality. In the 15th century, the mercantile classes of Italy and Flanders became important patrons, commissioning small portraits of themselves in oils as well as a growing range of luxury items such as jewellery,ivory caskets,cassonechests, andmaiolicapottery. These objects also included theHispano-Moresque wareproduced by mostlyMudéjarpotters in Spain. Although royalty owned huge plate collections, little survives except for theRoyal Gold Cup.[329]Italian silk manufacture developed so that Western churches and elites no longer needed to rely on imports from Byzantium or the Islamic world. In France and Flanders,tapestryweaving of sets likeThe Lady and the Unicornbecame a major luxury industry.[330]
The large external sculptural schemes of Early Gothic churches gave way to more sculpture inside the building, as tombs became more elaborate and other features such as pulpits were sometimes lavishly carved, as in thePulpit by Giovanni Pisano in Sant'Andrea. Painted or carved wooden reliefaltarpiecesbecame common, especially as churches created manyside-chapels.Early Netherlandish paintingby artists such asJan van Eyck(d. 1441) andRogier van der Weyden(d. 1464) rivalled that of Italy, as did northern illuminated manuscripts, which in the 15th century began to be collected on a large scale by secular elites, who also commissioned secular books, especially histories. From about 1450, printed books rapidly became popular, though still expensive. There were around 30,000 different editions ofincunabula, or works printed before 1500,[331]by which time illuminated manuscripts were commissioned only by royalty and a few others. Very smallwoodcuts, nearly all religious, were affordable even by peasants in parts of Northern Europe from the middle of the 15th century. More expensiveengravingssupplied a wealthier market with various images.[332]
The medieval period is frequently caricatured as a "time of ignorance and superstition" that placed "the word of religious authorities over personal experience and rational activity."[333]This is a legacy from both theRenaissanceandEnlightenmentwhen scholars favourably contrasted their intellectual cultures with those of the medieval period. Renaissance scholars saw the Middle Ages as a period of decline from the high culture and civilisation of the Classical world. Enlightenment scholars saw reason as superior to faith and thus viewed the Middle Ages as a time of ignorance and superstition.[16]
Others argue that reason was held in high regard during the Middle Ages. Science historianEdward Grantwrites, "If revolutionary rational thoughts were expressed [in the 18th century], they were only made possible because of the long medieval tradition that established the use of reason as one of the most important of human activities".[334]Also, contrary to common belief,David Lindbergwrites, "the late medieval scholar rarely experienced the coercive power of the Church and would have regarded himself as free (particularly in the natural sciences) to follow reason and observation wherever they led".[335]
The caricature of the period is also reflected in some more specific notions. One misconception, first propagated in the 19th century[336]and still very common, is that all people in the Middle Ages believed that theEarth was flat.[336]This is untrue, as lecturers in medieval universities commonly argued that evidence showed the Earth was a sphere.[337]Lindberg andRonald Numbers, another scholar of the period, state that there "was scarcely a Christian scholar of the Middle Ages who did not acknowledge [Earth's] sphericity and even know its approximate circumference".[338]Other misconceptions such as "the Church prohibited autopsies and dissections during the Middle Ages", "the rise of Christianity killed off ancient science", or "the medieval Christian Church suppressed the growth of natural philosophy", are all cited by Numbers as examples of widely popular myths that still pass as historical truth, although they are not supported by historical research.[339]
TheGreat Depressionwas a severe globaleconomic downturnfrom 1929 to 1939. The period was characterized by high rates ofunemploymentandpoverty, drastic reductions in industrial production and international trade, and widespread bank and business failures around the world. Theeconomic contagionbegan in 1929 in theUnited States, the largest economy in the world, with the devastatingWall Street stock market crash of October 1929often considered the beginning of the Depression. Among the countries with the most unemployed were the U.S., theUnited Kingdom, andGermany.
The Depression was preceded by a period of industrial growth and social development known as the "Roaring Twenties". Much of the profit generated by the boom was invested inspeculation, such as on thestock market, contributing to growingwealth inequality. Banks were subject tominimal regulation, resulting in loose lending and widespread debt. By 1929, declining spending had led to reductions in manufacturing output and rising unemployment. Share values continued to rise until the October 1929 crash, after which the slide continued until July 1932, accompanied by a loss of confidence in the financial system. By 1933, the U.S. unemployment rate had risen to 25%, about one-third of farmers had lost their land, and 9,000 of its 25,000 banks had gone out of business. PresidentHerbert Hooverwas unwilling to intervene heavily in the economy, and in 1930 he signed theSmoot–Hawley Tariff Act, which worsened the Depression. In the1932 presidential election, Hoover was defeated byFranklin D. Roosevelt, who from 1933 pursued a set of expansiveNew Dealprograms in order to provide relief and create jobs. In Germany, which depended heavily on U.S. loans, the crisis caused unemployment to rise to nearly 30% and fueled political extremism, paving the way forAdolf Hitler'sNazi Partyto rise to power in 1933.
Between 1929 and 1932, worldwidegross domestic product(GDP) fell by an estimated 15%; in the U.S., the Depression resulted in a 30% contraction in GDP.[1]Recovery varied greatly around the world. Some economies, such as the U.S., Germany and Japan started to recover by the mid-1930s; others, like France, did not return to pre-shock growth rates until later in the decade.[2]The Depression had devastating economic effects on both wealthy and poor countries: all experienced drops inpersonal income, prices (deflation), tax revenues, and profits. International trade fell by more than 50%, and unemployment in some countries rose as high as 33%.[3]Cities around the world, especially those dependent onheavy industry, were heavily affected. Construction virtually halted in many countries, and farming communities and rural areas suffered as crop prices fell by up to 60%.[4][5][6]Faced with plummeting demand and few job alternatives, areas dependent onprimary sector industriessuffered the most.[7]The outbreak ofWorld War IIin 1939 ended the Depression, as it stimulated factory production, providing jobs for women as militaries absorbed large numbers of young, unemployed men.
The precise causes for the Great Depression are disputed. One set of historians, for example, focuses on non-monetary economic causes. Among these, some regard the Wall Street crash itself as the main cause; others consider that the crash was a mere symptom of more general economic trends of the time, which had already been underway in the late 1920s.[3][8]A contrasting set of views, which rose to prominence in the later part of the 20th century,[9]ascribes a more prominent role to failures ofmonetary policy. According to those authors, while general economic trends can explain the emergence of the downturn, they fail to account for its severity and longevity; they argue that these were caused by the lack of an adequate response to the crises of liquidity that followed the initial economic shock of 1929 and the subsequent bank failures accompanied by a general collapse of the financial markets.[1]
After theWall Street crash of 1929, when theDow Jones Industrial Averagedropped from 381 to 198 over the course of two months, optimism persisted for some time. The stock market rose in early 1930, with the Dow returning to 294 (pre-depression levels) in April 1930, before steadily declining for years, to a low of 41 in 1932.[10]
At the beginning, governments and businesses spent more in the first half of 1930 than in the corresponding period of the previous year. On the other hand, consumers, many of whom suffered severe losses in the stock market the previous year, cut expenditures by 10%. In addition, beginning in the mid-1930s, asevere droughtravaged the agricultural heartland of the U.S.[11]
Interest rates dropped to low levels by mid-1930, but expecteddeflationand the continuing reluctance of people to borrow meant that consumer spending and investment remained low.[12]By May 1930, automobile sales declined to below the levels of 1928. Prices, in general, began to decline, although wages held steady in 1930. Then adeflationary spiralstarted in 1931. Farmers faced a worse outlook; declining crop prices and a Great Plains drought crippled their economic outlook. At its peak, the Great Depression saw nearly 10% of all Great Plains farms change hands despite federal assistance.[13]
At first, the decline in theU.S. economywas the factor that triggered economic downturns in most other countries due to a decline in trade, capital movement, and global business confidence. Then, internal weaknesses or strengths in each country made conditions worse or better. For example, the U.K. economy, which experienced an economic downturn throughout most of the late 1920s, was less severely impacted by the shock of the depression than the U.S. By contrast, the German economy saw a similar decline in industrial output as that observed in the U.S.[14]Some economic historians attribute the differences in the rates of recovery and relative severity of the economic decline to whether particular countries had been able to effectively devaluate their currencies or not. This is supported by the contrast in how the crisis progressed in, e.g., Britain, Argentina and Brazil, all of which devalued their currencies early and returned to normal patterns of growth relatively rapidly and countries which stuck to thegold standard, such as France or Belgium.[15]
Frantic attempts by individual countries to shore up their economies throughprotectionistpolicies – such as the 1930 U.S.Smoot–Hawley Tariff Actand retaliatory tariffs in other countries – exacerbated the collapse in global trade, contributing to the depression.[16]By 1933, the economic decline pushed world trade to one third of its level compared to four years earlier.[17]
While the precise causes for the occurrence of the Great Depression are disputed and can be traced to both global and national phenomena, its immediate origins are most conveniently examined in the context of the U.S. economy, from which the initial crisis spread to the rest of the world.[19]
In the aftermath ofWorld War I, theRoaring Twentiesbrought considerable wealth to the United States and Western Europe.[20]Initially, the year 1929 dawned with good economic prospects: despite a minor crash on 25 March 1929, the market seemed to gradually improve through September. Stock prices began to slump in September, and were volatile at the end of the month.[21]A large sell-off of stocks began in mid-October. Finally, on 24 October,Black Thursday, the American stock market crashed 11% at the opening bell. Actions to stabilize the market failed, and on 28 October, Black Monday, the market crashed another 12%. The panic peaked the next day on Black Tuesday, when the market saw another 11% drop.[22][23]Thousands of investors were ruined, and billions of dollars had been lost; many stocks could not be sold at any price.[23]The market recovered 12% on Wednesday but by then significant damage had been done. Though the market entered a period of recovery from 14 November until 17 April 1930, the general situation had been a prolonged slump. From September 1929 to 8 July 1932, the market lost 85% of its value.[24]
Despite the crash, the worst of the crisis did not reverberate around the world until after 1929. The crisis hit panic levels again in December 1930, with abank runon theBank of United States, a former privately run bank, bearing no relation to the U.S. government (not to be confused with theFederal Reserve). Unable to pay out to all of its creditors, the bank failed.[25][26]Among the 608 American banks that closed in November and December 1930, the Bank of United States accounted for a third of the total $550 million deposits lost and, with its closure, bank failures reached a critical mass.[27]
In an initial response to the crisis, the U.S. Congress passed theSmoot–Hawley Tariff Acton 17 June 1930. The Act was ostensibly aimed at protecting the American economy from foreign competition by imposing high tariffs on foreign imports. The consensus view amongeconomistsand economic historians (includingKeynesians,MonetaristsandAustrian economists) is that the passage of the Smoot–Hawley Tariff had, in fact, achieved an opposite effect to what was intended. It exacerbated the Great Depression[28]by preventing economic recovery after domestic production recovered, hampering the volume of trade; still there is disagreement as to the precise extent of the Act's influence.
In the popular view, the Smoot–Hawley Tariff was one of the leading causes of the depression.[29][30]In a 1995 survey of American economic historians, two-thirds agreed that theSmoot–Hawley Tariff Actat least worsened the Great Depression.[31]According to the U.S. Senate website, the Smoot–Hawley Tariff Act is among the most catastrophic acts in congressional history.[32]
Many economists have argued that the sharp decline in international trade after 1930 helped to worsen the depression, especially for countries significantly dependent on foreign trade. Most historians and economists blame the Act for worsening the depression by seriously reducing international trade and causing retaliatory tariffs in other countries. While foreign trade was a small part of overall economic activity in the U.S. and was concentrated in a few businesses like farming, it was a much larger factor in many other countries.[33]The averagead valorem(value based) rate of duties on dutiable imports for 1921–1925 was 25.9% but under the new tariff it jumped to 50% during 1931–1935. In dollar terms, American exports declined over the next four years from about $5.2 billion in 1929 to $1.7 billion in 1933; so, not only did the physical volume of exports fall, but also the prices fell by about1⁄3as written. Hardest hit were farm commodities such as wheat, cotton, tobacco, and lumber.[34]
Governments around the world took various steps into spending less money on foreign goods such as: "imposing tariffs, import quotas, and exchange controls". These restrictions triggered much tension among countries that had large amounts of bilateral trade, causing major export-import reductions during the depression. Not all governments enforced the same measures of protectionism. Some countries raised tariffs drastically and enforced severe restrictions on foreign exchange transactions, while other countries reduced "trade and exchange restrictions only marginally":[35]
Thegold standardwas the primary transmission mechanism of the Great Depression. Even countries that did not face bank failures and a monetary contraction first-hand were forced to join the deflationary policy since higher interest rates in countries that performed a deflationary policy led to a gold outflow in countries with lower interest rates. Under the gold standard'sprice–specie flow mechanism, countries that lost gold but nevertheless wanted to maintain the gold standard had to permit their money supply to decrease and the domestic price level to decline (deflation).[36][37]
Some economic studies have indicated that the rigidities of thegold standardnot only spread the downturn worldwide, but also suspended gold convertibility (devaluing the currency in gold terms) that did the most to make recovery possible.[39]
Every major currency left the gold standard during the Great Depression. The UK was the first to do so. Facingspeculative attackson thepoundand depletinggold reserves, in September 1931 theBank of Englandceased exchanging pound notes for gold and the pound was floated on foreign exchange markets. Japan and the Scandinavian countries followed in 1931. Other countries, such as Italy and the United States, remained on the gold standard into 1932 or 1933, while a few countries in the so-called "gold bloc", led by France and including Poland, Belgium and Switzerland, stayed on the standard until 1935–36.
According to later analysis, the earliness with which a country left the gold standard reliably predicted its economic recovery. For example, The UK and Scandinavia, which left the gold standard in 1931, recovered much earlier than France and Belgium, which remained on gold much longer. Countries such as China, which had asilver standard, almost avoided the depression entirely. The connection between leaving the gold standard as a strong predictor of that country's severity of its depression and the length of time of its recovery has been shown to be consistent for dozens of countries, includingdeveloping countries. This partly explains why the experience and length of the depression differed between regions and states around the world.[40]
The financial crisis escalated out of control in mid-1931, starting with the collapse of theCredit Anstaltin Vienna in May.[41][42]This put heavy pressure on Germany, which was already in political turmoil. With the rise in violence of National Socialist ('Nazi') and Communist movements, as well as investor nervousness at harsh government financial policies,[43]investors withdrew their short-term money from Germany as confidence spiraled downward. The Reichsbank lost 150 million marks in the first week of June, 540 million in the second, and 150 million in two days, 19–20 June. Collapse was at hand. U.S. President Herbert Hoover called for amoratorium on payment of war reparations. This angered Paris, which depended on a steady flow of German payments, but it slowed the crisis down, and the moratorium was agreed to in July 1931. An International conference in London later in July produced no agreements but on 19 August a standstill agreement froze Germany's foreign liabilities for six months. Germany received emergency funding from private banks in New York as well as the Bank of International Settlements and the Bank of England. The funding only slowed the process. Industrial failures began in Germany, a major bank closed in July and a two-day holiday for all German banks was declared. Business failures were more frequent in July, and spread toRomaniaand Hungary. The crisis continued to get worse in Germany, bringing political upheaval that finally led to thecoming to power of Hitler's Nazi regimein January 1933.[44]
The world financial crisis now began to overwhelm Britain; investors around the world started withdrawing their gold from London at the rate of £2.5 million per day.[45]Credits of £25 million each from the Bank of France and the Federal Reserve Bank of New York and an issue of £15 million fiduciary note slowed, but did not reverse, the British crisis. The financial crisis now caused a major political crisis in Britain in August 1931. With deficits mounting, the bankers demanded a balanced budget; the divided cabinet of Prime Minister Ramsay MacDonald's Labour government agreed; it proposed to raise taxes, cut spending, and most controversially, to cut unemployment benefits 20%. The attack on welfare was unacceptable to the Labour movement. MacDonald wanted to resign, but King George V insisted he remain and form an all-party coalition "National Government". The Conservative and Liberals parties signed on, along with a small cadre of Labour, but the vast majority of Labour leaders denounced MacDonald as a traitor for leading the new government. Britain went off thegold standard, and suffered relatively less than other major countries in the Great Depression. In the 1931 British election, the Labour Party was virtually destroyed, leaving MacDonald as prime minister for a largely Conservative coalition.[46][47]
In most countries of the world, recovery from the Great Depression began in 1933.[8]In the U.S., recovery began in early 1933,[8]but the U.S. did not return to 1929 GNP for over a decade and still had an unemployment rate of about 15% in 1940, albeit down from the high of 25% in 1933.
There is no consensus among economists regarding the motive force for the U.S. economic expansion that continued through most of theRoosevelt years(and the 1937 recession that interrupted it). The common view among most economists is that Roosevelt'sNew Dealpolicies either caused or accelerated the recovery, although his policies were never aggressive enough to bring the economy completely out of recession. Some economists have also called attention to the positive effects from expectations ofreflationand rising nominal interest rates that Roosevelt's words and actions portended.[49][50]It was the rollback of those same reflationary policies that led to the interruption of a recession beginning in late 1937.[51][52]One contributing policy that reversed reflation was theBanking Act of 1935, which effectively raised reserve requirements, causing a monetary contraction that helped to thwart the recovery.[53]GDP returned to its upward trend in 1938.[48]A revisionist view among some economists holds that the New Deal prolonged the Great Depression, as they argue thatNational Industrial Recovery Act of 1933andNational Labor Relations Act of 1935restricted competition and established price fixing.[54]John Maynard Keynesdid not think that the New Deal under Roosevelt single-handedly ended the Great Depression: "It is, it seems, politically impossible for a capitalistic democracy to organize expenditure on the scale necessary to make the grand experiments which would prove my case—except in war conditions."[55]
According toChristina Romer, the money supply growth caused by huge international gold inflows was a crucial source of the recovery of the United States economy, and that the economy showed little sign of self-correction. The gold inflows were partly due todevaluation of the U.S. dollarand partly due to deterioration of the political situation in Europe.[56]In their book,A Monetary History of the United States,Milton FriedmanandAnna J. Schwartzalso attributed the recovery to monetary factors, and contended that it was much slowed by poor management of money by theFederal Reserve System.Chairman of the Federal Reserve(2006–2014)Ben Bernankeagreed that monetary factors played important roles both in the worldwide economic decline and eventual recovery.[57]Bernanke also saw a strong role for institutional factors, particularly the rebuilding and restructuring of the financial system,[58]and pointed out that the Depression should be examined in an international perspective.[59]
Women's primary role was as housewives; without a steady flow of family income, their work became much harder in dealing with food and clothing and medical care. Birthrates fell everywhere, as children were postponed until families could financially support them. The average birthrate for 14 major countries fell 12% from 19.3 births per thousand population in 1930, to 17.0 in 1935.[60]In Canada, half of Roman Catholic women defied Church teachings and used contraception to postpone births.[61]
Among the few women in the labor force, layoffs were less common in the white-collar jobs and they were typically found in light manufacturing work. However, there was a widespread demand to limit families to one paid job, so that wives might lose employment if their husband was employed.[62][63][64]Across Britain, there was a tendency for married women to join the labor force, competing for part-time jobs especially.[65][66]
In France, very slow population growth, especially in comparison to Germany continued to be a serious issue in the 1930s. Support for increasing welfare programs during the depression included a focus on women in the family. The Conseil Supérieur de la Natalité campaigned for provisions enacted in the Code de la Famille (1939) that increased state assistance to families with children and required employers to protect the jobs of fathers, even if they were immigrants.[67]
In rural and small-town areas, women expanded their operation of vegetable gardens to include as much food production as possible. In the United States, agricultural organizations sponsored programs to teach housewives how to optimize their gardens and to raise poultry for meat and eggs.[68]Rural women madefeed sack dressesand other items for themselves and their families and homes from feed sacks.[69]In American cities, African American women quiltmakers enlarged their activities, promoted collaboration, and trained neophytes. Quilts were created for practical use from various inexpensive materials and increased social interaction for women and promoted camaraderie and personal fulfillment.[70]
Oral history provides evidence for how housewives in a modern industrial city handled shortages of money and resources. Often they updated strategies their mothers used when they were growing up in poor families. Cheap foods were used, such as soups, beans and noodles. They purchased the cheapest cuts of meat—sometimes even horse meat—and recycled theSunday roastinto sandwiches and soups. They sewed and patched clothing, traded with their neighbors for outgrown items, and made do with colder homes. New furniture and appliances were postponed until better days. Many women also worked outside the home, or took boarders, did laundry for trade or cash, and did sewing for neighbors in exchange for something they could offer. Extended families used mutual aid—extra food, spare rooms, repair-work, cash loans—to help cousins and in-laws.[71]
In Japan, official government policy was deflationary and the opposite of Keynesian spending. Consequently, the government launched a campaign across the country to induce households to reduce their consumption, focusing attention on spending by housewives.[72]
In Germany, the government tried to reshape private household consumption under the Four-Year Plan of 1936 to achieve German economic self-sufficiency. The Nazi women's organizations, other propaganda agencies and the authorities all attempted to shape such consumption as economic self-sufficiency was needed to prepare for and to sustain the coming war. The organizations, propaganda agencies and authorities employed slogans that called up traditional values of thrift and healthy living. However, these efforts were only partly successful in changing the behavior of housewives.[73]
The common view among economic historians is that the Great Depression ended with the advent ofWorld War II. Many economists believe that government spending on the war caused or at least accelerated recovery from the Great Depression, though some consider that it did not play a very large role in the recovery, though it did help in reducing unemployment.[8][74][75][76]
The rearmament policies leading up to World War II helped stimulate the economies of Europe in 1937–1939. By 1937, unemployment in Britain had fallen to 1.5 million. Themobilizationof manpower following the outbreak of war in 1939 ended unemployment.[77]
The American mobilization forWorld War IIat the end of 1941 moved approximately 10 million people out of the civilian labor force and into the war.[78]This finally eliminated the last effects from the Great Depression and brought the U.S. unemployment rate down below 10%.[79]
World War II had a dramatic effect on many parts of the American economy.[80]Government-financed capital spending accounted for only 5% of the annual U.S. investment in industrial capital in 1940; by 1943, the government accounted for 67% of U.S. capital investment.[80]The massive war spending doubled economic growth rates, either masking the effects of the Depression or essentially ending the Depression. Businessmen ignored the mountingnational debtand heavy new taxes, redoubling their efforts for greater output to take advantage of generous government contracts.[81]
DuringWorld War Imany countries suspended theirgold standardin varying ways. There was high inflation from WWI, and in the 1920s in theWeimar Republic,Austria, and throughout Europe. In the late 1920s there was a scramble to deflate prices to get the gold standard's conversation rates back on track to pre-WWI levels, by causingdeflationand high unemployment through monetary policy. In 1933FDRsignedExecutive Order 6102and in 1934 signed theGold Reserve Act.[82]
The two classic competing economic theories of the Great Depression are theKeynesian(demand-driven) and theMonetaristexplanation.[84]There are also variousheterodox theoriesthat downplay or reject the explanations of the Keynesians and monetarists. The consensus among demand-driven theories is that a large-scale loss of confidence led to a sudden reduction in consumption and investment spending. Once panic and deflation set in, many people believed they could avoid further losses by keeping clear of the markets. Holding money became profitable as prices dropped lower and a given amount of money bought ever more goods, exacerbating the drop in demand.[85]Monetarists believe that the Great Depression started as an ordinary recession, but the shrinking of themoney supplygreatly exacerbated the economic situation, causing a recession to descend into the Great Depression.[86]
Economists and economic historians are almost evenly split as to whether the traditional monetary explanation that monetary forces were the primary cause of the Great Depression is right, or the traditional Keynesian explanation that a fall in autonomous spending, particularly investment, is the primary explanation for the onset of the Great Depression.[87]Today there is also significant academic support for thedebt deflationtheory and theexpectations hypothesisthat – building on the monetary explanation ofMilton FriedmanandAnna Schwartz– add non-monetary explanations.[88][89]
There is a consensus that theFederal Reserve Systemshould have cut short the process of monetary deflation and banking collapse, by expanding the money supply and acting aslender of last resort. If they had done this, the economic downturn would have been far less severe and much shorter.[90]
Insufficient spending, the money supply reduction, and debt on margin led to falling prices and further bankruptcies (Irving Fisher's debt deflation).
The monetarist explanation was given by American economistsMilton FriedmanandAnna J. Schwartz.[91]They argued that the Great Depression was caused by the banking crisis that caused one-third of all banks to vanish, a reduction of bank shareholder wealth and more importantlymonetary contractionof 35%, which they called "TheGreat Contraction". This caused a price drop of 33% (deflation).[92]By not lowering interest rates, by not increasing the monetary base and by not injecting liquidity into the banking system to prevent it from crumbling, the Federal Reserve passively watched the transformation of a normal recession into the Great Depression. Friedman and Schwartz argued that the downward turn in the economy, starting with the stock market crash, would merely have been an ordinary recession if the Federal Reserve had taken aggressive action.[93][94]This view was endorsed in 2002 byFederal Reserve GovernorBen Bernankein a speech honoring Friedman and Schwartz with this statement:
Let me end my talk by abusing slightly my status as an official representative of the Federal Reserve. I would like to say to Milton and Anna: Regarding the Great Depression, you're right. We did it. We're very sorry. But thanks to you, we won't do it again.
The Federal Reserve allowed some large public bank failures – particularly that of theNew York Bank of United States– which produced panic and widespread runs on local banks, and the Federal Reserve sat idly by while banks collapsed. Friedman and Schwartz argued that, if the Fed had provided emergency lending to these key banks, or simply boughtgovernment bondson theopen marketto provide liquidity and increase the quantity of money after the key banks fell, all the rest of the banks would not have fallen after the large ones did, and the money supply would not have fallen as far and as fast as it did.[97]
With significantly less money to go around, businesses could not get new loans and could not even get their old loans renewed, forcing many to stop investing. This interpretation blames the Federal Reserve for inaction, especially theNew York branch.[98]
One reason why the Federal Reserve did not act to limit the decline of the money supply was thegold standard. At that time, the amount of credit the Federal Reserve could issue was limited by theFederal Reserve Act, which required 40% gold backing of Federal Reserve Notes issued. By the late 1920s, the Federal Reserve had almost hit the limit of allowable credit that could be backed by the gold in its possession. This credit was in the form of Federal Reserve demand notes.[99]A "promise of gold" is not as good as "gold in the hand", particularly when they only had enough gold to cover 40% of the Federal Reserve Notes outstanding. During the bank panics, a portion of those demand notes was redeemed for Federal Reserve gold. Since the Federal Reserve had hit its limit on allowable credit, any reduction in gold in its vaults had to be accompanied by a greater reduction in credit. On 5 April 1933, President Roosevelt signedExecutive Order 6102making the private ownership ofgold certificates, coins and bullion illegal, reducing the pressure on Federal Reserve gold.[99]
British economistJohn Maynard Keynesargued inThe General Theory of Employment, Interest and Moneythat loweraggregate expendituresin the economy contributed to a massive decline in income and to employment that was well below the average. In such a situation, the economy reached equilibrium at low levels of economic activity and high unemployment.
Keynes's basic idea was simple: to keep people fully employed, governments have to run deficits when the economy is slowing, as the private sector would not invest enough to keep production at the normal level and bring the economy out of recession. Keynesian economists called on governments during times ofeconomic crisisto pick up the slack by increasinggovernment spendingor cutting taxes.
As the Depression wore on,Franklin D. Roosevelttriedpublic works,farm subsidies, and other devices to restart the U.S. economy, but never completely gave up trying to balance the budget. According to the Keynesians, this improved the economy, but Roosevelt never spent enough to bring the economy out of recession until the start ofWorld War II.[100]
Irving Fisherargued that the predominant factor leading to the Great Depression was a vicious circle of deflation and growing over-indebtedness.[101]He outlined nine factors interacting with one another under conditions of debt and deflation to create the mechanics of boom to bust. The chain of events proceeded as follows:
During the Crash of 1929 preceding the Great Depression, margin requirements were only 10%.[102]Brokerage firms, in other words, would lend $9 for every $1 an investor had deposited. When the market fell, brokerscalled in these loans, which could not be paid back.[103]Banks began to fail as debtors defaulted on debt and depositors attempted to withdraw their depositsen masse, triggering multiplebank runs. Government guarantees and Federal Reserve banking regulations to prevent such panics were ineffective or not used. Bank failures led to the loss of billions of dollars in assets.[103]
Outstanding debts became heavier, because prices and incomes fell by 20–50% but the debts remained at the same dollar amount. After the panic of 1929 and during the first 10 months of 1930, 744 U.S. banks failed. (In all, 9,000 banks failed during the 1930s.) By April 1933, around $7 billion in deposits had been frozen in failed banks or those left unlicensed after theMarch Bank Holiday.[104]Bank failures snowballed as desperate bankers called in loans that borrowers did not have time or money to repay. With future profits looking poor,capital investmentand construction slowed or completely ceased. In the face of bad loans and worsening future prospects, the surviving banks became even more conservative in their lending.[103]Banks built up their capital reserves and made fewer loans, which intensified deflationary pressures. Avicious cycledeveloped and the downward spiral accelerated.
The liquidation of debt could not keep up with the fall of prices that it caused. The mass effect of the stampede to liquidate increased the value of each dollar owed, relative to the value of declining asset holdings. The very effort of individuals to lessen their burden of debt effectively increased it. Paradoxically, the more the debtors paid, the more they owed.[101]This self-aggravating process turned a 1930 recession into a 1933 great depression.
Fisher's debt-deflation theory initially lacked mainstream influence because of the counter-argument that debt-deflation represented no more than a redistribution from one group (debtors) to another (creditors). Pure re-distributions should have no significant macroeconomic effects.
Building on both the monetary hypothesis of Milton Friedman and Anna Schwartz and the debt deflation hypothesis of Irving Fisher,Ben Bernankedeveloped an alternative way in which the financial crisis affected output. He builds on Fisher's argument that dramatic declines in the price level and nominal incomes lead to increasing real debt burdens, which in turn leads to debtor insolvency and consequently lowersaggregate demand; a further price level decline would then result in a debt deflationary spiral. According to Bernanke, a small decline in the price level simply reallocates wealth from debtors to creditors without doing damage to the economy. But when the deflation is severe, falling asset prices along with debtor bankruptcies lead to a decline in the nominal value of assets on bank balance sheets. Banks will react by tightening their credit conditions, which in turn leads to acredit crunchthat seriously harms the economy. A credit crunch lowers investment and consumption, which results in declining aggregate demand and additionally contributes to the deflationary spiral.[105][106][107]
Since economic mainstream turned to thenew neoclassical synthesis, expectations are a central element of macroeconomic models. According toPeter Temin, Barry Wigmore, Gauti B. Eggertsson andChristina Romer, the key to recovery and to ending the Great Depression was brought about by a successful management of public expectations. The thesis is based on the observation that after years of deflation and a very severe recession important economic indicators turned positive in March 1933 whenFranklin D. Roosevelttook office. Consumer prices turned from deflation to a mild inflation, industrial production bottomed out in March 1933, and investment doubled in 1933 with a turnaround in March 1933. There were no monetary forces to explain that turnaround. Money supply was still falling and short-term interest rates remained close to zero. Before March 1933, people expected further deflation and a recession so that even interest rates at zero did not stimulate investment. But when Roosevelt announced major regime changes, people began to expect inflation and an economic expansion. With these positive expectations, interest rates at zero began to stimulate investment just as they were expected to do. Roosevelt's fiscal and monetary policy regime change helped make his policy objectives credible. The expectation of higher future income and higher future inflation stimulated demand and investment. The analysis suggests that the elimination of the policy dogmas of the gold standard, a balanced budget in times of crisis and small government led endogenously to a large shift in expectation that accounts for about 70–80% of the recovery of output and prices from 1933 to 1937. If the regime change had not happened and the Hoover policy had continued, the economy would have continued its free fall in 1933, and output would have been 30% lower in 1937 than in 1933.[108][109][110]
Therecession of 1937–1938, which slowed down economic recovery from the Great Depression, is explained by fears of the population that the moderate tightening of the monetary and fiscal policy in 1937 were first steps to a restoration of the pre-1933 policy regime.[111]
There is common consensus among economists today that the government and the central bank should work to keep the interconnected macroeconomic aggregates ofgross domestic productandmoney supplyon a stable growth path. When threatened by expectations of a depression,central banksshould expand liquidity in the banking system and the government should cut taxes and accelerate spending in order to prevent a collapse in money supply andaggregate demand.[112]
At the beginning of the Great Depression, most economists believed inSay's lawand the equilibrating powers of the market, and failed to understand the severity of the Depression. Outright leave-it-aloneliquidationismwas a common position, and was universally held byAustrian Schooleconomists.[113]The liquidationist position held that a depression worked to liquidate failed businesses and investments that had been made obsolete by technological development – releasingfactors of production(capital and labor) to be redeployed in other more productive sectors of the dynamic economy. They argued that even if self-adjustment of the economy caused mass bankruptcies, it was still the best course.[113]
Economists likeBarry EichengreenandJ. Bradford DeLongnote that PresidentHerbert Hoovertried to keep the federal budget balanced until 1932, when he lost confidence in his Secretary of the TreasuryAndrew Mellonand replaced him.[113][114][115]An increasingly common view among economic historians is that the adherence of many Federal Reserve policymakers to the liquidationist position led to disastrous consequences.[114]Unlike what liquidationists expected, a large proportion of the capital stock was not redeployed but vanished during the first years of the Great Depression. According to a study byOlivier BlanchardandLawrence Summers, the recession caused a drop of netcapital accumulationto pre-1924 levels by 1933.[116]Milton Friedman called leave-it-alone liquidationism "dangerous nonsense".[112]He wrote:
I think the Austrian business-cycle theory has done the world a great deal of harm. If you go back to the 1930s, which is a key point, here you had the Austrians sitting in London, Hayek and Lionel Robbins, and saying you just have to let the bottom drop out of the world. You've just got to let it cure itself. You can't do anything about it. You will only make it worse. ... I think by encouraging that kind of do-nothing policy both in Britain and in the United States, they did harm.[114]
Two prominent theorists in theAustrian Schoolon the Great Depression include Austrian economistFriedrich Hayekand American economistMurray Rothbard, who wroteAmerica's Great Depression(1963). In their view, much like the monetarists, theFederal Reserve(created in 1913) shoulders much of the blame; however, unlike theMonetarists, they argue that the key cause of the Depression was the expansion of themoney supplyin the 1920s which led to an unsustainable credit-driven boom.[117]
In the Austrian view, it was this inflation of the money supply that led to an unsustainable boom in both asset prices (stocks and bonds) andcapital goods. Therefore, by the time the Federal Reserve tightened in 1928 it was far too late to prevent an economic contraction.[117]In February 1929Hayekpublished a paper predicting the Federal Reserve's actions would lead to a crisis starting in thestockandcreditmarkets.[118]
According to Rothbard, the government support for failed enterprises and efforts to keep wages above their market values actually prolonged the Depression.[119]UnlikeRothbard, after 1970Hayekbelieved that the Federal Reserve had further contributed to the problems of the Depression by permitting the money supply to shrink during the earliest years of the Depression.[120]However, during the Depression (in 1932[121]and in 1934)[121]Hayek had criticized both theFederal Reserveand theBank of Englandfor not taking a more contractionary stance.[121]
Hans Sennholzargued that mostboom and buststhat plagued the American economy, such as those in1819–20,1839–1843,1857–1860,1873–1878,1893–1897, and1920–21, were generated by government creating a boom through easy money and credit, which was soon followed by the inevitable bust.[122]
Ludwig von Miseswrote in the 1930s: "Credit expansion cannot increase the supply of real goods. It merely brings about a rearrangement. It diverts capital investment away from the course prescribed by the state of economic wealth and market conditions. It causes production to pursue paths which it would not follow unless the economy were to acquire an increase in material goods. As a result, the upswing lacks a solid base. It is not real prosperity. It is illusory prosperity. It did not develop from an increase in economic wealth, i.e. the accumulation of savings made available for productive investment. Rather, it arose because the credit expansion created the illusion of such an increase. Sooner or later, it must become apparent that this economic situation is built on sand."[123][124]
Marxistsgenerally argue that the Great Depression was the result of the inherent instability of thecapitalist mode of production.[125]According toForbes, "The idea that capitalism caused the Great Depression was widely held among intellectuals and the general public for many decades."[126]
Two economists of the 1920s,Waddill CatchingsandWilliam Trufant Foster, popularized a theory that influenced many policy makers, includingHerbert Hoover,Henry A. Wallace,Paul Douglas, andMarriner Eccles. It held the economy produced more than it consumed, because the consumers did not have enough income. Thus the unequaldistribution of wealththroughout the 1920s caused the Great Depression.[127][128]
According to this view, the root cause of the Great Depression was a global over-investment in heavy industry capacity compared to wages and earnings from independent businesses, such as farms. The proposed solution was for the government to pump money into the consumers' pockets. That is, it must redistribute purchasing power, maintaining the industrial base, and re-inflating prices and wages to force as much of the inflationary increase in purchasing power intoconsumer spending. The economy was overbuilt, and new factories were not needed. Foster and Catchings recommended[129]federal and state governments to start large construction projects, a program followed by Hoover and Roosevelt.
It cannot be emphasized too strongly that the [productivity, output, and employment] trends we are describing are long-time trends and were thoroughly evident before 1929. These trends are in nowise the result of the present depression, nor are they the result of the World War. On the contrary, the present depression is a collapse resulting from these long-term trends.
The first three decades of the 20th century saw economic output surge withelectrification,mass production, and motorized farm machinery, and because of the rapid growth in productivity there was a lot of excess production capacity and the work week was being reduced. The dramatic rise inproductivityof major industries in the U.S. and the effects of productivity on output, wages and the workweek are discussed by Spurgeon Bell in his bookProductivity, Wages, and National Income(1940).[131]
The majority of countries set up relief programs and most underwent some sort of political upheaval, pushing them to theright. Many of the countries in Europe and Latin America, that were democracies, saw their democratic governments overthrown by some form of dictatorship or authoritarian rule, most famouslyin Germany in 1933.The Dominion of Newfoundland abandoneditsautonomy within the British Empire, becoming the only region ever to voluntarily relinquish democracy. There, too, were severe impacts across the Middle East and North Africa, including economic decline which led to social unrest.[132][133]
Decline in foreign trade hit Argentina hard. The British decision to stop importing Argentine beef led to the signing of theRoca–Runciman Treaty, which preserved a quota in exchange for significant concessions to British exports. By 1935, the economy had recovered to 1929 levels, and the same year, theCentral Bank of Argentinawas formed.[134]However, the Great Depression was the last time when Argentina was one of the richer countries of the world, as it stopped growing in the decades thereafter, and became underdeveloped.[135]
Australia's dependence on agricultural and industrial exports meant it was one of the hardest-hit developed countries.[136]Falling export demand and commodity prices placed massive downward pressures on wages. Unemployment reached a record high of 29% in 1932,[137]with incidents ofcivil unrestbecoming common.[138]After 1932, an increase in wool and meat prices led to a gradual recovery.[139]
Harshly affected by both the global economic downturn and theDust Bowl, Canadian industrial production had by 1932 fallen to only 58% of its 1929 figure, the second-lowest level in the world after the United States, and well behind countries such as Britain, which fell to only 83% of the 1929 level. Totalnational incomefell to 56% of the 1929 level, again worse than any country apart from the United States. Unemployment reached 27% at the depth of the Depression in 1933.[140]
TheLeague of NationslabeledChilethe country hardest-hit by the Great Depression, because 80% of government revenue came from exports of copper and nitrates, which were in low demand. Chile initially felt the impact of the Great Depression in 1930, when GDP dropped 14%, mining income declined 27%, and export earnings fell 28%. By 1932, GDP had shrunk to less than half of what it had been in 1929, exacting a terrible toll in unemployment and business failures.
Influenced profoundly by the Great Depression, many government leaders promoted the development of local industry in an effort to insulate the economy from future external shocks. After six years of governmentausterity measures, which succeeded in reestablishing Chile's creditworthiness, Chileans elected to office during the 1938–58 period a succession of center and left-of-center governments interested in promoting economic growth through government intervention.
Prompted in part by the devastating1939 Chillán earthquake, thePopular Frontgovernment ofPedro Aguirre Cerdacreated the Production Development Corporation (Corporación de Fomento de la Producción,CORFO) to encourage with subsidies and direct investments – an ambitious program ofimport substitution industrialization. Consequently, as in other Latin American countries,protectionismbecame an entrenched aspect of the Chilean economy.
China was largely unaffected by the Depression, mainly by having stuck to theSilver standard. However, the U.S. silver purchase act of 1934 created an intolerable demand on China's silver coins, and so, in the end, the silver standard was officially abandoned in 1935 in favor of the four Chinese national banks'[which?]"legal note" issues. China and theBritish colony of Hong Kong, which followed suit in this regard in September 1935, would be the last to abandon the silver standard. In addition, theNationalist Governmentalso acted energetically to modernize the legal and penal systems, stabilize prices, amortize debts, reform the banking and currency systems, build railroads and highways, improve public health facilities, legislate against traffic in narcotics, and augment industrial and agricultural production. On 3 November 1935, the government instituted the fiat currency (fapi) reform, immediately stabilizing prices and also raising revenues for the government.
The sharp fall in commodity prices and the steep decline in exports hurt the economies of the European colonies in Africa and Asia.[141][142]The agricultural sector was especially hard-hit. For example,sisalhad recently become a major export crop in Kenya and Tanganyika. During the depression, it suffered severely from low prices and marketing problems that affected all colonial commodities in Africa. Sisal producers established centralized controls for the export of their fibre.[143]There was widespread unemployment and hardship among peasants, labourers, colonial auxiliaries, and artisans.[144]The budgets of colonial governments were cut, which forced the reduction in ongoing infrastructure projects, such as the building and upgrading of roads, ports, and communications.[145]The budget cuts delayed the schedule for creating systems of higher education.[146]
The depression severely hurt the export-basedBelgian Congoeconomy because of the drop in international demand for raw materials and for agricultural products. For example, the price of peanuts fell from 125 to 25 centimes. In some areas, as in theKatangamining region, employment declined by 70%. In the country as a whole, the wage labour force decreased by 72,000 people, and many men returned to their villages. In Leopoldville, the population decreased by 33% because of this labour migration.[147]
Political protests were not common. However, there was a growing demand, that the paternalistic claims be honored by colonial governments to respond vigorously. The theme was, that economic reforms were more urgently needed than political reforms.[148]French West Africa launched an extensive program of educational reform, in which "rural schools", designed to modernize agriculture, would stem the flow of under-employed farm workers to cities where unemployment was high. Students were trained in traditional arts, crafts, and farming techniques and were then expected to return to their own villages and towns.[149]
The crisis affected France a bit later than other countries, hitting hard around 1931.[150]While the 1920s grew at the very strong rate of 4.43% per year, the 1930s rate fell to only 0.63%.[151]
The depression was relatively mild: unemployment peaked under 5%, the fall in production was at most 20% below the 1929 output; there was no banking crisis.[152]
However, the depression had drastic effects on the local economy, and partly explains theFebruary 6, 1934 riotsand even more the formation of thePopular Front, led bySFIO socialist leaderLéon Blum, which won the elections in 1936. Ultra-nationalist groups also saw increased popularity, though democracy prevailed intoWorld War II.
France's relatively high degree of self-sufficiency meant the damage was considerably less than in neighbouring states like Germany.
The Great Depression hit Germany hard. The impact of theWall Street crashforced American banks to end the new loans that had been funding the repayments under theDawes Planand theYoung Plan. The financial crisis escalated out of control in mid-1931, starting with the collapse of theCredit Anstaltin Vienna in May.[42]This put heavy pressure on Germany, which was already in political turmoil with the rise in violence ofnational socialistandcommunistmovements, as well as with investor nervousness at harsh government financial policies,[43]investors withdrew their short-term money from Germany as confidence spiraled downward. The Reichsbank lost 150 million marks in the first week of June, 540 million in the second, and 150 million in two days, 19–20 June. Collapse was at hand. U.S. President Herbert Hoover called for a moratorium on payment of war reparations. This angered Paris, which depended on a steady flow of German payments, but it slowed the crisis down, and the moratorium was agreed to in July 1931. An international conference in London later in July produced no agreements, but on 19 August, a standstill agreement froze Germany's foreign liabilities for six months. Germany received emergency funding from private banks in New York as well as the Bank of International Settlements and the Bank of England. The funding only slowed the process. Industrial failures began in Germany, a major bank closed in July, and a two-day holiday for all German banks was declared. Business failures became more frequent in July, and spread to Romania and Hungary.[44]
In 1932, 90% of German reparation payments were cancelled (in the 1950s, Germany repaid all its missed reparations debts). Widespread unemployment reached 25%, as every sector was hurt. The government did not increase government spending to deal with Germany's growing crisis, as they were afraid that a high-spending policy could lead to a return of thehyperinflationthat had affected Germany in 1923. Germany'sWeimar Republicwas hit hard by the depression, as American loans to help rebuild the German economy now stopped.[153]The unemployment rate reached nearly 30% in 1932.[154]
The German political landscape was dramatically altered, leading toAdolf Hitler's rise to power. TheNazi Partyrose from being peripheral to winning 18.3% of the vote in theSeptember 1930 election, and theCommunist Partyalso made gains, while moderate forces, like theSocial Democratic Party, theDemocratic Party, and thePeople's Partylost seats. The next two years were marked by increased street violence between Nazis and Communists, while governments under PresidentPaul von Hindenburgincreasingly relied onrule by decree, bypassing theReichstag.[155]Hitler ran for the Presidency in 1932, and while he lost to the incumbent Hindenburg in the election, it marked a point during which both Nazi Party and the Communist parties rose in the years following the crash to altogether possess a Reichstag majority following thegeneral election in July 1932.[154][156]Although the Nazis lost seats inNovember 1932 election, they remained the largest party, and Hitler was appointed as Chancellor the following January. The government formation deal was designed to give Hitler's conservative coalition partners many checks on his power, but over the next few months, the Nazis manoeuvred to consolidate a single-party dictatorship.[157]
Hitler followed an economic policy ofautarky, creating a network of client states and economic allies in central Europe and Latin America. By cutting wages and taking control of labor unions, plus public works spending, unemployment fell significantly by 1935. Large-scale military spending played a major role in the recovery.[158]The policies had the effect of driving up the cost of food imports and depleting foreign currency reserves, leading to economic impasse by 1936. Nazi Germany faced a choice of either reversing course or pressing ahead with rearmament and autarky. Hitler chose the latter route, which, according toIan Kershaw, "could only be partially accomplished without territorial expansion" and therefore war.[159][160]
The reverberations of the Great Depression hit Greece in 1932. TheBank of Greecetried to adopt deflationary policies to stave off the crises that were going on in other countries, but these largely failed. For a brief period, the drachma was pegged to the U.S. dollar, but this was unsustainable given the country's large trade deficit and the only long-term effects of this were Greece's foreign exchange reserves being almost totally wiped out in 1932. Remittances from abroad declined sharply, and the value of the drachma began to plummet from 77 drachmas to the dollar in March 1931 to 111 drachmas to the dollar in April 1931. This was especially harmful to Greece, as the country relied on imports from the UK, France, and the Middle East for many necessities. Greece went off the gold standard in April 1932, and declared a moratorium on all interest payments. The country also adopted protectionist policies, such as import quotas, which several European countries also did during the period.
Protectionist policies coupled with a weak drachma and the stifling of imports allowed the Greek industry to expand during the Great Depression. In 1939, the Greek industrial output was 179% that of 1928. These industries were for the most part "built on sand", as one report of the Bank of Greece put it, as without massive protection, they would not have been able to survive. Despite the global depression, Greece managed to suffer comparatively little, averaging an average growth rate of 3.5% from 1932 to 1939. The dictatorial regime ofIoannis Metaxastook over the Greek government in 1936, and economic growth was strong in the years leading up to the Second World War.
Icelandic post-World War I prosperity came to an end with the outbreak of the Great Depression. The Depression hit Iceland hard, as the value of exports plummeted. The total value of Icelandic exports fell from 74 million kronur in 1929 to 48 million in 1932, and was not to rise again to the pre-1930 level until after 1939.[161]Government interference in the economy increased: "Imports were regulated, trade with foreign currency was monopolized by state-owned banks, and loan capital was largely distributed by state-regulated funds".[161]Due to the outbreak of theSpanish Civil War, which cut Iceland's exports of saltfish by half, the Depression lasted in Iceland until the outbreak of World War II (when prices for fish exports soared).[161]
How much India was affected, has been hotly debated. Historians have argued, that the Great Depression slowed long-term industrial development.[162]Apart from two sectors –juteand coal – the economy was little-affected. However, there were major negative impacts on the jute industry, as world demand fell and prices plunged.[163]Otherwise, conditions were fairly stable. Local markets in agriculture and small-scale industry showed modest gains.[164]
The Great Depression hitItalyvery hard.[169]As industries came close to failure they were bought out by the banks in a largely illusionary bail-out—the assets used to fund the purchases were largely worthless. This led to a financial crisis peaking in 1932 and major government intervention. TheIndustrial Reconstruction Institute(IRI) was formed in January 1933 and took control of the bank-owned companies, suddenly giving Italy the largest state-owned industrial sector in Europe (excluding the USSR). IRI did rather well with its new responsibilities—restructuring, modernising and rationalising as much as it could. It was a significant factor in post-1945 development. But it took the Italian economy until 1935 to recover the manufacturing levels of 1930—a position that was only 60% better than that of 1913.[170][171]
The Great Depression did not strongly affect Japan. The Japanese economy shrank by 8% during 1929–31. Japan's Finance MinisterTakahashi Korekiyowas the first to implement what have come to be identified asKeynesianeconomic policies: first, by large fiscal stimulus involvingdeficit spending; and second, by devaluingthe currency. Takahashi used the Bank of Japan to sterilize the deficit spending and minimize resulting inflationary pressures. Econometric studies have identified the fiscal stimulus as especially effective.[172]
The devaluation of the currency had an immediate effect. Japanese textiles began to displace British textiles in export markets. The deficit spending proved to be most profound and went into the purchase of munitions for the armed forces. By 1933, Japan was already out of the depression. By 1934, Takahashi realized that the economy was in danger of overheating, and to avoid inflation, moved to reduce the deficit spending that went towards armaments and munitions.
This resulted in a strong and swift negative reaction from nationalists, especially those in the army, culminating in his assassination in the course of theFebruary 26 Incident. This had achilling effecton all civilian bureaucrats in the Japanese government. From 1934, the military's dominance of the government continued to grow. Instead of reducing deficit spending, the government introduced price controls and rationing schemes that reduced, but did not eliminate inflation, which remained a problem until the end of World War II.
The deficit spending had a transformative effect on Japan. Japan's industrial production doubled during the 1930s. Further, in 1929 the list of the largest firms in Japan was dominated by light industries, especially textile companies (many of Japan's automakers, such asToyota, have their roots in the textile industry). By 1940light industryhad been displaced by heavy industry as the largest firms inside the Japanese economy.[173]
Because of high levels of U.S. investment in Latin American economies, they were severely damaged by the Depression. Within the region,Chile,BoliviaandPeruwere particularly badly affected.[174]
Before the 1929 crisis, links between the world economy andLatin Americaneconomies had been established through American and British investment in Latin American exports to the world. As a result, Latin Americans export industries felt the depression quickly. World prices for commodities such as wheat, coffee and copper plunged. Exports from all of Latin America to the U.S. fell in value from $1.2 billion in 1929 to $335 million in 1933, rising to $660 million in 1940.
But on the other hand, the depression led the area governments to develop new local industries and expand consumption and production. Following the example of the New Deal, governments in the area approved regulations and created or improved welfare institutions that helped millions of new industrial workers to achieve a better standard of living.
From roughly 1931 to 1937, theNetherlandssuffered a deep and exceptionally long depression. This depression was partly caused by the after-effects of the American stock-market crash of 1929, and partly by internal factors in the Netherlands. Government policy, especially the very late dropping of the Gold Standard, played a role in prolonging the depression. The Great Depression in the Netherlands led to some political instability and riots, and can be linked to the rise of the Dutch fascist political partyNSB. The depression in the Netherlands eased off somewhat at the end of 1936, when the government finally dropped the Gold Standard, but real economic stability did not return until after World War II.[175]
New Zealandwas especially vulnerable to worldwide depression, as it relied almost entirely on agricultural exports to the United Kingdom for its economy. The drop in exports led to a lack of disposable income from the farmers, who were the mainstay of the local economy. Jobs disappeared and wages plummeted, leaving people desperate and charities unable to cope. Work relief schemes were the only government support available to the unemployed, the rate of which by the early 1930s was officially around 15%, but unofficially nearly twice that level (official figures excluded Māori and women). In 1932, riots occurred among the unemployed in three of the country's main cities (Auckland,Dunedin, andWellington). Many were arrested or injured through the tough official handling of these riots by police and volunteer "special constables".[176]
In Iran, then known as theImperial State of Persia, the Great Depression had negative impacts on its exports. In 1933 a new concession was signed with theAnglo-Persian Oil Company.[177]
Poland was affected by the Great Depression longer and stronger than other countries due to inadequate economic response of the government and the pre-existing economic circumstances of the country. At that time, Poland was under the authoritarian rule ofSanacja, whose leader,Józef Piłsudski, was opposed to leaving thegold standarduntil his death in 1935. As a result, Poland was unable to perform a more active monetary and budget policy. Additionally, Poland was a relatively young country that emerged merely 10 years earlier after being partitioned betweenGerman,Russian, and theAustro-Hungarian Empiresfor over a century. Prior to independence, the Russian part exported 91% of its exports to Russia proper, while the German part exported 68% to Germany proper. After independence, these markets were largely lost, as Russia transformed intoUSSRthat was mostly a closed economy, and Germany was in a tariff war with Poland throughout the 1920s.[178]
Industrial productionfell significantly: in 1932hard coalproduction was down 27% compared to 1928,steelproduction was down 61%, andiron oreproduction noted an 89% decrease.[179]On the other hand, electrotechnical, leather, and paper industries noted marginal increases in production output. Overall, industrial production decreased by 41%.[180]A distinct feature of the Great Depression in Poland was the de-concentration of industry, as larger conglomerates were less flexible and paid their workers more than smaller ones.
Unemployment raterose significantly (up to 43%) while nominalwagesfell by 51% in 1933 and 56% in 1934, relative to 1928. However, real wages fell less due to the government's policy of decreasing cost of living, particularly food expenditures (food prices were down by 65% in 1935 compared to 1928 price levels). Material conditions deprivation led to strikes, some of them violent or violently pacified – like inSanok(March of the Hungry in Sanok[pl]6 March 1930),Lesko county(Lesko uprising21 June – 9 July 1932) andZawiercie(Bloody Friday (1930)[pl]18 April 1930).
To adapt to the crisis, Polish government employed deflation methods such as highinterest rates, credit limits and budgetausterityto keep afixed exchange ratewith currencies tied to the gold standard. Only in late 1932 did the government effect a plan to fight the economic crisis.[181]Part of the plan was masspublic worksscheme, employing up to 100,000 people in 1935.[179]After Piłsudski's death, in 1936 the gold standard regime was relaxed, and launching the development of theCentral Industrial Regionkicked off the economy, to over 10% annual growth rate in the 1936–1938 period.
Already under the rule of a dictatorial junta, theDitadura Nacional, Portugal suffered no turbulent political effects of the Depression, althoughAntónio de Oliveira Salazar, already appointed Minister of Finance in 1928 greatly expanded his powers and in 1932 rose toPrime Minister of Portugalto found theEstado Novo, anauthoritariancorporatistdictatorship. With the budget balanced in 1929, the effects of the depression were relaxed through harsh measures towardsbudget balanceandautarky, causing social discontent but stability and, eventually, an impressive economic growth.[182]
In the years immediately preceding the depression, negative developments in the island and world economies perpetuated an unsustainable cycle of subsistence for many Puerto Rican workers. The 1920s brought a dramatic drop in Puerto Rico's two primary exports, raw sugar and coffee, due to a devastating hurricane in 1928 and the plummeting demand from global markets in the latter half of the decade. 1930 unemployment on the island was roughly 36% and by 1933 Puerto Rico's per capita income dropped 30% (by comparison, unemployment in the United States in 1930 was approximately 8% reaching a height of 25% in 1933).[183][184]To provide relief and economic reform, the United States government and Puerto Rican politicians such asCarlos ChardonandLuis Muñoz Maríncreated and administered first the Puerto Rico Emergency Relief Administration (PRERA) 1933 and then in 1935, thePuerto Rico Reconstruction Administration(PRRA).[185]
Romania was also affected by the Great Depression.[186][187]
As world trade slumped, demand for South African agricultural and mineral exports fell drastically. TheCarnegie Commission on Poor Whiteshad concluded in 1931 that nearly one-third ofAfrikanerslived as paupers. The social discomfort caused by the depression was a contributing factor in the 1933 split between the "gesuiwerde" (purified) and "smelter" (fusionist) factions within theNational Partyand the National Party's subsequent fusion with theSouth African Party.[188][189]Unemployment programs were begun that focused primarily on the white population.[190]
The Soviet Union was the only majorsocialist statein the world and had very little international trade. Its economy was not tied to the rest of the world and was mostly unaffected by the Great Depression.[191]
At the time of the Depression, the Soviet economy was growing steadily, fuelled by intensive investment in heavy industry. The apparent economic success of the Soviet Union at a time when the capitalist world was in crisis led many Western intellectuals to view the Soviet system favorably. Jennifer Burns wrote:
As the Great Depression ground on and unemployment soared, intellectuals began unfavorably comparing their faltering capitalist economy to Russian Communism. Karl Marx had predicted that capitalism would fall under the weight of its own contradictions, and now with the economic crisis gripping the West, his predictions seem to be coming true. By contrast Russia seemed an emblematic modern nation, making the staggering leap from a feudal past to an industrial future with ease.[192]
The early years of the Great Depression caused mass immigration to the Soviet Union, including 10,000 to 15,000 from Finland and thousands more from Poland, Sweden, Germany, and other nearby countries. The Kremlin was at first happy to help these immigrants settle, believing that they were victims of capitalism who had come to help the Soviet cause. However, by 1933, the worst of the Depression had come to an end in many countries, and word had been received that illegal migrants to the Soviet Union were being sent to Siberia.[citation needed]These factors caused immigration to the Soviet Union to slow significantly, and roughly a tenth of Finnish migrants returned to Finland, either legally or illegally.[193]
Spain had a relatively isolated economy, with high protective tariffs and was not one of the main countries affected by the Depression. The banking system held up well, as did agriculture.[194]
By far the most serious negative impact came after 1936 from the heavy destruction of infrastructure and manpower by thecivil war, 1936–39. Many talented workers were forced into permanent exile. By staying neutral in the Second World War, and selling to both sides[clarification needed], the economy avoided further disasters.[195]
By the 1930s, Sweden had what America'sLife magazinecalled in 1938 the "world's highest standard of living". Sweden was also the first country worldwide to recover completely from the Great Depression. Taking place amid a short-lived government and a less-than-a-decade old Swedish democracy, events such as those surroundingIvar Kreuger(who eventually committed suicide) remain infamous in Swedish history. TheSocial DemocratsunderPer Albin Hanssonformed their first long-lived government in 1932 based on stronginterventionistandwelfare statepolicies, monopolizing the office ofPrime Ministeruntil 1976 with the sole and short-lived exception ofAxel Pehrsson-Bramstorp's "summer cabinet" in 1936. During forty years of hegemony, it was the most successful political party in the history of Western liberal democracy.[196]
In Thailand, then known as theKingdom of Siam, the Great Depression contributed to the end of theabsolute monarchy of King Rama VIIin theSiamese revolution of 1932.[197]
The Great Depression came at a time when the relatively newly established Turkish state was still reforming its economic policy following the end of theOttomanera. As the depression began, the country's trade deficits saw an increase and the Turkish lira significantly lost value. Turkey's economy was predominantly agrarian, thus the fall in demand which caused a fall in export prices of many goods affected the country's economy badly. As a result of the depression, the government, which had been following increasingly more liberal economic policies up until then, started opting for more statist policies.[198]
The world depression broke at a time when the United Kingdom had still not fully recovered from the effects of theFirst World Warmore than a decade earlier. The country was driven off thegold standardin 1931.
The world financial crisis began to overwhelm Britain in 1931; investors around the world started withdrawing their gold from London at the rate of £2.5 million per day.[45]Credits of £25 million each from the Bank of France and the Federal Reserve Bank of New York and an issue of £15 million fiduciary note slowed, but did not reverse the British crisis. The financial crisis now caused a major political crisis in Britain in August 1931. With deficits mounting, the bankers demanded a balanced budget; the divided cabinet of Prime Minister Ramsay MacDonald's Labour government agreed; it proposed to raise taxes, cut spending and most controversially, to cut unemployment benefits by 20%. The attack on welfare was totally unacceptable to the Labour movement. MacDonald wanted to resign, but King George V insisted he remain and form an all-party coalition "National Government". The Conservative and Liberals parties signed on, along with a small cadre of Labour, but the vast majority of Labour leaders denounced MacDonald as a traitor for leading the new government. Britain went off the gold standard, and suffered relatively less than other major countries in the Great Depression. In the 1931 British election, the Labour Party was virtually destroyed, leaving MacDonald as prime minister for a largely Conservative coalition.[199][47]
The effects on the northern industrial areas of Britain were immediate and devastating, as demand for traditional industrial products collapsed. By the end of 1930 unemployment had more than doubled from 1 million to 2.5 million (20% of the insured workforce), and exports had fallen in value by 50%. In 1933, 30% ofGlaswegianswere unemployed due to the severe decline in heavy industry. In some towns and cities in the north east, unemployment reached as high as 70% as shipbuilding fell by 90%.[200]TheNational Hunger Marchof September–October 1932 was the largest[201]of a series ofhunger marchesin Britain in the 1920s and 1930s. About 200,000 unemployed men were sent to the work camps, which continued in operation until 1939.[202]
In the less industrialMidlandsandSouthern England, the effects were short-lived and the later 1930s were a prosperous time. Growth in modern manufacture of electrical goods and a boom in the motor car industry was helped by a growing southern population and an expandingmiddle class. Agriculture also saw a boom during this period.[203]
Hoover's first measures to combat the depression were based on encouraging businesses not to reduce their workforce or cut wages but businesses had little choice: wages were reduced, workers were laid off, and investments postponed.[204][205]
In June 1930, Congress approved theSmoot–Hawley Tariff Actwhich raised tariffs on thousands of imported items. The intent of the Act was to encourage the purchase of American-made products by increasing the cost of imported goods, while raising revenue for the federal government and protecting farmers. Most countries that traded with the U.S. increased tariffs on American-made goods in retaliation, reducing international trade, and worsening the Depression.[206]
In 1931, Hoover urged bankers to set up theNational Credit Corporation[207]so that big banks could help failing banks survive. But bankers were reluctant to invest in failing banks, and the National Credit Corporation did almost nothing to address the problem.[208]
By 1932, unemployment had reached 23.6%, peaking in early 1933 at 25%.[210]Those released from prison during this period had an especially difficult time finding employment given the stigma of their criminal records, which often led to recidivism out of economic desperation.[211]Drought persisted in the agricultural heartland, businesses and families defaulted on record numbers of loans, and more than 5,000 banks had failed.[212]Hundreds of thousands of Americans found themselves homeless, and began congregating inshanty towns– dubbed "Hoovervilles" – that began to appear across the country.[213]In response, President Hoover and Congress approved theFederal Home Loan Bank Act, to spur new home construction, and reduce foreclosures. The final attempt of the Hoover Administration to stimulate the economy was the passage of theEmergency Relief and Construction Act(ERA) which included funds forpublic worksprograms such as dams and the creation of theReconstruction Finance Corporation(RFC) in 1932. The Reconstruction Finance Corporation was a Federal agency with the authority to lend up to $2 billion to rescue banks and restore confidence in financial institutions. But $2 billion was not enough to save all the banks, andbank runsand bank failures continued.[204]Quarter by quarter the economy went downhill, as prices, profits and employment fell, leading to thepolitical realignmentin 1932 that brought to powerFranklin Delano Roosevelt.
Shortly after PresidentFranklin Delano Rooseveltwas inaugurated in 1933, drought and erosion combined to cause theDust Bowl, shifting hundreds of thousands ofdisplaced personsoff their farms in the Midwest. From his inauguration onward, Roosevelt argued that restructuring of the economy would be needed to prevent another depression or avoid prolonging the current one. New Deal programs sought to stimulatedemandand provide work and relief for the impoverished through increased government spending and the institution of financial reforms.
During a "bank holiday" that lasted five days, theEmergency Banking Actwas signed into law. It provided for a system of reopening sound banks underTreasurysupervision, with federal loans available if needed. TheSecurities Act of 1933comprehensively regulated the securities industry. This was followed by theSecurities Exchange Act of 1934which created theSecurities and Exchange Commission. Although amended, key provisions of both Acts are still in force. Federal insurance ofbank depositswas provided by theFDIC, and theGlass–Steagall Act.
TheAgricultural Adjustment Actprovided incentives to cut farm production in order to raise farming prices. TheNational Recovery Administration(NRA) made a number of sweeping changes to the American economy. It forced businesses to work with government to set price codes through the NRA to fightdeflationary"cut-throat competition" by the setting of minimum prices andwages, labor standards, and competitive conditions in all industries. It encouraged unions that would raise wages, to increase thepurchasing powerof theworking class. The NRAwas deemed unconstitutionalby theSupreme Court of the United Statesin 1935.
These reforms, together with several other relief and recovery measures, are called theFirst New Deal. Economic stimulus was attempted through a newalphabet soup of agenciesset up in 1933 and 1934 and previously extant agencies such as theReconstruction Finance Corporation. By 1935, the "Second New Deal" addedSocial Security(which was later considerably extended through theFair Deal), a jobs program for the unemployed (theWorks Progress Administration, WPA) and, through theNational Labor Relations Board, a strong stimulus to the growth of labor unions. In 1929, federal expenditures constituted only 3% of theGDP. The national debt as a proportion of GNP rose under Hoover from 20% to 40%. Roosevelt kept it at 40% until the war began, when it soared to 128%.
By 1936, the maineconomic indicatorshad regained the levels of the late 1920s, except for unemployment, which remained high at 11%, although this was considerably lower than the 25% unemployment rate seen in 1933. In the spring of 1937, American industrial production exceeded that of 1929 and remained level until June 1937. In June 1937, the Roosevelt administration cut spending and increased taxation in an attempt to balance the federal budget.[216]The American economy then took a sharp downturn, lasting for 13 months through most of 1938. Industrial production fell almost 30% within a few months and production ofdurable goodsfell even faster. Unemployment jumped from 14.3% in 1937 to 19.0% in 1938, rising from 5 million to more than 12 million in early 1938.[217]Manufacturing output fell by 37% from the 1937 peak and was back to 1934 levels.[218]
Producers reduced their expenditures on durable goods, and inventories declined, but personal income was only 15% lower than it had been at the peak in 1937. As unemployment rose, consumers' expenditures declined, leading to further cutbacks in production. By May 1938 retail sales began to increase, employment improved, and industrial production turned up after June 1938.[219]After the recovery from the Recession of 1937–38, conservatives were able to form a bipartisanconservative coalitionto stop further expansion of the New Deal and, when unemployment dropped to 2% in the early 1940s, they abolished WPA, CCC and the PWA relief programs. Social Security remained in place.
Between 1933 and 1939, federal expenditure tripled, and Roosevelt's critics charged that he was turning America into asocialiststate.[220]The Great Depression was a main factor in the implementation ofsocial democracyandplanned economiesin European countries after World War II (seeMarshall Plan).Keynesianismgenerally remained the most influential economic school in the United States and in parts of Europe until the periods between the 1970s and the 1980s, whenMilton Friedmanand otherneoliberaleconomists formulated and propagated the newly created theories ofneoliberalismand incorporated them into theChicago School of Economicsas an alternative approach to the study of economics. Neoliberalism went on to challenge the dominance of the Keynesian school of Economics in the mainstream academia and policy-making in the United States, having reached its peak in popularity in the election of the presidency ofRonald Reaganin the United States, andMargaret Thatcherin the United Kingdom.[221]
And the great owners, who must lose their land in an upheaval, the great owners with access to history, with eyes to read history and to know the great fact: when property accumulates in too few hands it is taken away. And that companion fact: when a majority of the people are hungry and cold they will take by force what they need. And the little screaming fact that sounds through all history: repression works only to strengthen and knit the repressed.
The Great Depression has been the subject of much writing, as authors have sought to evaluate an era that caused both financial and emotional trauma. Perhaps the most noteworthy and famous novel written on the subject isThe Grapes of Wrath, published in 1939 and written byJohn Steinbeck, who was awarded thePulitzer Prizefor the work, and in 1962 was awarded theNobel Prizefor literature. The novel focuses on a poor family of sharecroppers who are forced from their home as drought, economic hardship, and changes in theagricultural industryoccur during the Great Depression. Steinbeck'sOf Mice and Menis another important novella about a journey during the Great Depression. Additionally, Harper Lee'sTo Kill a Mockingbirdis set during the Great Depression. Margaret Atwood's Booker prize-winningThe Blind Assassinis likewise set in the Great Depression, centering on a privileged socialite's love affair with a Marxist revolutionary. The era spurred the resurgence of social realism, practiced by many who started their writing careers on relief programs, especially theFederal Writers' Projectin the U.S.[223][224][225][226]Nonfiction works from this time also capture important themes. The 1933 memoirPrison Days and NightsbyVictor Folke Nelsonprovides insight into criminal justice ramifications of the Great Depression, especially in regard to patterns of recidivism due to lack of economic opportunity.[211]
A number of works for younger audiences are also set during the Great Depression, among them theKit Kittredgeseries ofAmerican Girlbooks written byValerie Trippand illustrated byWalter Rane, released to tie in with the dolls and playsets sold by the company. The stories, which take place during the early to mid 1930s inCincinnati, focuses on the changes brought by the Depression to the titular character's family and how the Kittredges dealt with it.[227]A theatrical adaptation of the series entitledKit Kittredge: An American Girlwas later released in 2008 to positive reviews.[228][229]Similarly,Christmas After All, part of theDear Americaseries of books for older girls, take place in 1930sIndianapolis; whileKit Kittredgeis told in a third-person viewpoint,Christmas After Allis in the form of a fictional journal as told by the protagonist Minnie Swift as she recounts her experiences during the era, especially when her family takes in an orphan cousin from Texas.[230]
The term "The Great Depression" is most frequently attributed to British economistLionel Robbins, whose 1934 bookThe Great Depressionis credited with formalizing the phrase,[231]though Hoover is widely credited with popularizing the term,[231][232]informally referring to the downturn as a depression, with such uses as "Economic depression cannot be cured by legislative action or executive pronouncement" (December 1930, Message to Congress), and "I need not recount to you that the world is passing through a great depression" (1931).
The term "depression" to refer to an economic downturn dates to the 19th century, when it was used by varied Americans and British politicians and economists. The first major American economic crisis, thePanic of 1819, was described by then-presidentJames Monroeas "a depression",[231]and the most recent economic crisis, theDepression of 1920–21, had been referred to as a "depression" by then-presidentCalvin Coolidge.
Financial crises were traditionally referred to as "panics", most recently the majorPanic of 1907, and the minorPanic of 1910–11, though the 1929 crisis was called "The Crash", and the term "panic" has since fallen out of use. At the time of the Great Depression, the term "The Great Depression" was already used to refer to the period 1873–96 (in the United Kingdom), or more narrowly 1873–79 (in the United States), which has retroactively been renamed theLong Depression.[233]
Thecollapse of the Soviet Union, and the breakdown of economic ties which followed, led to a severe economic crisis and catastrophic fall in thestandards of livingin the 1990s inpost-Soviet statesand the formerEastern Bloc,[234][235]which was even worse than the Great Depression.[236][237]Even before Russia'sfinancial crisisof 1998, Russia's GDP was half of what it had been in the early 1990s.[237]
Theworldwide economic decline after 2008has been compared to the 1930s.[238][239][240][241][242]
1928 and 1929 were the times in the 20th century that thewealth gapreached such skewed extremes;[243]half the unemployed had been out of work for over six months, something that was not repeated until the late-2000s recession. 2007 and 2008 eventually saw the world reach new levels of wealth gap inequality that rivalled the years of 1928 and 1929.
Thecivil rights movement[b]was a social movement in theUnited Statesfrom 1954 to 1968 which aimed to abolish legalizedracial segregation,discrimination, anddisenfranchisementin the country, which most commonly affectedAfrican Americans. The movement had origins in theReconstruction erain the late 19th century, and modern roots in the 1940s.[1]After years ofnonviolentprotests andcivil disobediencecampaigns, the civil rights movement achieved many of its legislative goals in the 1960s, during which it secured new protections infederal lawfor thecivil rightsof all Americans.
Following theAmerican Civil War(1861–1865), the threeReconstruction Amendmentsto theU.S. Constitutionabolished slavery and granted citizenship to all African Americans, the majority of whom had recently been enslaved in the southern states. During Reconstruction, African-American men in the South voted and held political office, but after 1877 they were increasingly deprived of civil rights under racistJim Crow laws(which for examplebanned interracial marriage, introducedliteracy tests for voters, andsegregated schools) and were subjected to violence fromwhite supremacistsduring thenadir of American race relations. African Americans who moved to the North in order to improve their prospects in theGreat Migrationalso faced barriers in employment and housing. Legal racial discrimination was upheld by theSupreme Courtin its 1896 decision inPlessy v. Ferguson, which established the doctrine of "separate but equal". The movement for civil rights, led by figures such asW. E. B. Du BoisandBooker T. Washington, achieved few gains until afterWorld War II. In 1948, PresidentHarry S. Trumanissuedan executive orderabolishing discrimination in the armed forces.
In 1954, the Supreme Court struck down state laws establishingracial segregation in public schoolsinBrown v. Board of Education. A mass movement for civil rights, led byMartin Luther King Jr.and others, began a campaign ofnonviolentprotests andcivil disobedienceincluding theMontgomery bus boycottin 1955–1956, "sit-ins" inGreensboroandNashvillein 1960, theBirmingham campaignin 1963, and a march fromSelma to Montgomeryin 1965. Press coverage of events such as the lynching ofEmmett Tillin 1955 and the use of fire hoses and dogs against protesters in Birmingham increased public support for the civil rights movement. In 1963, about 250,000 people participated in theMarch on Washington, after which PresidentJohn F. Kennedyasked Congress to pass civil rights legislation. Kennedy's successor,Lyndon B. Johnson, overcame the opposition of southern politicians to pass three major laws: theCivil Rights Act of 1964, which prohibited discrimination based on race, color, religion, sex, or national origin inpublic accommodations, employment, and federally assisted programs; theVoting Rights Act of 1965, which outlawed discriminatory voting laws and authorized federal oversight of election law in areas with a history of voter suppression; and theFair Housing Act of 1968, which banned housing discrimination. The Supreme Court made further pro–civil rights rulings in cases includingBrowder v. Gayle(1956) andLoving v. Virginia(1967), banning segregation in public transport and striking down laws against interracial marriage.
The new civil rights laws ended most legal discrimination against African Americans, though informal racism remained. In the mid-1960s, theBlack power movementemerged, which criticized leaders of the civil rights movement for their moderate and incremental tendencies.A wave of civil unrestin Black communities between 1964 and 1969, which peaked in 1967 and after theassassination of Kingin 1968, weakened support for the movement from White moderates. Despiteaffirmative actionand other programs which expanded opportunities for Black and other minorities in the U.S. by the early 21st century, racial gaps in income, housing, education, and criminal justice continue to persist.
Before theAmerican Civil War,eight serving presidents had owned slaves, almost four million black people remainedenslaved in the South, generally only white men with property could vote, and theNaturalization Act of 1790limited U.S. citizenship towhites.[2][3][4]Following the Civil War, three constitutional amendments were passed, including the13th Amendment(1865) that ended slavery; the14th Amendment(1869) that gave black people citizenship, adding their total forCongressional apportionment; and the15th Amendment(1870) that gave black males the right to vote (only males could vote in the U.S. at the time).[5]From 1865 to 1877, the United States underwent a turbulentReconstruction eraduring which the federal government tried to establish free labor and thecivil rightsof freedmen in the South after the end of slavery. Many whites resisted the social changes, leading to the formation of insurgent movements such as theKu Klux Klan(KKK), whose members attacked black and whiteRepublicansin order to maintainwhite supremacy. In 1871, PresidentUlysses S. Grant, the U.S. Army, and U.S. Attorney GeneralAmos T. Akerman, initiated a campaign to repress the KKK under theEnforcement Acts.[6]Some states were reluctant to enforce the federal measures of the act. In addition, by the early 1870s, other white supremacist and insurgentparamilitarygroups arose that violently opposed African-American legal equality and suffrage, intimidating and suppressing black voters, and assassinating Republican officeholders.[7][8]However, if the states failed to implement the acts, the laws allowed theFederal Governmentto get involved.[8]Many Republican governors were afraid of sending black militia troops to fight the Klan for fear of war.[8]
After thedisputed electionof 1876, which resulted in the end of Reconstruction and the withdrawal of federal troops, whites in the South regained political control of the region's state legislatures. They continued to intimidate and violently attack blacks before and during elections to suppress their voting, but the last African Americans were elected to Congress from the South before disenfranchisement of blacks by states throughout the region, as described below.
From 1890 to 1908, southern states passed new constitutions and laws todisenfranchiseAfrican Americans and manyPoor Whitesby creating barriers to voter registration; voting rolls were dramatically reduced as blacks and poor whites were forced out of electoral politics. After the landmarkSupreme Courtcase ofSmith v. Allwright(1944), which prohibitedwhite primaries, progress was made in increasing black political participation in the Rim South andAcadiana– although almost entirely in urban areas[9]and a few rural localities where most blacks worked outside plantations.[10]Thestatus quo anteof excluding African Americans from the political system lasted in the remainder of the South, especiallyNorth Louisiana, Mississippi and Alabama, until national civil rights legislation was passed in the mid-1960s to provide federal enforcement of constitutional voting rights. For more than sixty years, blacks in the South were essentially excluded from politics, unable to elect anyone to represent their interests in Congress or local government.[8]Since they could not vote, they could not serve on local juries.
During this period, the white-dominatedDemocratic Partymaintained political control of the South. With whites controlling all the seats representing the total population of the South, they had a powerfulvoting blocin Congress. TheRepublican Party—the "party of Lincoln" and the party to which most blacks had belonged—shrank to insignificance except in remoteUnionistareas ofAppalachiaand theOzarksas black voter registration was suppressed. The Republicanlily-white movementalso gained strength by excluding blacks. Until 1965, the "Solid South" was a one-party system under the white Democrats. Excepting the previously noted historic Unionist strongholds the Democratic Party nomination was tantamount to election for state and local office.[11]In 1901, PresidentTheodore RooseveltinvitedBooker T. Washington, president of theTuskegee Institute, to dine at theWhite House, making him the first African American to attend an official dinner there. "The invitation was roundly criticized by southern politicians and newspapers."[12]Washington persuaded the president to appoint more blacks to federal posts in the South and to try to boost African-American leadership in state Republican organizations. However, these actions were resisted by both white Democrats and white Republicans as an unwanted federal intrusion into state politics.[12]
During the same time as African Americans were being disenfranchised, white southerners imposedracial segregationby law. Violence against blacks increased, with numerouslynchingsthrough the turn of the century. The system ofde jurestate-sanctioned racial discrimination and oppression that emerged from the post-Reconstruction South became known as the "Jim Crow" system. The United States Supreme Court made up almost entirely of Northerners, upheld the constitutionality of those state laws that required racial segregation in public facilities in its 1896 decisionPlessy v. Ferguson, legitimizing them through the "separate but equal" doctrine.[14]Segregation, which began with slavery, continued with Jim Crow laws, with signs used to show blacks where they could legally walk, talk, drink, rest, or eat.[15]For those places that were racially mixed, non-whites had to wait until all white customers were served first.[15]Elected in 1912, PresidentWoodrow Wilsongave in to demands by Southern members of his cabinet who ordered segregation of workplaces in the federal government.[16]
The early 20th century is a period often referred to as the "nadir of American race relations", when the number of lynchings was highest. While tensions andcivil rightsviolations were most intense in the South, social discrimination affected African Americans in other regions as well.[17]At the national level, the Southern bloc controlled important committees in Congress, defeated passage of federal laws against lynching, and exercised considerable power beyond the number of whites in the South.
African Americans rejected this regime. They resisted it in numerous ways and sought better opportunities through lawsuits, new organizations, political redress, and labor organizing (see theCivil rights movement (1896–1954)). TheNational Association for the Advancement of Colored People(NAACP) was founded in 1909. It fought to end race discrimination through litigation, education, and lobbying efforts. Its crowning achievement was its legal victory in the Supreme Court decisionBrown v. Board of Education(1954), when theSupreme Courtruled that segregation of public schools in the US was unconstitutional and, by implication, overturned the "separate but equal" doctrine established inPlessy v. Fergusonof 1896.[19][20]Following the unanimous Supreme Court ruling, many states began to gradually integrate their schools, but some areas of the South resisted by closing public schools altogether.[19][20]
The integration of Southern public libraries followed demonstrations and protests that used techniques seen in other elements of the larger civil rights movement.[21]This included sit-ins, beatings, and white resistance.[21]For example, in 1963 in the city ofAnniston, Alabama, two black ministers were brutally beaten for attempting to integrate the public library.[21]Though there was resistance and violence, the integration of libraries was generally quicker than the integration of other public institutions.[21]
The situation for blacks outside the South was somewhat better (in most states they could vote and have their children educated, though they still faced discrimination in housing and jobs). In 1900 Reverend Matthew Anderson, speaking at the annualHampton Negro Conferencein Virginia, said that "...the lines along most of the avenues of wage-earning are more rigidly drawn in the North than in the South. There seems to be an apparent effort throughout the North, especially in the cities to debar the colored worker from all the avenues of higher remunerative labor, which makes it more difficult to improve his economic condition even than in the South."[22]From 1910 to 1970, blacks sought better lives by migrating north and west out of the South. A total of nearly seven million blacks left the South in what was known as theGreat Migration, most during and after World War II. So many people migrated that the demographics of some previously black-majority states changed to a white majority (in combination with other developments). The rapid influx of blacks altered the demographics of Northern and Western cities; happening at a period of expanded European, Hispanic, and Asian immigration, it added to social competition and tensions, with the new migrants and immigrants battling for a place in jobs and housing.
Reflecting social tensions after World War I, as veterans struggled to return to the workforce and labor unions were organizing, theRed Summer of 1919was marked by hundreds of deaths and higher casualties across the U.S. as a result of white race riots against blacks that took place in more than three dozen cities. Urban problems such as crime and disease were blamed on the large influx of Southern blacks to cities in the north and west, based on stereotypes of rural southern African-Americans. Overall, blacks in Northern and Western cities experiencedsystemic discriminationin a plethora of aspects of life. Within employment, economic opportunities for blacks were routed to the lowest status and restrictive in potential mobility. Within the housing market, stronger discriminatory measures were used in correlation to the influx, resulting in a mix of "targeted violence,restrictive covenants,redliningandracial steering".[23]The Great Migration resulted in many African Americans becoming urbanized, and they began to realign from the Republican to the Democratic Party, especially because of opportunities under theNew Dealof theFranklin D. Rooseveltadministration during the Great Depression in the 1930s.[24]Substantially under pressure from African-American supporters who began theMarch on Washington Movement, President Roosevelt issued the first federal order banning discrimination and created theFair Employment Practice Committee. After both World Wars, black veterans of the military pressed for full civil rights and often led activist movements. In 1948, PresidentHarry TrumanissuedExecutive Order 9981, which endedsegregation in the military.[25]
Housing segregationbecame a nationwide problem following the Great Migration of black people out of the South.Racial covenantswere employed by manyreal estate developersto "protect" entiresubdivisions, with the primary intent to keep "white" neighborhoods "white". Ninety percent of the housing projects built in the years following World War II were racially restricted by such covenants.[26]Cities known for their widespread use of racial covenants includeChicago,Baltimore,Detroit,Milwaukee,[27]Los Angeles,Seattle, andSt. Louis.[28]
Said premises shall not be rented, leased, or conveyed to, or occupied by, any person other than of the white or Caucasian race.
While many whites defended their space with violence, intimidation, or legal tactics toward black people, many other whites migrated to more racially homogeneoussuburbanorexurbanregions, a process known aswhite flight.[30]From the 1930s to the 1960s, the National Association of Real Estate Boards (NAREB) issued guidelines that specified that a realtor "should never be instrumental in introducing to a neighborhood a character or property or occupancy, members of any race or nationality, or any individual whose presence will be clearly detrimental to property values in a neighborhood." The result was the development of all-blackghettosin the North and West, where much housing was older, as well as South.[31]
The firstanti-miscegenation lawwas passed by theMaryland General Assemblyin 1691, criminalizinginterracial marriage.[32]In a speech inCharleston, Illinoisin 1858,Abraham Lincolnstated, "I am not, nor ever have been in favor of making voters or jurors of negroes, nor of qualifying them to hold office, nor to intermarry with white people".[33]By the late 1800s, 38 US states had anti-miscegenation statutes.[32]By 1924, the ban on interracial marriage was still in force in 29 states.[32]While interracial marriage had been legal in California since 1948, in 1957 actorSammy Davis Jr.faced a backlash for his involvement with white actressKim Novak.[34]Davis briefly married a black dancer in 1958 to protect himself from mob violence.[34]In 1958, officers inVirginiaentered the home ofMildred and Richard Lovingand dragged them out of bed for living together as an interracial couple, on the basis that "any white person intermarry with a colored person"— or vice versa—each party "shall be guilty of a felony" and face prison terms of five years.[32]
Invigorated by the victory ofBrownand frustrated by the lack of immediate practical effect, private citizens increasingly rejected gradualist, legalistic approaches as the primary tool to bring aboutdesegregation. They were faced with "massive resistance" in the South by proponents of racial segregation andvoter suppression. In defiance, African-American activists adopted a combined strategy ofdirect action,nonviolence,nonviolent resistance, and many events described ascivil disobedience, giving rise to the civil rights movement of 1954 to 1968.
A. Philip Randolphhad planned a march on Washington, D.C., in 1941 to support demands for elimination ofemployment discriminationin thedefense industry; he called off the march when theRooseveltadministration met the demand by issuingExecutive Order 8802, which barred racial discrimination and created anagencyto oversee compliance with the order.[35]
The strategy of public education, legislative lobbying, and litigation that had typified the civil rights movement during the first half of the 20th century broadened afterBrownto a strategy that emphasized "direct action": boycotts,sit-ins,Freedom Rides, marches or walks, and similar tactics that relied on mass mobilization, nonviolent resistance, standing in line, and, at times, civil disobedience.[36]
Churches, local grassroots organizations, fraternal societies, and black-owned businesses mobilized volunteers to participate in broad-based actions. This was a more direct and potentially more rapid means of creating change than the traditional approach of mounting court challenges used by the NAACP and others.
In 1952, theRegional Council of Negro Leadership(RCNL), led byT. R. M. Howard, a black surgeon, entrepreneur, and planter organized a successful boycott of gas stations in Mississippi that refused to provide restrooms for blacks. Through the RCNL, Howard led campaigns to expose brutality by the Mississippi state highway patrol and to encourage blacks to make deposits in the black-owned Tri-State Bank ofNashvillewhich, in turn, gave loans to civil rights activists who were victims of a "credit squeeze" by theWhite Citizens' Councils.[37]
AfterClaudette Colvinwas arrested for not giving up her seat on aMontgomery, Alabamabus in March 1955, a bus boycott was considered and rejected. But whenRosa Parkswas arrested in December,Jo Ann Gibson Robinsonof the Montgomery Women's Political Council put the bus boycott protest in motion. Late that night, she, John Cannon (chairman of the Business Department atAlabama State University) and others mimeographed and distributed thousands of leaflets calling for a boycott.[38][39]The eventual success of the boycott made its spokesmanMartin Luther King Jr., a nationally known figure. It also inspired other bus boycotts, such as the successfulTallahassee, Floridaboycott of 1956–57.[40]This movement also sparked the1956 Sugar Bowlriots in Atlanta which later became a major organizing center of the civil rights movement, with Martin Luther King Jr.[41][42]
In 1957, King andRalph Abernathy, the leaders of the Montgomery Improvement Association, joined with other church leaders who had led similar boycott efforts, such asC. K. Steeleof Tallahassee andT. J. Jemisonof Baton Rouge, and other activists such asFred Shuttlesworth,Ella Baker,A. Philip Randolph,Bayard RustinandStanley Levison, to form theSouthern Christian Leadership Conference(SCLC). The SCLC, with its headquarters inAtlanta,Georgia, did not attempt to create a network of chapters as the NAACP did. It offered training and leadership assistance for local efforts to fight segregation. The headquarters organization raised funds, mostly from Northern sources, to support such campaigns. It made nonviolence both its central tenet and its primary method of confronting racism.
In 1959,Septima Clarke, Bernice Robinson, andEsau Jenkins, with the help ofMyles Horton'sHighlander Folk SchoolinTennessee, began the first Citizenship Schools inSouth Carolina'sSea Islands. They taught literacy to enable blacks to pass voting tests. The program was an enormous success and tripled the number of black voters onJohns Island. SCLC took over the program and duplicated its results elsewhere.
In the spring of 1951, black students inVirginiaprotested their unequal status in the state's segregated educational system. Students atMoton High Schoolprotested the overcrowded conditions and failing facility.[43]Some local leaders of the NAACP had tried to persuade the students to back down from their protest against the Jim Crow laws of school segregation. When the students did not budge, the NAACP joined their battle against school segregation. The NAACP proceeded with five cases challenging the school systems; these were later combined under what is known today asBrown v. Board of Education.[43]Under the leadership ofWalter Reuther, theUnited Auto Workersdonated $75,000 to help pay for the NAACP's efforts at the Supreme Court.[44]
On May 17, 1954, theU.S. Supreme Courtunder Chief JusticeEarl Warrenruled unanimously inBrown v. Board of Educationof Topeka, Kansas, that mandating, or even permitting,public schools to be segregatedby race wasunconstitutional.[19]Chief JusticeWarren wrote in the court majority opinion that[19][20]
Segregation of white and colored children in public schools has a detrimental effect upon the colored children. The impact is greater when it has the sanction of the law; for the policy of separating the races is usually interpreted as denoting the inferiority of the Negro group.[45]
The lawyers from the NAACP had to gather plausible evidence in order to win the case ofBrown vs. Board of Education. Their method of addressing the issue of school segregation was to enumerate several arguments. One pertained to having exposure to interracial contact in a school environment. It was argued that interracial contact would, in turn, help prepare children to live with the pressures that society exerts in regard to race and thereby afford them a better chance of living in a democracy. In addition, another argument emphasized how "'education' comprehends the entire process of developing and training the mental, physical and moral powers and capabilities of human beings".[46]
Risa Goluboffwrote that the NAACP's intention was to show the Courts that African American children were the victims of school segregation and their futures were at risk. The Court ruled that bothPlessy v. Ferguson(1896), which had established the "separate but equal" standard in general, andCumming v. Richmond County Board of Education(1899), which had applied that standard to schools, was unconstitutional.
The federal government filed afriend of the court briefin the case urging the justices to consider the effect that segregation had on America's image in theCold War. Secretary of StateDean Achesonwas quoted in the brief stating that"The United States is under constant attack in the foreign press, over the foreign radio, and in such international bodies as the United Nations because of various practices of discrimination in this country."[47][48]
The following year, in the case known asBrown II, the Court ordered segregation to be phased out over time, "with all deliberate speed".[49]Brown v. Board of Education of Topeka, Kansas(1954) did not overturnPlessy v. Ferguson(1896).Plessy v. Fergusonwas segregation in transportation modes.Brown v. Board of Educationdealt with segregation in education.Brown v. Board of Educationdid set in motion the future overturning of 'separate but equal'.
On May 18, 1954,Greensboro, North Carolina, became the first city in the South to publicly announce that it would abide by the Supreme Court'sBrown v. Board of Educationruling. "It is unthinkable,' remarked School Board Superintendent Benjamin Smith, 'that we will try to [override] the laws of the United States."[50]This positive reception for Brown, together with the appointment of African American David Jones to the school board in 1953, convinced numerous white and black citizens that Greensboro was heading in a progressive direction. Integration in Greensboro occurred rather peacefully compared to the process in Southern states such as Alabama,Arkansas, and Virginia where "massive resistance" was practiced by top officials and throughout the states. In Virginia, some counties closed their public schools rather than integrate, and many whiteChristianprivate schools were founded to accommodate students who used to go to public schools. Even in Greensboro, much local resistance to desegregation continued, and in 1969, the federal government found the city was not in compliance with the 1964 Civil Rights Act. Transition to a fully integrated school system did not begin until 1971.[50]
Many Northern cities also hadde facto segregationpolicies, which resulted in a vast gulf in educational resources between black and white communities. InHarlem, New York, for example, neither a single new school was built since the turn of the century, nor did a single nursery school exist – even as theSecond Great Migrationwas causing overcrowding. Existing schools tended to be dilapidated and staffed with inexperienced teachers.Brownhelped stimulate activism amongNew York Cityparents likeMae Mallorywho, with the support of the NAACP, initiated a successful lawsuit against the city and state onBrown'sprinciples. Mallory and thousands of other parents bolstered the pressure of the lawsuit with a school boycott in 1959. During the boycott, some of the firstfreedom schoolsof the period were established. The city responded to the campaign by permitting more open transfers to high-quality, historically white schools. (New York's African-American community, and Northern desegregation activists generally, now found themselves contending with the problem ofwhite flight, however.)[51][52]
Emmett Till, a 14-year-old African American from Chicago, visited his relatives inMoney, Mississippi, for the summer. He allegedly had an interaction with a white woman, Carolyn Bryant, in a small grocery store that violated the norms of Mississippi culture, and Bryant's husband Roy and his half-brother J. W. Milam brutally murdered young Emmett Till. They beat and mutilated him before shooting him in the head and sinking his body in theTallahatchie River. Three days later, Till's body was discovered and retrieved from the river. After Emmett's mother,Mamie Till,[53]came to identify the remains of her son, she decided she wanted to "let the people see what I have seen".[54]Till's mother then had his body taken back to Chicago where she had it displayed in an open casket during the funeral services where many thousands of visitors arrived to show their respects.[54]A later publication of an image at the funeral inJetis credited as a crucial moment in the civil rights era for displaying in vivid detail the violent racism that was being directed at black people in America.[55][54]In a column forThe Atlantic, Vann R. Newkirk wrote: "The trial of his killers became a pageant illuminating the tyranny ofwhite supremacy".[56]The state of Mississippi tried two defendants, but they were speedily acquitted by anall-white jury.[57]
"Emmett's murder," historian Tim Tyson writes, "would never have become a watershed historical moment without Mamie finding the strength to make her private grief a public matter."[58]The visceral response to his mother's decision to have an open-casket funeral mobilized the black community throughout the U.S.[56]The murder and resulting trial ended up markedly impacting the views of several young black activists.[58]Joyce Ladnerreferred to such activists as the "Emmett Till generation."[58]One hundred days after Emmett Till's murder, Rosa Parks refused to give up her seat on the bus in Montgomery, Alabama.[59]Parks later informed Till's mother that her decision to stay in her seat was guided by the image she still vividly recalled of Till's brutalized remains.[59]The glass topped casket that was used for Till's Chicago funeral was found in a cemetery garage in 2009. Till had been reburied in a different casket after being exhumed in 2005.[60]Till's family decided to donate the original casket to the Smithsonian's National Museum of African American Culture and History, where it is now on display.[61]In 2007, Bryant said that she had fabricated the most sensational part of her story in 1955.[55][62]
On December 1, 1955, nine months after a 15-year-old high school student,Claudette Colvin, refused to give up her seat to a white passenger on a public bus in Montgomery, Alabama, and was arrested,Rosa Parksdid the same thing. Parks soon became the symbol of the resulting Montgomery bus boycott and received national publicity. She was later hailed as the "mother of the civil rights movement".[63]
Parks was secretary of the Montgomery NAACP chapter and had recently returned from a meeting at theHighlander Folk Schoolin Tennessee where nonviolence as a strategy was taught byMyles Hortonand others. After Parks' arrest, African Americans gathered and organized the Montgomery bus boycott to demand a bus system in which passengers would be treated equally.[64]The organization was led by Jo Ann Robinson, a member of the Women's Political Council who had been waiting for the opportunity to boycott the bus system. Following Rosa Parks' arrest, Jo Ann Robinson mimeographed 52,500 leaflets calling for a boycott. They were distributed around the city and helped gather the attention of civil rights leaders. After the city rejected many of its suggested reforms, the NAACP, led byE. D. Nixon, pushed for full desegregation of public buses. With the support of most of Montgomery's 50,000 African Americans, the boycott lasted for 381 days, until the local ordinance segregating African Americans and whites on public buses was repealed. Ninety percent of African Americans in Montgomery partook in the boycotts, which reduced bus revenue significantly, as they comprised the majority of the riders. This movement also sparked riots leading up to the1956 Sugar Bowl.[65]In November 1956, the United States Supreme Court upheld a district court ruling in the case ofBrowder v. Gayleand ordered Montgomery's buses desegregated, ending the boycott.[64]
Local leaders established the Montgomery Improvement Association to focus their efforts.Martin Luther King Jr.was elected President of this organization. The lengthy protest attracted national attention for him and the city. His eloquent appeals to Christian brotherhood and American idealism created a positive impression on people both inside and outside the South.[39]
The Little Rock Nine were a group of nine students who attended segregated black high schools inLittle Rock, the capital of the state of Arkansas.  They each volunteered when the NAACP and the national civil rights movement obtained federal court orders to integrate the prestigiousLittle Rock Central High Schoolin September, 1957. The Nine faced intense harassment and threats of  violence from white parents and students, as well as organized white supremacy groups.  The enraged opposition emphasized miscegenation as the threat to white society.Arkansas Governor,Orval Faubus, claiming his only goal was to preserve the peace, deployed the Arkansas National Guard to prevent the black students from entering the school. Faubus defied federal court orders, whereupon President Dwight D. Eisenhower intervened. He federalized the Arkansas National Guard and sent them home. Then he sent in an elite Army unit to escort the students to school and protect them between classes  during the 1957–58 school year. In class, however, the Nine were teased and ridiculed every day. In the city compromise efforts all failed and political tensions continued to fester. A year later in September 1958 the Supreme Court ruled that all the city's high schools had to be integrated immediately. Governor Faubus and the legislature responded by immediately shutting down all the public high schools in the city for the entire 1958–1959 school year, despite the harm it did to all the students. The decision to integrate the school was a landmark event in the civil rights movement, and the students' bravery and determination in the face of violent opposition is remembered as a key moment in American history. The city and state were entangled in very expensive legal disputes for decades, while suffering a reputation for hatred and obstruction.[66][67]
During the time period considered to be the "African-American civil rights" era, the predominant use of protest was nonviolent, or peaceful.[68]Often referred to as pacifism, the method of nonviolence is considered to be an attempt to impact society positively. Although acts of racial discrimination have occurred historically throughout the United States, perhaps the most violent regions have been in the former Confederate states. During the 1950s and 1960s, the nonviolent protesting of the civil rights movement caused definite tension, which gained national attention.
In order to prepare for protests physically and psychologically, demonstrators received training in nonviolence. According to former civil rights activist Bruce Hartford, there are two main components of nonviolence training. There is the philosophical method, which involves understanding the method of nonviolence and why it is considered useful, and there is the tactical method, which ultimately teaches demonstrators "how to be a protestor—how to sit-in, how to picket, how to defend yourself against attack, giving training on how to remain cool when people are screaming racist insults into your face and pouring stuff on you and hitting you" (Civil Rights Movement Archive). The philosophical basis of the practice of nonviolence in the American civil rights movement was largely inspired byMahatma Gandhi's"non-cooperation" policiesduring his involvement in theIndian independence movement, which were intended to gain attention so that the public would either "intervene in advance" or "provide public pressure in support of the action to be taken" (Erikson, 415). As Hartford explains it, philosophical nonviolence training aims to "shape the individual person's attitude and mental response to crises and violence" (Civil Rights Movement Archive). Hartford and activists like him, who trained in tactical nonviolence, considered it necessary in order to ensure physical safety, instill discipline, teach demonstrators how to demonstrate, and form mutual confidence among demonstrators (Civil Rights Movement Archive).[68][69]
For many, the concept of nonviolent protest was a way of life, a culture. However, not everyone agreed with this notion. James Forman, formerSNCC(and later Black Panther) member, and nonviolence trainer was among those who did not. In his autobiography,The Making of Black Revolutionaries, Forman revealed his perspective on the method of nonviolence as "strictly a tactic, not a way of life without limitations." Similarly,Bob Moses, who was also an active member ofSNCC, felt that the method of nonviolence was practical. When interviewed by author Robert Penn Warren, Moses said "There's no question that he (Martin Luther King Jr.) had a great deal of influence with the masses. But I don't think it's in the direction of love. It's in a practical direction … ." (Who Speaks for the Negro? Warren).[70][71]
According to a 2020 study in theAmerican Political Science Review, nonviolent civil rights protests boosted vote shares for the Democratic party in presidential elections in nearby counties, but violent protests substantially boosted white support for Republicans in counties near to the violent protests.[72]
In July 1958, theNAACP Youth Councilsponsored sit-ins at the lunch counter of aDockum Drug Storein downtownWichita, Kansas. After three weeks, the movement successfully got the store to change its policy of segregated seating, and soon afterward all Dockum stores in Kansas were desegregated. This movement was quickly followed in the same year by astudent sit-in at a Katz Drug StoreinOklahoma Cityled byClara Luper, which also was successful.[73]
Mostly black students from area colleges led a sit-in at aWoolworth's store inGreensboro, North Carolina.[74]On February 1, 1960, four students,Ezell A. Blair Jr., David Richmond,Joseph McNeil, andFranklin McCainfromNorth Carolina Agricultural & Technical College, an all-black college, sat down at the segregated lunch counter to protest Woolworth's policy of excluding African Americans from being served food there.[75]The four students purchased small items in other parts of the store and kept their receipts, then sat down at the lunch counter and asked to be served. After being denied service, they produced their receipts and asked why their money was good everywhere else at the store, but not at the lunch counter.[76]
The protesters had been encouraged to dress professionally, to sit quietly, and to occupy every other stool so that potential white sympathizers could join in. The Greensboro sit-in was quickly followed by other sit-ins inRichmond, Virginia;[77][78]Nashville, Tennessee; and Atlanta, Georgia.[79][80]The most immediately effective of these was in Nashville, where hundreds of well organized and highly disciplined college studentsconducted sit-insin coordination with a boycott campaign.[81][82]As students across the south began to "sit-in" at the lunch counters of local stores, police and other officials sometimes used brutal force to physically escort the demonstrators from the lunch facilities.
The "sit-in" technique was not new—as far back as 1939, African-American attorneySamuel Wilbert Tuckerorganized a sit-in at the then-segregatedAlexandria, Virginia, library.[83]In 1960 the technique succeeded in bringing national attention to the movement.[84]On March 9, 1960, anAtlanta University Centergroup of students releasedAn Appeal for Human Rightsas a full-page advertisement in newspapers, including theAtlanta Constitution,Atlanta Journal, andAtlanta Daily World.[85]Known as theCommittee on Appeal for Human Rights(COAHR), the group initiated theAtlanta Student Movementand began to lead sit-ins starting on March 15, 1960.[80][86]By the end of 1960, the process of sit-ins had spread to every southern andborder state, and even to facilities inNevada,Illinois, andOhiothat discriminated against blacks.
Demonstrators focused not only on lunch counters but also on parks, beaches, libraries, theaters, museums, and other public facilities. In April 1960 activists who had led these sit-ins were invited by SCLC activistElla Bakerto hold a conference atShaw University, ahistorically black universityinRaleigh, North Carolina. This conference led to the formation of theStudent Nonviolent Coordinating Committee(SNCC).[87]SNCC took these tactics of nonviolent confrontation further, and organized the freedom rides. As the constitution protected interstate commerce, they decided to challenge segregation on interstate buses and in public bus facilities by putting interracial teams on them, to travel from the North through the segregated South.[88]
Freedom Rides were journeys by civil rights activists on interstate buses into the segregated southern United States to test the United States Supreme Court decisionBoynton v. Virginia(1960), which ruled that segregation was unconstitutional for passengers engaged in interstate travel. Organized byCORE, the first Freedom Ride of the 1960s left Washington D.C. on May 4, 1961, and was scheduled to arrive in New Orleans on May 17.[89]
During the first and subsequent Freedom Rides, activists traveled through theDeep Southto integrate seating patterns on buses and desegregate bus terminals, including restrooms and water fountains. That proved to be a dangerous mission. InAnniston, Alabama, one buswas firebombed, forcing its passengers to flee for their lives.[90]
InBirmingham, Alabama, anFBIinformant reported that Public Safety CommissionerEugene "Bull" Connorgave Ku Klux Klan members fifteen minutes to attack an incoming group of freedom riders before having police "protect" them. The riders were severely beaten "until it looked like a bulldog had got a hold of them."James Peck, a white activist, was beaten so badly that he required fifty stitches to his head.[90]
In a similar occurrence in Montgomery, Alabama, the Freedom Riders followed in the footsteps of Rosa Parks and rode an integrated Greyhound bus from Birmingham. Although they were protesting interstate bus segregation in peace, they were met with violence in Montgomery as a large, white mob attacked them for their activism. They caused an enormous, 2-hour long riot which resulted in 22 injuries, five of whom were hospitalized.[91]
Mob violence in Anniston and Birmingham temporarily halted the rides. SNCC activists from Nashville brought in new riders to continue the journey from Birmingham to New Orleans. In Montgomery, Alabama, at theGreyhound Bus Station, a mob charged another busload of riders, knockingJohn Lewis[92]unconscious with a crate and smashingLifephotographerDon Urbrockin the face with his own camera. A dozen men surroundedJames Zwerg,[93]a white student fromFisk University, and beat him in the face with a suitcase, knocking out his teeth.[90]
On May 24, 1961, the freedom riders continued their rides intoJackson, Mississippi, where they were arrested for "breaching the peace" by using "white only" facilities. New Freedom Rides were organized by many different organizations and continued to flow into the South. As riders arrived in Jackson, they were arrested. By the end of summer, more than 300 had been jailed in Mississippi.[89]
… When the weary Riders arrive in Jackson and attempt to use "white only" restrooms and lunch counters they are immediately arrested for Breach of Peace and Refusal to Obey an Officer. Says Mississippi GovernorRoss Barnettin defense of segregation: "The Negro is different because God made him different to punish him." From lockup, the Riders announce "Jail No Bail"—they will not pay fines for unconstitutional arrests and illegal convictions—and by staying in jail they keep the issue alive. Each prisoner will remain in jail for 39 days, the maximum time they can serve without losing their right to appeal the unconstitutionality of their arrests, trials, and convictions. After 39 days, they file an appeal and post bond...[94]
The jailed freedom riders were treated harshly, crammed into tiny, filthy cells and sporadically beaten. In Jackson, some male prisoners were forced to do hard labor in 100 °F (38 °C) heat. Others were transferred to theMississippi State Penitentiaryat Parchman, where they were treated to harsh conditions. Sometimes the men were suspended by "wrist breakers" from the walls. Typically, the windows of their cells were shut tight on hot days, making it hard for them to breathe.
Public sympathy and support for the freedom riders ledJohn F. Kennedy's administration to order theInterstate Commerce Commission(ICC) to issue a new desegregation order. When the new ICC rule took effect on November 1, 1961, passengers were permitted to sit wherever they chose on the bus; "white" and "colored" signs came down in the terminals; separate drinking fountains, toilets, and waiting rooms were consolidated; and lunch counters began serving people regardless of skin color.
The student movement involved such celebrated figures as John Lewis, a single-minded activist;James Lawson,[95]the revered "guru" of nonviolent theory and tactics;Diane Nash,[96]an articulate and intrepid public champion of justice;Bob Moses, pioneer of voting registration in Mississippi; andJames Bevel, a fiery preacher and charismatic organizer, strategist, and facilitator. Other prominent student activists includedDion Diamond,[97]Charles McDew,Bernard Lafayette,[98]Charles Jones,Lonnie King,Julian Bond,[99]Hosea Williams, andStokely Carmichael.
After the Freedom Rides, local black leaders in Mississippi such asAmzie Moore,Aaron Henry,Medgar Evers, and others asked SNCC to help register black voters and to build community organizations that could win a share of political power in the state. Since Mississippi ratified its new constitution in 1890 with provisions such as poll taxes, residency requirements, and literacy tests, it made registration more complicated and stripped blacks from voter rolls and voting. Also, violence at the time of elections had earlier suppressed black voting.
By the mid-20th century, preventing blacks from voting had become an essential part of the culture of white supremacy. In June and July 1959, members of the black community in Fayette County, TN formed theFayette County Civic and Welfare Leagueto spur voting. At the time, there were 16,927 blacks in the county, yet only 17 of them had voted in the previous seven years. Within a year, some 1,400 blacks had registered, and the white community responded with harsh economic reprisals. Using registration rolls, the White Citizens Council circulated a blacklist of all registered black voters, allowing banks, local stores, and gas stations to conspire to deny registered black voters essential services. What's more, sharecropping blacks who registered to vote were getting evicted from their homes. All in all, the number of evictions came to 257 families, many of whom were forced to live in a makeshift Tent City for well over a year. Finally, in December 1960, the Justice Department invoked its powers authorized by the Civil Rights Act of 1957 to file a suit against seventy parties accused of violating the civil rights of black Fayette County citizens.[100]In the following year the first voter registration project inMcComband the surrounding counties in the Southwest corner of the state. Their efforts were met with violent repression from state and local lawmen, theWhite Citizens' Council, and the Ku Klux Klan. Activists were beaten, there were hundreds of arrests of local citizens, and the voting activist Herbert Lee was murdered.[101]
White opposition to black voter registration was so intense in Mississippi that Freedom Movement activists concluded that all of the state's civil rights organizations had to unite in a coordinated effort to have any chance of success. In February 1962, representatives of SNCC, CORE, and the NAACP formed theCouncil of Federated Organizations(COFO). At a subsequent meeting in August, SCLC became part of COFO.[102]
In the Spring of 1962, with funds from theVoter Education Project, SNCC/COFO began voter registration organizing in the Mississippi Delta area aroundGreenwood, and the areas surroundingHattiesburg,Laurel, andHolly Springs. As in McComb, their efforts were met with fierce opposition—arrests, beatings, shootings, arson, and murder. Registrars used theliteracy testto keep blacks off the voting roles by creating standards that even highly educated people could not meet. In addition, employers fired blacks who tried to register, and landlords evicted them from their rental homes.[103]Despite these actions, over the following years, the black voter registration campaign spread across the state.
Similar voter registration campaigns—with similar responses—were begun by SNCC, CORE, and SCLC inLouisiana,Alabama, southwestGeorgia, andSouth Carolina. By 1963, voter registration campaigns in the South were as integral to the Freedom Movement as desegregation efforts. After the passage of theCivil Rights Act of 1964,[104]protecting and facilitating voter registration despite state barriers became the main effort of the movement. It resulted in the passage of theVoting Rights Actof 1965, which had provisions to enforce the constitutional right to vote for all citizens.
Beginning in 1956,Clyde Kennard, a blackKorean War-veteran, wanted to enroll at Mississippi Southern College (now theUniversity of Southern Mississippi) atHattiesburgunder theG.I. Bill.William David McCain, the college president, used theMississippi State Sovereignty Commission, in order to prevent his enrollment by appealing to local black leaders and the segregationist state political establishment.[105]
The state-funded organization tried to counter the civil rights movement by positively portraying segregationist policies. More significantly, it collected data on activists, harassed them legally, and used economic boycotts against them by threatening their jobs (or causing them to lose their jobs) to try to suppress their work.
Kennard was twice arrested on trumped-up charges, and eventually convicted and sentenced to seven years in the state prison.[106]After three years athard labor, Kennard was paroled byMississippi GovernorRoss Barnett. Journalists had investigated his case and publicized the state's mistreatment of hiscolon cancer.[106]
McCain's role in Kennard's arrests and convictions is unknown.[107][108][109][110]While trying to prevent Kennard's enrollment, McCain made a speech in Chicago, with his travel sponsored by the Mississippi State Sovereignty Commission. He described the blacks' seeking to desegregate Southern schools as "imports" from the North. (Kennard was a native and resident of Hattiesburg.) McCain said:
We insist that educationally and socially, we maintain asegregatedsociety...In all fairness, I admit that we are not encouraging Negro voting...The Negroes prefer that control of the government remain in the white man's hands.[107][109][110]
Note: Mississippi had passed a new constitution in 1890 that effectivelydisfranchisedmost blacks by changing electoral and voter registration requirements; although it deprived them of constitutional rights authorized under post-Civil War amendments, it survivedU.S. Supreme Courtchallenges at the time. It was not until after the passage of the 1965Voting Rights Actthat most blacks in Mississippi and other southern states gained federal protection to enforce the constitutional right of citizens to vote.
In September 1962,James Meredithwon a lawsuit to secure admission to the previously segregatedUniversity of Mississippi. He attempted to enter campus on September 20, on September 25, and again on September 26. He was blocked by Governor Ross Barnett, who said, "[N]o school will be integrated in Mississippi while I am your Governor." TheFifth U.S. Circuit Court of Appealsheld Barnett and Lieutenant GovernorPaul B. Johnson Jr.incontempt, ordering them arrested and fined more than $10,000 for each day they refused to allow Meredith to enroll.
Attorney GeneralRobert F. Kennedysent in a force ofU.S. Marshalsand deputizedU.S. Border Patrolagents andFederal Bureau of Prisonsofficers. On September 30, 1962, Meredith entered the campus under their escort. Students and other whites began rioting that evening, throwing rocks and firing on the federal agents guarding Meredith at Lyceum Hall. Rioters ended up killing two civilians, including a French journalist; 28 federal agents suffered gunshot wounds, and 160 others were injured. PresidentJohn F. KennedysentU.S. Armyand federalizedMississippi National Guardforces to the campus to quell the riot. Meredith began classes the day after the troops arrived.[111]
Kennard and other activists continued to work on public university desegregation. In 1965Raylawni BranchandGwendolyn Elaine Armstrongbecame the first African-American students to attend theUniversity of Southern Mississippi. By that time, McCain helped ensure they had a peaceful entry.[112]In 2006, Judge Robert Helfrich ruled that Kennard was factually innocent of all charges for which he had been convicted in the 1950s.[106]
The SCLC, which had been criticized by some student activists for its failure to participate more fully in the freedom rides, committed much of its prestige and resources to a desegregation campaign inAlbany, Georgia, in November 1961. King, who had been criticized personally by some SNCC activists for his distance from the dangers that local organizers faced—and given the derisive nickname "De Lawd" as a result—intervened personally to assist the campaign led by both SNCC organizers and local leaders.
The campaign was a failure because of the canny tactics ofLaurie Pritchett, the local police chief, and divisions within the black community. The goals may not have been specific enough. Pritchett contained the marchers without violent attacks on demonstrators that inflamed national opinion. He also arranged for arrested demonstrators to be taken to jails in surrounding communities, allowing plenty of room to remain in his jail. Pritchett also foresaw King's presence as a danger and forced his release to avoid King's rallying the black community. King left in 1962 without having achieved any dramatic victories. The local movement, however, continued the struggle, and it obtained significant gains in the next few years.[113]
The Albany movement was shown to be an important education for the SCLC, however, when it undertook the Birmingham campaign in 1963. Executive DirectorWyatt Tee Walkercarefully planned the early strategy and tactics for the campaign. It focused on one goal—the desegregation of Birmingham's downtown merchants, rather than total desegregation, as in Albany.
The movement's efforts were helped by the brutal response of local authorities, in particularEugene "Bull" Connor, the Commissioner of Public Safety. He had long held much political power but had lost a recent election for mayor to a less rabidly segregationist candidate. Refusing to accept the new mayor's authority, Connor intended to stay in office.
The campaign used a variety of nonviolent methods of confrontation, including sit-ins, kneel-ins at local churches, and a march to the county building to mark the beginning of a drive to register voters. The city, however, obtained aninjunctionbarring all such protests. Convinced that the order was unconstitutional, the campaign defied it and prepared formass arrestsof its supporters. King elected to be among those arrested on April 12, 1963.[114]
While in jail, King wrote his famous "Letter from Birmingham Jail"[115]on the margins of a newspaper, since he had not been allowed any writing paper while held in solitary confinement.[116]Supporters appealed to the Kennedy administration, which intervened to obtain King's release.Walter Reuther, president of theUnited Auto Workers, arranged for $160,000 to bail out King and his fellow protestors.[117]King was allowed to call his wife, who was recuperating at home after the birth of their fourth child and was released early on April 19.
The campaign, however, faltered as it ran out of demonstrators willing to risk arrest.James Bevel, SCLC's Director of Direct Action and Director of Nonviolent Education, then came up with a bold and controversial alternative: to train high school students to take part in the demonstrations. As a result, in what would be called theChildren's Crusade, more than one thousand students skipped school on May 2 to meet at the 16th Street Baptist Church to join the demonstrations. More than six hundred marched out of the church fifty at a time in an attempt to walk to City Hall to speak to Birmingham's mayor about segregation. They were arrested and put into jail. In this first encounter, the police acted with restraint. On the next day, however, another one thousand students gathered at the church. When Bevel started them marching fifty at a time, Bull Connor finally unleashed police dogs on them and then turned the city's fire hoses water streams on the children. National television networks broadcast the scenes of the dogs attacking demonstrators and the water from the fire hoses knocking down the schoolchildren.[118]
Widespread public outrage led theKennedy administrationto intervene more forcefully in negotiations between the white business community and the SCLC. On May 10, the parties announced an agreement to desegregate the lunch counters and other public accommodations downtown, to create a committee to eliminate discriminatory hiring practices, to arrange for the release of jailed protesters, and to establish regular means of communication between black and white leaders.
Not everyone in the black community approved of the agreement—Fred Shuttlesworthwas particularly critical, since he was skeptical about the good faith of Birmingham's power structure from his experience in dealing with them. Parts of the white community reacted violently. Theybombed the Gaston Motel, which housed the SCLC's unofficial headquarters, and the home of King's brother, the Reverend A. D. King. In response,thousands of blacks rioted, burning numerous buildings and one of them stabbed and wounded a police officer.[119]
Kennedy prepared to federalize theAlabama National Guardif the need arose. Four months later, on September 15, a conspiracy of Ku Klux Klan membersbombed the Sixteenth Street Baptist Churchin Birmingham, killing four young girls.
Birmingham was only one of over a hundred cities rocked by the chaotic protest that spring and summer, some of them in the North but mainly in the South. During the March on Washington, Martin Luther King Jr. would refer to such protests as "the whirlwinds of revolt." In Chicago, blacks rioted through the South Side in late May after a white police officer shot a fourteen-year-old black boy who was fleeing the scene of a robbery.[120]Violent clashes between black activists and white workers took place in both Philadelphia and Harlem in successful efforts to integrate state construction projects.[121][122]On June 6, over a thousand whites attacked a sit-in in Lexington, North Carolina; blacks fought back and one white man was killed.[123][124]Edwin C. Berry of the National Urban League warned of a complete breakdown in race relations: "My message from the beer gardens and the barbershops all indicate the fact that the Negro is ready for war."[120]
InCambridge, Maryland, a working‐class city on theEastern Shore,Gloria Richardsonof SNCC led a movement that pressed for desegregation but also demanded low‐rent public housing, job‐training, public and private jobs, and an end to police brutality.[125]On June 11, struggles between blacks and whitesescalated into violent rioting, leading Maryland GovernorJ. Millard Tawesto declaremartial law. When negotiations between Richardson and Maryland officials faltered, Attorney General Robert F. Kennedy directly intervened to negotiate a desegregation agreement.[126]Richardson felt that the increasing participation of poor and working-class blacks was expanding both the power and parameters of the movement, asserting that "the people as a whole really do have more intelligence than a few of their leaders.ʺ[125]
In their deliberations during this wave of protests, the Kennedy administration privately felt that militant demonstrations were ʺbad for the countryʺ and that "Negroes are going to push this thing too far."[127]On May 24, Robert Kennedy had ameeting with prominent black intellectualsto discuss the racial situation. The black delegation criticized Kennedy harshly for vacillating on civil rights and said that the African-American community's thoughts were increasingly turning to violence. The meeting ended with ill will on all sides.[128][129][130]Nonetheless, the Kennedys ultimately decided that new legislation for equal public accommodations was essential to drive activists "into the courts and out of the streets."[127][131]
On June 11, 1963,George Wallace, Governor of Alabama, triedto block[132]the integration of theUniversity of Alabama. President John F. Kennedy sent a military force to make Governor Wallace step aside, allowing the enrollment ofVivian Malone JonesandJames Hood. That evening, President Kennedy addressed the nation on TV and radio with his historiccivil rights speech, where he lamented "a rising tide of discontent that threatens the public safety." He called on Congress to pass new civil rights legislation, and urged the country to embrace civil rights as "a moral issue...in our daily lives."[133]In the early hours of June 12,Medgar Evers, field secretary of the Mississippi NAACP, was assassinated by a member of the Klan.[134][135]The next week, as promised, on June 19, 1963, President Kennedy submitted his Civil Rights bill to Congress.[136]
Randolph andBayard Rustinwere the chief planners of theMarch on Washington for Jobs and Freedom, which they proposed in 1962. In 1963, the Kennedy administration initially opposed the march out of concern it would negatively impact the drive for passage of civil rights legislation. However, Randolph and King were firm that the march would proceed.[137]With the march going forward, the Kennedys decided it was important to work to ensure its success. Concerned about the turnout, President Kennedy enlisted the aid of white church leaders andWalter Reuther, president of theUAW, to help mobilize white supporters for the march.[138][139]
The march was held on August 28, 1963. Unlike the planned 1941 march, for which Randolph included only black-led organizations in the planning, the 1963 march was a collaborative effort of all of the major civil rights organizations, the more progressive wing of the labor movement, and other liberal organizations. The march had six official goals:
Of these, the march's major focus was on passage of the civil rights law that the Kennedy administration had proposed after the upheavals in Birmingham.
National media attention also greatly contributed to the march's national exposure and probable impact. In the essay "The March on Washington and Television News",[140]historian William Thomas notes: "Over five hundred cameramen, technicians, and correspondents from the major networks were set to cover the event. More cameras would be set up than had filmed the last presidential inauguration. One camera was positioned high in the Washington Monument, to give dramatic vistas of the marchers". By carrying the organizers' speeches and offering their own commentary, television stations framed the way their local audiences saw and understood the event.[140]
The march was a success, although not without controversy. An estimated 200,000 to 300,000 demonstrators gathered in front of theLincoln Memorial, where King delivered his famous "I Have a Dream" speech. While many speakers applauded the Kennedy administration for the efforts it had made toward obtaining new, more effective civil rights legislation protecting the right to vote and outlawing segregation,John LewisofSNCCtook the administration to task for not doing more to protect southern blacks and civil rights workers under attack in the Deep South.
After the march, King and other civil rights leaders met with President Kennedy at theWhite House. While the Kennedy administration appeared sincerely committed to passing the bill, it was not clear that it had enough votes in Congress to do so. However, whenPresident Kennedy was assassinatedon November 22, 1963,[136]the new PresidentLyndon Johnsondecided to use his influence inCongressto bring about much of Kennedy's legislative agenda.
InSt. Augustine, Floridaa local movement had been picketing segregated local institutions since 1963. In the fall of 1964, four teenagers who came to be known as "The St. Augustine Four" sat in at a local Woolworth's lunch counter, seeking to get served. They were arrested and convicted of trespassing, and sentenced to six months in jail and reform school. It took a special act of the governor and cabinet of Florida to release them after national protests by thePittsburgh Courier,Jackie Robinson, and others.
In response to the repression, the St. Augustine movement practiced armed self-defense in addition to nonviolent direct action. In June 1963, Hayling publicly stated that "I and the others have armed. We will shoot first and answer questions later. We are not going to die like Medgar Evers." The comment made national headlines.[141]When Klan nightriders terrorized black neighborhoods in St. Augustine, Hayling's NAACP members often drove them off with gunfire. In October 1963, a Klansman was killed.[142]
In 1964, Hayling and other activists urged theSouthern Christian Leadership Conferenceto come to St. Augustine. Four prominent Massachusetts women—Mary Parkman Peabody, Esther Burgess, Hester Campbell (all of whose husbands were Episcopal bishops), and Florence Rowe (whose husband was vice president of an insurance company)—also came to lend their support. The arrest of Peabody, the 72-year-old mother of the governor of Massachusetts, for attempting to eat at the segregated Ponce de Leon Motor Lodge in an integrated group, made front-page news across the country and brought the movement in St. Augustine to the attention of the world.[143]
Widely publicized activities continued in the ensuing months. When King was arrested, he sent a "Letter from the St. Augustine Jail" to a northern supporter,RabbiIsrael S. Dresner. A week later, in the largest mass arrest of rabbis in American history took place, while they were conducting a pray-in at the segregated Monson Motel. A well-known photograph taken in St. Augustine showsthe manager of the Monson Motelpouringhydrochloric acidin the swimming pool while blacks and whites are swimming in it. As he did so he yelled that he was "cleaning the pool", a presumed reference to it now being, in his eyes, racially contaminated.[144]The photograph was run on the front page of a Washington newspaper the day the Senate was to vote on passing the Civil Rights Act of 1964.
From November 1963 through April 1964, theChester school protestswere a series of civil rights protests led byGeorge Raymondof theNational Association for the Advancement of Colored Persons(NAACP) andStanley Brancheof the Committee for Freedom Now (CFFN) that madeChester, Pennsylvaniaone of the key battlegrounds of the civil rights movement.James Farmer, the national director of theCongress of Racial Equalitycalled Chester "the Birmingham of the North".[145]
In 1962, Branche and the CFFN focused on improving conditions at the predominantly black Franklin Elementary school in Chester. Although the school was built to house 500 students, it had become overcrowded with 1,200 students. The school's average class size was 39, twice the number of nearby all-white schools.[146]The school was built in 1910 and had never been updated. Only two bathrooms were available for the entire school.[147]In November 1963, CFFN protesters blocked the entrance to Franklin Elementary school and the Chester Municipal Building resulting in the arrest of 240 protesters. Following public attention to the protests stoked by media coverage of the mass arrests, the mayor and school board negotiated with the CFFN and NAACP.[145]The Chester Board of Education agreed toreduce class sizesat Franklin school, remove unsanitary toilet facilities, relocate classes held in the boiler room and coal bin and repair school grounds.[147]
Emboldened by the success of the Franklin Elementary school demonstrations, the CFFN recruited new members, sponsored voter registration drives and planned a citywide boycott of Chester schools. Branche built close ties with students at nearbySwarthmore College,Pennsylvania Military CollegeandCheyney State Collegein order to ensure large turnouts at demonstrations and protests.[145]Branche invitedDick GregoryandMalcolm Xto Chester to participate in the "Freedom Now Conference"[148]and other national civil rights leaders such asGloria Richardsoncame to Chester in support of the demonstrations.[149]
In 1964, a series of almost nightly protests brought chaos to Chester as protestors argued that the Chester School Board hadde factosegregationof schools. The mayor of Chester,James Gorbey, issued "The Police Position to Preserve the Public Peace", a ten-point statement promising an immediate return to law and order. The city deputized firemen and trash collectors to help handle demonstrators.[145]The State of Pennsylvania deployed 50 state troopers to assist the 77-member Chester police force.[147]The demonstrations were marked by violence and charges of police brutality.[150]Over six hundred people were arrested over a two-month period of civil rights rallies, marches, pickets, boycotts and sit-ins.[145]Pennsylvania GovernorWilliam Scrantonbecame involved in the negotiations and convinced Branche to obey a court-ordered moratorium on demonstrations.[148]Scranton created the Pennsylvania Human Relations Commission to conduct hearings on the de facto segregation of public schools. All protests were discontinued while the commission held hearings during the summer of 1964.[151]
In November 1964, the Pennsylvania Human Relations Commission concluded that the Chester School Board had violated the law and ordered the Chester School District to desegregate the city's six predominantly African-American schools. The city appealed the ruling, which delayed implementation.[147]
In the summer of 1964,COFObrought nearly 1,000 activists to Mississippi—most of them white college students from the North and West—to join with local black activists to register voters, teach in "Freedom Schools", and organize theMississippi Freedom Democratic Party(MFDP).[152]
Many of Mississippi's white residents deeply resented the outsiders and attempts to change their society. State and local governments, police, theWhite Citizens' Counciland the Ku Klux Klan used arrests, beatings, arson, murder, spying, firing, evictions, and other forms of intimidation and harassment to oppose the project and prevent blacks from registering to vote or achieving social equality.[153]
On June 21, 1964,three civil rights workers disappeared:James Chaney, a young black Mississippian and plasterer's apprentice; and twoJewishactivists,Andrew Goodman, aQueens Collegeanthropology student; andMichael Schwerner, aCOREorganizer fromManhattan'sLower East Side. They were found weeks later, murdered by conspirators who turned out to be local members of the Klan, some of the members of theNeshoba Countysheriff's department. This outraged the public, leading the U.S. Justice Department along with the FBI (the latter which had previously avoided dealing with the issue of segregation and persecution of blacks) to take action. The outrage over these murders helped lead to the passage of the Civil Rights Act of 1964 and the Voting Rights Act of 1965.
From June to August, Freedom Summer activists worked in 38 local projects scattered across the state, with the largest number concentrated in theMississippi Deltaregion. At least 30 Freedom Schools, with close to 3,500 students, were established, and 28 community centers were set up.[154]
Over the course of the Summer Project, some 17,000 Mississippi blacks attempted to become registered voters in defiance of the red tape and forces ofwhite supremacyarrayed against them—only 1,600 (less than 10%) succeeded. But more than 80,000 joined theMississippi Freedom Democratic Party(MFDP), founded as an alternative political organization, showing their desire to vote and participate in politics.[155]
ThoughFreedom Summerfailed to register many voters, it had a significant effect on the course of the civil rights movement. It helped break down the decades of people's isolation and repression that were the foundation of theJim Crowsystem. Before Freedom Summer, the national news media had paid little attention to the persecution of black voters in the Deep South and the dangers endured by black civil rights workers. The progression of events throughout the South increased media attention to Mississippi.[156]
The deaths of affluent northern white students and threats to non-Southerners attracted the full attention of the media spotlight to the state. Many black activists became embittered, believing the media valued the lives of whites and blacks differently. Perhaps the most significant effect of Freedom Summer was on the volunteers, almost all of whom—black and white—still consider it to have been one of the defining periods of their lives.[156]
Although President Kennedy hadproposed civil rights legislationand it had support from Northern Congressmen and Senators of both parties, Southern Senators blocked the bill by threateningfilibusters. After considerable parliamentary maneuvering and 54 days of filibuster on the floor of the United States Senate, President Johnson got a bill through the Congress.[157]
On July 2, 1964, Johnson signed theCivil Rights Act of 1964,[104]which banned discrimination based on "race, color, religion, sex or national origin" in employment practices and public accommodations. The bill authorized the Attorney General to file lawsuits to enforce the new law. The law also nullified state and local laws that required such discrimination.
Blacks in Mississippi had beendisfranchisedby statutory and constitutional changes since the late 19th century. In 1963 COFO held aFreedom Ballotin Mississippi to demonstrate the desire of black Mississippians to vote. More than 80,000 people registered and voted in the mock election, which pitted an integrated slate of candidates from the "Freedom Party" against the official state Democratic Party candidates.[158]
In 1964, organizers launched the Mississippi Freedom Democratic Party (MFDP) to challenge the all-white official party. When Mississippi voting registrars refused to recognize their candidates, they held their own primary. They selectedFannie Lou Hamer,Annie Devine, andVictoria Grayto run forCongress, and a slate of delegates to represent Mississippi at the 1964 Democratic National Convention.[152]
The presence of the Mississippi Freedom Democratic Party inAtlantic City, New Jersey, was inconvenient, however, for the convention organizers. They had planned a triumphant celebration of the Johnson administration's achievements in civil rights, rather than a fight over racism within the Democratic Party. All-white delegations from other Southern states threatened to walk out if the official slate from Mississippi was not seated. Johnson was worried about the inroads that RepublicanBarry Goldwater's campaign was making in what previously had been the white Democratic stronghold of the "Solid South", as well as support thatGeorge Wallacehad received in the North during the Democratic primaries.
Johnson could not, however, prevent the MFDP from taking its case to the Credentials Committee. ThereFannie Lou Hamertestified eloquently about the beatings that she and others endured and the threats they faced for trying to register to vote. Turning to the television cameras, Hamer asked, "Is this America?"
Johnson offered the MFDP a "compromise" under which it would receive two non-voting, at-large seats, while the white delegation sent by the official Democratic Party would retain its seats. The MFDP angrily rejected the "compromise."
The MFDP kept up its agitation at the convention after it was denied official recognition. When all but three of the "regular" Mississippi delegates left because they refused to pledge allegiance to the party, the MFDP delegates borrowed passes from sympathetic delegates and took the seats vacated by the official Mississippi delegates. National party organizers removed them. When they returned the next day, they found convention organizers had removed the empty seats that had been there the day before. They stayed and sang "freedom songs".
The 1964 Democratic Party convention disillusioned many within the MFDP and the civil rights movement, but it did not destroy the MFDP. The MFDP became more radical after Atlantic City. It invitedMalcolm Xto speak at one of its conventions and opposed thewar in Vietnam.
SNCChad undertaken an ambitious voter registration program inSelma, Alabama, in 1963, but by 1965 little headway had been made in the face of opposition from Selma's sheriff, Jim Clark. After local residents asked the SCLC for assistance, King came to Selma to lead several marches, at which he was arrested along with 250 other demonstrators. The marchers continued to meet violent resistance from the police.Jimmie Lee Jackson, a resident of nearby Marion, was killed by police at a later march on February 17, 1965. Jackson's death promptedJames Bevel, director of the Selma Movement, to initiate and organize a plan to march from Selma toMontgomery, the state capital.
On March 7, 1965, acting on Bevel's plan,Hosea Williamsof the SCLC and John Lewis of SNCC led a march of 600 people to walk the 54 miles (87 km) from Selma to the state capital in Montgomery. Six blocks into the march, at theEdmund Pettus Bridgewhere the marchers left the city and moved into the county, state troopers, and local county law enforcement, some mounted on horseback, attacked the peaceful demonstrators with billy clubs,tear gas, rubber tubes wrapped in barbed wire, and bullwhips. They drove the marchers back into Selma. Lewis was knocked unconscious and dragged to safety. At least 16 other marchers were hospitalized. Among those gassed and beaten wasAmelia Boynton Robinson, who was at the center of civil rights activity at the time.
The national broadcast of the news footage of lawmen attacking unresisting marchers seeking to exercise their constitutional right to vote provoked a national response and hundreds of people from all over the country came for a second march. These marchers were turned around by King at the last minute so as not to violate a federal injunction. This displeased many demonstrators, especially those who resented King's nonviolence (such asJames FormanandRobert F. Williams).
That night, local Whites attackedJames Reeb, a voting rights supporter. He died of his injuries in a Birmingham hospital on March 11. Due to the national outcry at a White minister being murdered so brazenly (as well as the subsequent civil disobedience led by Gorman and other SNCC leaders all over the country, especially in Montgomery and at the White House), the marchers were able to lift the injunction and obtain protection from federal troops, permitting them to make the march across Alabama without incident two weeks later; during the march, Gorman, Williams, and other more militant protesters carried bricks and sticks of their own.
Four Klansmen shot and killedDetroithomemakerViola Liuzzoas she drove marchers back to Selma that night.
Eight days after the first march, but before the final march, President Johnson delivered a televised address to support the voting rights bill he had sent to Congress. In it he stated:
Their cause must be our cause too. Because it is not just Negroes, but really it is all of us, who must overcome the crippling legacy of bigotry and injustice. And we shall overcome.
On May 26, the Senate passed S. 1564, theVoting Rights Act, by a vote of 77–19, with only Southern Senators opposing the bill.[159][160]On July 9, the House of Representatives passed H.R. 6400, the House version of the bill, by a vote of 333-85.[159]: 163–165[161]On August 3–4, the two houses of Congress reconciled the two bill, and on August 6, President Johnson signed theVoting Rights Act of 1965. The bill suspended literacy tests and other subjective voter registration tests. It authorized Federal supervision of voter registration in states and individual voting districts where such tests were being used and where African Americans were historically under-represented in voting rolls compared to the eligible population. African Americans who had been barred from registering to vote finally had an alternative to taking suits to local or state courts, which had seldom prosecuted their cases to success. If discrimination in voter registration occurred, the 1965 act authorized theAttorney General of the United Statesto send Federal examiners to replace local registrars.
Within months of the bill's passage, 250,000 new black voters had been registered, one-third of them by federal examiners. Within four years, voter registration in the South had more than doubled. In 1965, Mississippi had the highest black voter turnout at 74% and led the nation in the number of black public officials elected. In 1969, Tennessee had a 92% turnout among black voters; Arkansas, 78%; and Texas, 73%.
Several whites who had opposed the Voting Rights Act paid a quick price. In 1966Sheriff Jim Clarkof Selma, Alabama, infamous for usingcattle prodsagainst civil rights marchers, was up for reelection. Although he took off the notorious "Never" pin on his uniform, he was defeated. At the election, Clark lost as blacks voted to get him out of office.
Blacks' regaining the power to vote changed the political landscape of the South. When Congress passed the Voting Rights Act, only about 100 African Americans held elective office, all in northern states. By 1989, there were more than 7,200 African Americans in office, including more than 4,800 in the South. Nearly every county where populations were majority black in Alabama had a black sheriff. Southern blacks held top positions in city, county, and state governments.
Atlanta elected a black mayor in 1982,Andrew Young, as didJackson, Mississippiin 1997, withHarvey Johnson Jr., andNew Orleansin 1978, withErnest Morial. Black politicians on the national level includedBarbara Jordan, elected as a Representative from Texas in Congress in 1973, and President Jimmy Carter appointed Andrew Young asUnited States Ambassador to the United Nations.Julian Bondwas elected to theGeorgia State Legislaturein 1965, although political reaction to his publicopposition to the U.S. involvement in the Vietnam Warprevented him from taking his seat until 1967.John Lewiswas first elected in 1986 to representGeorgia's 5th congressional districtin theUnited States House of Representatives, where he served from 1987 until his death in 2020. There were only twoBlack members of Congressfrom the states of the former Confederacyelected in 1980, and fourelected in 1990, but this rose to 16in 2000.
The first major blow against housing segregation in the era, theRumford Fair Housing Act, was passed inCaliforniain 1963. It was overturned by white California voters and real estate lobbyists the following year withProposition 14, a move which helped precipitate theWatts riots.[162][163]In 1966, theCalifornia Supreme Courtinvalidated Proposition 14 and reinstated the Rumford Fair Housing Act.[164]
Working and organizing forfair housinglaws became a major project of the movement over the next two years, with Martin Luther King Jr., James Bevel, andAl Rabyleading theChicago Freedom Movementaround the issue in 1966. In the following year, FatherJames Groppiand theNAACP Youth Councilalso attracted national attention with a fair housing campaign in Milwaukee.[165][166]Both movements faced violent resistance from white homeowners and legal opposition from conservative politicians.
The Fair Housing Bill was the most contentious civil rights legislation of the era. SenatorWalter Mondale, who advocated for the bill, noted that over successive years, it was the mostfilibusteredlegislation in U.S. history. It was opposed by most Northern and Southern senators, as well as theNational Association of Real Estate Boards. A proposed "Civil Rights Act of 1966" had collapsed completely because of its fair housing provision.[167]Mondale commented that:
A lot of civil rights [legislation] was about making the South behave and taking the teeth from George Wallace, [but] this came right to the neighborhoods across the country. This was civil rights getting personal.[168]
In the mid-1960s, the U.S. experienced a series of "long hot summers" ofcivil unrest. While the early civil rights movement primarily focused on legal challenges to segregation in the South, the "long hot summers" brought attention to the racial disparities and issues within urban communities in the North.[169]Systemic racism, police brutality, high unemployment rates, poor living conditions in urban Black neighborhoods, and a sense of hopelessness contributed to the widespread unrest.[170][171][172]
The momentum for the advancement of civil rights came to a sudden halt in August 1965 withriotsin theWatts districtofLos Angeles. The riots were ignited by the arrest of Marquette Frye during a traffic stop, which escalated into a physical confrontation with police officers and drew a large crowd of onlookers. During the six days of unrest, rioters engaged in widespreadlootingof stores, burning buildings througharson, and in some cases, usingsnipertactics to fire at authorities. To quell the violence,National Guardtroops were deployed to the area, imposing acurfew.[173][174]
After 34 people were killed and $35 million (equivalent to $349.22 million in 2024) in property was damaged, the public feared an expansion of the violence to other cities, and so the appetite for additional programs in President Lyndon Johnson's agenda was lost.[175][176]
Occurring well after the conclusion of the 1966 Chicago open housing movement, in what is known as the "Long hot summer of 1967" more than 150 riots erupted across the United States, with the most destructive occurring inDetroit, Michigan andNewark, New Jersey.[177]TheBoston Globecalled it "a revolution of black Americans against white Americans, a violent petition for the redress of long-standing grievances." TheGlobeasserted thatGreat Societylegislation had affected little fundamental improvement.[178]
TheNewark riotswere sparked by the arrest and beating of John William Smith, a Black cab driver, by police officers. The unrest lasted for five days, involving widespread looting, arson, and violent confrontations with police and National Guard troops. Some 26 people were killed, more than 700 were injured, and more than 1,000 residents were arrested.[178][179]
In Detroit, a largeblack middle classhad begun to develop among those African Americans who worked at unionized jobs in the automotive industry. These workers complained of persisting racist practices, limiting the jobs they could have and opportunities for promotion. TheUnited Auto Workerschanneled these complaints into bureaucratic and ineffective grievance procedures.[180]Violent white mobs enforced the segregation of housing up through the 1960s.[181]TheDetroit riotswere sparked by a police raid on an unlicensed after-hours bar, commonly called the "Blind Pig," in a predominantly Black neighborhood. The riots lasted for five days, causing significant property damage, 1,200 injuries, and at least 43 deaths (33 of those killed were Black residents of the city).[171]GovernorGeorge Romneysent in 7,400 National Guard troops to quell fire bombings, looting, and attacks on businesses and police. President Lyndon Johnson deployedU.S. Armytroops withtanksandmachine guns. Residents reported that police officers and National Guardsmen shot at black civilians and suspects indiscriminately.[181][182][183]
At an August 2, 1967 cabinet meeting, Attorney GeneralRamsey Clarkwarned that untrained and undisciplined local police forces and National Guardsmen might trigger a "guerrilla warin the streets," as evidenced by the climate ofsniperfire in Newark and Detroit.[184][185][186][187]Snipers were a significant element in many of the riots, creating a dangerous situation for both law enforcement and civilians, with shooters often targeting from rooftops and other concealed locations.[188][189]
The riots confounded many civil rights activists of both races due to the recent passage of major civil rights legislation. They also caused a backlash among Northern whites, many of whom stopped supporting civil rights causes.[190]President Johnson formed the National Advisory Commission on Civil Disorders, informally known as theKerner Commission, on July 28, 1967 to explore the causes behind the recurring outbreaks of urban civil disorder.[191][192]The commission's scope included the 164 disorders occurring in the first nine months of 1967. The president had directed them, in simple words, to document what happened, find out why it happened, and find out how to prevent it.[193]
The commission's 1968 report identified police practices, unemployment and underemployment, and lack of adequate housing as the most significant grievances motivating the rage.[194]It suggested legislative measures to promote racial integration and alleviate poverty and concluded that the nation was "moving toward two societies, one black, one white—separate and unequal."[195]The president, fixated on the Vietnam War and keenly aware of budgetary constraints, barely acknowledged the report.[196]
As 1968 began, the fair housing bill was beingfilibusteredonce again, but two developments revived it.[168]TheKerner Commissionreport on the1967 ghetto riotswas delivered to Congress on March 1, and it strongly recommended "a comprehensive and enforceable federal open housing law" as a remedy to the civil disturbances. The Senate was moved to end their filibuster that week.[197]
James Lawsoninvited King toMemphis, Tennessee, in March 1968 to support asanitation workers' strike. These workers launched a campaign forunionrepresentation after two workers were accidentally killed on the job; they were seeking fair wages and improved working conditions. King considered their struggle to be a vital part of thePoor People's Campaignhe was planning.
A day after delivering his stirring "I've Been to the Mountaintop" sermon, which has become famous for his vision of American society, King was assassinated on April 4, 1968, at theLorraine Motelin Memphis.Riots broke outin black neighborhoods in more than 110 cities across the United States in the days that followed, notablyin Chicago,Baltimore, andWashington, D.C.
The day beforeKing's funeral, April 8, a completely silent march withCoretta Scott King,SCLC, and UAW presidentWalter Reutherattracted approximately 42,000 participants.[198][199]Armed National Guardsmen lined the streets, sitting onM-48 tanks, to protect the marchers, and helicopters circled overhead. On April 9, Mrs. King led another 150,000 people in a funeral procession through the streets of Atlanta.[200]Her dignity revived courage and hope in many of the Movement's members, confirming her place as the new leader in the struggle for racial equality.
Martin Luther King Jr.gave his life for the poor of the world, the garbage workers of Memphis and the peasants of Vietnam. The day that Negro people and others in bondage are truly free, on the day want is abolished, on the day wars are no more, on that day I know my husband will rest in a long-deserved peace.
Ralph Abernathysucceeded King as the head of the SCLC and attempted to carry forth King's plan for a Poor People's March. It was to unite blacks and whites to campaign for fundamental changes in American society and economic structure. The march went forward under Abernathy's plainspoken leadership but did not achieve its goals.
TheU.S. House of Representativeshad been deliberating its Fair Housing Act in early April, before King's assassination and the aforementionedwave of unrestthat followed, the largest since the Civil War.[202]SenatorCharles Mathiaswrote:
[S]ome Senators and Representatives publicly stated they would not be intimidated or rushed into legislating because of the disturbances. Nevertheless, the news coverage of the riots and the underlying disparities in income, jobs, housing, and education, between White and Black Americans helped educate citizens and Congress about the stark reality of an enormous social problem. Members of Congress knew they had to act to redress these imbalances in American life to fulfill the dream that King had so eloquently preached.[197]
The House passed the legislation on April 10, less than a week after King was murdered, and President Johnson signed it the next day. TheCivil Rights Act of 1968prohibited discrimination concerning the sale, rental, and financing of housing based on race, religion, and national origin. It also made it a federal crime to "by force or by the threat of force, injure, intimidate, or interfere with anyone...by reason of their race, color, religion, or national origin."[203]
Conditions at theMississippi State PenitentiaryatParchman, then known as Parchman Farm, became part of the public discussion of civil rights afteractivistswere imprisoned there. In the spring of 1961, Freedom Riders came to the South to test thedesegregationof public facilities. By the end of June 1963, Freedom Riders had been convicted inJackson, Mississippi.[204]Many were jailed in Mississippi State Penitentiary at Parchman. Mississippi employed thetrusty system, a hierarchical order of inmates that used some inmates to control and enforce punishment of other inmates.[205]
In 1970 the civil rights lawyer Roy Haber began taking statements from inmates. He collected 50 pages of details of murders, rapes, beatings and other abuses suffered by the inmates from 1969 to 1971 at Mississippi State Penitentiary. In alandmark caseknown asGates v. Collier(1972), four inmates represented by Haber sued the superintendent of Parchman Farm for violating their rights under theUnited States Constitution.
Federal JudgeWilliam C. Keadyfound in favor of the inmates, writing that Parchman Farm violated the civil rights of the inmates by inflictingcruel and unusual punishment. He ordered an immediate end to all unconstitutional conditions and practices. Racial segregation of inmates was abolished, as was the trusty system, which allowed certain inmates to have power and control over others.[206]
The prison was renovated in 1972 after the scathing ruling by Keady, who wrote that the prison was an affront to "modern standards of decency." Among other reforms, the accommodations were made fit for human habitation. The system of trusties was abolished. (The prison had armedliferswith rifles and given them authority to oversee and guard other inmates, which led to many cases of abuse and murders.)[207]
In integrated correctional facilities in northern and western states, blacks represented a disproportionate number of prisoners, in excess of their proportion of the general population. They were often treated as second-class citizens by white correctional officers. Blacks also represented a disproportionately high number ofdeath rowinmates.Eldridge Cleaver's bookSoul on Icewas written from his experiences in the California correctional system; it contributed to black militancy.[208]
Civil rights protest activity had an observable impact on white American's views on race and politics over time.[209]White people who live in counties in which civil rights protests of historical significance occurred have been found to have lower levels of racial resentment against blacks, are more likely to identify with theDemocratic Partyas well as more likely to supportaffirmative action.[209]
One study found that non-violent activism of the era tended to produce favorable media coverage and changes in public opinion focusing on the issues organizers were raising, but violent protests tended to generate unfavorable media coverage that generated public desire to restore law and order.[210]
The 1964 Act was passed to enddiscriminationin various fields based on race, color, religion, sex, or national origin in the areas of employment and public accommodation.[211][212]The 1964 Act did not prohibit sex discrimination against persons employed at educational institutions. A parallel law, Title VI, had also been enacted in 1964 to prohibit discrimination in federally funded private and public entities. It covered race, color, and national origin but excluded sex. Feminists during the early 1970s lobbied Congress to add sex as a protected class category. In 1972,Title IXwas enacted to fill this gap and prohibit discrimination in all federally funded education programs. Title IX, or theEducation Amendments of 1972was later renamed thePatsy T. MinkEqual Opportunity in Education Actfollowing Mink's death in 2002.[213]
African-Americanwomen in the civil rights movement were pivotal to its success.[214]They volunteered as activists, advocates, educators, clerics, writers, spiritual guides, caretakers and politicians for the civil rights movement; leading and participating in organizations that contributed to the cause of civil rights.[214]Rosa Parks's refusal to sit at the back of apublic busresulted in the year-longMontgomery bus boycott,[214]and the eventualdesegregationof interstate travel in theUnited States.[215]Women were members of the NAACP because they believed it could help them contribute to the cause of civil rights.[214]Some of those involved with the Black Panthers were nationally recognized as leaders, and still others did editorial work on theBlack Panthernewspaper spurring internal discussions about gender issues.[216]Ella Bakerfounded theSNCCand was a prominent figure in the civil rights movement.[217][218]Female students involved with the SNCC helped to organize sit-ins and the Freedom Rides.[217]At the same time many elderly black women in towns across the Southern US cared for the organization's volunteers at their homes, providing the students food, a bed, healing aid and motherly love.[217]Other women involved also formed church groups, bridge clubs, and professional organizations, such as theNational Council of Negro Women, to help achieve freedom for themselves and their race.[216]Several who participated in these organizations lost their jobs because of their involvement.[216]
Many women who participated in the movement experiencedgender discriminationandsexual harassment.[219]In the SCLC,Ella Baker's input was discouraged in spite of her being the oldest and most experienced person on the staff.[220]There are many other accounts and examples.[221][222][223][224]
On December 17, 1951, theCommunist Party–affiliatedCivil Rights Congressdelivered the petitionWe Charge Genocide: The Crime of Government Against the Negro Peopleto the United Nations, arguing that the U.S. federal government, by its failure to act againstlynching in the United States, was guilty ofgenocideunder Article II of theUN Genocide Convention(seeBlack genocide).[225]The petition was presented to the United Nations at two separate venues:Paul Robeson, a concert singer and activist, presented it to a UN official in New York City, whileWilliam L. Patterson, executive director of the CRC, delivered copies of the drafted petition to a UN delegation in Paris.[226]
Patterson, the editor of the petition, was a leader of the Communist Party USA and head of theInternational Labor Defense, a group that offered legal representation to communists, trade unionists, and African Americans who were involved in cases that involved issues of political or racial persecution. The ILD was known for leading the defense of theScottsboro BoysinAlabamain 1931, where the Communist Party had a considerable amount of influence among African Americans in the 1930s. This influence had largely declined by the late 1950s, although it could command international attention. As earlier civil rights figures such as Robeson, Du Bois and Patterson became more politically radical (and therefore targets of Cold Waranti-Communismby the U.S. Government), they lost favor with mainstream Black America as well as with the NAACP.[226]
In order to secure a place in the political mainstream and gain the broadest base of support, the new generation of civil rights activists believed that it had to openly distance itself from anything and anyone associated with the Communist party. According toElla Baker, the Southern Christian Leadership Conference added the word "Christian" to its name in order to deter charges that it was associated withCommunism.[227]UnderJ. Edgar Hoover, the FBI had been concerned about communism since the early 20th century, and it kept civil rights activists under close surveillance and labeled some of them "Communist" or "subversive", a practice that continued during the civil rights movement. In the early 1960s, the practice of distancing the civil rights movement from "Reds" was challenged by theStudent Nonviolent Coordinating Committeewhich adopted a policy of accepting assistance and participation from anyone who supported the SNCC's political program and was willing to "put their body on the line, regardless of political affiliation." At times the SNCC's policy of political openness put it at odds with the NAACP.[226]
While most popular representations of the movement are centered on the leadership and philosophy of Martin Luther King Jr., some scholars note that the movement was too diverse to be credited to one person, organization, or strategy. SociologistDoug McAdamhas stated that, "in King's case, it would be inaccurate to say that he was the leader of the modern civil rights movement...but more importantly, there was no singular civil rights movement. The movement was, in fact, a coalition of thousands of local efforts nationwide, spanning several decades, hundreds of discrete groups, and all manner of strategies and tactics—legal, illegal, institutional, non-institutional, violent, non-violent. Without discounting King's importance, it would be sheer fiction to call him the leader of what was fundamentally an amorphous, fluid, dispersed movement."[228]Decentralized grassroots leadership has been a major focus of movement scholarship in recent decades through the work of historiansJohn Dittmer,Charles Payne,Barbara Ransby, and others.
The Jim Crow system employed "terror as a means of social control,"[229]with the most organized manifestations being theKu Klux Klanand their collaborators in local police departments. This violence played a key role in blocking the progress of the civil rights movement in the late 1950s. Some black organizations in the South began practicing armed self-defense. The first to do so openly was the Monroe, North Carolina, chapter of theNAACPled byRobert F. Williams. Williams had rebuilt the chapter after its membership was terrorized out of public life by the Klan. He did so by encouraging a new, more working-class membership to arm itself thoroughly and defend against attack.[230]When Klan nightriders attacked the home of NAACP member Albert Perry in October 1957, Williams' militia exchanged gunfire with the stunned Klansmen, who quickly retreated. The following day, the city council held an emergency session and passed an ordinance banning KKK motorcades.[231]One year later, Lumbee Indians in North Carolina would have a similarly successful armed stand-off with the Klan (known as theBattle of Hayes Pond) which resulted in KKK leader James W. "Catfish" Cole being convicted of incitement to riot.[232]
After the acquittal of several white men charged with sexually assaulting black women in Monroe, Williams announced to United Press International reporters that he would "meet violence with violence" as a policy. Williams' declaration was quoted on the front page ofThe New York Times, andThe Carolina Timesconsidered it "the biggest civil rights story of 1959".[233]NAACP National chairman Roy Wilkins immediately suspended Williams from his position, but the Monroe organizer won support from numerous NAACP chapters across the country. Ultimately, Wilkins resorted to bribing influential organizer Daisy Bates to campaign against Williams at the NAACP national convention and the suspension was upheld. The convention nonetheless passed a resolution which stated: "We do not deny, but reaffirm the right of individual and collective self-defense against unlawful assaults."[234]Martin Luther King Jr. argued for Williams' removal,[235]butElla Baker[236]andWEB Dubois[237]both publicly praised the Monroe leader's position.
Williams—along with his wife, Mabel Williams—continued to play a leadership role in the Monroe movement, and to some degree, in the national movement. The Williamses publishedThe Crusader, a nationally circulated newsletter, beginning in 1960, and the influential bookNegroes With Gunsin 1962. Williams did not call for full militarization in this period, but "flexibility in the freedom struggle."[238]Williams was well-versed in legal tactics and publicity, which he had used successfully in the internationally known "Kissing Case" of 1958, as well as nonviolent methods, which he used atlunch countersit-ins in Monroe—all with armed self-defense as a complementary tactic.
Williams led the Monroe movement in another armed stand-off with white supremacists during an August 1961 Freedom Ride; he had been invited to participate in the campaign byElla BakerandJames Formanof the Student Nonviolent Coordinating Committee (SNCC). The incident (along with his campaigns for peace with Cuba) resulted in him being targeted by the FBI and prosecuted for kidnapping; he was cleared of all charges in 1976.[239]Meanwhile, armed self-defense continued discreetly in the Southern movement with such figures as SNCC'sAmzie Moore,[239]Hartman Turnbow,[240]andFannie Lou Hamer[241]all willing to use arms to defend their lives from nightrides. Taking refuge from the FBI in Cuba, the Willamses broadcast the radio showRadio Free Dixiethroughout the eastern United States via Radio Progresso beginning in 1962. In this period, Williams advocated guerilla warfare against racist institutions and saw the large ghetto riots of the era as a manifestation of his strategy.
University of North Carolinahistorian Walter Rucker has written that "the emergence of Robert F Williams contributed to the marked decline in anti-black racial violence in the U.S....After centuries of anti-black violence, African Americans across the country began to defend their communities aggressively—employing overt force when necessary. This in turn evoked in whites real fear of black vengeance..." This opened up space for African Americans to use nonviolent demonstrations with less fear of deadly reprisal.[242]Of the many civil rights activists who share this view, the most prominent was Rosa Parks. Parks gave the eulogy at Williams' funeral in 1996, praising him for "his courage and for his commitment to freedom," and concluding that "The sacrifices he made, and what he did, should go down in history and never be forgotten."[243]
Jewish Americans played an active role supporting the Civil Rights Movement and were actively involved in establishing and supporting a number of the most important civil rights organizations, including the NAACP, the Leadership Conference on Civil and Human Rights, the Southern Christian Leadership Conference (SCLC), and the Student Nonviolent Coordinating Committee (SNCC). These organizations played pivotal roles in the civil rights movement, advocating for racial equality and justice.[244]
Despite representing less than 2% of the US population, Jews made up roughly half of all civil rights lawyers in the South during the 1960s and half of the white northern volunteers involved in the 1964 Mississippi Freedom Summer project.[245]
Organizers from theCongress of Racial Equalityand theStudent Nonviolent Coordinating Committeeconfronted police violence with sit-ins at precinct stations and pickets outside department headquarters, and by blocking traffic to bring attention to officer misdeeds. In return, activists found themselves the targets of political repression in the form of pervasive police surveillance, infiltration by undercover officers, and retaliatory prosecutions aimed at discrediting their movement. Many civil rights leaders—includingMartin Luther King Jr.,Ella Baker,James Forman,Fannie Lou HamerandJohn Lewis—criticized police brutality in writing and speeches at various points.[246]
Partly in response to theMarch on Washington Movementunder Truman's predecessor, Franklin D. Roosevelt, theFair Employment Practices Committeewas created to address racial discrimination in employment,[247]and in 1946, Truman created thePresident's Committee on Civil Rights. On June 29, 1947, Truman became the first president to address the demands of theNational Association for the Advancement of Colored People(NAACP). The speech took place at theLincoln Memorialduring the NAACP convention and was carried nationally on radio. In that speech, Truman laid out his agreement on the need to end discrimination, which would be advanced by the first comprehensive, presidentially proposed civil rights legislation. Truman on "civil rights and human freedom" declared:[248]
… Our immediate task is to remove the last remnants of the barriers which stand between millions of our citizens and their birthright. There is no justifiable reason for discrimination because of ancestry, or religion, or race, or color. We must not tolerate such limitations on the freedom of any of our people and on their enjoyment of basic rights which every citizen in a truly democratic society must possess.
In February 1948, Truman delivered a formal message to Congress requesting adoption of his 10-point program to secure civil rights, including anti-lynching, voter rights, and elimination of segregation. "No political act since theCompromise of 1877," argued biographerTaylor Branch, "so profoundly influenced race relations; in a sense it was a repeal of 1877."[249]Truman was opposed by theconservative coalitionin congress, so instead issued Executive Orders 9980 and 9981 ending discrimination in federal employment and in the armed forces.[249]
While not a key focus of his administration, President Eisenhower made several conservative strides toward making America a racially integrated country. The year he was elected, Eisenhower desegregated Washington D.C. after hearing a story about an African American man who was unable to rent a hotel room, buy a meal, access drinking water, and attend a movie.[250]Shortly after this act, Eisenhower utilized Hollywood personalities to pressure movie theatres into desegregating as well.[251]
Under the previous administration, President Truman signedExecutive Order 9981 to desegregate the military. However, Truman's executive order had hardly been enforced. President Eisenhower made it a point to enforce the executive order. By October 30, 1954, there were no segregated combat units in the United States.[250]Not only this, but Eisenhower also desegregated the Veterans Administration and military bases in the South, including federal schools for military dependents. Expanding his work beyond the military, Eisenhower formed two non-discrimination committees, one to broker nondiscrimination agreements with government contractors, and a second to end discrimination within government departments and agencies.[250]
The first major piece of civil rights legislation since the Civil Rights Act of 1875 was also passed under the Eisenhower administration. President Eisenhower proposed, championed, and signed theCivil Rights Act of 1957. The legislation established the Civil Rights Commission and the Justice Department's Civil Rights Division and banned intimidating, coercing, and other means of interfering with a citizen's right to vote. Eisenhower's work in desegregating the judicial system is also notable. The judges he appointed were liberal when it came to the subject of civil rights / desegregation, and he actively avoided placing segregationists in federal courts.[250]
For the first two years of the Kennedy administration, civil rights activists had mixed opinions of both the president and his younger brother,Robert F. Kennedy, theAttorney General. HistorianDavid Halberstamwrote that the race question was for a long time a minor ethnic political issue inMassachusettswhere the Kennedy brothers came from, and had they been from another part of the country, "they might have been more immediately sensitive to the complexities and depth of black feelings."[253]A well of historical skepticism toward liberal politics had left African Americans with a sense of uneasy disdain for any white politician who claimed to share their concerns for freedom, particularly ones connected to the historically pro-segregationist Democratic Party. Still, many were encouraged by the discreet support Kennedy gave to King, and the administration's willingness, after dramatic pressure from civil disobedience, to bring forth racially egalitarian initiatives.
Many of the initiatives resulted from Robert Kennedy's passion. The younger Kennedy gained a rapid education in the realities of racism through events such as theBaldwin–Kennedy meeting. The president came to share his brother's sense of urgency on the matter, resulting in the landmarkCivil Rights Addressof June 1963 and the introduction of the first major civil rights act of the decade.[254][255]
That same month, during theFreedom Rides, Robert Kennedy became concerned with the issue when photographs of the burning bus and savage beatings inAnnistonand Birmingham were broadcast around the world. They came at an especially embarrassing time, as President Kennedy was about to have asummit with the Soviet premierin Vienna. The White House was concerned with its image among the populations of newly independent nations in Africa and Asia, and Robert Kennedy responded with an address forVoice of Americastating that great progress had been made on the issue of race relations. Meanwhile, behind the scenes, the administration worked to resolve the crisis with a minimum of violence and prevent the Freedom Riders from generating a fresh crop of headlines that might divert attention from the President's international agenda. TheFreedom Ridersdocumentary notes that, "The back burner issue of civil rights had collided with the urgent demands of Cold Warrealpolitik."[256]
On May 21, when a white mob attacked and burned the First Baptist Church in Montgomery, Alabama, where King was holding out with protesters, Robert Kennedy telephoned King to ask him to stay in the building until the U.S. Marshals and National Guard could secure the area. King proceeded to berate Kennedy for "allowing the situation to continue". King later publicly thanked Kennedy for deploying the force to break up an attack that might otherwise have ended King's life.
With a very small majority in Congress, the president's ability to press ahead with legislation relied considerably on a balancing game with the Senators and Congressmen of the South. Without the support of Vice-president Johnson, a former Senator who had years of experience in Congress and longstanding relations there, many of the Attorney-General's programs would not have progressed.
By late 1962, frustration at the slow pace of political change was balanced by the movement's strong support for legislative initiatives, including administrative representation across all U.S. Government departments and greater access to the ballot box. From squaring off against GovernorGeorge Wallace, to "tearing into" Vice-president Johnson (for failing to desegregate areas of the administration), to threatening corrupt white Southern judges with disbarment, to desegregating interstate transport, Robert Kennedy came to be consumed by the civil rights movement. He continued to work on these social justice issues in his bid for the presidency in 1968.
On the night of Governor Wallace's capitulation to African-American enrollment at theUniversity of Alabama, President Kennedy gave anaddressto the nation, which marked the changing tide, an address that was to become a landmark for the ensuing change in political policy as to civil rights. In 1966, Robert Kennedy visited South Africa and voiced his objections toapartheid, the first time a major US politician had done so:
At theUniversity of Natalin Durban, I was told the church to which most of the white population belongs teaches apartheid as a moral necessity. A questioner declared that few churches allow black Africans to pray with the white because the Bible says that is the way it should be, because God created Negroes to serve. "But suppose God is black", I replied. "What if we go to Heaven and we, all our lives, have treated the Negro as an inferior, and God is there, and we look up and He is not white? What then is our response?" There was no answer. Only silence.
Robert Kennedy's relationship with the movement was not always positive. As attorney general, he was called to account by activists—who booed him at a June 1963 speech—for the Justice Department's own poor record of hiring blacks.[252]He also presided overFBI DirectorJ. Edgar Hooverand hisCOINTELPROprogram. This program ordered FBI agents to "expose, disrupt, misdirect, discredit, or otherwise neutralize" the activities of Communist front groups, a category in which the paranoid Hoover included most civil rights organizations.[258][259]Kennedy personally authorized some of the programs.[260]According toTim Weiner, "RFK knew much more about this surveillance than he ever admitted." Although Kennedy only gave approval for limited wiretapping of King's phones "on a trial basis, for a month or so." Hoover extended the clearance so his men were "unshackled" to look for evidence in any areas of the black leader's life they deemed important; they then used this information to harass King.[261]Kennedy directly ordered surveillance onJames Baldwinafter their antagonistic racial summit in 1963.[262][263]
Lyndon Johnson made civil rights one of his highest priorities, coupling it with a "war on poverty." However, the increasing opposition to the Vietnam War, coupled with the cost of the war, undercut support for his domestic programs.[264]
Under Kennedy, major civil rights legislation had been stalled in Congress. His assassination changed everything. On one hand, President Lyndon Johnson was a much more skillful negotiator than Kennedy, but he had behind him a powerful national momentum demanding immediate action on moral and emotional grounds. Demands for immediate action originated from unexpected directions, especially white Protestant church groups. The Justice Department, led by Robert Kennedy, moved from a posture of defending Kennedy from the quagmire minefield of racial politics to acting to fulfill his legacy. The violent death and public reaction dramatically moved the conservative Republicans, led by SenatorEverett McKinley Dirksen, whose support was the margin of victory for theCivil Rights Act of 1964. The act immediately ended de jure (legal) segregation and the era of Jim Crow.[265]
With the civil rights movement at full blast, Lyndon Johnson coupled black entrepreneurship with his war on poverty, setting up special programs in the Small Business Administration, the Office of Economic Opportunity, and other agencies.[266]This time there was money for loans designed to boost minority business ownership. Richard Nixon greatly expanded the program, setting up the Office of Minority Business Enterprise (OMBE) in the expectation that black entrepreneurs would help defuse racial tensions and possibly support his reelection.[267]
In China,Mao Zedongin August 1963 expressed support for the U.S. civil rights movement, stating that the "fascist atrocities" committed against black people in the U.S. demonstrated the link between reactionary domestic U.S. policies and its policies of aggression abroad.[268]: 34In 1968, a mass rally in China condemned the assassination of Martin Luther King Jr.[269]: 91Mao stated that racial discrimination in the U.S. resulted from its colonial system and that the struggle of Black people in the U.S. was ananti-imperialiststruggle.[268]: 34TheChinese Communist Partyechoed this view of the civil rights movement.[269]: 91During theCultural Revolution,People's Dailyrepeated cited the example that King advocated nonviolence, but was violently killed, as an example of its view that violent struggle was necessary for the oppressed masses of the world to free themselves.[270]
Maoisminfluenced some components of the Black liberation movement, including the Black Panther Party and black self-defense advocateRobert F. Williams.[268]: 34
In March 1964,Malcolm X(el-Hajj Malik el-Shabazz), national representative of theNation of Islam, formally broke with that organization, and made a public offer to collaborate with any civil rights organization that accepted the right to self-defense and the philosophy of Black nationalism (which Malcolm said no longer requiredBlack separatism).Gloria Richardson, head of theCambridge, Maryland, chapter ofSNCC, and leader of the Cambridge rebellion,[271]an honored guest at The March on Washington, immediately embraced Malcolm's offer. Mrs. Richardson, "the nation's most prominent woman [civil rights] leader,"[272]toldThe Baltimore Afro-Americanthat "Malcolm is being very practical...The federal government has moved into conflict situations only when matters approach the level of insurrection. Self-defense may force Washington to intervene sooner."[272]Earlier, in May 1963, writer and activistJames Baldwinhad stated publicly that "the Black Muslim movement is the only one in the country we can callgrassroots, I hate to say it...Malcolm articulates for Negroes, their suffering...he corroborates their reality..."[273]On the local level, Malcolm and the NOI had been allied with the Harlem chapter of the Congress of Racial Equality (CORE) since at least 1962.[274]
On March 26, 1964, as the Civil Rights Act was facing stiff opposition in Congress, Malcolm had a public meeting with Martin Luther King Jr. at the Capitol. Malcolm had tried to begin a dialog with King as early as 1957, but King had rebuffed him. Malcolm had responded by calling King an "Uncle Tom", saying he had turned his back on black militancy in order to appease the white power structure. But the two men were on good terms at their face-to-face meeting.[275]There is evidence that King was preparing to support Malcolm's plan to formally bring the U.S. government before the United Nations on charges of human rights violations against African Americans.[276]Malcolm now encouraged Black nationalists to get involved in voter registration drives and other forms of community organizing to redefine and expand the movement.[277]
Civil rights activists became increasingly combative in the 1963 to 1964 period, seeking to defy such events as the thwarting of the Albany campaign, police repression andKu Klux Klan terrorisminBirmingham, and the assassination ofMedgar Evers. The latter's brother Charles Evers, who took over as Mississippi NAACP Field Director, told a public NAACP conference on February 15, 1964, that "non-violence won't work in Mississippi...we made up our minds...that if a white man shoots at a Negro in Mississippi, we will shoot back."[278]The repression of sit-ins inJacksonville, Florida, provoked a riot in which black youth threwMolotov cocktailsat police on March 24, 1964.[279]Malcolm X gave numerous speeches in this period warning that such militant activity would escalate further if African Americans' rights were not fully recognized. In his landmark April 1964 speech "The Ballot or the Bullet", Malcolm presented an ultimatum to white America: "There's new strategy coming in. It'll be Molotov cocktails this month, hand grenades next month, and something else next month. It'll be ballots, or it'll be bullets."[280]
As noted in the PBS documentaryEyes on the Prize, "Malcolm X had a far-reaching effect on the civil rights movement. In the South, there had been a long tradition of self-reliance. Malcolm X's ideas now touched that tradition".[281]Self-reliance was becoming paramount in light of the1964 Democratic National Convention's decision to refuse seating to theMississippi Freedom Democratic Party(MFDP) and instead to seat the regular state delegation, which had been elected in violation of the party's own rules, and byJim Crow lawinstead.[282]SNCC moved in an increasingly militant direction and worked with Malcolm X on two Harlem MFDP fundraisers in December 1964.
WhenFannie Lou Hamerspoke to Harlemites about the Jim Crow violence that she'd suffered in Mississippi, she linked it directly to the Northern police brutality against blacks that Malcolm protested against;[283]When Malcolm asserted that African Americans should emulate theMau Mau armyofKenyain efforts to gain their independence, many in SNCC applauded.[284]
During theSelma campaignfor voting rights in 1965, Malcolm made it known that he'd heard reports of increased threats of lynching around Selma. In late January he sent an open telegram toGeorge Lincoln Rockwell, the head of theAmerican Nazi Party, stating:
"if your present racist agitation against our people there in Alabama causes physical harm to Reverend King or any other black Americans...you and your KKK friends will be met with maximum physical retaliation from those of us who are not handcuffed by the disarming philosophy of nonviolence."[285]
The following month, the Selma chapter of SNCC invited Malcolm to speak to a mass meeting there. On the day of Malcolm's appearance, President Johnson made his first public statement in support of the Selma campaign.[286]Paul Ryan Haygood, a co-director of theNAACP Legal Defense Fund, credits Malcolm with a role in gaining support by the federal government. Haygood noted that "shortly after Malcolm's visit to Selma, a federal judge, responding to a suit brought by theDepartment of Justice, requiredDallas County, Alabama, registrars to process at least 100 Black applications each day their offices were open."[287]
Many in theJewishcommunity supported the civil rights movement. In fact, statistically, Jews were one of the most actively involved non-black groups in the Movement. Many Jewish students worked in concert with African Americans for CORE, SCLC, and SNCC as full-time organizers and summer volunteers during the Civil Rights era. Jews made up roughly half of the white northern and western volunteers involved in the 1964 MississippiFreedom Summerproject and approximately half of the civil rights attorneys active in the South during the 1960s.[288]
Jewish leaders were arrested while heeding a call from Martin Luther King Jr. inSt. Augustine, Florida, in June 1964, where the largest mass arrest of rabbis in American history took place at the Monson Motor Lodge.Abraham Joshua Heschel, a writer, rabbi, and professor of theology at theJewish Theological Seminary of Americain New York, was outspoken on the subject of civil rights. He marched arm-in-arm with King in the 1965Selma to Montgomery march. In the 1964murders of Chaney, Goodman, and Schwerner, the two white activists killed,Andrew GoodmanandMichael Schwerner, were both Jewish.
Brandeis University, the only nonsectarian Jewish-sponsored college university in the world, created the Transitional Year Program (TYP) in 1968, in part response to theassassination of Martin Luther King Jr.The faculty created it to renew the university's commitment to social justice. Recognizing Brandeis as a university with a commitment to academic excellence, these faculty members created a chance for disadvantaged students to participate in an empowering educational experience.
TheAmerican Jewish Committee,American Jewish Congress, andAnti-Defamation League(ADL) actively promoted civil rights. While Jews were very active in the civil rights movement in the South, in the North, many had experienced a more strained relationship with African Americans.  It has been argued that with Black militancy and theBlack Powermovements on the rise, "Black Anti-Semitism" increased leading to strained relations between Blacks and Jews in Northern communities. In New York City, most notably, there was a major socio-economic class difference in the perception of African Americans by Jews.[289]Jews from better educated Upper-Middle-Class backgrounds were often very supportive of African American civil rights activities while the Jews in poorer urban communities that became increasingly minority were often less supportive largely in part due to more negative and violent interactions between the two groups.
According to political scientistMichael Rogin, Jewish-Black hostility was a two-way street extending to earlier decades. In the post-World War II era, Jews were grantedwhite privilegeand most moved into the middle-class while Blacks were left behind in the ghetto.[290]Urban Jews engaged in the same sort of conflicts with Blacks—overintegration busing, local control of schools, housing, crime, communal identity, and class divides—that otherwhite ethnicsdid, leading to Jews participating inwhite flight. The culmination of this was the1968 New York City teachers' strike, pitting largely Jewish schoolteachers against predominantly Black parents inBrownsville, New York.[291]
Many Jews in the Southern states who supported civil rights for African Americans tended to keep a low profile on "the race issue", in order to avoid attracting the attention of the anti-Black and antisemitic Ku Klux Klan.[292]However, Klan groups exploited the issue of African-American integration and Jewish involvement in the struggle in order to commit violently antisemitichate crimes. As an example of this hatred, in one year alone, from November 1957 to October 1958, temples and other Jewish communal gatherings were bombed and desecrated inAtlanta,Nashville,Jacksonville, andMiami, anddynamitewas found undersynagoguesinBirmingham,Charlotte, andGastonia, North Carolina. Somerabbisreceiveddeath threats, but there were no injuries following these outbursts ofviolence.[292]
Despite the common notion that the ideas ofMartin Luther King Jr.,Malcolm XandBlack Poweronly conflicted with each other and were the only ideologies of the civil rights movement, there were other sentiments felt by many blacks. Fearing the events during the movement was occurring too quickly, there were some blacks who felt that leaders should take their activism at an incremental pace. Others had reservations on how focused blacks were on the movement and felt that such attention was better spent on reforming issues within the black community.
While Conservatives, in general, supported integration, some defended incrementally phased out segregation as a backstop against assimilation. Based on her interpretation of a 1966 study made by Donald Matthews and James Prothro detailing the relative percentage of blacks for integration, against it or feeling something else, Lauren Winner asserts that:
Black defenders of segregation look, at first blush, very much like black nationalists, especially in their preference for all-black institutions; but black defenders of segregation differ from nationalists in two key ways. First, while both groups criticizeNAACP-style integration, nationalists articulate a third alternative to integration andJim Crow, while segregationists preferred to stick with the status quo. Second, absent from black defenders of segregation's political vocabulary was the demand forself-determination. They called for all-black institutions, but not autonomous all-black institutions; indeed, some defenders of segregation asserted that black people needed white paternalism and oversight in order to thrive.[293]
Oftentimes, African-American community leaders would be staunch defenders of segregation. Church ministers, businessmen, and educators were among those who wished to keep segregation and segregationist ideals in order to retain the privileges they gained from patronage from whites, such as monetary gains. In addition, they relied on segregation to keep their jobs and economies in their communities thriving. It was feared that if integration became widespread in the South, black-owned businesses and other establishments would lose a large chunk of their customer base to white-owned businesses, and many blacks would lose opportunities for jobs that were presently exclusive to their interests.[294]On the other hand, there were the everyday, average black people who criticized integration as well. For them, they took issue with different parts of the civil rights movement and the potential for blacks to exercise consumerism and economic liberty without hindrance from whites.[295]
For Martin Luther King Jr., Malcolm X and other leading activists and groups during the movement, these opposing viewpoints acted as an obstacle against their ideas. These different views made such leaders' work much harder to accomplish, but they were nonetheless important in the overall scope of the movement. For the most part, the black individuals who had reservations on various aspects of the movement and ideologies of the activists were not able to make a game-changing dent in their efforts, but the existence of these alternate ideas gave some blacks an outlet to express their concerns about the changing social structure.
During the Freedom Summer campaign of 1964, numerous tensions within the civil rights movement came to the forefront. Many blacks inSNCCdeveloped concerns that white activists from the North and West were taking over the movement. The participation by numerous white students was not reducing the amount of violence that SNCC suffered, but seemed to exacerbate it. Additionally, there was profound disillusionment at Lyndon Johnson's denial of voting status for the Mississippi Freedom Democratic Party at the Democratic National Convention.[296][297]Meanwhile, duringCORE's work in Louisiana that summer, that group found the federal government would not respond to requests to enforce the provisions of the Civil Rights Act of 1964, or to protect the lives of activists who challenged segregation. The Louisiana campaign survived by relying on a local African-American militia called theDeacons for Defense and Justice, who used arms to repel white supremacist violence and police repression. CORE's collaboration with the Deacons was effective in disrupting Jim Crow in numerous Louisiana areas.[298][299]
In 1965, SNCC helped organize an independent political party, theLowndes County Freedom Organization(LCFO), in the heart of the Alabama Black Belt, also Klan territory. It permitted its black leaders to openly promote the use of armed self-defense. Meanwhile, the Deacons for Defense and Justice expanded into Mississippi and assistedCharles Evers' NAACP chapter with a successful campaign inNatchez. Charles had taken the lead after his brotherMedgar Everswas assassinated in 1963.[300]The same year, the 1965Watts Rebelliontook place in Los Angeles. Many black youths were committed to the use of violence to protest inequality and oppression.[301]
During theMarch Against Fearin 1966, initiated byJames Meredith, SNCC and CORE fully embraced the slogan of "black power" to describe these trends towards militancy and self-reliance. In Mississippi, Stokely Carmichael declared, "I'm not going to beg the white man for anything that I deserve, I'm going to take it. We need power."[302]
Some people engaging in the Black Power movement claimed a growing sense of black pride and identity. In gaining more of a sense of a cultural identity, blacks demanded that whites no longer refer to them as "Negroes" but as "Afro-Americans," similar to other ethnic groups, such as Irish Americans and Italian Americans. Until the mid-1960s, blacks had dressed similarly to whites and oftenstraightened their hair. As a part of affirming their identity, blacks started to wear African-baseddashikisand grow their hair out as a naturalafro. The afro, sometimes nicknamed the "'fro," remained a popular black hairstyle until the late 1970s. Other variations of traditional African styles have become popular, often featuring braids, extensions, and dreadlocks.
TheBlack Panther Party(BPP), which was founded byHuey NewtonandBobby SealeinOakland, California, in 1966, gained the most attention for Black Power nationally. The group began following the revolutionary pan-Africanism of late-periodMalcolm X, using a "by-any-means necessary" approach to stopping racial inequality. They sought to rid African-American neighborhoods ofpolice brutalityand to establishsocialistcommunity controlin the ghettos. While they conducted armed confrontation with police, they also set up free breakfast and healthcare programs for children.[303]Between 1968 and 1971, the BPP was one of the most important black organizations in the country and had support from the NAACP, SCLC,Peace and Freedom Party, and others.[304]
Black Power was taken to another level inside prison walls. In 1966,George Jacksonformed theBlack Guerrilla Familyin the CaliforniaSan Quentin State Prison. The goal of this group was to overthrow the white-run government in America and the prison system. In 1970, this group displayed their dedication after a white prison guard was found not guilty of shooting and killing three black prisoners from the prison tower. They retaliated by killing a white prison guard.
Numerous popular cultural expressions associated with black power appeared at this time. Released in August 1968, the number oneRhythm & Blues singlefor theBillboardYear-Endlist wasJames Brown's "Say It Loud – I'm Black and I'm Proud".[305]In October 1968,Tommie SmithandJohn Carlos, while being awarded the gold and bronze medals, respectively, at the1968 Summer Olympics, donned human rights badges and each raised a black-gloved Black Power salute during their podium ceremony.
King was not comfortable with the "Black Power" slogan, which sounded too much likeblack nationalismto him. When King was assassinated in 1968, Stokely Carmichael said that whites had murdered the one person who would prevent rampant rioting and that blacks would burn every major city to the ground. Riots broke out in more than 100 cities across the country. Some cities did not recover from the damage for more than a generation; other city neighborhoods never recovered.
King and the civil rights movement inspired theNative American rights movementof the 1960s and many of its leaders.[306]Native Americans had beendehumanizedas "merciless Indian savages" in theUnited States Declaration of Independence,[307]and in King's 1964 bookWhy We Can't Waithe wrote: "Our nation was born in genocide when it embraced the doctrine that the original American, the Indian, was an inferior race."[308]John Echohawk, a member of thePawnee tribeand the executive director and one of the founders of theNative American Rights Fund, stated: "Inspired by Dr. King, who was advancing the civil rights agenda of equality under the laws of this country, we thought that we could also use the laws to advance our Indianship, to live as tribes in our territories governed by our own laws under the principles of tribal sovereignty that had been with us ever since 1831. We believed that we could fight for a policy of self-determination that was consistent with U.S. law and that we could govern our own affairs, define our own ways and continue to survive in this society".[309]Native Americans were also active supporters of King's movement throughout the 1960s, which included a sizable Native American contingent at the 1963 March on Washington for Jobs and Freedom.[306]
Due to policies ofsegregationand disenfranchisement present in Northern Ireland many Irish activists took inspiration from American civil rights activists.People's Democracyhad organized a"Long March" from Belfast to Derrywhich was inspired by theSelma to Montgomery marches.[310]During the civil rights movement in Northern Ireland protesters often sang the American protest songWe Shall Overcomeand sometimes referred to themselves as the "negroes of Northern Ireland".[311]
There was an international context for the actions of the U.S. federal government during these years. The Soviet media frequently coveredracial discrimination in the U.S.[312]Deeming American criticism ofits own human rights abuseshypocritical, the Soviet government would respond by stating "And you are lynching Negroes".[313]In his 1934 bookRussia Today: What Can We Learn from It?,Sherwood Eddywrote: "In the most remote villages of Russia today Americans are frequently asked what they are going to do to theScottsboro Negro boysand why they lynch Negroes."[314]
InCold War Civil Rights: Race and the Image of American Democracy, the historianMary L. Dudziakwrote that Communists who were critical of the United States accused it of practicing hypocrisy when it portrayed itself as the "leader of the free world," while so many of its citizens were being subjected to severe racial discrimination and violence; she argued that this was a major factor in moving the government to support civil rights legislation.[315]
A majority ofWhite Southernershave been estimated to have neither supported nor resisted the civil rights movement.[316]Many did not enjoy the idea of expanding civil rights but were uncomfortable with the language and often violent tactics used by those who resisted the civil rights movement as part of theMassive resistance.[317]Many only reacted to the movement once forced to by their changing environment, and when they did their response was usually whatever they felt would disturb their daily life the least. Most of their personal reactions, whether eventually in support or resistance were not in extreme.[316]
King reached the height of popular acclaim during his life in 1964, when he was awarded theNobel Peace Prize. After that point, his career was filled with frustrating challenges. Theliberalcoalition that had gained passage of theCivil Rights Act of 1964and theVoting Rights Act of 1965began to fray.
King was becoming more estranged from the Johnson administration. In 1965 he broke with it by calling for peace negotiations and a halt to thebombing of Vietnam. He moved furtherleftin the following years, speaking about the need for economic justice and thoroughgoing changes in American society. He believed that change was needed beyond the civil rights which had been gained by the movement.
However, King's attempts to broaden the scope of the civil rights movement were halting and largely unsuccessful. In 1965 King made several attempts to take the Movement north in order to addresshousing discrimination. The SCLC's campaign in Chicago publicly failed, because Chicago's MayorRichard J. Daleymarginalized the SCLC's campaign by promising to "study" the city's problems. In 1966, white demonstrators in notoriously racistCicero, a suburb of Chicago, held "white power" signs and threw stones at marchers who were demonstrating againsthousing segregation.[318]
Politicians and journalists quickly blamed this whitebacklashon the movement's shift towards Black Power in the mid-1960s; today most scholars believe the backlash was a phenomenon that was already developing in the mid-1950s, and it was embodied in the "massive resistance" movement in the South where even the few moderate white leaders (includingGeorge Wallace, who had once been endorsed by the NAACP) shifted to openly racist positions.[319][320]Northern and Western racists opposed the southerners on a regional and cultural basis, but also held segregationist attitudes which became more pronounced as the civil rights movement headed north and west. For instance, prior to the Watts riot, California whites had already mobilized torepeal the state's 1963 fair housing law.[318]
Even so, the backlash which occurred at the time was not able to roll back the major civil rights victories which had been achieved or swing the country into reaction. Social historians Matthew Lassiter andBarbara Ehrenreichnote that the backlash's primary constituency wassuburbanand middle-class, not working-class whites: "among the white electorate, one half of blue-collar voters…cast their ballot for [the liberal presidential candidate]Hubert Humphreyin 1968…only in the South didGeorge Wallacedraw substantially more blue-collar than white-collar support."[321]
The 1954 to 1968 civil rights movement contributed strong cultural threads to American and international theater, song, film, television, and art.
TheSpace Race(Russian:космическая гонка,romanized:kosmicheskaya gonka,IPA:[kɐsˈmʲitɕɪskəjəˈɡonkə]) was a 20th-century competition between theCold Warrivals, theUnited Statesand theSoviet Union, to achieve superiorspaceflightcapability. It had its origins in theballistic missile-basednuclear arms racebetween the two nations followingWorld War IIand the onset of thecold war. The technological advantage demonstrated by spaceflight achievement was seen as necessary for national security, particularly in regard tointercontinental ballistic missileandsatellite reconnaissancecapability, but also became part of the cultural symbolism and ideology of the time. The Space Race brought pioneering launches ofartificial satellites, robotic landers to theMoon,Venus, andMars, andhuman spaceflightinlow Earth orbitand ultimately to the Moon.[1][2][3][4]
Public interest in space travel originated in the 1951 publication of a Soviet youth magazine and was promptly picked up by US magazines.[5]The competition began on July 30, 1955, when the United States announced its intent to launch artificialsatellitesfor theInternational Geophysical Year. Four days later, the Soviet Union responded by declaring they would also launch a satellite "in the near future". The launching of satellites was enabled by developments in ballistic missile capabilities since the end of World War II.[6]The competition gained Western public attention with the "Sputnik crisis", when the USSR achieved the first successful satellite launch,Sputnik 1, on October 4, 1957. It gained momentum when the USSR sent the first human,Yuri Gagarin, into space with the orbital flight ofVostok 1on April 12, 1961. These were followed by a string of other firsts achieved by the Soviets over the next few years.[7][8][9]
Gagarin's flight led US presidentJohn F. Kennedyto raise the stakes on May 25, 1961, by asking the US Congress to commit to the goal of "landing a man on the Moon and returning him safely to the Earth" before the end of the decade.[10][2]Both countries began developingsuper heavy-lift launch vehicles, with the US successfully deploying theSaturn V, which was large enough to send a three-person orbiter and two-person lander to the Moon. Kennedy's Moon landing goal was achieved in July 1969, with the flight ofApollo 11.[11][12][13]The USSR continued to pursuecrewed lunar programsto launch and land on the Moon before the US with itsN1 rocketbut did not succeed, and eventually canceled it to concentrate onSalyut, the firstspace stationprogram, and the first landingson Venusandon Mars. Meanwhile, the US landed five more Apollo crews on the Moon,[14]and continuedexploration of other extraterrestrial bodiesrobotically.
A period ofdétentefollowed with the April 1972 agreement on a cooperativeApollo–Soyuz Test Project(ASTP), resulting in the July 1975 rendezvous in Earth orbit of a US astronaut crew with a Soviet cosmonaut crew and joint development of an international docking standardAPAS-75. Being considered as the final act of the Space Race by many observers,[15]the competition was however only gradually replaced with cooperation.[16]Thecollapse of the Soviet Unioneventually allowed the US and the newly reconstitutedRussian Federationto end their Cold War competition also in space, by agreeing in 1993 on theShuttle–MirandInternational Space Station programs.[17][18]
Although Germans,Americansand Soviets experimented with small liquid-fuel rockets before World War II, launching satellites and humans into space required the development of largerballistic missilessuch asWernher von Braun'sAggregat-4 (A-4), which became known as theVergeltungswaffe 2(V-2) developed byNazi Germanyto bomb the Allies in the war.[19]After the war, both the US and USSR acquired custody of German rocket development assets which they used to leverage the development of their own missiles.
Public interest in space flight was first aroused in October 1951 when the Soviet rocketry engineerMikhail Tikhonravovpublished "Flight to the Moon" in the newspaperPionerskaya pravdafor young readers. He described a two-person interplanetary spaceship of the future and the industrial and technological processes required to create it. He ended the short article with a clear forecast of the future: "We do not have long to wait. We can assume that the bold dream ofKonstantin Tsiolkovskywill be realized within the next 10 to 15 years."[20]From March 1952 to April 1954, the USCollier'smagazinereacted with a series of seven articlesMan Will Conquer Space Soon!detailingWernher von Braun's plans for crewed spaceflight. In March 1955, Disneyland's animated episode "Man in Space" which was broadcast on US television with an audience of about 40 million people, eventually fired the public enthusiasm for space travel and raised government interest, both in the US and USSR.
Soon after the end of World War II, the two former allies became engaged in a state of political conflict and military tension known as theCold War(1947–1991), which polarized Europe between the Soviet Union'ssatellite states(often referred to as theEastern Bloc) and the states of theWestern worldallied with the U.S.[21]
In August 1949, the Soviet Union became the second nuclear power after the United States with the successfulRDS-1nuclear weapon test. In October 1957, the Soviet Union conducted the world's first successful test of an intercontinental ballistic missile (ICBM), this was theR-7 Semyorka(also known as SS-6 by NATO) and was seen as capable of striking U.S. territory with a nuclear payload.[22][23]Fears in the US due to this perceived threat became known as theMissile gap. The first American ICBM, theAtlas missile, was tested in late 1958.[22][24]
ICBMs presented the ability to strike targets on the other side of the globe in a very short amount of time and in a manner which was impervious to air interception such as bombers might have been. The value which ICBMs presented in a nuclear standoff were very substantial, and this fact greatly accelerated efforts to develop rocket and rocket interception technology.[25]
The first Soviet development of artillery rockets was in 1921 when the Soviet military sanctioned theGas Dynamics Laboratory, a small research laboratory to explore solid-fuel rockets, led byNikolai Tikhomirov, who had begun studying solid and liquid-fueled rockets in 1894, and obtained a patent in 1915 for "self-propelled aerial and water-surface mines.[26][27]The first test-firing of a solid fuel rocket was carried out in 1928.[28]
Further development was carried out in the 1930s by theGroup for the Study of Reactive Motion(GIRD), where Soviet rocket pioneersSergey Korolev,Friedrich Zander,Mikhail TikhonravovandLeonid Dushkin[29]launchedGIRD-X, the first Soviet liquid-fueled rocket in 1933.[30]In 1933 the twodesign bureauswere combined into theReactive Scientific Research Institute[31]and produced the RP-318, the USSR's firstrocket-powered aircraftand theRS-82 and RS-132 missiles,[32]which became the basis for theKatyushamultiple rocket launcher,[33][34]During the 1930s Soviet rocket technology was comparable to Germany's,[35]butJoseph Stalin'sGreat Purgefrom 1936 to 1938 severely damaged its progress.
In 1945 the Soviets captured several keyNazi GermanA-4 (V-2) rocket production facilities, and also gained the services of someGerman scientists and engineersrelated to the project. A-4s were assembled and studied and the experience derived from assembling and launching A4 rockets was directly applied to the Soviet copy, called theR-1,[36][37]with NII-88 chief designerSergei Korolevoverseeing the R-1's development.,[38]The R-1 entered into service in theSoviet Armyon 28 November 1950.[39][40]By the latter half of 1946, Korolev and rocket engineerValentin Glushkohad, with extensive input from German engineers, outlined a successor to the R-1, theR-2with an extended frame and a new engine designed by Glushko,[41]which entered service in November, 1951, with a range of 600 kilometres (370 mi), twice that of the R-1.[42]This was followed in 1951 with the development of theR-5 Pobeda, the Soviet Union's first real strategic missile, with a range of 1,200 km (750 mi) and capable of carrying a 1 megaton (mt)thermonuclearwarhead. The R-5 entered service in 1955.[43]Scientific versions of the R-1, R-2 and R-5 undertook various experiments between 1949 and 1958, including flights withspace dogs.[44]: 21–23
Design work began in 1953 on theR-7 Semyorkawith the requirement for a missile with a launch mass of 170 to 200 tons, range of 8,500 km and carrying a 3,000 kg (6,600 lb) nuclear warhead, powerful enough to launch a nuclear warhead against the United States. In late 1953 the warhead's mass was increased to 5.5 to 6 tons to accommodate the then plannedtheromonuclear bomb.[45][46]The R-7 was designed in a two-stage configuration, with four boosters that would jettison when empty.[47]On the 21 August 1957 the R-7 flew 6,000 km (3,700 mi), and became the worlds's first intercontinental ballistic missile.[48][46]Two months later the R-7 launchedSputnik 1, the first artificial satellite, into orbit, and became the basis for theR-7 familywhich includesSputnik,Luna,Molniya,Vostok, andVoskhodspace launchers, as well as laterSoyuzvariants. Several versions are still in use and it has become the world's most reliable space launcher.[49][50]
Although American rocket pioneerRobert H. Goddarddeveloped, patented, and flew small liquid-propellant rockets as early as 1914, the United States was the only one of the three major allied World War II powers to not have its own rocket program, until Von Braun and his engineers were expatriated from Nazi Germany in 1945. The US acquired a large number of V-2 rockets and recruited von Braun and most of his engineering team inOperation Paperclip.[51]The team was sent to the Army'sWhite Sands Proving Groundin New Mexico, in 1945.[52]They set about assembling the captured V-2s and began a program of launching them and instructing American engineers in their operation.[53]These tests led to thefirst photos of Earth from space, and the first two-stage rocket, theWAC Corporal-V-2combination, in 1949.[53]The German rocket team was moved fromFort Blissto the Army's newRedstone Arsenal, located inHuntsville, Alabama, in 1950.[54]From here, von Braun and his team developed the Army's first operational medium-range ballistic missile, theRedstone rocket, derivatives of which launched both America's first satellite, and the first piloted Mercury space missions.[54]It became the basis for both theJupiterandSaturn family of rockets.[54]
Each of the United States armed services had its own ICBM development program. The Air Force began ICBM research in 1945 with theMX-774.[55]In 1950, von Braun began testing the Air ForcePGM-11 Redstonerocket family at Cape Canaveral.[56]By 1957, a descendant of the Air Force MX-774 received top-priority funding.[55]and evolved into theAtlas-A, the first successful American ICBM.[55]The Atlas made use of a thin stainless steel fuel tank which relied on the internal pressure of the tank for structural integrity, this allowed an overall lighter weight design.[57]WD-40was developed to prevent rust on the Atlas rockets so that rust protecting paint could be avoided, to further reduce weight.[58][59]
A later variant of the Atlas, theAtlas-D, served as a nuclear ICBM and as the orbital launch vehicle forProject Mercuryand the remote-controlledAgena Target Vehicleused inProject Gemini.[55]
The period from 1955 to 1960 saw the first artificial satellites put into earth orbit by both the USSR and the US, the first animals sent into orbit, and the first robotic probes to impact and flyby the Moon by the Soviets.
In 1955, with both the United States and the Soviet Union building ballistic missiles that could be used to launch objects into space, the stage was set for nationalistic competition.[6]On July 29, 1955,James C. Hagerty, PresidentDwight D. Eisenhower's press secretary, announced that the United States intended to launch "small Earth circling satellites" between July 1, 1957, and December 31, 1958, as part of the US contribution to theInternational Geophysical Year(IGY).[6]On August 2, at theSixth Congress of the International Astronautical Federationin Copenhagen, scientistLeonid I. Sedovtold international reporters at the Soviet embassy of his country's intention to launch a satellite as well, in the "near future".[6]
On August 30, 1955,Sergei Korolevsucceeded in convincing theSoviet Academy of Sciencesto establish a commission dedicated to achieving the goal of launching a satellite into Earth orbit before the United States,[6]this can be viewed as thede factostart date of the space race. TheCouncil of Ministers of the Soviet Unionbegan a policy of treating development of its space program as top-secret. When the Sputnik project was first approved, one of the immediate courses of action thePolitburotook was to consider what to announce to the world regarding their event. TheTelegraph Agency of the Soviet Union(TASS) established precedents for all official announcements on the Soviet space program. The information eventually released did not offer details on who built and launched the satellite or why it was launched.[60]
The Soviet space program's use of secrecy served as both a tool to prevent the leaking ofclassified informationbetween countries, and to avoid revealing specifics to the Soviet populace in regards to their short and long term goals; the program's nature embodied ambiguous messages concerning its goals, successes, and values. Launches were not announced until they took place,cosmonautnames were not released until they flew, and outside observers did not know the size or shape of their rockets or cabins of most of their spaceships, except for the first Sputniks, lunar probes, and Venus probe.[61]
The Soviet military maintained control over the space program; Korolev'sOKB-1design bureau was subordinated under theMinistry of General Machine Building,[60]tasked with the development of intercontinental ballistic missiles, and continued to give its assets random identifiers into the 1960s.[60]Information about failures was systematically withheld, historian James Andrews notes that Soviet media coverage of the space program, particularly human space missions, rarely reported any failures or difficulties, creating the impression of a flawless operation:
"With almost no exceptions, coverage of Soviet space exploits, especially in the case of human space missions, omitted reports of failure or trouble".[60]
Dominic Phelan noted in the bookCold War Space Sleuths(Springer-Praxis 2013): "TheUSSRwas famously described byWinston Churchillas 'a riddle, wrapped in a mystery, inside an enigma' and nothing signified this more than the search for the truth behind its space program during the Cold War. Although the Space Race was literally played out above our heads, it was often obscured by a figurative 'space curtain' that took much effort to see through".[61]
Initially, President Eisenhower was worried that a satellite passing above a nation at over 100 kilometers (62 mi) might be seen as violating that nation's airspace.[62]He was concerned that the Soviet Union would accuse the Americans of an illegal overflight, thereby scoring a propaganda victory at his expense.[63]Eisenhower and his advisors were of the opinion that a nation's airspace sovereignty did not extend past theKármán line, and they used the 1957–58 International Geophysical Year launches to establish this principle in international law.[62]Eisenhower also feared that he might cause an international incident and be called a "warmonger" if he were to use military missiles as launchers. Therefore, he selected the untriedNaval Research Laboratory'sVanguard rocket, which was a research-only rocket.[64]This meant that von Braun's team was not allowed to put a satellite into orbit with their Jupiter-C rocket, because of its intended use as a future military vehicle.[64]On September 20, 1956, von Braun and his team did launch a Jupiter-C that was capable of putting a satellite into orbit, but the launch was used only as a suborbital test of reentry vehicle technology.[64]
Korolev received word about von Braun's 1956 Jupiter-C test and, mistakenly thinking it was a satellite mission that failed, expedited plans to get his own satellite in orbit. Since the R-7 was substantially more powerful than any of the USlaunch vehicles, he made sure to take full advantage of this capability by designingObject Das his primary satellite.[65]It was given the designation 'D', to distinguish it from other R-7 payload designations 'A', 'B', 'V', and 'G' which were nuclear weapon payloads.[66]Object D dwarfed the proposed US satellites, having a weight of 1,400 kilograms (3,100 lb), of which 300 kilograms (660 lb) would be composed of scientific instruments that would photograph the Earth, take readings on radiation levels, and check on the planet's magnetic field.[66]However, things were not going along well with the design and manufacturing of the satellite, so in February 1957, Korolev sought and received permission from the Council of Ministers to build aProsteishy Sputnik(PS-1), or simple satellite.[65]The council also decreed that Object D be postponed until April 1958.[67]The newSputnikwas a metallic sphere that would be a much lighter craft, weighing 83.8 kilograms (185 lb) and having a 58-centimeter (23 in) diameter.[68]The satellite would not contain the complex instrumentation that Object D had, but had two radio transmitters operating on differentshort wave radiofrequencies, the ability to detect if a meteoroid were to penetrate its pressure hull, and the ability to detect the density of the Earth'sthermosphere.[69]
Korolev was buoyed by the first successful launches of the R-7 rocket in August and September, which paved the way for the launch ofSputnik.[70]Word came that the US was planning to announce a major breakthrough at an International Geophysical Year conference at theNational Academy of Sciencesin Washington D.C., with a paper titled "Satellite Over the Planet", on October 6, 1957.[71]Korolev anticipated that von Braun might launch a Jupiter-C with a satellite payload on or around October 4 or 5, in conjunction with the paper.[71]He hastened the launch, moving it to October 4.[71]The launch vehicle for PS-1 was a modified R-7 – vehicle 8K71PS number M1-PS – without much of the test equipment and radio gear that was present in the previous launches.[70]It arrived at the Soviet missile baseTyura-Tamin September and was prepared for its mission atlaunch site number one.[70]
The first launch took place on Friday, October 4, 1957, at exactly 10:28:34 pm Moscow time, with the R-7 and the now named Sputnik 1 satellite lifting off the launch pad and placing the artificial "moon" into an orbit a few minutes later.[72]This "fellow traveler", as the name is translated in English, was a small, beeping ball, less than two feet in diameter and weighing less than 200 pounds. But the celebrations were muted at the launch control center until the down-range far east tracking station atKamchatkareceived the first distinctive beep ... beep ... beep sounds fromSputnik 1's radio transmitters, indicating that it was on its way to completing its first orbit.[72]About 95 minutes after launch, the satellite flew over its launch site, and its radio signals were picked up by the engineers and military personnel at Tyura-Tam: that's when Korolev and his team celebrated the first successful artificial satellite placed into Earth-orbit.[73]
The next satellite sent by the Soviets after Sputnik 1 wasSputnik 2, launched on November 3, 1957, just a month later. This would put the first animal into orbit.[74][75]
At the latest, the successful start ofSputnik 2with the satellite weighing more than 500 kg proved that the USSR had achieved a leading advantage in rocket technology. The CIA, initially astonished, estimated the launch weight of the rocket at 500 metric tons, requiring an initial thrust exceeding 1,000 tons, and assumed the use of a three-stage rocket. In a classified report, the agency described the event as a "stupendous scientific achievement" and concluded that the USSR had likely perfected an intercontinental ballistic missile (ICBM) capable of accurately targeting any location.[76]In reality, the launch weight of the Soviet rocket was 267 metric tons with an initial thrust of 410 tons with one and a half stages. The CIA's misjudgement was caused by extrapolating the parameters of the USAtlasrocket developed at the same time (launch weight 82 tons, initial thrust 135 tones, maximum payload of 70 kg forlow Earth orbit).[77]In part, the favourable data of the Soviet launcher was based on concepts proposed by the German rocket scientists headed byHelmut GröttruponGorodomlya Island, such as, among other things, the rigorous weight saving, the control of the residual fuel quantities and a reduced thrust to weight relation of 1.4 instead of usual factor 2.[78]The CIA had heard about such details already in January 1954 when it interrogated Göttrup after his return from the USSR but did not take him seriously.[79]
The Soviet success raised a great deal of concern in the United States. For example, economist Bernard Baruch wrote in an open letter titled "The Lessons of Defeat" to theNew York Herald Tribune: "While we devote our industrial and technological power to producing new model automobiles and more gadgets, the Soviet Union is conquering space. ... It is Russia, not the United States, who has had the imagination to hitch its wagon to the stars and the skill to reach for the moon and all but grasp it. America is worried. It should be."[80]
Eisenhower ordered project Vanguard to move up its timetable and launch its satellite much sooner than originally planned.[81]The December 6, 1957Project Vanguard launch failureoccurred atCape Canaveral Air Force Stationin Florida. It was a monumental failure, exploding a few seconds after launch, and it became an international joke. The satellite appeared in newspapers under the names Flopnik, Stayputnik, Kaputnik,[82]and Dudnik.[83]In the United Nations, the Soviet delegate offered the US representative aid "under the Soviet program of technical assistance to backwards nations."[82]Only in the wake of this very public failure did von Braun's Redstone team get the go-ahead to launch their Jupiter-C rocket as soon as they could. In Britain, the US's Western Cold War ally, the reaction was mixed: some celebrated the fact that the Soviets had reached space first, while others feared the destructive potential that military uses of spacecraft might bring.[84]TheDaily Expresspredicted that the US would catch up to and pass the USSR in space; "never doubt for a moment that America would be successful".[85]
On January 31, 1958, nearly four months after the launch ofSputnik 1, von Braun and the United States successfully launched its first satellite on a four-stageJuno Irocket derived from the US Army's Redstone missile, at Cape Canaveral.[86]The satelliteExplorer 1was 30.66 pounds (13.91 kg) in mass.[86]The payload of Explorer 1 weighed 18.35 pounds (8.32 kg). It carried a micrometeorite gauge and aGeiger-Müller tube. It passed in and out of the Earth-encompassing radiation belt with its 194-by-1,368-nautical-mile (360 by 2,534 km) orbit, therefore saturating the tube's capacity and proving what Dr.James Van Allen, a space scientist at theUniversity of Iowa, had theorized.[86]The belt, named theVan Allen radiation belt, is a doughnut-shaped zone of high-level radiation intensity around the Earth above the magnetic equator.[87]Van Allen was also the man who designed and built the satellite instrumentation ofExplorer 1. The satellite measured three phenomena: cosmic ray and radiation levels, the temperature in the spacecraft, and the frequency of collisions with micrometeorites. The satellite had nomemoryfor data storage, therefore it had to transmit continuously.[88]The next successful mission wasExplorer 3, launched later that month (March 26, 1958), which carried similar scientific instruments and successfully recorded cosmic ray data.[89][90][91]
On April 2, 1958, President Eisenhower reacted to the Soviet space lead in launching the first satellite by recommending to the US Congress that a civilian agency be established to direct nonmilitary space activities. Congress, led bySenate Majority LeaderLyndon B. Johnson, responded by passing theNational Aeronautics and Space Act, which Eisenhower signed into law on July 29, 1958. This law turned theNational Advisory Committee on Aeronauticsinto theNational Aeronautics and Space Administration(NASA). It also created a Civilian-Military Liaison Committee, appointed by the President, responsible for coordinating the nation's civilian and military space programs.[92]
On October 21, 1959, Eisenhower approved the transfer of the Army's remaining space-related activities to NASA. On July 1, 1960, the Redstone Arsenal became NASA'sGeorge C. Marshall Space Flight Center, with von Braun as its first director. Development of theSaturn rocket family, which when mature gave the US parity with the Soviets in terms of lifting capability, was thus transferred to NASA.[93]
The US and the USSR sent animals into space to determine the safety of the environment before sending the first humans. The USSR useddogsfor this purpose, and the US usedmonkeys and apes. The first mammal in space wasAlbert II, a rhesus monkey launched by the US on a sub-orbital flight on June 14, 1949, who died on landing due to a parachute malfunction.[94]
The USSR sent the dogLaikainto orbit onSputnik 2, the second satellite launched, on November 3, 1957, for an intended ten-day flight.[74]They did not yet have the technology to return Laika safely to Earth, and the government reported Laika died when the oxygen ran out,[95]but in October 2002 her true cause of death was reported as stress and overheating on the fourth orbit[96]due to failure of the air conditioning system.[97]At a Moscow press conference in 1998Oleg Gazenko, a senior Soviet scientist involved in the project, stated "The more time passes, the more I'm sorry about it. We did not learn enough from the mission to justify the death of the dog...".[98]
In 1958, Korolev upgraded the R-7 to be able to launch a 400-kilogram (880 lb) payload to the Moon. TheLuna programbegan with three failed secret 1958 attempts to launchLuna E-1-classimpactorprobes.[99]The fourth attempt,Luna 1, launched successfully on January 2, 1959, but missed the Moon. The fifth attempt on June 18 also failed at launch. The 390-kilogram (860 lb)Luna 2successfully impacted the Moon on September 14, 1959. The 278.5-kilogram (614 lb)Luna 3successfully flew by the Moon and sent back pictures of its far side on October 7, 1959.[100]
The US first embarked on thePioneer programin 1958 by launching thefirst probe, albeit ending in failure. A subsequent probe namedPioneer 1was launched with the intention of orbiting the Moon only to result in a partial mission success when it reached an apogee of 113,800 km before falling back to Earth. The missions ofPioneer 2andPioneer 3failed whereasPioneer 4had one partially successful lunar flyby in March 1959.[101][102]
The period from 1961 to 1968 began with the first men sent to space, the first robotic explorations of other planets; with missions to Venus and Mars conducted by both the Soviet Union and the United States, robotic landings on the Moon, and the gestation of US ambition to land a man on the moon. The 60s saw significant advancements in crewed spaceflight by both cold war adversaries, as well as the first nuclear detonation in space, research into anti-satellite technology, and the signing of historic international outer space treaties.
The Soviets designed their first humanspace capsuleusing the samespacecraft busas theirZenit spy satellite,[103]forcing them to keep the details and true appearance secret until after the Vostok program was over. The craft consisted of a spherical descent module with a mass of 2.46 tonnes (5,400 lb) and a diameter of 2.3 meters (7.5 ft), with a cylindrical inner cabin housing the cosmonaut, instruments, and escape system; and abiconicinstrument module with a mass of 2.27 tonnes (5,000 lb), 2.25 meters (7.4 ft) long and 2.43 meters (8.0 ft) in diameter, containing the engine system and propellant. After reentry, the cosmonaut would eject at about 7,000 meters (23,000 ft) over the USSR and descend via parachute, while the capsule would land separately, because the descent module made an extremely rough landing that could have left a cosmonaut seriously injured.[104]The "Vostok spaceship" was first displayed at the July 1961Tushino air show, mounted on its launch vehicle's third stage, with the nose cone in place concealing the spherical capsule. A tail section with eight fins was added in an apparent attempt to confuse western observers. This also appeared on official commemorative stamps and a documentary.[105]The Soviets finally revealed the true appearance of their Vostok capsule at the April 1965 Moscow Economic Exhibition.[106]
On April 12, 1961, the USSR surprised the world by launchingYuri Gagarininto a single, 108-minute orbit around the Earth in a craft calledVostok 1.[104]They dubbed Gagarin the firstcosmonaut, roughly translated from Russian and Greek as "sailor of the universe". Gagarin's capsule was flown in automatic mode, since doctors did not know what would happen to a human in the weightlessness of space; but Gagarin was given an envelope containing the code that would unlock manual control in an emergency.[104]
Gagarin became a national hero of the Soviet Union and the Eastern Bloc, and a worldwide celebrity. Moscow and other cities in the USSR held mass demonstrations, the scale of which was second only to theWorld War II Victory Parade of 1945.[107]April 12 was declaredCosmonautics Dayin the USSR, and is celebrated today in Russia as one of the official "Commemorative Dates of Russia."[108]In 2011, it was declared the International Day of Human Space Flight by theUnited Nations.[109]
The USSR demonstrated 24-hour launch pad turnaround and launched two piloted spacecraft,Vostok 3andVostok 4, in essentially identical orbits, on August 11 and 12, 1962.[110]The two spacecraft came within approximately 6.5 kilometers (3.5 nautical miles) of one another, close enough for radio communication,[111]but then drifted as far apart as 2,850 kilometers (1,540 nautical miles). The Vostok had no maneuvering rockets to keep the two craft a controlled distance apart.[112]Vostok 4 also set a record of nearly four days in space. The first woman,Valentina Tereshkova, was launched into space onVostok 6on June 16, 1963,[113]as (possibly) a medical experiment. She was the only one to fly of a small group of female parachutist factory workers (unlike the male cosmonauts who were military test pilots),[114]chosen by the head of cosmonaut training because he read a tabloid article about the "Mercury 13" group of women wanting to become astronauts, and got the mistaken idea that NASA was actually entertaining this.[115][113]Five months after her flight, Tereshkova marriedVostok 3cosmonautAndriyan Nikolayev,[116]and they had a daughter.[117]
The US Air Force had been developing a program to launch the first man in space, namedMan in Space Soonest. This program studied several different types of one-man space vehicles, settling on aballistic re-entry capsulelaunched on a derivativeAtlas missile, and selecting a group of nine candidate pilots. After NASA's creation, the program was transferred over to the civilian agency'sSpace Task Groupand renamed Project Mercury on November 26, 1958. The Mercury spacecraft was designed by the STG's chief engineerMaxime Faget. NASA selected a new group ofastronaut(from the Greek for "star sailor") candidates fromNavy,Air ForceandMarinetest pilots, and narrowed this down toa group of sevenfor the program. Capsule design and astronaut training began immediately, working toward preliminary suborbital flights on theRedstone missile, followed by orbital flights on the Atlas. Each flight series would first start unpiloted, then carry a non-human primate, then finally humans.[118]
The Mercury spacecraft's principal designer wasMaxime Faget, who started research for human spaceflight during the time of the NACA.[119]It consisted of a conical capsule with a cylindrical pack of three solid-fuelretro-rocketsstrapped over aberylliumorfiberglassheat shieldon the blunt end. Base diameter at the blunt end was 6.0 feet (1.8 m) and length was 10.8 feet (3.3 m); with the launch escape system added, the overall length was 25.9 feet (7.9 m).[120]With 100 cubic feet (2.8 m3) of habitable volume, the capsule was just large enough for a single astronaut.[121]The first suborbital spacecraft weighed 3,000 pounds (1,400 kg); the heaviest, Mercury-Atlas 9, weighed 3,000 pounds (1,400 kg) fully loaded.[122]On reentry, the astronaut would stay in the craft through splashdown by parachute in the Atlantic Ocean.
On May 5, 1961,Alan Shepardbecame the first American in space, launching in aballistic trajectoryonMercury-Redstone 3, in a spacecraft he namedFreedom 7.[123]Though he did not achieve orbit like Gagarin, he was the first person to exercise manual control over his spacecraft'sattitudeandretro-rocketfiring.[124]After his successful return, Shepard was celebrated as a national hero, honored with parades in Washington, New York and Los Angeles, and received theNASA Distinguished Service MedalfromPresidentJohn F. Kennedy.[125]
AmericanVirgil "Gus" Grissomrepeated Shepard's suborbital flight inLiberty Bell 7on July 21, 1961.[126]Almost a year after the Soviet Union put a human into orbit, astronautJohn Glennbecame the first American to orbit the Earth, on February 20, 1962.[127]HisMercury-Atlas 6mission completed three orbits in theFriendship 7spacecraft, and splashed down safely in the Atlantic Ocean, after a tense reentry, due to what falsely appeared from the telemetry data to be a loose heat-shield.[127]On February 23, 1962, President Kennedy awarded Glenn with theNASA Distinguished Service Medalin a ceremony atCape Canaveral Air Force Station.[128]As the first American in orbit, Glenn became a national hero, and received aticker-tape paradein New York City, reminiscent of that given forCharles Lindbergh.
The United States launched three more Mercury flights after Glenn's:Aurora 7on May 24, 1962, duplicated Glenn's three orbits,Sigma 7on October 3, 1962, six orbits, andFaith 7on May 15, 1963, 22 orbits (32.4 hours), the maximum capability of the spacecraft. NASA at first intended to launch one more mission, extending the spacecraft's endurance to three days, but since this would not beat the Soviet record, it was decided instead to concentrate on developing Project Gemini.[129]
These are extraordinary times. And we face an extraordinary challenge. Our strength, as well as our convictions, have imposed upon this nation the role of leader in freedom's cause.
... if we are to win the battle that is now going on around the world between freedom and tyranny, the dramatic achievements in space which occurred in recent weeks should have made clear to us all, as did the Sputnik in 1957, the impact of this adventure on the minds of men everywhere, who are attempting to make a determination of which road they should take. ... Now it is time to take longer strides – time for a great new American enterprise – time for this nation to take a clearly leading role in space achievement, which in many ways may hold the key to our future on Earth.
... Recognizing the head start obtained by the Soviets with their large rocket engines, which gives them many months of lead-time, and recognizing the likelihood that they will exploit this lead for some time to come in still more impressive successes, we nevertheless are required to make new efforts on our own.
... I believe that this nation should commit itself to achieving the goal, before this decade is out, of landing a man on the Moon and returning him safely to the Earth. No single space project in this period will be more impressive to mankind, or more important for the long-range exploration of space, and none will be so difficult or expensive to accomplish.
... Let it be clear that I am asking the Congress and the country to accept a firm commitment to a new course of action—a course which will last for many years and carry very heavy costs: 531 million dollars in fiscal '62—an estimated seven to nine billion dollars additional over the next five years. If we are to go only half way, or reduce our sights in the face of difficulty, in my judgment it would be better not to go at all.
Before Gagarin's flight, US PresidentJohn F. Kennedy's support for America's piloted space program was lukewarm.Jerome Wiesnerof MIT, who served as a science advisor to presidents Eisenhower and Kennedy, and himself an opponent of sending humans into space, remarked, "If Kennedy could have opted out of a big space program without hurting the country in his judgment, he would have."[130]As late as March 1961, when NASA administrator James E. Webb submitted a budget request to fund a Moon landing before 1970, Kennedy rejected it because it was simply too expensive.[131]Some were surprised by Kennedy's eventual support of NASA and the space program because of how often he had attacked the Eisenhower administration's inefficiency during the election.[132]
Gagarin's flight changed this; now Kennedy sensed the humiliation and fear on the part of the American public over the Soviet lead. Additionally, theBay of Pigs invasion, planned before his term began but executed during it, was an embarrassment to his administration due to the colossal failure of the US forces.[133]Looking for something to save political face, he sent a memo dated April 20, 1961, to Vice PresidentLyndon B. Johnson, asking him to look into the state of America's space program, and into programs that could offerNASAthe opportunity to catch up.[134]The two major options at the time were either the establishment of an Earth orbital space station or a crewed landing on the Moon. Johnson, in turn, consulted with von Braun, who answered Kennedy's questions based on his estimates of US and Soviet rocket lifting capability.[135]Based on this, Johnson responded to Kennedy, concluding that much more was needed to reach a position of leadership, and recommending that the crewed Moon landing was far enough in the future that the US had a fighting chance to achieve it first.[136]
Kennedy ultimately decided to pursue what became the Apollo program, and on May 25 took the opportunity to ask for Congressional support in a Cold War speech titled "Special Message on Urgent National Needs".Full textHe justified the program in terms of its importance to national security, and its focus of the nation's energies on other scientific and social fields.[137]He rallied popular support for the program in his "We choose to go to the Moon" speech, on September 12, 1962, before a large crowd atRice UniversityStadium, in Houston, Texas, near the construction site of the newLyndon B. Johnson Space Centerfacility.[137]Full text
Khrushchev responded to Kennedy's challenge with silence, refusing to publicly confirm or deny the Soviets were pursuing a "Moon race".[138]As later disclosed, the Soviet Union secretly pursuedtwo competing crewed lunar programs. Soviet Decree 655–268,On Work on the Exploration of the Moon and Mastery of Space, issued in August 1964, directedVladimir Chelomeito develop a Moon flyby program with a projected first flight by the end of 1966, and directed Korolev to develop the Moon landing program with a first flight by the end of 1967.[139]In September 1965, Chelomei's flyby program was assigned to Korolev, who redesigned the cislunar mission to use his ownSoyuz 7K-L1spacecraft and Chelomei'sProton rocket. After Korolev's death in January 1966, another government decree of February 1967 moved the first crewed flyby to mid-1967, and the first crewed landing to the end of 1968.[140][141]
After a first US-USSRDryden-Blagonravovagreement and cooperation on theEcho II balloon satellitein 1962,[16]President Kennedy proposed on September 20, 1963, in a speech before theUnited Nations General Assembly, that the United States and the Soviet Union join forces in an effort to reach the Moon.[142]Kennedy thus changed his mind regarding the desirability of the space race, preferring instead to ease tensions with the Soviet Union by cooperating on projects such as a joint lunar landing.[143]Soviet PremierNikita Khrushchevinitially rejected Kennedy's proposal.[144]However, on October 2, 1997, it was reported that Khrushchev's sonSergeiclaimed Khrushchev was poised to accept Kennedy's proposal at the time ofKennedy's assassinationon November 22, 1963. During the next few weeks he reportedly concluded that both nations might realize cost benefits and technological gains from a joint venture, and decided to accept Kennedy's offer based on a measure of rapport during their years as leaders of the world's two superpowers, but changed his mind and dropped the idea since he lacked the same trust for Kennedy's successor, Lyndon Johnson.[144]
Some cooperation in robotic space exploration nevertheless did take place,[145]such as a combinedVenera 4–Mariner 5data analysis under a joint Soviet–American working group ofCOSPARin 1969, allowing a more complete drawing of the profile of theatmosphere of Venus.[146][147]Eventually theApollo–Soyuz missionwas realized afterall, which furthermore laid the foundations for theShuttle-Mir programand theISS.
As President, Johnson steadfastly pursued the Gemini and Apollo programs, promoting them as Kennedy's legacy to the American public. One week after Kennedy's death, he issuedExecutive Order 11129renaming the Cape Canaveral andApollo launchfacilities after Kennedy.
TheRanger program, started in 1959 by NASA'sJet Propulsion Laboratory, aimed to conduct hard impacts on the moon and had its first success in 1962, after 3 failures due to launch aborts (Ranger 1andRanger 2) and a failure to reach the moon (Ranger 3), when the 730-pound (330 kg)Ranger 4became the first US spacecraft to reach the Moon, but itssolar panelsand navigational system failed near the Moon and it impacted the far side without returning any scientific data.Ranger 5ran out of power and missed the Moon by 725 kilometers (391 nmi) on October 21, 1962. The first successful Ranger mission was the 806-pound (366 kg) Block IIIRanger 7which impacted on July 31, 1964.[148]Ranger had three successful impacts out of nine attempts.[149]
In 1963, the Soviet Union's "2nd Generation" Luna programme was less successful than the earlier Luna probes;Luna 4,Luna 5,Luna 6,Luna 7, andLuna 8were all met with mission failures. However, in 1966 theLuna 9achieved the first soft-landing on the Moon, and successfully transmitted photography from the surface.[150]Luna 10marked the first man-made object to establish an orbit around the Moon,[151]followed byLuna 11,Luna 12, andLuna 14which also successfully established orbits.Luna 12was able to transmit detailed photography of the surface from orbit.[152]Luna 10, 12, andLuna 14conductedGamma ray spectrometryof the Moon, among other tests.
TheZond programmewas orchestrated alongside theLunaprogramme withZond 1andZond 2launching in 1964, intended as flyby missions, however both failed.[153][154]Zond 3however was successful, and transmitted high quality photography from the far side of the moon.[155][156]
Partly to aid the Apollo missions, theSurveyor programwas conducted by NASA, with five successful soft landings out of seven attempts from 1966 to 1968. TheLunar Orbiter programhad five successes out of five attempts in 1966–1967.[157][158]
In late 1966,Luna 13became the third spacecraft to make a soft-landing on the Moon, with the AmericanSurveyor 1having now taken second. Luna 13 made use of inflatable air-bags to soften it's landing.[159][160][161]Surveyor 1 was a 995 kg lander, notably larger than the 112 kg Luna 13 E-6M lander.[159][162]Surveyor 1 was equipped with aDoppler velocity sensing systemthat fed information into the spacecraft computer to implement a controllable descent to the surface. Each of the three landing pads also carried aircraft-type shock absorbers and strain gauges to provide data on landing characteristics, important for future Apollo missions.[163][164]
Surveyor 3, which successfully touched down on the Moon April 20, 1967, carried a 'surface sampler' which facilitated tests of the Lunar soil. Based on these experiments, scientists concluded that lunar soil had a consistency similar to wet sand, with a bearing strength of about 10 pounds per square inch (0.7 kilograms per square centimeter, or 98 kilopascals), which was concluded to be solid enough to support an Apollo Lunar Module.[165]The Surveyor 3 lander would be later visited byApollo 12astronauts.[166]
On Nov. 17, 1967, before mission termination,Surveyor 6fired its thrusters for 2.5 seconds, becoming the first spacecraft launched from the lunar surface. It rose about 10 feet (3 meters) before landing 8 feet (2.5 meters) west of its original spot. Cameras then examined the original landing site to assess the soil's properties.[167][168]
From the early 60s both cold war adversaries almost simultaneously initiated their own programmes which sought to reach other planets in the solar system for the first time; namely Venus and Mars.
Venus was of great interest in the field ofplanetary sciencedue to its thick and opaque atmosphere, the atmospheres of other planets being a novel area of research at the time.
In 1961 theVenera Programmewas initiated by the Soviet Union, with the launch ofVenera 1. The programme would go on to mark many firsts in the exploration of another planet. Despite the later successes however,Venera 1andVenera 2, intended to flyby Venus, resulted in failure due to losses of contact.[169][170]
NASA would then initiate theMariner programwith the launch ofMariner 1andMariner 2. Mariner 1 failed shortly after launch,[171]however Mariner 2 would become the first man-made object to flyby another Planet in December 1962 when the probe passed by Venus.[172][173]
Later in 1965/66,Venera 3, marked the first time a man-made object made contact with another planet after it impacted Venus on March 1, 1966, despite operational difficulties resulting in loss of contact with the craft.[174]
In 1967,Mariner 5flew by Venus and conducted atmospheric analysis.[175]
In 1964, NASA'sMariner 4became the first successful Mars flyby, transmitting 21 pictures of the planets surface. This was followed byMariner 6 and 7in 1969.
Focused by the commitment to a Moon landing, in January 1962 the US announced Project Gemini, a two-person spacecraft that would support the later three-person Apollo by developing the key spaceflight technologies ofspace rendezvousanddocking of two craft, flight durations of sufficient length to go to the Moon and back, andextra-vehicular activityto perform work outside the spacecraft.[176][177]
Meanwhile, Korolev had planned further long-term missions for the Vostok spacecraft, and had four Vostoks in various stages of fabrication in late 1963 at hisOKB-1facilities.[178]The Americans' announced plans for Gemini represented major advances over the Mercury and Vostok capsules, and Korolev felt the need to try to beat the Americans to many of these innovations.[178]He had already begun designing the Vostok's replacement, the next-generationSoyuz, a multi-cosmonaut spacecraft that had at least the same capabilities as the Gemini spacecraft.[179]Soyuz would not be available for at least three years, and it could not be called upon to deal with this new American challenge in 1964 or 1965.[180]Political pressure in early 1964 – which some sources claim was from Khrushchev while other sources claim was from other Communist Party officials – pushed him to modify his four remaining Vostoks to beat the Americans to new space firsts in the size of flight crews, and the duration of missions.[178]
Korolev's conversion of his surplus Vostok capsules to theVoskhod spacecraftallowed the Soviet space program to beat the Gemini program in achieving the first spaceflight with a multi-person crew, and the first "spacewalk". Gemini took a year longer than planned to make its first flight, soVoskhod 1became the first spaceflight with a three-person crew on October 12, 1964.[181]The USSR touted another "technological achievement" during this mission: it was the first space flight during which cosmonauts performed in a shirt-sleeve-environment.[182]However, flying without spacesuits was not due to safety improvements in the Soviet spacecraft's environmental systems; rather this was because the craft's limited cabin space did not allow for spacesuits. Flying without spacesuits exposed the cosmonauts to significant risk in the event of potentially fatal cabin depressurization.[182]This was not repeated until the USApollo Command Moduleflew in 1968; the command module cabin was designed to transport three astronauts in a low pressure, pure oxygenshirt-sleeve environmentwhile in space.
On March 18, 1965, about a week before the first piloted Project Gemini space flight, the USSR launched the two-cosmonautVoskhod 2mission withPavel BelyayevandAlexei Leonov.[183]Voskhod 2's design modifications included the addition of an inflatable airlock to allow forextravehicular activity(EVA), also known as a spacewalk, while keeping the cabin pressurized so that the capsule's electronics would not overheat.[184]Leonov performed the first-ever EVA as part of the mission.[183]A fatality was narrowly avoided when Leonov's spacesuit expanded in the vacuum of space, preventing him from re-entering the airlock.[185]To overcome this, he had to partially depressurize his spacesuit to a potentially dangerous level.[185]He succeeded in safely re-entering the spacecraft, but he and Belyayev faced further challenges when the spacecraft's atmospheric controls flooded the cabin with 45% pure oxygen, which had to be lowered to acceptable levels before re-entry.[186]The reentry involved two more challenges: an improperly timed retrorocket firing caused the Voskhod 2 to land 386 kilometers (240 mi) off its designated target area, the city ofPerm; and the instrument compartment's failure to detach from the descent apparatus caused the spacecraft to become unstable during reentry.[186]
By October 16, 1964,Leonid Brezhnevand a small cadre of high-ranking Communist Party officials deposed Khrushchev as Soviet government leader a day after Voskhod 1 landed, in what was called the "Wednesday conspiracy".[187]The new political leaders, along with Korolev, ended the technologically troublesome Voskhod program, canceling Voskhod 3 and 4, which were in the planning stages, and started concentrating on reaching the Moon.[188]Voskhod 2 ended up being Korolev's final achievement before his death on January 14, 1966, as it became the last of the space firsts that the USSR achieved during the early 1960s. According to historian Asif Siddiqi, Korolev's accomplishments marked "the absolute zenith of the Soviet space program, one never, ever attained since."[7]There was a two-year pause in Soviet piloted space flights while Voskhod's replacement, the Soyuz spacecraft, was designed and developed.[189]
Though delayed a year to reach its first flight, Gemini was able to take advantage of the USSR's two-year hiatus after Voskhod, which enabled the US to catch up and surpass the previous Soviet superiority in piloted spaceflight. Gemini had ten crewed missions between March 1965 and November 1966:Gemini 3,Gemini 4,Gemini 5,Gemini 6A,Gemini 7,Gemini 8,Gemini 9A,Gemini 10,Gemini 11, andGemini 12; and accomplished the following:
Gemini 8 experienced the first in-space mission abort on March 17, 1966, just after achieving the world's first docking, when a stuck or shorted thruster sent the craft into an uncontrolled spin. Command pilotNeil Armstrongwas able to shut off the stuck thruster and stop the spin by using the re-entry control system.[191]He and his crewmateDavid Scottlanded and were recovered safely.[192]
Most of the novice pilots on the early missions would command the later missions. In this way, Project Gemini built up spaceflight experience for the pool of astronauts for the Apollo lunar missions. With the completion of Gemini, the US had demonstrated many of the key technologies necessary to make Kennedy's goal of landing a man on the Moon, namely crewed spacecraft docking, with the exception of developing a large enough launch vehicle.[193]
Korolev's design bureau produced two prospectuses for circumlunar spaceflight (March 1962 and May 1963), the main spacecraft for which were early versions of his Soyuz design. At the same time, another bureau,OKB-52, headed byVladimir Chelomey, was developing theLK-1lunar flyby spacecraft, which would be launched by Chelomey'sProton UR-500rocket. The Soviet government rejected Korolev's proposals, opting to support Chelomey's project, who gained favor with Khrushchev by employing his son.[194]
Officially, the Soviet lunar program was established on August 3, 1964, with the adoption of Soviet Communist Party Central Committee Command 655-268 (On Work on the Exploration of the Moon and Mastery of Space).[140]The circumlunar flights were planned to occur in 1967, and the landings to start in 1968, intending to land a person on the Moon before the Apollo flights.[195]Both of the bureaus submitted their projects for a crewed lunar landing.[140]
Korolev's lunar landing program was designated N1/L3, for itsN1 super rocketand a more advancedSoyuz 7K-L3spacecraft, also known as the lunar orbital module ("Lunniy Orbitalny Korabl", LOK), with a crew of two. A separate lunar lander ("Lunniy Korabl",LK), would carry a single cosmonaut to the lunar surface.[195]
The N1/L3 launch vehicle had three stages to Earth orbit, a fourth stage for Earth departure, and a fifth stage for lunar landing assist. The combined space vehicle was roughly the same height and takeoff mass as the three-stage USApollo-Saturn V and exceeded its takeoff thrust by 28% (45,400 kN vs. 33,000 kN.[196]The N1/3L was never successfully tested, the first flight suffered a fire in the first-stage Block A due to a loose bolt, leading to a catastrophic explosion 70 seconds into the flight. Further variations of the N1 had similar catastrophic results in testing.[197]If successful, the N1 would have been capable of carrying a 95 metric tons payload into low earth orbit.[197]The Saturn V comparatively usedliquid hydrogen fuelin its two upper stages, and carried a 140.6 metric tons payload to orbit,[198][197]enough for a three-personorbiterand two-personlander.
Chelomey's program assumed using adirect ascentlander based on the LK-1,LK-700, which would be launched using his proposedUR-700rocket. Following Khrushchev's ouster from power, Chelomey lost his support in the Soviet government, and his proposal didn't receive any funding. Additionally, in August 1965, due to Korolev's opposition, work on the LK-1 was suspended, and later stopped completely. As a replacement, the circumlunar mission would use a stripped-downSoyuz 7K-L1"Zond", while still retaining the Proton UR-500 booster. To fit two crewmembers, the Zond had to omit the Soyuz orbital module, sacrificing equipment for habitable cabin volume.[194][199]
The US and USSR began discussions on the peaceful uses of space as early as 1958, presenting issues for debate to the United Nations,[200][201][202]which created aCommittee on the Peaceful Uses of Outer Spacein 1959.[203]
On May 10, 1962, Vice President Johnson addressed the Second National Conference on the Peaceful Uses of Space revealing that the United States and the USSR both supported a resolution passed by the Political Committee of the UN General Assembly in December 1962, which not only urged member nations to "extend the rules of international law to outer space," but to also cooperate in its exploration. Following the passing of this resolution, Kennedy commenced his communications proposing a cooperative American and Soviet space program.[204]
In 1963, thePartial Nuclear Test Ban Treatywas signed by more than 100 signatories, including both the United States and the Soviet Union. This treaty followed the US test of a nuclear bomb detonated in outer space the year earlier calledStarfish Prime.
The UN ultimately created aTreaty on Principles Governing the Activities of States in the Exploration and Use of Outer Space, including the Moon and Other Celestial Bodies, which was signed by the United States, the USSR, and theUnited Kingdomon January 27, 1967, and came into force the following October 10.[205]
The treaty remains in force, signed by 107 member states. – As of July 2017[update]
In November 1968, dismay gripped the United States Central Intelligence Agency when a successful satellite destruction simulation was successfully orchestrated by the Soviet Union.[206]As a part of theIstrebitel Sputnikovanti-satellite weaponsresearch programme, the Kosmos 248 Soviet satellite was successfully destroyed by Kosmos 252 which was able to intercept within the 5 km 'kill radius' and destroyed Kosmos 248 by detonating its onboard warhead. This wasn't the beginning of the programme, years earlier intercept attempts had begun with maneuvering test of thePolyotsatellites in 1964.[207][208][209][210]
Possibly as a response to the Soviet programme, the United States beganProject SAINT, which was intended to provide anti-satellite capability to be used in the case of war with the Soviet Union.[211][206][212]However, less is known about the mission profiles of this project compared to the Soviet programme, and the project was cancelled early on due to budget constraints.[212]
In 1967, both nations' space programs faced serious challenges that brought them to temporary halts.
On January 27, 1967, the same day the US and USSR signed the Outer Space Treaty, the crew of the first crewed Apollo mission, Command PilotVirgil "Gus" Grissom, Senior PilotEd White, and PilotRoger Chaffee, were killed in a fire that swept through their spacecraft cabin during a ground test, less than a month before the planned February 21 launch. An investigative board determined the fire was probably caused by an electrical spark and quickly grew out of control, fed by the spacecraft's atmosphere of pure oxygen at greater than one standard atmosphere. Crew escape was made impossible by inability to open theplug doorhatch cover against the internal pressure.[213]The board also found design and construction flaws in the spacecraft, and procedural failings, including failure to appreciate the hazard of the pure-oxygen atmosphere, as well as inadequate safety procedures.[213]All these flaws had to be corrected over the next twenty-two months until the first piloted flight could be made.[213]Mercury and Gemini veteran Grissom had been a favored choice ofDeke Slayton, NASA's Director of Flight Crew Operations, to make the first piloted landing.[214]
On April 24, 1967, the single pilot of Soyuz 1,Vladimir Komarov, became the first in-flight spaceflight fatality. The mission was planned to be a three-day test, to include the first Soviet docking with an unpilotedSoyuz 2, but the mission was plagued with problems. Problems began shortly after launch when one solar panel failed to unfold, leading to a shortage of power for the spacecraft's systems. Further problems with the orientation detectors complicated maneuvering the craft. By orbit 13, the automatic stabilisation system was completely dead, and the manual system was only partially effective.[215]The mission was aborted, Soyuz 1 fired itsretrorocketsandreenteredtheEarth's atmosphere. During the emergency re-entry, a fault in the landing parachute system caused the primary chute to fail, and the reserve chute became tangled with the drogue chute, causing descent speed to reach as high as 40 m/s (140 km/h; 89 mph). Shortly thereafter,Soyuz 1impacted the ground 3 km (1.9 mi) west of Karabutak, and was found on fire. The official autopsy states Komarov died of blunt force trauma on impact.[216][217][218]In the US during subsequent years, stories began circulating that in his last transmissions Komarov cursed the engineers and flight staff as he descended, or even that he cursed the Soviet leadership, and that these transmissions were received by anNSAlistening station nearIstanbul.[219][220][221]This would contradict Soviet records of the radio transcripts, and historians such asAsif Azam SiddiqiandRobert Pearlmanregard these claims to be fabrications.[222][223]
The United States recovered from the Apollo 1 fire, fixing the fatal flaws in an improved version of theBlock II command module. The US proceeded with unpiloted test launches of the Saturn V launch vehicle (Apollo 4andApollo 6) and the Lunar Module (Apollo 5) during the latter half of 1967 and early 1968.[224]The first Saturn V flight was an unqualified success, and although the second suffered some non-catastrophic engine failures, it was considered a partial success and the launcher achieved human rating qualification. Apollo 1's mission to check out the Apollo Command and Service Module in Earth orbit was accomplished by Grissom's backup crew onApollo 7, launched on October 11, 1968.[225]The eleven-day mission was a total success, as the spacecraft performed a virtually flawless mission, paving the way for the United States to continue with its lunar mission schedule.[226]
The Soviet Union also fixed the parachute and control problems with Soyuz, and the next piloted missionSoyuz 3was launched on October 26, 1968.[227]The goal was to complete Komarov's rendezvous and docking mission with the un-piloted Soyuz 2.[227]Ground controllers brought the two craft to within 200 meters (660 ft) of each other, then cosmonautGeorgy Beregovoytook control.[227]He got within 40 meters (130 ft) of his target, but was unable to dock before expending 90 percent of his maneuvering fuel, due to a piloting error that put his spacecraft into the wrong orientation and forced Soyuz 2 to automatically turn away from his approaching craft.[227]The first docking of Soviet spacecraft was finally realized in January 1969 by theSoyuz 4andSoyuz 5missions. It was the first-ever docking of two crewed spacecraft, and the first transfer of crew from one space vehicle to another.[228]
The SovietZond spacecraftwas not yet ready for pilotedcircumlunarmissions in 1968, after six unsuccessful automated test launches:Kosmos 146on March 10, 1967;Kosmos 154on April 8, 1967;Zond 1967Aon September 28, 1967;Zond 1967Bon November 22, 1967;Zond 1968Aon April 23, 1968; andZond 1968Bin July 1968.[229]Zond 4was launched on March 2, 1968, and successfully made a circumlunar flight,[230]but encountered problems with its Earth reentry on March 9, and was ordered destroyed by an explosive charge 15,000 meters (49,000 ft) over theGulf of Guinea.[231]The Soviet official announcement said that Zond 4 was an automated test flight which ended with its intentional destruction, due to its recovery trajectory positioning it over the Atlantic Ocean instead of over the USSR.[230]
During the summer of 1968, the Apollo program hit another snag: the first pilot-rated Lunar Module (LM) was not ready for orbital tests in time for a December 1968 launch. NASA planners overcame this challenge by changing the mission flight order, delaying the first LM flight until March 1969, and sendingApollo 8into lunar orbit without the LM in December.[232]This mission was in part motivated by intelligence rumors the Soviet Union might be ready for a piloted Zond flight in late 1968.[233]In September 1968,Zond 5made a circumlunar flight withtortoiseson board and returned safely to Earth, accomplishing the first successful water landing of the Soviet space program in the Indian Ocean.[234]It also scared NASA planners, as it took them several days to figure out that it was only an automated flight, not piloted, because voice recordings were transmitted from the craft en route to the Moon.[235]On November 10, 1968, another automated test flight,Zond 6, was launched. It encountered difficulties in Earth reentry, and depressurized and deployed its parachute too early, causing it to crash-land only 16 kilometers (9.9 mi) from where it had been launched six days earlier.[236]It turned out there was no chance of a piloted Soviet circumlunar flight during 1968, due to the unreliability of the Zonds.[237]
On December 21, 1968,Frank Borman,James Lovell, andWilliam Andersbecame the first humans to ride the Saturn V rocket into space, on Apollo 8. They also became the first to leave low-Earth orbit and go to another celestial body, entering lunar orbit on December 24.[238]They made ten orbits in twenty hours, and transmitted one of the most watched TV broadcasts in history, with theirChristmas Eve programfrom lunar orbit, which concluded with a reading from the biblicalBook of Genesis.[238]Two and a half hours after the broadcast, they fired their engine to perform the firsttrans-Earth injectionto leave lunar orbit and return to the Earth.[238]Apollo 8 safely landed in the Pacific Ocean on December 27, in NASA's first dawn splashdown and recovery.[238]
The American Lunar Module was finally ready for a successful piloted test flight in low Earth orbit onApollo 9in March 1969. The next mission,Apollo 10, conducted a "dress rehearsal" for the first landing in May 1969, flying the LM in lunar orbit as close as 47,400 feet (14.4 km) above the surface, the point where the powered descent to the surface would begin.[239]With the LM proven to work well, the next step was to attempt the landing.
Unknown to the Americans, the Soviet Moon program was in deep trouble.[237]After two successive launch failures of the N1 rocket in 1969, Soviet plans for a piloted landing suffered delay.[240]The launch pad explosion of the N-1 on July 3, 1969, was a significant setback.[241]The rocket hit the pad after an engine shutdown, destroying itself and the launch facility.[241]Without the N-1 rocket, the USSR could not send a large enough payload to the Moon to land a human and return him safely.[242]
The latter period of the space race began with the United States landing the first men on the moon, and was followed by the Soviets operating the first space stations and putting the first robotic landers on Venus and Mars, the US space shuttles marking the first significant reusable space vehicles, and a cooling down of tensions with the first docking between a Soviet and American vessel.
Apollo 11 was prepared with the goal of a July landing in theSea of Tranquility, just half a year after the first crewed flight to the Moon.[243]The crew, selected in January 1969, consisted of commander (CDR) Neil Armstrong, Command Module Pilot (CMP)Michael Collins, and Lunar Module Pilot (LMP)Edwin "Buzz" Aldrin.[244]They trained for the mission until just before the launch day.[245]On July 16, 1969, at 9:32 amEDT, the Saturn V rocket, AS-506, lifted off fromKennedy Space Center Launch Complex 39in Florida.[246]
The trip to the Moon took just over three days.[247]After achieving orbit, Armstrong and Aldrin transferred into the Lunar Module namedEagle, leaving Collins in theCommand and Service ModuleColumbia, and began their descent. Despite the interruption of alarms from an overloadedcomputercaused by an antenna switch left in the wrong position, Armstrong took over manual flight control at about 180 meters (590 ft) to correct a slight downrange guidance error, and set theEagledown on a safelanding spotat 20:18:04UTC, July 20, 1969 (3:17:04 pmCDT). Six hours later, at 02:56 UTC, July 21 (9:56 pm CDT July 20), Armstrong left theEagleto become the first human to set foot on the Moon.[248]
The first step was witnessed on live television by at least one-fifth of the population of Earth, or about 723 million people.[249]His first words when he stepped off the LM's landing footpad were, "That's one small step for [a] man, one giant leap for mankind."[248]Aldrin joined him on the surface almost 20 minutes later.[250]Altogether, they spent just under two and one-quarter hours outside their craft.[251]The next day, they performed the first crewed launch from another celestial body, and rendezvoused back with Collins inColumbia.[252]But before they return ascended the Space Race came to a particular culmination.[253]Few days before Apollo 11 left Earth the Soviet Union launched theLuna 15probe, entering lunar orbit just before Apollo 11 and eventually sharing it with Apollo 11. Aware of Luna 15, Apollo 8 astronaut Frank Borman was asked to use his goodwill contacts in the Soviet Union to prevent any collision. Subsequently, in one of the first instances of Soviet–American space communication the Soviet Union released Luna 15's flight plan to ensure it would not collide with Apollo 11, although its exact mission was not publicized.[254]But as Apollo 11 was wrapping up surface activities, the Soviet mission command hastened Luna 15 and attempted its roboticsample-return missionbefore Apollo 11 would return. As Luna 15 descended just two hours before Apollo 11's launch and impacted at 15:50 UTC some hundred kilometers away from Apollo 11, British astronomers monitoring Luna 15 recorded the situation, with one commenting:“I say, this has really been drama of the highest order”.[255]
Apollo 11 left lunar orbit and returned to Earth, landing safely in the Pacific Ocean on July 24, 1969.[256]When the spacecraft splashed down, 2,982 days had passed since Kennedy's commitment to landing a man on the Moon and returning him safely to the Earth before the end of the decade; the mission was completed with 161 days to spare.[257]With the safe completion of the Apollo 11 mission, the Americans won the race to the Moon.[258]
Armstrong and his crew became worldwide celebrities, feted with ticker-tape parades on August 13 in New York City and Chicago, attended by an estimated six million.[259][260]That evening in Los Angeles they were honored at an officialstate dinnerattended by members of Congress, 44 governors, theChief Justice of the United States, and ambassadors from 83 nations. The President and Vice president presented each astronaut with thePresidential Medal of Freedom.[259][261]The astronauts spoke before ajoint session of Congresson September 16, 1969.[262]This began a 38-day world tour to 22 foreign countries and included visits with the leaders of many countries.[263]
The public's reaction in the Soviet Union was mixed. The Soviet government limited the release of information about the lunar landing, which affected the reaction. A portion of the populace did not give it any attention, and another portion was angered by it.[264]
The first landing was followed by another, precision landing onApollo 12in November 1969, within walking distance of theSurveyor 3spacecraft which landed on April 20, 1967.
In total the Apollo programme involved six crewed Moon landings from 1969 to 1972, and a total of twelve astronauts walked on the surface of the Moon. These wereApollo 11,Apollo 12,Apollo 14,Apollo 15,Apollo 16, andApollo 17.
NASA had ambitious follow-on human spaceflight plans as it reached its lunar goal but soon discovered it had expended most of its political capital to do so.[265]A victim of its own success, Apollo had achieved its first landing goal with enough spacecraft and Saturn V launchers left for a total of ten lunar landings through Apollo 20, conducting extended-duration missions and transporting the landing crews inLunar Roving Vehicleson the last five. NASA also planned anApollo Applications Program(AAP) to develop a longer-duration Earth orbital workshop (later namedSkylab) from a spentS-IVBupper stage, to be constructed in orbit using several launches of the smallerSaturn IBlaunch vehicle.
In February 1969, PresidentRichard M. Nixonconvened a "space task group" to set recommendations for the future US civilian space program, headed by his vice president,Spiro T. Agnew.[266]Agnew was an enthusiastic proponent of NASA's follow-up plans for permanentspace stationsin Earth and lunar orbit, perhaps a base on the lunar surface, and the first human flight to Mars as early as 1986 or as late as 2000.[267]These would be serviced by an infrastructure of a reusableSpace Transportation System, including an Earth-to-orbitSpace Shuttle. Nixon had a 'better sense' of the declining political support in Congress for new Apollo-style programs, which had disappeared with the achievement of the landing, and he intended to pursue détente with the USSR and China, which he hoped might ease Cold War tensions. He cut the spending proposal he sent to Congress to include funding for only the Space Shuttle, with perhaps an option to pursue the Earth orbital space station for the foreseeable future.[268]
AAP planners decided the Earth orbital workshop could be accomplished more efficiently by prefabricating it on the ground and launching it with a single Saturn V, which immediately eliminated Apollo 20. Budget cuts soon led NASA to cut Apollo 18 and 19 as well.Apollo 13had to abort its lunar landing in April 1970 due to an in-flight spacecraft failure but returned its crew safely to Earth. The Apollo program made itsfinallunar landing in December 1972; the two unused Saturn Vs were used as outdoor visitor displays and allowed to deteriorate due to the effects of weathering.
The USSR continued trying to develop its N1 rocket, after two more launch failures in 1971 and 1972, finally canceling it in May 1974, without achieving a single successful uncrewed test flight.[269]
In late 1970Luna 16was launched by the Soviet Union, and became the first uncrewed probe to return a sample from the Moon. This was followed byLuna 20andLuna 24in subsequent years.[270][271]
The Soviet Union was also able to successfully land the first robotic rover on the moon in 1970, followed by another in 1973, with theLunokhodmissions.[272]
These missions demonstrated continued Soviet willingness to compete with the US in the space race despite having lost the manned moon landing aspect of the space race.
Having lost the race to the Moon, the USSR seemed to decide to concentrate on orbital space stations instead of pursuing a crewed lunar mission. During 1969 and 1970, they launched six more Soyuz flights after Soyuz 3 and then launched a series of six successful space stations (plus two failures to achieve orbit and one station rendered uninhabitable due to damage from explosion of the launcher's upper stage) on theirProton-Kheavy-lift launcher in theirSalyut programdesigned byKerim Kerimov. Each one weighed between 18,500 and 19,824 kilograms (40,786 and 43,704 lb), was 20 meters (66 ft) long by 4 meters (13 ft) in diameter, and had a habitable volume of 99 cubic meters (3,500 cu ft). All of the Salyuts were presented to the public as non-military scientific laboratories, but three of them were covers for militaryAlmazreconnaissance stations:Salyut 2(failed),[273]Salyut 3,[274]andSalyut 5.[275][276]
Salyut 1, the first space station, was launched by the Soviets on April 19, 1971. Three days later, theSoyuz 10crew attempted to dock with it, but failed to achieve a secure enough connection to safely enter the station. TheSoyuz 11crew ofVladislav Volkov,Georgi DobrovolskiandViktor Patsayevsuccessfully docked on June 7, and completed a then record 22-day stay. The crew became the second in-flight space fatality during their reentry on June 30, when they wereasphyxiateddue to the spacecraft's cabin becoming depressurized, shortly after undocking.[277][278]The disaster was blamed on a faulty cabin pressure valve, that allowed the air to vent into space. The crew was not wearing pressure suits and had no chance of survival once the leak occurred.[279]To prevent a recurrence of the Soyuz 11 tragedy, Soviet engineers redesigned the Soyuz spacecraft and mandated that cosmonauts wear Sokol pressure suits during launch and landing, a requirement still in place today.[280]
The United States launched a single orbital workstation,Skylab, on May 14, 1973. It was launched using a leftover Saturn-5 rocket from the Apollo programme.[280]Skylab weighed 169,950 pounds (77,090 kg), was 58 feet (18 m) long by 21.7 feet (6.6 m) in diameter, and had a habitable volume of over 10,000 cubic feet (280 m3).Skylabwas damaged during the ascent to orbit, losing one of its solar panels and a meteoroid thermal shield. Subsequent crewed missions repaired the station, and conducted valuable research. The third and final mission's crew,Skylab 4, set a human endurance record (at the time) with 84 days in orbit when the mission ended on February 8, 1974.Skylabstayed in orbit another five years before reentering the Earth's atmosphere over the Indian Ocean and Western Australia on July 11, 1979.[281]
Salyut 4brokeSkylab's occupation record at 92 days.Salyut 6andSalyut 7were second-generation stations designed for long duration, and were occupied for 683 and 816 days.Salyut 7improved upon earlier designs by allowing long-duration crewed missions and more complex experiments. These stations, with their expanded crew capacity and amenities for long term stay, carrying electric stoves, a refrigerator, and constant hot water.[282]
In 1970, the Soviet Union'sVenera 7marked the first time a spacecraft was able to return data after landing on another planet.[283]Venera 7 held a resistantthermometerand an aneroidbarometerto measure the temperature and atmospheric pressure on the surface, the transmitted data showed 475 C at the surface, and a pressure of 92 bar.[284][285][283][286]
In 1975,Venera 9established an orbit around Venus and successfully returned the first photography of the surface of Venus.[287][288]Venera 10landed on Venus and followed with further photography shortly after.[289]
NASA initiated thePioneer Venus projectin 1978, successfully deploying four small probes into the Venusian atmosphere on December 9, 1978. The probes confirmed that Venus has little if any magnetic field, and cameras detected lightning in the atmosphere. The last transmissions were received on October 8, 1992, as its decaying orbit no longer permitted communications. The spacecraft burned up the atmosphere soon after, ending a successful 14-year mission that was planned to last only eight months.[290]
In 1981,Venera 13performed a successful soft-landing on Venus and marked the first probe to drill into the surface of another planet and take a sample.[291][292]Venera 13 also took an audio sample of the Venusian environment, marking another first.[293]Venera 13 returned the first color images of the surface of Venus, revealing an orange-brown flat bedrock surface covered with looseregolithand small flat thin angular rocks.[291]Venera 14, an identical spacecraft to Venera 13, was launched 5 days apart with a similar mission profile.[294]
In total ten Venera probes achieved a soft landing on the surface of Venus.
In 1984, the SovietVega programmebegan and ended with the launch of two crafts launched six days apart,Vega 1andVega 2. Both crafts deployed a balloon in addition to a lander, marking a first in spaceflight.[295][296][297]
The US never caught up or matched the Soviet efforts to explore the surface of Venus, but did claim the title of the first successful probe to have flown by the planet and had notable success with the Pioneer atmospheric probes.
In 1971, the Soviet'sMars 2successfully established Mars orbit and attempted a soft landing but crashed, becoming the first man-made object to impact Mars. This was shortly followed byMars 3, a 358 kg lander, which successfully landed but the lander only transmitted data for 14.5 seconds before losing contact.[300]
In 1976, NASA followed suit, and put two successful landers on Mars. These wereViking 1andViking 2. These landers were significantly larger than the Soviet Mars landers (Viking 1 was 3,527 kilograms). They were able to take the first photographs from the surface of Mars.[301][302]
Viking 1 operated on the surface of Mars for around six years (On Nov 11, 1982 the Lander stopped operating after getting a faulty command) and Viking 2 for over three years (mission ended in early 1980). Both landers were equipped with a robotic sampler arm which successfully scooped up soil samples and tested them with instruments such as aGas chromatography–mass spectrometer. The landers measured temperatures ranging from negative 86 degrees Celsius before dawn to negative 33 degrees Celsius in the afternoon. Both landers had issues obtaining accurate results from theirseismometers.[302][303][304][305]
Photographs from the landers and orbiters surpassed expectations in quality and quantity. The total exceeded 4,500 from the landers and 52,000 from the orbiters.
The Viking landers recorded atmospheric pressures ranging from below 7 millibars (0.0068 bars) to over 10 millibars (0.0108 bars) over the Martian year, leading to the conclusion that atmospheric pressure varies by 30 percent during the Martian year because carbon dioxide condenses and sublimes at the polar caps. Martian winds generally blow more slowly than expected, scientists had expected them to reach speeds of several hundred miles an hour from observing global dust storms, but neither lander recorded gusts over 120 kilometers (74 miles) an hour, and average velocities were considerably lower. Nevertheless, the orbiters observed more than a dozen small dust storms. The Viking landers detectednitrogenin the atmosphere for the first time, and that it was a significant component of the Martian atmosphere. There was speculation from the atmospheric analysis that the atmosphere of Mars used to be much denser.[306][307]
The Soviets did not match the Martian lander achievements of NASA, but did claim the title of the first lander.[308]
In May 1972, President Richard M. Nixon and SovietPremierLeonid Brezhnevnegotiated an easing of relations known as détente, creating a temporary "thaw" in the Cold War. The two nations planned a joint mission to dock the last US Apollo craft with a Soyuz, known as theApollo-Soyuz Test Project(ASTP). To prepare, the US designed a docking module for the Apollo that was compatible with the Soviet docking system, which allowed any of their craft to dock with any other (e.g. Soyuz-to-Soyuz as well as Soyuz-to-Salyut). The module was also necessary as an airlock to allow the men to visit each other's craft, which had incompatible cabin atmospheres. The USSR used theSoyuz 16mission in December 1974 to test modifications of the Soyuz atmosphere and the docking adapter to prepare for ASTP.[309][310]
The joint mission began whenSoyuz 19was first launched on July 15, 1975, at 12:20UTC, and the Apollo craft was launched with the docking module six and a half hours later. The two craft rendezvoused and docked on July 17 at 16:19 UTC. The three astronauts conducted joint experiments with the two cosmonauts, and the crew shook hands, exchanged gifts, and visited each other's craft.[311]
NASA achieved the first approach and landing test of itsSpace Shuttle orbiteron aBoeing 747 carrier planeon August 12, 1977, and thefirst orbital test flightof a complete, crewed Space Shuttle, consisting of the orbiter, anexternal fuel tank, and twosolid rocket boosters, on April 12, 1981. The designers underestimated the time and cost of refurbishment between flights, which reduced the cost benefit of its reusability. They also overestimated its safety: two of the fleet of five orbiters were lost in fatal flight accidents: one during launch, due to failure of a solid rocket booster seal; and one on reentry, due to launch damage of a wing heat shield. The Air Force was also supposed to use the Shuttle to launch its military payloads, but shunned it in favor of its expendable launchers after the first Shuttle loss. NASA ceased production of its Apollo spacecraft and Saturn IB launcher, and used the Shuttle as its orbital workhorse until2011, thenretiredit due to the safety concern. Originally, more than 150 flights over a 15-year operation were expected; actually, the Shuttles made 135 flights in the 30-year lifespan of the series.[312]
The Soviets interpreted the Shuttle as a military surveillance vehicle, and decided they had to develop their own shuttle, which they namedBuran, beginning in 1974. They copied the aerodynamic design of NASA's Shuttle orbiter, which they strapped to the side of their expendable,liquid hydrogen-fueledEnergia launcher. The Buran could be fitted with fourSaturn AL-31turbofanengines and a fuel tank in its payload bay, allowing it to make its own atmospheric test flights, which began in November 1985. Also unlike the US Shuttle, it could be flown pilotlessly and landed automatically. Energia-Buran made only one orbital test flight in November 1988, but UScounterintelligencebaited the Soviets with disinformation about the heat shield design, and it was not reusable for repeated flight.[313]Buran was the largest and most expensive Soviet program in the history of the Space Race,[314]and was effectively canceled by thecollapse of the Soviet Unionin 1991, due to lack of funding. The Energia was also canceled at the same time, after only two flights.
The first woman in space was from the Soviet Union,Valentina Tereshkova. NASA did not welcome female astronauts into its corps until 1978, when six femalemission specialistswere recruited. This first class included scientistSally Ride, who became America's first woman in space onSTS-7in June 1983. NASA included women mission specialists in the next four astronaut candidate classes, and admitted female pilots starting in 1990.Eileen Collinsfrom this class became the first pilot to fly on Space Shuttle flightSTS-63in February 1995, and the first female commander of a spaceflight onSTS-93in July 1999.
The USSR admitted its first female test pilot as a cosmonaut,Svetlana Savitskaya, in 1980. She became the first female to fly since Tereshkova, onSalyut 7in December 1981.
The USSR turned its space program to the development of the low Earth orbit modular space stationMir(peaceorworld) assembled in orbit from 1986 to 1996. At 129,700 kilograms (285,900 lb), it held records for the largest spacecraft and the longest continuous human presence in space at 3,644 days, until theInternational Space Stationwas built starting in 1998.[315]Mir's operation continued after the 1991 replacement of the USSR's space program with theRussian Federal Space Agencyuntil 2001, supported by Soyuz spacecraft.
The question of who won the Space Race has sparked considerable debate among historians and analysts. The United States is widely seen as the victor due to the Apollo crewed landing and moonwalk missions, which achieved President John F. Kennedy's ambitious goal of landing a man on the Moon and returning him safely to Earth by the end of the 1960s. This achievement, completed in July 1969, marked the pinnacle of U.S. space exploration efforts of the time and was regarded by most observers as the culmination of the Space Race. Political scientist Richard J. Samuels describes Apollo 11 as a "decisive American victory."[15]
The Moon race is often analyzed as a microcosm of the Space Race's broader dynamics. Historians such as Jennifer Frost argue that if the Space Race is measured in terms of overall spaceflight capability, the Soviet Union "won it hands down."[316]Asif A. Siddiqi, a noted space historian, provides a more nuanced view, emphasizing the Soviet Union's dominance in smaller aspects of the race to the moon, yet critical, benchmarks such as the first lunar impact, first photos of the Moon's far side, first soft lunar landing, and first lunar orbit.[317]These accomplishments laid the groundwork for lunar exploration, though they are often overshadowed by the Apollo 11 mission.
Before that landing [Apollo 11], there was an enormous amount of investment in the robotic exploration of the Moon, both by the Soviets and the US, in terms of all sorts of smaller benchmarks like the first lunar impact, the first pictures of the far side of the Moon, the first soft lunar landing, and the first lunar orbit. We forget, but in those little races, the Soviet Union dominated almost every benchmark, but it is forgotten as the United States won the big one.
The Space Race was deeply intertwined with Cold War rivalries and reflected broader ideological contests between the United States and the Soviet Union. Historian Walter McDouglass highlights how space exploration served as a demonstration of each superpower's political and technological systems, with the U.S. emphasizing transparency and democratic values, and the USSR showcasing the capabilities of its centralized, state-driven model.[318][319]Asif A. Siddiqistresses the importance of viewing the Space Race as more than a single-event competition. He notes that while the U.S. achieved the symbolic "big one" with the Apollo missions, the Soviet Union's early and sustained achievements in robotic lunar and interplanetary exploration reveal the broader, multi-faceted nature of the rivalry.[317]
After the end of the Cold War in 1991, the assets of the USSR's space program passed mainly toRussia. Since then, the United States and Russia have cooperated in space with theShuttle-MirProgram, and theInternational Space Station(ISS).[320]The Russians continue to use theirR-7 rocket familyas their orbital workhorse to launch the Soyuz crewed spacecraft and itsProgressderivative uncrewed cargo craft as shuttles to the ISS. After the 2011 retirement of the Space Shuttle, American crews were dependent on the R-7–Soyuz to reach the ISS,[321]until the 2020 first flight of the USCrew DragonCommercial Crew Developmentvehicle.
In 2023 the Russian Federation resumed theLunamissions as a part of theLuna-Globprogramme with the launch ofLuna 25(47 years after the SovietLuna 24),[322]amidst American reignition of interest in the Moon with theArtemis programbeginning with the launch ofArtemis Iin 2022.[323]
Thefall of the Berlin Wall(German:Mauerfall,pronounced[ˈmaʊ̯ɐˌfal]ⓘ) on9 November1989, during thePeaceful Revolution, marked the beginning of the destruction of theBerlin Walland the figurativeIron Curtain, asEast Berlintransit restrictions were overwhelmed and discarded.  Sections of the wall were breached, and planned deconstruction began the following June. It was one of the series of events that started thefall of communisminCentral and Eastern Europe. Thefall of the inner German bordertook place shortly afterward. An end to theCold Warwas declared at theMalta Summitin early December, andGerman reunificationtook place in October the following year.
The opening of theIron Curtainbetween Austria and Hungary at thePan-European Picnicon 19 August 1989 set in motion a peaceful chain reaction, at the end of which there was no longer anEast Germanyand theEastern Blochad disintegrated. After the picnic, which was based on an idea byOtto von Habsburgto test the reaction of theUSSRandMikhail Gorbachevto an opening of the border, tens of thousands of media-informed East Germans set off for Hungary.Erich Honeckerdictated to theDaily Mirrorfor the Pan-European Picnic: "Habsburg distributed leaflets far into Poland, on which the East German holidaymakers were invited to a picnic. When they came to the picnic, they were given gifts, food andDeutsche Mark, and then they were persuaded to come to the West." The leadership of the GDR in East Berlin did not dare to completely block the borders of their own country and the USSR did not respond at all. Thus the bracket of the Eastern Bloc was broken.[2][3][4][5][6][7]
Following the summer of 1989, by early November refugees were finding their way toHungaryvia Czechoslovakia or via theWest German embassy in Prague. On 30 September, following negotiations with East Germany and the Soviet Union, the West GermanForeign MinisterHans-Dietrich Genscherwent to the Prague embassy to personally inform the thousands of refugees that they were allowed to leave for West Germany. His speech from the embassy's balcony, which included the line,"Wir sind zu Ihnen gekommen, um Ihnen mitzuteilen, dass heute Ihre Ausreise..."("We came to you, to let you know that today, your departure...") was met with loud cheers and jubilations. The next day, the first of the embassy refugees leftPragueforBavaria.[8][9]
The emigration was initially tolerated because of long-standing agreements with the communist Czechoslovak government, allowing free travel across their common border. However, this movement of people grew so large it caused difficulties for both countries. In addition, East Germany was struggling to meet loan payments on foreign borrowings;Egon KrenzsentAlexander Schalck-Golodkowskito unsuccessfully ask West Germany for a short-term loan to make interest payments.[10]: 344
The Berlin Wall was made up of two walls. Both walls were 4 metres tall and had a length of 155 kilometres. They were separated by a mined corridor called thedeath strip. This strip was heavily guarded and included 302 watchtowers (by 1989). Guards had authorisation to shoot people who attempted to go through this strip.[11][12]
On 18 October 1989, longtimeSocialist Unity Party of Germany(SED) leaderErich Honeckerstepped down in favor of Krenz. Honecker had been seriously ill, and those looking to replace him were initially willing to wait for a "biological solution", but by October were convinced that the political and economic situation was too grave.[10]: 339Honecker approved the choice, naming Krenz in his resignation speech,[13]and theVolkskammerduly elected him. Although Krenz promised reforms in his first public speech,[14]he was considered by the East German public to be following his predecessor's policies, and public protests demanding his resignation continued.[10]: 347Despite promises of reform, public opposition to the regime continued to grow.
On 1 November, Krenz authorized the reopening of the border withCzechoslovakia, which had been sealed to prevent East Germans from fleeing to West Germany.[15]On 4 November, theAlexanderplatz demonstrationtook place.[16]
On 6 November, the Interior Ministry published a draft of new travel regulations, which made cosmetic changes to Honecker-era rules, leaving the approval process opaque and maintaining uncertainty regarding access to foreign currency. The draft enraged ordinary citizens, and was denounced as "complete trash" byWest Berlin MayorWalter Momper.[17]Hundreds of refugees crowded onto the steps of the West German embassy in Prague, enraging the Czechoslovaks, who threatened to seal off the East German–Czechoslovak border.[18]
On 7 November, Krenz approved the resignation of Prime MinisterWilli Stophand two-thirds of the Politburo; however, Krenz was unanimously re-elected as General Secretary by the Central Committee.[10]: 341
On 19 October, Krenz askedGerhard Lauterto draft a new travel policy.[19]Lauter was a formerPeople's Policeofficer.  After rising rapidly through the ranks he had recently been promoted to a position with theInterior Ministry ("Home Office" / "Department of the Interior")as head of the department responsible for issuing passports and the registration of citizens.[20]
On 8 November, the East GermanPolitburoenacted a portion of the draft travel regulations addressing permanent emigration immediately. Initially, the Politburo planned to create a special border crossing nearSchirndingspecifically for this emigration.[21]However,Interior Ministry officialsandStasibureaucrats charged with drafting the new text concluded the proposal was not feasible, and instead crafted new text relating to both emigration and temporary travel. The revised text stipulated East German citizens could apply for permission to travel abroad, without having to meet the previous requirements for those trips.[22]To ease the difficulties, the Krenz-led Politburo decided on 9 November refugees could exit directly through crossing points between East Germany and West Germany, including between East and West Berlin. Later the same day, the ministerial administration modified the proposal to include private, round-trip, travel. The new regulations would take effect the next day.[23]
Zur Veränderung der Situation der ständigen Ausreise von DDR-Bürgern nach der BRD über die CSSR wird festgelegt:
1) Die Verordnung vom 30. November 1988 über Reisen von Bürgern der DDR in das Ausland (GBl. I Nr. 25 S. 271) findet bis zur Inkraftsetzung des neuen Reisegesetzes keine Anwendung mehr.
2) Ab sofort treten folgende zeitweilige Übergangsregelungen für Reisen und ständige Ausreisen aus der DDR in das Ausland in Kraft:
a. Privatreisen nach dem Ausland können ohne Vorliegen von Voraussetzungen (Reiseanlässe und Verwandtschaftsverhältnisse) beantragt werden. Die Genehmigungen werden kurzfristig erteilt. Versagungsgründe werden nur in besonderen Ausnahmefällen angewandt.
b. Die zuständigen Abteilungen Paß- und Meldewesen der VPKÄ in der DDR sind angewiesen, Visa zur ständigen Ausreise unverzüglich zu erteilen, ohne daß dafür noch geltende Voraussetzungen für eine ständige Ausreise vorliegen müssen. Die Antragstellung auf ständige Ausreise ist wie bisher auch bei den Abteilungen Innere Angelegenheiten möglich.
c. Ständige Ausreisen können über alle Grenzübergangsstellen der DDR zur BRD bzw. zu Berlin (West) erfolgen.
d. Damit entfällt die vorübergehend ermöglichte Erteilung von entsprechenden Genehmigungen in Auslandsvertretungen der DDR bzw. die ständige Ausreise mit dem Personalausweis der DDR über Drittstaaten.
3) Über die zeitweiligen Übergangsregelungen ist die beigefügte Pressemitteilung am 10. November 1989 zu veröffentlichen.
1. The decree from 30 November 1988 about travel abroad of East German citizens will no longer be applied until the new travel law comes into force.
2. Starting immediately, the following temporary transition regulations for travel abroad and permanent exits from East Germany are in effect:
a) Applications by private individuals for travel abroad can now be made without the previously existing requirements (of demonstrating a need to travel or proving familial relationships). The travel authorizations will be issued within a short period of time. Grounds for denial will only be applied in particularly exceptional cases.
b) The responsible departments of passport and registration control in thePeople's Policedistrict offices in East Germany are instructed to issue visas for permanent exit without delays and presentation of the existing requirements for permanent exit. It is still possible to apply for permanent exit in the departments for internal affairs [of the local district or city councils].
c) Permanent exits are possible via all East German border crossings to West Germany and (West) Berlin.
d) The temporary practice of issuing (travel) authorizations through East German consulates and permanent exit with only an East German personal identity card via third countries ceases.
3. The attached press release explaining the temporary transition regulation will be issued on 10 November.
Verantwortlich: 
Regierungssprecher beim Ministerrat der DDR
Wie die Presseabteilung des Ministeriums des Innern mitteilt, hat der Ministerrat der DDR beschlossen, daß bis zum Inkrafttreten einer entsprechenden gesetzlichen Regelung durch die Volkskammer folgende zeitweilige Übergangsregelung für Reisen und ständige Ausreisen aus der DDR ins Ausland in Kraft gesetzt wird:
1. Privatreisen nach dem Ausland können ohne Vorliegen von Voraussetzungen (Reiseanlässe und Verwandtschaftsverhältnisse) beantragt werden. Die Genehmigungen werden kurzfristig erteilt. Versagungsgründe werden nur in besonderen Ausnahmefällen angewandt.
2. Die zuständigen Abteilungen Paß- und Meldewesen der VPKÄ in der DDR sind angewiesen, Visa zur ständigen Ausreise unverzüglich zu erteilen, ohne daß dafür noch geltende Voraussetzungen für eine ständige Ausreise vorliegen müssen. Die Antragstellung auf ständige Ausreise ist wie bisher auch bei den Abteilungen Innere Angelegenheiten möglich.
3. Ständige Ausreisen können über alle Grenzübergangsstellen der DDR zur BRD bzw. zu Berlin (West) erfolgen.
4. Damit entfällt die vorübergehend ermöglichte Erteilung von entsprechenden Genehmigungen in Auslandsvertretungen der DDR bzw. die ständige Ausreise mit dem Personalausweis der DDR über Drittstaaten.
Responsible: Government spokesman of East Germany; Council of Ministers
As the Press Office of the Ministry of the Interior has announced, the East German Council of Ministers has decided that the following temporary transition regulation for travel abroad and permanent exit from East Germany will be effective until a corresponding law is put into effect by theVolkskammer:
1) Applications by private individuals for travel abroad can now be made without the previously existing requirements (of demonstrating a need to travel or proving familial relationships). The travel authorizations will be issued within a short period of time. Grounds for denial will only be applied in particularly exceptional cases.
2) The responsible departments of passport and registration control in the People's Police district offices in East Germany are instructed to issue visas for permanent exit without delays and without presentation of the existing requirements for permanent exit. It is still possible to apply for permanent exit in the departments for internal affairs [of the local district or city councils].
3) Permanent exits are possible via all East German border crossings to West Germany and (West) Berlin.
4) This decision revokes the temporary practice of issuing (travel) authorizations through East German consulates and permanent exit with only an East German personal identity card via third countries ceases.
The announcement of the regulations which brought down the Wall took place at an hour-long press conference led byGünter Schabowski, the outgoing party leader in East Berlin and top party spokesman as Secretary for Information, beginning at 18:00 CET on 9 November and broadcast live onEast German televisionandradio. Schabowski was joined by Minister of Foreign TradeGerhard BeilandCentral CommitteemembersHelga LabsandManfred Banaschak.[1][10]: 352
Topics of the press conference included the results of votes at the Central Committee meeting, the surprising removal of SED Bezirk First SecretariesHans-Joachim BöhmeandWerner Walde, the SED's electoral and press reform plans and the new travel regulations.
Schabowski had not been involved in the discussions about the new regulations and had not been fully updated.[26]Shortly before the press conference, he was handed a note from Krenz announcing the changes, but given no further instructions on how to handle the information. The text stipulated that East German citizens could apply for permission to travel abroad without having to meet the previous requirements for those trips, and also allowed for permanent emigration between all border crossings—including those between East and West Berlin.[22]
At 18:53, near the end of the press conference,ANSA'sRiccardo Ehrmanasked if the draft travel law of 6 November was a mistake. Schabowski gave a confusing answer that asserted it was necessary because West Germany had exhausted its capacity to accept fleeing East Germans, then remembered the note he had been given and added that a new regulation had been drafted to allow permanent emigration at any border crossing. This caused a stir in the room; amid several questions at once, Schabowski expressed surprise that the reporters had not yet seen this regulation, and started reading from the note.[1]After this, a reporter, either Ehrman orBild-Zeitungreporter Peter Brinkmann, both of whom were sitting in the front row at the press conference,[27][28][29]asked when the regulations would take effect.[1]After a few seconds' hesitation, Schabowski replied, "As far as I know, it takes effect immediately, without delay" (German:Das tritt nach meiner Kenntnis ... ist das sofort ... unverzüglich).[30][31][10]: 352This was an apparent assumption based on the note's opening paragraph; as Beil attempted to interject that it was up to theCouncil of Ministersto decide when it took effect, Schabowski proceeded to read this clause, which stated it was in effect until a law on the matter was passed by theVolkskammer. Crucially, a journalist then asked if the regulation also applied to the crossings toWest Berlin. Schabowski shrugged and read item 3 of the note, which confirmed that it did.[1][28]
After this exchange,Daniel JohnsonofThe Daily Telegraphasked what this law meant for the Berlin Wall. Schabowski sat frozen before giving a rambling statement about the Wall being tied to the larger disarmament question.[32][27]He then ended the press conference promptly at 19:00 as journalists hurried from the room.[28][1]
After the press conference, Schabowski sat for an interview withNBC NewsanchorTom Brokawin which he repeated that East Germans would be able to emigrate through the border and the regulations would go into effect immediately.[33][34]
The news began spreading immediately: the West GermanDeutsche Presse-Agenturissued a bulletin at 19:04 which reported that East German citizens would be able to cross the inner German border "immediately". Excerpts from Schabowski's press conference were broadcast on West Germany's two main news programs that night—at 19:17 onZDF'sheute, which came on the air as the press conference was ending, and as the lead story at 20:00 onARD'sTagesschau. As ARD and ZDF had broadcast tonearly all of East Germanysince the late 1950s, were far more widely viewed than the East German channels, and had become accepted by the East German authorities, this is how most of the population heard the news. Later that night, on ARD'sTagesthemen, anchormanHanns Joachim Friedrichsproclaimed, "This 9 November is a historic day. The GDR has announced that, starting immediately, its borders are open to everyone. The gates in the Wall stand open wide."[10]: 353[26]
In 2009, Ehrman claimed that a member of the Central Committee had called him and urged him to ask about the travel law during the press conference, but Schabowski called that absurd.[29]Ehrman later recanted this statement in a 2014 interview with an Austrian journalist, admitting that the caller wasGünter Pötschke, head of the East German news agencyADN, and he only asked if Ehrman would attend the press conference.[35]
Despite the policy ofstate atheismin East Germany, Christian pastorChristian Führerhad regularly met with his congregation atSt. Nicholas Churchfor prayer since 1982.[36][37]Over the next seven years the church's congregation grew, despite authorities' barricading the adjacent streets, and peaceful candlelit marches took place following its services.[36]The secret police issued death threats and even attacked some of the marchers, but the crowds still continued to gather.[36]On 9 October 1989, the police and army units were given permission to use force against those assembled, but this did not deter the church service and march from taking place, which gathered 70,000 people and in which not a single shot was fired.[36][37]
After hearing the 9 November broadcast, East Germans began gathering at the Wall, at thesix checkpoints between East and West Berlin, demanding thatborder guardsimmediately open the gates.[26]The surprised and overwhelmed guards made many hectic telephone calls to their superiors about the problem. At first, they were ordered to find the "more aggressive" people gathered at the gates and stamp their passports with a special stamp that barred them from returning to East Germany—in effect, revoking their citizenship. However, this still left thousands of people demanding to be let through "as Schabowski said we can".[10]: 353It soon became clear that no one among the East German authorities would take personal responsibility for issuing orders to use lethal force, so the vastly outnumbered soldiers had no way to hold back the huge crowd of East German citizens.Mary Elise Sarottein a 2009Washington Poststory characterized the series of events leading to the fall of the Wall as an accident, saying "One of the most momentous events of the past century was, in fact, an accident, a semicomical and bureaucratic mistake that owes as much to the Western media as to the tides of history".[26]
Finally, at 22:45 (alternatively given as 23:30) on 9 November,Harald Jäger, commander of theBornholmer Straße border crossing, yielded, allowing guards to open the checkpoints and let people through with little or no identity-checking.[38][39]As theOssisswarmed through, they were greeted byWessiswaiting with flowers and champagne amid wild rejoicing. Soon afterward, a crowd of West Berliners jumped on top of the Wall and were soon joined by East German youngsters.[40]The evening of 9 November 1989 is known as the night the Wall came down.[41]
Another border crossing to the south may have been opened earlier. An account byHeinz Schäferindicates that he also acted independently and ordered the opening of the gate at Waltersdorf-Rudow a couple of hours earlier.[42]This may explain reports of East Berliners appearing in West Berlin earlier than the opening of the Bornholmer Straße border crossing.[42]
Removal of the Wall began on the evening of 9 November 1989 and continued over the following days and weeks, with people nicknamedMauerspechte(wallpeckers) using various tools to chip off souvenirs, demolishing lengthy parts in the process, and creating several unofficial border crossings.[43]In the season holidays this became a sort of international action. People from all over the western world went to West Berlin and local youth provided a range of appropriate demolition tools.
Television coverage of citizens demolishing sections of the Wall on 9 November was soon followed by the East German regime announcing ten newborder crossings, including the historically significant locations ofPotsdamer Platz,Glienicker Brücke, andBernauer Straße. Crowds gathered on both sides of the historic crossings waiting for hours to cheer the bulldozers that tore down portions of the Wall to reconnect the divided roads. While the Wall officially remained guarded at a decreasing intensity, new border crossings continued for some time. Initially theEast German Border Troopsattempted repairing the damage done by the "wallpeckers"; gradually these attempts ceased, and guards became laxer, tolerating the increasing demolitions and "unauthorized" border crossing through the holes.[44]
TheBrandenburg Gatein the Berlin Wall was opened on 22 December 1989; on that date, West German ChancellorHelmut Kohlwalked through the gate and was greeted by East German Prime MinisterHans Modrow.[45]West Germans and West Berliners were allowed visa-free travel starting 23 December.[44]Until then, they could only visit East Germany and East Berlin under restrictive conditions that involved application for a visa several days or weeks in advance and obligatory exchange of at least 25DMper day of their planned stay, all of which hindered spontaneous visits. Thus, in the weeks between 9 November and 23 December, East Germans could actually travel more freely than Westerners.[44]
On 13 June 1990, the East German Border Troops officially began dismantling the Wall,[46][47]beginning inBernauer Straßeand around the Mitte district. From there, demolition continued through Prenzlauer Berg/Gesundbrunnen, Heiligensee and throughout the city of Berlin until December 1990. According to estimates by the border troops, a total of around 1.7 million tonnes of building rubble was produced by the demolition. Unofficially, the demolition of the Bornholmer Straße crossing began because of construction work on the railway. This involved a total of 300 GDR border guards and—after 3 October 1990—600 Pioneers of the Bundeswehr. These were equipped with 175 trucks, 65 cranes, 55 excavators and 13 bulldozers. Virtually every road that was severed by the Berlin Wall, every road that once linked from West Berlin to East Berlin, was reconstructed and reopened by 1 August 1990. In Berlin alone, 184 km (114 mi) of wall, 154 km (96 mi) border fence, 144 km (89 mi) signal systems and 87 km (54 mi) barrier ditches were removed. What remained were six sections that were to be preserved as a memorial. Various military units dismantled the Berlin/Brandenburg border wall, completing the job in November 1991. Painted wall segments with artistically valuable motifs were put up for auction in 1990 inBerlinandMonte Carlo.[44]
On 1 July 1990, the day East Germany adopted theWest German currency, allde jureborder controls ceased, although the inter-German border had become meaningless for some time before that.[48]The demolition of the Wall was completed in 1994.[46]
The fall of the Wall marked the first critical step towardsGerman reunification, which formally concluded a mere 339 days later on 3 October 1990 with the dissolution of East Germany and the official reunification of the German state along the democratic lines of the West GermanBasic Law.[43]
French PresidentFrançois Mitterrandand British Prime MinisterMargaret Thatcherboth opposed the eventual reunification of Germany, fearing potential German designs on its neighbours using its increased strength. In September 1989, Margaret Thatcher privately confided toSoviet General SecretaryMikhail Gorbachevthat she wanted the Soviet leader to do what he could to stop it.[49][50]Thatcher told Gorbachev:
We do not want a united Germany. This would lead to a change to postwar borders and we cannot allow that because such a development would undermine the stability of the whole international situation and could endanger our security.[49]
After the fall of the Berlin Wall, Mitterrand warned Thatcher that a unified Germany could make more ground thanAdolf Hitlerever had and that Europe would have to bear the consequences.[51]
On 21 November 1989,Crosby, Stills & Nashperformed the song "Chippin' Away" fromGraham Nash's 1986 solo albumInnocent Eyesin front of the Brandenburg Gate.[52]
On 25 December 1989,Leonard Bernsteingave a concert in Berlin celebrating the end of the Wall, includingBeethoven's9th symphony(Ode to Joy) with the word "Joy"(Freude)changed to "Freedom"(Freiheit)in the lyrics sung. The poetSchillermay have originally written "Freedom" and changed it to "Joy" out of fear. The orchestra and choir were drawn from both East and West Germany, as well as the United Kingdom, France, the Soviet Union, and the United States.[53]On New Year's Eve 1989,David Hasselhoffperformed his song "Looking for Freedom" while standing atop the partly demolished Wall in front of 200 000 people.[54]Roger WatersperformedthePink FloydalbumThe Walljust north ofPotsdamer Platzon 21 July 1990, with guests includingScorpions,Bryan Adams,Sinéad O'Connor,Cyndi Lauper,Thomas Dolby,Joni Mitchell,Marianne Faithfull,Levon Helm,Rick DankoandVan Morrison.[55]
Over the years, there has been a repeated controversial debate[56]as to whether9 Novemberwould make a suitable German national holiday, often initiated by former members of political opposition in East Germany, such asWerner Schulz.[57]Besides being the emotional apogee of East Germany's peaceful revolution, 9 November is also the date of the 1918 abdication of KaiserWilhelm IIand declaration of theWeimar Republic, the first German republic. However, 9 November is also the anniversary of the execution ofRobert Blumfollowing the1848 Vienna revolts, the 1923Beer Hall Putschand the infamousKristallnachtpogromsof the Nazis in 1938.NobelLaureateElie Wieselcriticised the first euphoria, noting that "they forgot that 9 November has already entered into history—51 years earlier it marked the Kristallnacht."[58]As reunification was not official and complete until 3 October (1990), that day was finally chosen asGerman Unity Day.
On 9 November 1999,  the 10th anniversary was observed with a concert and fireworks at theBrandenburg Gate. Russian cellistMstislav Rostropovichplayed music byJohann Sebastian Bach, while German rock bandScorpionsperformed their 1990 songWind of Change. Wreaths were placed for victims shot down when theyattempted to escape to the west, and politicians delivered speeches.[59][60]
On 9 November 2009, Berlin celebrated the 20th anniversary of the fall of the Berlin Wall with a "Festival of Freedom" with dignitaries from around the world in attendance for an evening celebration around the Brandenburg Gate. A high point was when over 1,000 colourfully designed foam domino tiles, each over 8 feet (2.4 m) tall, that were stacked along the former route of the Wall in the city center were toppled in stages, converging in front of the Brandenburg Gate.[61]
A Berlin Twitter Wall was set up to allow Twitter users to post messages commemorating the 20th anniversary. TheChinese governmentquickly shut down access to the Twitter Wall after internet users in China began using it to protest theGreat Firewall.[62][63][64]
In the United States, the German Embassy coordinated a public diplomacy campaign with the motto "Freedom Without Walls", to commemorate the 20th anniversary of the fall of the Berlin Wall. The campaign was focused on promoting awareness of the fall of the Berlin Wall among current college students. Students at over 30 universities participated in "Freedom Without Walls" events in late 2009. First place winner of the Freedom Without Walls Speaking Contest[65]Robert Cannon received a free trip to Berlin for 2010.[66]
An international project calledMauerreise(Journey of the Wall) took place in various countries. Twenty symbolic Wall bricks were sent from Berlin starting in May 2009, with the destinations being Korea, Cyprus, Yemen, and other places where everyday life is characterised by division and border experience. In these places, the bricks would become a blank canvas for artists, intellectuals and young people to tackle the "Wall" phenomenon.[67]
To commemorate the 20th anniversary of the fall of the Berlin Wall, the 3D online virtual worldTwinityreconstructed a true-to-scale section of the Wall in virtual Berlin.[68]TheMTV Europe Music Awards, on 5 November, hadU2andTokio Hotelperform songs dedicated to and about the Berlin Wall. U2 performed at the Brandenburg Gate, and Tokio Hotel performed "World Behind My Wall".
Palestiniansin the town ofKalandia,West Bankpulled down parts of theIsraeli West Bank barrier, in a demonstration marking the 20th anniversary of the fall of the Berlin Wall.[69]
TheInternational Spy Museumin Washington D.C., hosted aTrabantcar rally where 20 Trabants gathered in recognition of the 20th anniversary of the fall of the Berlin Wall. Rides were raffled every half-hour and a Trabant crashed through a Berlin Wall mock up. The Trabant was the East German people's car that many used to leave DDR after the collapse.[70][71]
TheAllied Museumin theDahlemdistrict of Berlin hosted a number of events to mark the 20th anniversary of the fall of the Berlin Wall. The museum held a Special Exhibition entitled "Wall Patrol – The Western Powers and the Berlin Wall 1961–1990" which focused on the daily patrols deployed by the Western powers to observe the situation along the Berlin Wall and the fortifications on theGDRborder.[72]A sheet of "Americans in Berlin" CommemorativeCinderella stampsdesigned byT.H.E. Hillwas presented to the Museum by David Guerra, Berlin veteran and webmaster of the sitewww.berlinbrigade.com.[73]
Berlin planned a week-long arts festival from 4 to 10 November 2019 and a citywide music festival on 9 November to celebrate the 30th anniversary.[74][75]On 4 November, outdoor exhibits opened at Alexanderplatz, the Brandenburg Gate, the East Side Gallery, Gethsemane Church, Kurfürstendamm, Schlossplatz, and the former Stasi headquarters in Lichtenberg.[75]
A small minority still support the wall or even support rebuilding the wall back up. In 2008 a poll found that 11% of participants from the former West Berlin and 12% from the former East Berlin said it would be better if the wall was still in place.[76]
A November 2009 poll found that 12% of Germans said the wall should be rebuilt. The poll also found that in the formerWest German statessupport was at 12% and in the formerEast German statesit was 13%. A September 2009 poll found 15% of Germans supported a wall, while in the west it was 16% and in the east it was at 10%.[77][78]
A 2010 poll from Emnid forBild, found that 24% of West Germans and 23% of East Germans wished for the wall still being in place.[79][80][81][82]
A 2011 poll fromBerliner Zeitungon the 30th anniversary, found that 8% of Berliners supported the idea if the wall was still standing, The overwhelming majority of Berliners at 87% however supported the fall of the wall. The poll also found that 28% of theAlternative for Germany(AfD) and 16% ofFree Democratic Party(FDP) supporters supported bringing back the wall.[83]A 2019 Yougov poll found that 13% of Germans wanted the wall back, in the West support was at 14% and in the East it was 13%.[84]
A 2019 poll fromForsafound 35% of Berliners thought the construction of the Wall was not so wrong with supporters of theleft partyDie Linke at 74%.[85]
External colonies were first founded in Africaduring antiquity.Ancient GreeksandRomansestablished colonies on the African continent inNorth Africa, similar to how they established settler-colonies in parts ofEurasia. Some of these endured for centuries; however, popular parlance ofcolonialismin Africa usually focuses on theEuropean conquestsofAfrican statesandsocietiesin theScramble for Africa(1884–1914) during the age ofNew Imperialism, followed by gradualdecolonisationafterWorld War II.
The principal powers involved in the modern colonisation of Africa wereBritain,France,Germany,Portugal,Spain,Belgium, andItaly.  European rule hadsignificant impacts on Africa's societiesand the suppression of communal autonomy disrupted local customary practices and caused the irreversible transformation of Africa'ssocioeconomic systems.[1]Colonies were maintained for the purpose of economic exploitation andextractionofnatural resources. In nearly all African countries today, thelanguage used in government and mediais the one used by a recent colonial power, though most people speak theirnative African languages.
In the early historical period, colonies were founded inNorth Africaby migrants from Europe and Western Asia, particularlyGreeksandPhoenecians.
UnderEgypt's PharaohAmasis(570–526 BC) a Greek mercantile colony was established atNaucratis, some 50 miles from the later Alexandria.[2]Greeks colonisedCyrenaicaaround the same time.[3]There was an attempt in 513 BC to establish a Greek colony betweenCyreneandCarthage, which resulted in the combined local and Carthaginian expulsion two years later of the Greek colonists.[4]Alexander the Great(356–323 BC) foundedAlexandriaduring his conquest of Egypt. This became one of the major cities of Hellenistic and Roman times, a trading and cultural centre as well as a military headquarters and communications hub.
Phoenicians established several colonies along the coast of North Africa. Some of these were founded relatively early. For example,Uticawas foundedc.1100 BC. Carthage, which means New City, has a traditional foundation date of 814 BC. It was established in what is nowTunisiaand became a major power in theMediterraneanby the 4th century BC. The Carthaginians sent out expeditions to explore and establish colonies along Africa's Atlantic coast. A surviving account of such is that ofHannoc.425 BC.[5]
Carthage encountered and struggled with theRomans. After the third and final war between them, theThird Punic War(150–146 BC), Rome completely destroyed Carthage. Scullard mentions plans by such asGaius Gracchusin the late 2nd century BC,Julius CaesarandAugustusin the 1st century BC to establish a new Roman colony near the same site. This was established and under Augustus served as the capital city of African continentRoman province of Africa.[6]GothicVandalsbriefly established a kingdom there in the 5th century, which shortly thereafter fell to the Romans again, this time theByzantines. The whole of Roman/Byzantine North Africaeventually fell to the Arabs in the 7th century. Arabs introduced the Arabic language andIslamin the early medieval period, while aMalayo-Polynesian-speaking group introducedMalagasyto Madagascar even earlier.
Early European expeditions concentrated on establishing coastaltrading postsas a base for trade, while also colonising previously uninhabited islands such as theCape VerdeIslands andSão Tomé Island. The Spanishcolonised the Canary Islandsoff the north African coast in the 15th century, causing the genocide of thenative Berber population.[7]
The oldest modern city founded by Europeans on the African continent isCape Town, which was founded by theDutch East India Companyin 1652, as a halfway stop for passing European ships sailing to the east.
Established empires—notably Britain, France, Spain and Portugal—had already claimed coastal areas but had not penetrated deeply inland. By 1870, Europeans controlled one tenth of Africa, primarily along the Mediterranean and in the far south. A significant early proponent of colonising inland wasKing Leopold of Belgium, who oppressed theCongo Basinas his own private domain until 1908. The 1885Berlin Conference, initiated byOtto von Bismarckto establish international guidelines and avoiding violent disputes among European Powers, formalized the "New Imperialism", driven by theSecond Industrial Revolution.[8]This allowed the imperialists to move inland, with relatively few disputes among themselves. The only serious threat of inter-Imperial violence came in theFashoda Incidentof 1898 between Britain and France; It was settled without significant military violence between the colonising countries. Between 1870 and 1914 Europe acquired almost 23,000,000 sq. km —one-fifth of the land area of the globe—to its overseas colonial possessions.
Imperialism generated self-esteem across Europe.  The Allies of World War I and World War II made extensive use of African labour and soldiers during the wars.[9][10]In terms of administrative styles, "[t]he French, the Portuguese, the Germans and the Belgians exercised a highly centralised type of administration called 'direct rule.'"[11]The British by contrast sought to rule by identifying local power holders and encouraging or forcing them to administer for the British Empire. This was indirect rule.[12]France ruled from Paris, appointing chiefs individually without considering traditional criteria, but rather loyalty to France. France established two large colonial federations in Africa,French West AfricaandFrench Equatorial Africa. France appointed the state officials, passed laws and had to approve any measures passed by colonial assemblies.
Local groups inGerman East Africaresisted German enforced labour and taxation. In theAbushiri revolt, the Germans were almost driven out of the area in 1888.[13]A decade later the colony seemed conquered, though, "It had been a long-drawn-out struggle and inland administration centres were in reality little more than a series of small military fortresses." In 1905, the Germans were astonished by the widely supportedMaji Maji Rebellion. This resistance was at first successful. However, within a year, the insurrection was suppressed by reinforcing troops armed with machine guns. German attempts to seize control in Southwest Africa also produced ardent resistance, which was very forcefully repressed leading to theHerero and Namaqua Genocide.[14]
King Leopold II of Belgiumcalled his vast private colony theCongo Free State.  His barbaric treatment of the Africans sparked a strong international protest and the European powers forced him to relinquish control of the colony to the Belgian Parliament.[15]
Vincent Khapoya notes the significant attention colonial powers paid to the economics of colonisation. This included: acquisition of land, often enforced labour, the introduction of cash crops, sometimes even to the neglect of food crops, changing inter-African trading patterns of pre-colonial times, the introduction of labourers from India, etc. and the continuation of Africa as a source of raw materials for European industry.[16]Colonial powers later focused on abolishing slavery, developing infrastructure, and improving health and education.[17][18][19]
Khapoya notes the significant resistance of powers faced to their domination in Africa. Technical superiority enabled conquest and control. Pro-independence Africans recognised the value of European education in dealing with Europeans in Africa. Some Africans established their own churches. Africans also noticed the unequal evidence of gratitude they received for their efforts to support Imperialist countries during the world wars.[20]While European-imposed borders did not correspond to traditional territories, such new territories provided entities to focus efforts by movements for increased political voice up to independence.[21]Among local groups so concerned were professionals such as lawyers and doctors, thepetite bourgeoisie(clerks, teachers, small merchants), urban workers, cash crop farmers, peasant farmers, etc. Trade unions and other initially non-political associations evolved into political movements.
While the British sought to follow a process of gradual transfer of power and thus independence, the French policy of assimilation faced some resentment, especially in North Africa.[22]The granting of independence in March 1956 to Morocco and Tunisia allowed a concentration on Algeria where there was a long (1954–62) and bloody armed struggle to achieve independence.[23]When PresidentCharles de Gaulleheld a referendum in 1958 on the issue, only Guinea voted for outright independence. Nevertheless, in 1959 France amended the constitution to allow other colonies this option.[24]
Farmers inBritish East Africawere upset by attempts to take their land and to impose agricultural methods against their wishes and experience. InTanganyika,Julius Nyerereexerted influence not only among Africans, united by the commonSwahili language, but also on some white leaders whose disproportionate voice under a racially weighted constitution was significant. He became the leader of an independent Tanganyika in 1961. In Kenya, whites had evicted African tenant farmers in the 1930s; since the 1940s there has been conflict, which intensified in 1952. By 1955, Britain had suppressed the revolt, and by 1960 Britain accepted the principle of African majority rule. Kenya became independent three years later.[25]
The main period ofdecolonisationin Africa began after World War II. Growing independence movements, indigenous political parties and trade unions coupled with pressure from within the imperialist powers and from the United States and the Soviet Union ensured the decolonisation of the majority of the continent by 1980. Some areas (in particular South Africa and Namibia) retain a large population ofEuropean descent. Only the SpanishenclavesofCeutaandMelillaare still governed by a European country. While the islands ofRéunionandMayotte,Saint Helena, Ascension, and Tristan Da Cunha, theCanary IslandsandMadeiraall remain under eitherFrench,British,Spanish, orPortuguesecontrol, the latter two of which were never part of any African polity and have overwhelmingly European populations.
The theory of colonialism addresses the problems and consequences of the colonisation of a country, and there has been much research conducted exploring these concepts.
Guyanese historian and activistWalter Rodneyproposes in his bookHow Europe Underdeveloped Africathat Africa was pillaged and plundered by the West through economic exploitation. Using a Marxist analysis, he analyses the modes of resource extraction and systematic underdevelopment of Africa by Europe. He concludes that the structure of present-day Africa and Europe can, through a comparative analysis be traced to theAtlantic slave tradeand colonialism. He includes an analysis of gender and states the rights of African women were further diminished during colonialism.[citation needed]
Mahmood Mamdaniwrote his bookCitizen and Subjectin 1996. The main point of his argument is that the colonial state in Africa took the form of a bifurcated state, "two forms of power under a single hegemonic authority".[26]The colonial state in Africa was divided into two. One state for the colonial European population and one state for the indigenous population. The colonial power was mainly in urban towns and cities and were served by elected governments. The indigenous power was found in rural villages and were ruled by tribal authority, which seemed to be more in keeping with their history and tradition. Mamdani mentions that in urban areas, native institutions were not recognised. The natives, who were portrayed as uncivilised by theEuropeans, were excluded from the rights of citizenship.[27]The division of the colonial state created a racial segregation between the European 'citizen' and African 'subject', and a division between institutions of government.
Achille Mbembeis a Cameroonian historian, political theorist, and philosopher who has written and theorized extensively on life in the colony and postcolony. His 2000 bookOn the Postcolonycritically examines postcolonial life in Africa and is an important work within the field ofpostcolonialism. It is through this examination of the postcolony that Mbembe reveals the modes through which power was exerted in colonial Africa. He reminds the reader that colonial powers demanded use of African bodies in particularly violent ways for the purpose of labor as well as the shaping of subservient colonised identities.
By comparing power in the colony and postcolony, Mbembe demonstrates that violence in the colony was exerted on African bodies largely for labor and submission.[28]European colonial powers sought natural resources in African colonies and needed the requisite labor force to extract them and simultaneously build the colonial city around these industries. Because Europeans viewed native bodies as degenerate and in need of taming, violence was necessary to create a submissive laborer.[28]Colonisers viewed this violence as necessary and good because it shaped the African into a productive worker.[28]They had the simultaneous goals of utilizing the raw labor and shaping the identity and character of the African. By beating into the African a docile nature, colonisers ultimately shaped and enforced the way Africans could move through colonial spaces.[28]The African’s day-to-day life then became a show of submission done through exercises likepublic worksprojects andmilitary conscription.[28]
Mbembe contrasts colonial violence with that of the postcolony. Mbembe demonstrates that violence in the postcolony is cruder and more generally for the purpose of demonstrating raw power. Expressions of excess and exaggeration characterize this violence.[28]Mbembe's theorization of violence in the colony illuminates the unequal relationship between the coloniser and colonised and reminds us of the violence inflicted on African bodies throughout the process of colonisation. It cannot be understood nor should be taught without the context of this violence.
Stephanie Terreni Brown is an academic in the field of colonialism. In her 2014 paper she examines how sanitation and dirt is used in colonial narratives through the example ofKampala.[29]Brown describesabjectionas the process whereby one group others or dehumanizes another. Those who are deemed abject are often avoided by others and seen as inferior.  Abjectivication is continually used as a mechanism to dominate a group of people and control them. In the case of colonialism, she argues that it is used by the west to dominate over and control the indigenous population of Africa.[29]
Abjectivication through discourses of dirt and sanitation are used to draw distinctions between the Western governing figures and the local population. Dirt being seen as something out of place, whilst cleanliness being attributed to the “in group”, the colonisers, and dirt being paralleled with the indigenous people. The reactions of disgust and displeasure to dirt and uncleanliness are often linked social norms and the wider cultural context, shaping the way in which Africa is still thought of today.[29]
Brown discusses how the colonial authorities were only concerned with constructing a working sewage system to cater for the colonials and were not concerned with the Ugandan population. This rhetoric of sanitation is important because it is seen as a key part of modernity and being civilised. The lack of sanitation and proper sewage systems symbolize that Africans are savages and uncivilised, playing a central role in how the west justified the case of the civilising process. Brown refers to this process of abjectification using discourses of dirt as a physical and material legacy of colonialism that is still very much present in Kampala and other African cities today.[29]
Critical theory on the colonisation of Africa is largely unified in a condemnation of imperial activities. Postcolonial theory has been derived from this anti-colonial/anti-imperial concept and writers such as Mbembe, Mamdani and Brown, and many more, have used it as a narrative for their work on the colonisation of Africa.
Post colonialism can be described as a powerful interdisciplinary mood in the social sciences and humanities that is refocusing attention on the imperial/colonial past, and critically revising understanding of the place of the west in the world.[30]
Postcolonial geographers are consistent with the notion that colonialism, although maybe not in such clear-cut forms, is still concurrent today. Both Mbembe, Mamdani and Brown’s theories have a consistent theme of the indigenous Africans having been treated as uncivilised, second class citizens and that in many former colonial cities this has continued into the present day with a switch from race to wealth divide.
On the Postcolonyhas faced criticism from academics such as Meredith Terreta for focusing too much on specific African nations such as Cameroon.[31]Echoes of this criticism can also be found when looking at the work of Mamdani with his theories questioned for generalising across an Africa that, in reality, was colonised in very different ways, by fundamentally different European imperial ideologies.[32]
TheIndian independence movementwas a series of historic events inSouth Asiawith the ultimate aim of endingBritish colonial rule. It lasted until 1947, when theIndian Independence Act 1947was passed.
The first nationalistic movement took root in the newly formedIndian National Congresswith prominent moderate leaders seeking the right to appear forIndian Civil Serviceexaminations in British India, as well as more economic rights for natives. The first half of the 20th century saw a more radical approach towards self-rule.
The stages of the independence struggle in the 1920s were characterised by the leadership ofMahatma Gandhiand Congress's adoption of Gandhi's policy ofnon-violenceandcivil disobedience. Some of the leading followers of Gandhi's ideology wereJawaharlal Nehru,Vallabhbhai Patel,Abdul Ghaffar Khan,Maulana Azad, and others. Intellectuals such asRabindranath Tagore,Subramania Bharati, andBankim Chandra Chattopadhyayspread patriotic awareness. Female leaders likeSarojini Naidu,Vijaya Lakshmi Pandit,Pritilata Waddedar, andKasturba Gandhipromoted the emancipation of Indian women and their participation in the freedom struggle.
Few leaders followed a more violent approach, which became especially popular after theRowlatt Act, which permittedindefinite detention. The Act sparked protests across India, especially in thePunjab Province, where they were violently suppressed in theJallianwala Bagh massacre.
The Indian independence movement was in constant ideological evolution. Essentiallyanti-colonial, it was supplemented by visions of independent, economic development with a secular, democratic, republican, and civil-libertarian political structure. After the 1930s, the movement took on a strong socialist orientation. It culminated in theIndian Independence Act 1947, which endedCrownsuzeraintyandpartitionedBritish India into theDominion of Indiaand theDominion of Pakistan. On 26 January 1950, theConstitution of Indiaestablished the Republic of India. Pakistan adopted its first constitution in 1956.[1]In 1971,East Pakistandeclaredits own independence asBangladesh.[2]
The first European to reach India via the Atlantic Ocean was the Portuguese explorerVasco da Gama, who reachedCalicutin 1498 in search of spice.[3]Just over a century later, the Dutch and English established trading outposts on the Indian subcontinent, with the first English trading post set up atSuratin 1613.[4]
Over the next two centuries, the British[note 1]defeated the Portuguese and Dutch but remained in conflict with the French. The decline of theMughal Empirein the first half of the eighteenth century allowed the British to establish a foothold in Indian politics.[5]During theBattle of Plassey, the East India Company's Army defeatedSiraj ud-Daulah, theNawab of Bengal, and the company established itself as a major player in Indian affairs. After theBattle of Buxarof 1764, it gained administrative rights overBengal,Biharand the Midnapur part ofOdisha.[6]
After the defeat ofTipu Sultan, most of southern India came either under the company's direct rule, or under its indirect political control in asubsidiary alliance. The Company subsequently seized control of regions ruled by theMaratha Empire, after defeating them in a series of wars. Much of Punjab was annexed in the year 1849, after the defeat of Sikh armies in theFirst(1845–46) andSecond(1848–49) Anglo-Sikh Wars.[7]
Maveeran Alagumuthu Konewas an early revolutionary against the British presence in Tamil Nadu. He became a military leader in the town ofEttayapuramand was defeated in battle against the British and Maruthanayagam's forces. He was executed in 1757.[8][better source needed]Puli Thevaropposed theNawab of Arcot, who was supported by the British.[9]Maruthanayagam Pillai was a commandant of the British East India Company'sMadras Army. He was born in a TamilVellalar castefamily in a village called Panaiyur inBritish India, what is now in Nainarkoil Taluk,Ramanathapuram DistrictofTamil Nadu, India. He converted to Islam and was named Muhammad Yusuf Khan. He was popularly known as Khan Sahib when he became a ruler of Madurai. He became a warrior in theArcottroops, and later a commandant for theBritish East India Companytroops. TheBritishand theArcot Nawabemployed him to suppress thePolygar(a.k.a. Palayakkarar) uprising inSouth India. Later he was entrusted to administer theMaduraicountry when theMadurai Nayakrule ended. He later fought war against the British and the Arcot Nawab. A dispute arose with the British and Arcot Nawab, and three of Khan's associates were bribed to capture him. He was captured during his morning prayer (Thozhugai) and hanged on 15 October 1764 at Sammatipuram near Madurai. Local legends state that he survived two earlier attempts at hanging, and that the Nawab feared Yusuf Khan would come back to life and so had his body dismembered and buried in different locations around Tamil Nadu. 
InEastern Indiaand across the country, Indigenous communities organised numerous uprising against the British and their fellow members, especiallylandlordsand moneylenders.[10][11][12]One of the earliest of these on record was led by Binsu Manki around 1771 over the transfer ofJharkhandto theEast India Company.[10]The Rangpur Dhing took place from 1782 to 1783 in nearbyRangpur, Bengal.[13]Following Binsu Manki's revolt in Jharkhand, numerous uprisings across the region took place, including the rebellion led byTilka Manjhiin 1784;Bhumij RevoltofManbhumfrom 1798 to 1799; theCheroUprising ofPalamuin 1800 under the leadership of Bhukan Singh, and two uprising of theMundacommunity in Tamar region, during 1807 led by Dukan Mank, and 1819–20 under the leadership Bundu and Konta.[10]The Ho Rebellion took place when theHo communityfirst came in contact with the British, from 1820 to 1821 nearChaibasaon the Roro River inWest Singhbhum, but were defeated by the technologically enhanced colonial cavalry.[14][15]A largerBhumij Revoltoccurred nearMidnapurin Bengal, under the leadership ofGanga Narain Singhwho had previously also been involved in co-leading theChuar Rebellionsin these regions from 1771 to 1809.[16]Syed Mir Nisar Ali Titumirwas an Islamic preacher who led a peasant uprising against theHindu Zamindars of Bengaland the British during the 19th century. Along with his followers, he built a bamboo fort (Bansher Kellain Bengali) in Narkelberia Village, which gained a prominent place into Bengali folk legend. After the storming of the fort by British soldiers, Titumir died of his wounds on 19 November 1831.[17]These rebellions lead to larger regional movements in Jharkhand and beyond such as theKol Insurrectionled by Singhray and Binray Manki, where theKol(Munda,Oraon,BhumijandHocommunities) united to rebel against the "outsiders" from 1830 -1833.[10][11][14][18]
TheSanthal Hulwas a movement of over 60,000Santhalsthat happened from 1855 to 1857 (but started as early as 1784) and was particularly led by siblings – brothersSidhu, Kanhu, Chand and Bhairav and their sisters Phulo and Jhano from theMurmuclan in its most fervent years that lead up to theRevolt of 1857.[19][20][21][22]More than 100 years of such escalating rebellions created grounds for a large, impactful, millenarian movement in Eastern India that again shook the foundations of British rule in the region, under the leadership ofBirsa Munda. Birsa Munda belonged to theMunda communityand lead thousands of people from Munda,Oraon, andKhariacommunities in "Ulgulaan" (revolt) against British political expansion and those who advanced it, against forceful conversions of Indigenous peoples into Christianity (even creating a Birsaite movement), and against the displacement of Indigenous peoples from their lands.[23][24][25]To subdue these rising tensions which were getting increasingly out of control of the British, they aggressively set out to search for Birsa Munda, even setting up a reward for him. They brutally attacked the Dombari Hills where Birsa had repaired a water tank and made his revolutionary headquarters between 7–9 January 1900, murdering a minimum of 400 of the Munda warriors who had congregated there, akin to the attacks on the people atJallianwallah Bagh, however, receiving much less attention.[24][26]The hills are known as "Topped Buru" today – the mound of the dead.[26]Birsa was ultimately captured in the Jamkopai forest inSinghbhum, and assassinated by the British in jail in 1900, with a rushed cremation/burial conducted to ensure his movement was subdued.[23][24][26]
The toughest resistance the Company experienced was offered by Mysore. TheAnglo-Mysore Warswere a series of wars fought in over the last three decades of the 18th century between theKingdom of Mysoreon the one hand, and the British East India Company (represented chiefly by theMadras Presidency), andMaratha Confederacyand theNizam of Hyderabadon the other.Hyder Aliand his successorTipu Sultanfought a war on four fronts with the British attacking from the west, south, and east, while the Marathas and the Nizam's forces attacked from the north. The fourth war resulted in the overthrow of the house of Hyder Ali and Tipu (who was killed in the final war, in 1799), and the dismantlement of Mysore to the benefit of the East India Company, which won and took control of much of India.[27]Pazhassi Rajawas the prince regent of the princely state ofCotiotein North Malabar, nearKannur, India between 1774 and 1805. He fought a guerrilla war with tribal people from Wynad supporting him. He was captured by the British and his fort was razed to the ground.
In 1766 theNizam of Hyderabadtransferred theNorthern Circarsto the British authority. The independent kingJagannatha Gajapati Narayan Deo IIofParalakhemundiestate situated in today'sOdishaand in the northernmost region of the then political division was continuously revolting against theFrenchoccupants since 1753 as per the Nizam's earlier handover of his estate to them on similar grounds. Narayan Deo II fought the British at Jelmur fort on 4 April 1768 and was defeated due to superior firepower of the British. He fled to the tribal hinterlands of his estate and continued his efforts against the British until his natural death on the Fifth of December 1771.
Rani Velu Nachiyar(1730–1796), was a queen ofSivagangafrom 1760 to 1790. Rani Nachiyar was trained in war match weapons usage, martial arts like Valari, Silambam (fighting using stick), horse riding and archery. She was a scholar in many languages and she had proficiency with languages like French, English, and Urdu. When her husband, Muthuvaduganathaperiya Udaiyathevar, was killed in battle with British soldiers and the forces of theNawab of Arcot, she was drawn into battle. She formed an army and sought an alliance with Gopala Nayaker andHyder Aliwith the aim of attacking the British, whom she successfully challenged in 1780. When the inventories of the Britishers were discovered, she is said to have arranged a suicide attack by a faithful follower,Kuyili, dousing herself in oil and setting herself alight and walking into the storehouse. Rani formed a women's army named "Udaiyaal" in honour of her adopted daughter, who had died detonating a British arsenal. Rani Nachiyar was one of the few rulers who regained her kingdom, and ruled it for a decade more.[28][29]
Veerapandiya Kattabommanwas an eighteenth-centuryPolygarand chieftain fromPanchalankurichiinTamil Nadu, India who waged thePolygar waragainst the East India Company. He was captured by the British and hanged in 1799 CE.[30]Kattabomman refused to accept the sovereignty of East India Company, and fought against them.[31]Dheeran Chinnamalaiwas aKongu Naduchieftain andPalayakkararfrom Tamil Nadu who fought against the East India Company.[32]AfterKattabommanand Tipu Sultan's deaths, Chinnamalai sought the help ofMarathasandMaruthu Pandiyarto attack the British atCoimbatorein 1800. The British forces managed to stop the armies of the allies, forcing Chinnamalai to attack Coimbatore on his own. His army was defeated and he escaped from the British forces. Chinnamalai engaged inguerrilla warfareand defeated the British in battles atCauveryin 1801, Odanilai in 1802 andArachalurin 1804.[33][34]
In 1804 the King ofKhordha,Kalingawas deprived of his traditional rights to theJagannathTemple. In retaliation, a group of armed Paiks attacked the British atPipili.Jayee Rajguru, the chief of Army of Kalinga requested a common alliance against the British.[35]After Rajguru's death,Bakshi Jagabandhulaunched an armed revolution against the East India Company's rule in Odisha. This is now known as thePaik Rebellion, the first uprising against the British East India Company.[36][37][38]
The Indian war of independence of 1857 was a large uprising in northern and central India against the East India Company.[39]The conditions of service in the company's army andcantonmentshad increasingly come into conflict with the religious beliefs and prejudices of thesepoys.[40]The predominance of members from the upper castes in the army, perceived loss of caste due to overseas deployment, and rumours of secret designs of the government to convert them to Christianity led to growing discontent.[41]The sepoys were also disillusioned by their low salaries and the racial discrimination practised by British officers in matters of promotion and privileges.[42]
The indifference of the British towards native Indian rulers and the annexation ofOudhfurthered dissent. TheMarquess of Dalhousie's policy of annexation, thedoctrine of lapseand the projected removal of the Mughals from their ancestral palace atRed Fortalso led to popular anger.
The final spark was provided by the rumoured use of tallow (from cows) and lard (pig fat) in the newly introducedPattern 1853 Enfieldrifle cartridges. Soldiers had to bite the cartridges with their teeth before loading them into their rifles, ingesting the fat. This was sacrilegious to both Hindus and Muslims.[43]
Mangal Pandeywas sepoy who played a key part in the events immediately preceding the outbreak of theIndian rebellion of 1857. His defiance to his British superiors led to his execution, contributing to the first outbreak atMeerut.
On 10 May 1857, the sepoys atMeerutbroke ranks and turned on their commanding officers, killing some of them. They reached Delhi on 11 May, set the company'stoll houseon fire, and marched into the Red Fort, where they asked theMughal emperor,Bahadur Shah II, to become their leader and reclaim his throne. The emperor eventually agreed and was proclaimedShahenshah-e-Hindustanby the rebels.[39]The rebels also murdered much of the European,Eurasian, and Christian population of the city, including natives who had converted to Christianity,[44]while sparing British men and women who had converted to Islam.[45]
Revolts broke out in other parts ofOudhand theNorth-Western Provincesas well, wherecivil rebellionfollowed the mutinies, leading to popular uprisings.[46]The British were initially caught off-guard and were thus slow to react, but eventually responded with force. The lack of effective organisation among the rebels, coupled with the military superiority of the British, brought an end to the rebellion.[47]The British fought the main army of the rebels near Delhi, and after prolonged fighting and a siege, defeated them and reclaimed the city on 20 September 1857.[48]Subsequently, revolts in other centres were also crushed. The last significant battle was fought inGwalioron 17 June 1858, during whichRani Lakshmibaiwas killed. Sporadic fighting andguerrilla warfare, led byTatya Tope, continued until spring 1859, but most of the rebels were eventually subdued.
The Indian Rebellion of 1857 was a turning point. While affirming the military and political power of the British,[49]it led to a significant change in how India was to be controlled by them. Under theGovernment of India Act 1858, the East India Company's territory was transferred to the British government.[50]At the apex of the new system was aCabinet minister, theSecretary of State for India, who was to be formally advised by astatutory council;[51]theGovernor-General of India(Viceroy) was made responsible to him, while he in turn was responsible to the government.
In aroyal proclamationmade to the people of India,Queen Victoriapromised equal opportunity of public service under British law, and also pledged to respect the rights of native princes.[52]The British stopped the policy of seizing land from the princes, decreedreligious toleranceand began to admit Indians into the civil service. However, they also increased the number of British soldiers in relation to native Indian ones, and allowed only British soldiers to handle artillery. Bahadur Shah II was exiled toRangoonwhere he died in 1862.
In 1876 the British Prime MinisterBenjamin Disraeliproclaimed Queen Victoria theEmpress of India. The British Liberals objected as the title was foreign to British traditions.[53]
The decades following the Rebellion were a period of growing political awareness, the manifestation of Indian public opinion and the emergence of Indian leadership at both national and provincial levels.Dadabhai Naorojiformed theEast India Associationin 1866 andSurendranath Banerjeefounded theIndian National Associationin 1876. Inspired by a suggestion made byA.O. Hume, a retired Scottish civil servant, seventy-two Indian delegates met inBombayin 1885 and founded the Indian National Congress.[55]They were mostly members of the upwardly mobile and successful western-educated provincial elites, engaged in professions such as law,teachingand journalism. At its inception, Congress had no well-defined ideology and commanded few of the resources essential to a political organisation. Instead, it functioned more as a debating society that met annually to express its loyalty to the British and passed numerous resolutions on less controversial issues such as civil rights or opportunities in government (especially in the civil service). These resolutions were submitted to the Indian government and occasionally to the British Parliament, but the Congress's early gains were slight. "Despite its claim to represent all India, the Congress voiced the interests of urban elites;[55]the number of participants from other social and economic backgrounds remained negligible.[55]However, this period of history is still crucial because it represented the first political mobilisation of Indians, coming from all parts of the subcontinent and the first articulation of the idea of India as one nation, rather than a collection of independent princely states.[55]
Religious groups played a role in reforming Indian society. These were of several religions from Hindu groups such as theArya Samaj, theBrahmo Samaj, to other religions, such as theNamdhari(orKuka) sect ofSikhism.[56]The work of men likeSwami Vivekananda,Ramakrishna,Sri Aurobindo,V. O. Chidambaram Pillai,Subramanya Bharathy,Bankim Chandra Chatterjee,Rabindranath TagoreandDadabhai Naoroji, as well as women such as the Scots–IrishSister Nivedita, spread the passion for rejuvenation and freedom. The rediscovery of India's indigenous history by several European and Indian scholars also fed into the rise of nationalism among Indians.[55]The triumvirate also is known asLal Bal Pal(Bal Gangadhar Tilak,Bipin Chandra Pal,Lala Lajpat Rai), along withV. O. Chidambaram Pillai,Sri Aurobindo,Surendranath Banerjee, andRabindranath Tagorewere some of the prominent leaders of movements in the early 20th century. TheSwadeshi movementwas the most successful. The name of Lokmanya began spreading around and people started following him in all parts of the country.
The Indian textile industry also played an important role in the freedom struggle of India. The merchandise of the textile industry pioneered the Industrial Revolution in India and soon England was producing cotton cloth in such great quantities that the domestic market was saturated, and the products had to be sold in foreign markets.
On the other hand, India was rich in cotton production and was in a position to supply British mills with the raw material they required. This was the time when India was under British rule and the East India Company had already established its roots in India. Raw materials were exported to England at very low rates while cotton cloth of refined quality was imported to India and sold at very high prices. This was draining India's economy, causing the textile industry of India to suffer greatly. This led to great resentment among cotton cultivators and traders.
After Lord Curzon announced the partition of Bengal in 1905, there was massive opposition from the people of Bengal. Initially, the partition plan was opposed through press campaign. The total follower of such techniques led to the boycott of British goods and the people of India pledged to use only swadeshi or Indian goods and to wear only Indian cloth.  Imported garments were viewed with hate. At many places, public burnings of foreign cloth were organised.  Shops selling foreign cloths were closed. The cotton textile industry is rightly described as the Swadeshi industry. The period witnessed the growth of swadeshitextile mills. Swadeshi factories came into existence everywhere.
According to Surendranath Banerji, the Swadeshi movement changed the entire texture of Indian social and domestic life. The songs composed by Rabindranath Tagore, Rajanikanta Sen and Syed Abu Mohd became the moving spirit for the nationalists. The movement soon spread to the rest of the country and the partition of Bengal had to be firmly inhaled on the first of April 1912.
By 1900, although the Congress had emerged as an all-India political organisation, it did not have the support of most Indian Muslims.[57]Attacks by Hindu reformers against religious conversion, cow slaughter, and the preservation ofUrduinArabicscript deepened their concerns of minority status and denial of rights if the Congress alone were to represent the people of India. SirSyed Ahmed Khanlaunched a movement for Muslim regeneration that culminated in the founding in 1875 of the Muhammadan Anglo-Oriental College atAligarh, Uttar Pradesh (renamedAligarh Muslim Universityin 1920). Its objective was to educate students by emphasising the compatibility of Islam with modern western knowledge.  The diversity among India's Muslims, however, made it impossible to bring about uniform cultural and intellectual regeneration.
The Hindu faction of the Independence movement was led by Nationalist leaderLokmanya Tilak, who was regarded as the "father of Indian Unrest" by the British. Along with Tilak were leaders likeGopal Krishna Gokhale, who was the inspiration, political mentor and role model ofMahatma Gandhiand inspired several other freedom activists.
Nationalistic sentiments among Congress members led to a push to be represented in the bodies of government, as well as to have a say in the legislation and administration of India. Congressmen saw themselves as loyalists, but wanted an active role in governing their own country, albeit as part of the Empire. This trend was personified byDadabhai Naoroji, who went as far as contesting, successfully, an election to theHouse of Commons of the United Kingdom, becoming its first Indian member.
Dadabhai Naorojiwas the first Indian nationalist to embraceSwarajas the destiny of the nation.[58]Bal Gangadhar Tilakdeeply opposed a British education system that ignored and defamed India's culture, history, and values. He resented the denial of freedom of expression for nationalists, and the lack of any voice or role for ordinary Indians in the affairs of their nation. For these reasons, he considered Swaraj as the natural and only solution. His popular sentence "Swaraj is my birthright, and I shall have it" became the source of inspiration for Indians.
In 1907, Congress was split into two factions: Theradicals, led byTilak, advocated civil agitation and direct revolution to overthrow the British Empire and the abandonment of all British goods. This movement gained traction and huge following of the masses in the western and eastern parts of India.  Themoderates, led by leaders like Dadabhai Naoroji andGopal Krishna Gokhale, on the other hand, wanted reform within the framework of British rule. Tilak was backed by rising public leaders likeBipin Chandra PalandLala Lajpat Rai, who held the same point of view. Under them, India's three great states –Maharashtra, Bengal andPunjabshaped the demand of the people and India's nationalism. Gokhale criticised Tilak for encouraging acts of violence and armed resistance. But the Congress of 1906 did not have public membership, and thus Tilak and his supporters were forced to leave the party.
But with Tilak's arrest, all hopes for an Indian offensive were stalled. The Indian National Congress lost credibility with the people. A Muslim deputation met with the Viceroy,Minto(1905–10), seeking concessions from the impending constitutional reforms, including special considerations in government service and electorates.  The British recognised some of theMuslim League's petitions by increasing the number of elective offices reserved for Muslims in theIndian Councils Act 1909.  The Muslim League insisted on its separateness from the Hindu-dominated Congress, as the voice of a "nation within a nation".
TheGhadar Partywas formed overseas in 1913 to fight for the Independence of India with members coming from the United States and Canada, as well as Shanghai, Hong Kong, and Singapore.[59]Members of the party aimed forHindu, Sikh, and Muslim unityagainst the British.[60]
In colonial India, theAll India Conference of Indian Christians(AICIC), which was founded in 1914, played a role in the Indian independence movement, advocating forswarajandopposing the partition of India.[61]The AICIC also was opposed to separate electorates for Christians, believing that the faithful "should participate as common citizens in the one common, national political system".[62][63]The All India Conference of Indian Christians and theAll India Catholic Unionformed a working committee with M. Rahnasamy ofAndhra Universityserving as president and B.L. Rallia Ram of Lahore serving as general secretary. In its meeting on 16 and 17 April 1947, the joint committee prepared a 13-point memorandum that was sent to theConstituent Assembly of India, which asked forreligious freedomfor both organisations and individuals; this came to be reflected in theConstitution of India.[64][63]
Thetemperance movement in Indiabecame aligned with Indian nationalism under the direction ofMahatma Gandhi, who saw alcohol as a foreign importation to the culture of the subcontinent.[65][66]
In July 1905,Lord Curzon, the Viceroy and Governor-General (1899–1905), ordered thepartition of the province of Bengal. The stated aim was to improve administration.[68]However, this was seen as an attempt to quench nationalistic sentiment throughdivide and rule.  The Bengali Hindu intelligentsia exerted considerable influence on local and national politics. The partition outraged Bengalis. Widespread agitation ensued in the streets and in the press, and the Congress advocated boycotting British products under the banner ofswadeshi, or indigenous industries. A growing movement emerged, focussing on indigenous Indian industries, finance, and education, which saw the founding ofNational Council of Education, the birth of Indian financial institutions and banks, as well as an interest in Indian culture and achievements in science and literature. Hindus showed unity by tyingRakhion each other's wrists and observingArandhan(not cooking any food). During this time, Bengali Hindu nationalists likeSri Aurobindo,Bhupendranath Datta, andBipin Chandra Palbegan writing virulent newspaper articles challenging the legitimacy of British rule in India in publications such asJugantarandSandhya, and were charged with sedition.
The Partition also precipitated increasing activity from the then still Nascent militant nationalistrevolutionary movement, which was particularly gaining strength in Bengal and Maharashtra from the last decade of the 1800s.  In Bengal,Anushilan Samiti, led by brothers Aurobindo and Barin Ghosh organised a number of attacks of figureheads of the Raj, culminating in the attempt on the life of a British judge in Muzaffarpur. This precipitated theAlipore bomb case, whilst a number of revolutionaries were killed, or captured and put on trial. Revolutionaries likeKhudiram Bose,Prafulla Chaki, Kanailal Dutt who were either killed or hanged became household names.[67]
Khudiram Bose was executed this morning;... it is alleged that he mounted the scaffold with his body erect. He was cheerful and smiling.
Jugantarwas a paramilitary organisation. Led byBarindra Ghosh, with 21 revolutionaries, includingBagha Jatin, started to collect arms and explosives and manufactured bombs.
Some senior members of the group were sent abroad for political and military training. One of them,Hemchandra Kanungoobtained his training in Paris. After returning toKolkatahe set up a combined religious school and bomb factory at a garden house inManiktalasuburb ofCalcutta. However, the attempted murder of district Judge Kingsford ofMuzaffarpurbyKhudiram BoseandPrafulla Chaki(30 April 1908) initiated a police investigation that led to the arrest of many of the revolutionaries.
Bagha Jatinwas one of the senior leaders in Jugantar. He was arrested, along with several other leaders, in connection with theHowrah-Sibpur Conspiracy case. They were tried for treason, the charge being that they had incited various regiments of the army against the ruler.[70]
Several leaders of theJugantarparty includingAurobindo Ghoshwere arrested in connection with bomb-making activities inKolkata.[71]Several of the activists were deported to theAndamanCellular Jail.
TheDelhi-Lahore Conspiracy, hatched in 1912, planned to assassinate the thenViceroy of India,Lord Hardinge, on the occasion of transferring the capital ofBritish IndiafromCalcuttato New Delhi. Involving revolutionary underground inBengaland headed byRash Behari Bosealong withSachin Sanyal, the conspiracy culminated on the attempted assassination on 23 December 1912, when the ceremonial procession moved through theChandni Chowksuburb ofDelhi. The Viceroy escaped with his injuries, along with Lady Hardinge, although theMahoutwas killed.
The investigations in the aftermath of the assassination attempt led to the Delhi Conspiracy trial.Basant Kumar Biswaswas convicted of having thrown the bomb and executed, along withAmir Chand BombwalandAvadh Beharifor their roles in the conspiracy.[72][73][74][75]
Most of the eminentJugantarleaders includingBagha JatinaliasJatindra Nath Mukherjeewho were not arrested earlier, were arrested in 1910, in connection with the murder of Shamsul Alam. Thanks to Bagha Jatin's new policy of a decentralised federated action, most of the accused were released in 1911.[76]
TheAll-India Muslim Leaguewas founded by theAll India Muhammadan Educational ConferenceatDacca(now Dhaka,Bangladesh), in 1906. Being a political party to secure the interests of the Muslim inBritish India, the Muslim League played a decisive role behind the creation of Pakistan in theIndian subcontinent.[77]
In 1916,Muhammad Ali Jinnahjoined the Indian National Congress, which was the largest Indian political organisation. Like most of the Congress at the time, Jinnah did not favour outright self-rule, considering British influences on education, law, culture, and industry as beneficial to India. Jinnah became a member of the sixty-memberImperial Legislative Council. The council had no real power or authority, and included a large number of unelected pro-Raj loyalists and Europeans. Nevertheless, Jinnah was instrumental in the passing of theChild Marriages Restraint Act, the legitimisation of the Muslimwaqf(religious endowments) and was appointed to the Sandhurst committee, which helped establish theIndian Military AcademyatDehradun.[78]During theFirst World War, Jinnah joined other Indian moderates in supporting the British war effort.
The initial response throughout India to Lord Hardinge's announcement was, for the most part, enthusiastic support. Indian princes volunteered their men, money, and personal service. Support from the Congress Party was primarily offered on the hopes that Britain would repay such loyal assistance with substantial political concessions—if not immediate independence or at least dominion status following the war, then surely its promise soon after the Allies achieved victory. Contrary to initial British fears of an Indian revolt, Indians contributed considerably to the British war effort by providing men and resources.  About 1.3 million Indian soldiers and labourers served in Europe, Africa, and the Middle East, while both the Indian government and the princes sent large supplies of food, money, and ammunition. The major threat for the British Government in South Asia came from the armed tribes in North Western frontier and Afghanistan. The source of the second potential threat for the colonial government was the Indian Muslims whom the British believed shall sympathise with the Ottoman Empire.. Nationalism in Bengal, increasingly associated with theunrest in Punjab, was of significant ferocity to almost complete the paralysis of the regional administration. Meanwhile,failed conspiracieswere triggered by revolutionaries lack of preparedness to organise a nationalist revolt.[79][80]
None of the revolutionary conspiracies made a significant impact inside India. The prospect that subversive violence would have an effect on a popular war effort drew support from the Indian population for special measures against anti-colonial activities in the form ofDefence of India Act 1915.  There were no major mutinies occurring during wartime, yet conspiracies exacerbated profound fears of insurrection among British officials, preparing them to use extreme force to frighten Indians into submission.[81]
TheHindu–German Conspiracy, was a series of plans between 1914 and 1917 by Indian nationalist groups to attempt Pan-Indian rebellion against theBritish Rajduring World War I, formulated between theIndian revolutionary undergroundand exiled or self-exiled nationalists who formed, in the United States, theGhadar Party, and in Germany, theIndian independence committee, in the decade preceding theGreat War.[82][83][84]The conspiracy was drawn up at the beginning of the war, with extensive support from theGerman Foreign Office, the German consulate in San Francisco, as well as some support fromOttoman Turkeyand theIrish republican movement. The most prominent plan attempted to foment unrest and trigger a Pan-Indian mutiny in theBritish Indian ArmyfromPunjabtoSingapore. This plot was planned to be executed in February 1915 with the aim of overthrowing British rule over theIndian subcontinent. TheFebruary mutinywas ultimately thwarted when British intelligence infiltrated theGhadaritemovement and arrested key figures. Mutinies in smaller units and garrisons within India were also crushed.
Other related events include the1915 Singapore Mutiny, theAnnie Larsen arms plot, theJugantar–German plot, theGerman mission to Kabul, the mutiny of theConnaught Rangersin India, as well as, by some accounts, theBlack Tom explosionin 1916. Parts of the conspiracy included efforts to subvert theBritish Indian Armyin theMiddle Eastern theatre of World War I.
TheGhadar Mutinywas a plan to initiate a pan-Indianmutinyin theBritish Indian Armyin February 1915 to end theBritish Rajin India. The plot originated at the onset ofWorld War I, between theGhadar Partyin the United States, theBerlin Committeein Germany, theIndian revolutionary undergroundin British India and the German Foreign Office through the consulate in San Francisco. The incident derives its name from the North AmericanGhadar Party, whose members of thePunjabi Sikhcommunity in Canada and the United States were among the most prominent participants in the plan. It was the most prominent amongst a number of plans of the much largerHindu–German Mutiny, formulated between 1914 and 1917 to initiate a Pan-Indian rebellion against theBritish Rajduring World War I.[82][83][84]The mutiny was planned to start in the key state ofPunjab, followed by mutinies in Bengal and rest of India. Indian unitsas far as Singaporewere planned to participate in the rebellion. The plans were thwarted through a coordinated intelligence and police response. British intelligence infiltrated the Ghadarite movement in Canada and in India, and last-minute intelligence from a spy helping to crush the planned uprising in Punjab before it started. Key figures were arrested, mutinies in smaller units and garrisons within India were also crushed.
Intelligence about the threat of the mutiny led to a number of important war-time measures introduced in India, including the passages ofIngress into India Ordinance, 1914, the Foreigners act 1914, and theDefence of India Act 1915. The conspiracy was followed by theFirst Lahore Conspiracy TrialandBenares Conspiracy Trialwhich saw death sentences awarded to a number of Indian revolutionaries, and exile to a number of others. After the end of the war, fear of a second Ghadarite uprising led to the recommendations of theRowlatt Actsand thence theJallianwala Bagh massacre.
The firstChristmas Day plotwas a conspiracy made by the Indian revolutionary movement in 1909: during the year-ending holidays, the Governor of Bengal organised at his residence a ball in the presence of the Viceroy, the Commander-in-Chief and all the high-ranking officers and officials of the Capital (Calcutta). The 10th Jat Regiment was in charge of the security. Indoctrinated byJatindranath Mukherjee, its soldiers decided to blow up the ballroom and take advantage of destroying the colonial Government. In keeping with his predecessor Otto (William Oskarovich) von Klemm, a friend of LokamanyaTilak, on 6 February 1910, M. Arsenyev, the Russian Consul-General, wrote to St Petersburg that it had been intended to "arouse in the country a general perturbation of minds and, thereby, afford the revolutionaries an opportunity to take the power in their hands."[85]According toR. C. Majumdar, "The police had suspected nothing and it is hard to say what the outcome would have been had the soldiers not been betrayed by one of their comrades who informed the authorities about the impending coup".[86]
The second Christmas Day plot was to initiate an insurrection inBengalinBritish Indiaduring World War I with German arms and support. Scheduled for Christmas Day, 1915, the plan was conceived and led by theJugantar groupunder the Bengali Indian revolutionary Jatindranath Mukherjee, to be coordinated with simultaneous uprising in the British colony of Burma and Kingdom ofSiamunder direction of theGhadar Party, along with a German raid on the South Indian city ofMadrasand the Britishpenal colony in Andaman Islands. The aim of the plot was to seize the Fort William, isolate Bengal and capture the capital city ofCalcutta, which was then to be used as a staging ground for a pan-Indian revolution. The Christmas Day plot wasone ofthe later plans for pan-Indian mutiny during the war that were coordinated between the Indian nationalist underground, the "Indian independence committee" set up by the Germans in Berlin, the Ghadar Party in North America, and the German Foreign office.[87]The plot was ultimately thwarted after British intelligence uncovered the plot through German and Indian double agents in Europe and Southeast Asia.
TheNiedermayer–Hentig Expeditionwas adiplomatic missiontoAfghanistansent by theCentral Powersin 1915–1916. The purpose was to encourage Afghanistan to declare full independence from theBritish Empire, enterWorld War Ion the side of the Central Powers, and attackBritish India. The expedition was part of theHindu–German Conspiracy, a series of Indo-German efforts to provoke a nationalist revolution in India. Nominally headed by the exiledIndian princeRaja Mahendra Pratap, the expedition was a joint operation ofGermanyandTurkeyand was led by the German Army officersOskar NiedermayerandWerner Otto von Hentig. Other participants included members of an Indian nationalist organisation called theBerlin Committee, includingMaulavi BarkatullahandChempakaraman Pillai, while the Turks were represented byKazim Bey, a close confidante ofEnver Pasha.
Britain saw the expedition as a serious threat. Britain and its ally, theRussian Empire, unsuccessfully attempted to intercept it inPersiaduring the summer of 1915. Britain waged a covert intelligence and diplomatic offensive, including personal interventions by theViceroyLord HardingeandKing George V, to maintain Afghan neutrality.
The mission failed in its main task of rallying Afghanistan, under EmirHabibullah Khan, to the German and Turkish war effort, but it influenced other major events. In Afghanistan, the expedition triggered reforms and drove political turmoil that culminated in the assassination of the Emir in 1919, which in turn precipitated theThird Afghan War. It influenced theKalmyk Projectof nascentBolshevik Russiato propagate socialist revolution in Asia, with one goal being the overthrow of the British Raj. Other consequences included the formation of theRowlatt Committeeto investigatesedition in Indiaas influenced by Germany and Bolshevism, and changes in the Raj's approach to the Indian independence movement immediately after World War I.
In the aftermath of the First World War, high casualty rates, soaring inflation compounded by heavy taxation, awidespread influenza pandemicand the disruption of trade during the war escalated human suffering in India.
The pre-war nationalist movement revived moderate and extremist groups within the Congress submerged their differences in order to stand together as a unified front.  They argued that their enormous services to the British Empire during the war demanded a reward to demonstrate Indian capacity for self-rule. In 1916, Congress succeeded in forging theLucknow Pact, a temporary alliance with the All India Muslim League over the issues of devolution and the future of Islam in the region.[88]
The British themselves adopted a "carrot and stick" approach in recognition of India's support during the war and in response to renewed nationalist demands. In August 1917,Edwin Montagu, Secretary of state for India, made an historic announcement in Parliament that the British policy was for: "increasing association of Indians in every branch of the administration and the gradual development of self-governing institutions with a view to the progressive realisation of responsible government in India as an integral part of the British Empire." The means of achieving the proposed measures were later enshrined in theGovernment of India Act, 1919, which introduced the principle of a dual-mode of administration, or diarchy, in which both elected Indian legislators and, appointed British officials shared power. The act also expanded the central and provincial legislatures and widened the franchise considerably. The diarchy set in motion certain real changes at the provincial level: a number of non-controversial or "transferred" portfolios, such as agriculture, local government, health, education, and public works, were handed over to Indians, while more sensitive matters such as finance, taxation, and maintaining law and order were retained by the provincial British administrators.[89]
Gandhi had been a leader of the Indian nationalist movement inSouth Africa. He had also been a vocal opponent of basic discrimination and abusive labour treatment as well as suppressive police control such as theRowlatt Acts.  During these protests, Gandhi had perfected the concept ofsatyagraha.  In January 1914 (well before the First World War began) Gandhi was successful. The legislation against Indians was repealed and all Indian political prisoners were released by GeneralJan Smuts.[90]Gandhi accomplished this through extensive use of non-violent protests, such as boycotting, protest marching, and fasting by him and his followers.[91]
Gandhi returned to India on 9 January 1915, and initially entered the political fray not with calls for a nation-state, but in support of the unified commerce-oriented territory that the Congress Party had been asking for.  Gandhi believed that the industrial development and educational development that the Europeans had brought were long required to alleviate many of India's chronic problems.Gopal Krishna Gokhale, a veteran Congressman and Indian leader, became Gandhi's mentor.  Gandhi's ideas and strategies of non-violentcivil disobedienceinitially appeared impractical to some Indians and their Congress leaders.  In the Mahatma's own words, "civil disobedience is civil breach of immoral statutory enactments."  It had to be carried out non-violently by withdrawing co-operation with the corrupt state.  Gandhi had great respect forLokmanya Tilak. His programmes were all inspired by Tilak's "Chatusutri" programme.
The positive impact of reform was seriously undermined in 1919 by theRowlatt Act, named after the recommendations made the previous year to theImperial Legislative Councilby theRowlatt Committee. The commission was set up to look into the war-time conspiracies by the nationalist organisations and recommend measures to deal with the problem in the post-war period.  Rowlatt recommended the extension of the war-time powers of theDefence of India actinto the post-war period. The war-time act had vested the Viceroy's government with extraordinary powers to quell sedition by silencing the press, detainingpolitical activistswithout trial, and arresting any individuals suspected of sedition or treason without a warrant. It was increasingly reviled within India due to widespread and indiscriminate use.  Many popular leaders, includingAnnie Besantand Ali brothers had been detained.  The Rowlatt Act was, therefore, passed in the face of universal opposition among the (non-official) Indian members in the Viceroy's council. The extension of the act drew widespread critical opposition.  A nationwide cessation of work (hartal) was called, marking the beginning of widespread, although not nationwide, popular discontent.
The agitation unleashed by the acts led to demonstrations and British repressions, culminating on 13 April 1919, in theJallianwala Bagh massacre(also known as the Amritsar Massacre) inAmritsar, Punjab. In response to agitation in Amritsar, Brigadier-GeneralReginald Dyerblocked the main, and only entrance, and ordered troops under his command to fire into an unarmed and unsuspecting crowd of some 15,000 men, women, and children. They had assembled peacefully at Jallianwala Bagh, a walled courtyard, but Dyer had wanted to execute the imposed ban on all meetings and proposed to teach all protestors a lesson the harsher way.[93]A total of 1,651 rounds were fired, killing 379 people (as according to an official British commission; Indian officials' estimates ranged as high as 1,499 and wounding 1,137 in the massacre.)[94]Dyer was forced to retire but was hailed as a hero by some in Britain, demonstrating to Indian nationalists that the Empire was beholden to public opinion in Britain, but not in India.[95]The episode dissolved wartime hopes of home rule and goodwill and opened a rift that could not be bridged short of complete self-rule.[96]
From 1920 to 1922, Gandhi started the Non-Cooperation Movement.  At the Kolkata session of the Congress in September 1920, Gandhi convinced other leaders of the need to start a non-co-operation movement in support ofKhilafatas well as for dominion status. The first satyagraha movement urged the use ofkhadiand Indian material as alternatives to those shipped from Britain. It also urged people to boycott British educational institutions and law courts, resign from government employment, refuse to pay taxes, and forsake British titles and honors. Although this came too late to influence the framing of the newGovernment of India Act 1919, the movement enjoyed widespread popular support, and the resulting unparalleled magnitude of disorder presented a serious challenge to foreign rule. However, Gandhi called off the movement because he was scared after theChauri Chaura incident, which saw the death of twenty-two policemen at the hands of an angry mob that India would descend into anarchy.
In 1920, under Gandhi's leadership, the Congress was reorganized and given a new constitution, whose goal wasswaraj. Membership in the party was opened to anyone prepared to pay a token fee, and a hierarchy of committees was established and made responsible for discipline and control over a hitherto amorphous and diffuse movement. The party was transformed from an elite organisation to one of mass national appeal.[97]
Gandhi was sentenced in 1922 to six years in prison, but was released after serving two. On his release from prison, he set up theSabarmati AshraminAhmedabad. On the banks of the riverSabarmati, he established the newspaperYoung India, introducing a series of reforms aimed at the socially disadvantaged within Hindu society — the rural poor, and theuntouchables.[98][99]This era saw the emergence of a new generation of Indians from within the Congress Party, includingMaulana Azad,C. Rajagopalachari,Jawaharlal Nehru,Vallabhbhai Patel,Subhas Chandra Boseand others- who would, later on, come to form the most prominent voices of the Indian self-rule movement, whether keeping with Gandhian Values, or, as in the case of Bose'sIndian National Army, diverging from it.
The Indian political spectrum was further broadened in the mid-1920s by the emergence of both moderate and militant parties, such as theSwaraj Party,Hindu Mahasabha,Communist Party of Indiaand theRashtriya Swayamsevak Sangh. Regional political organisations also continued to represent the interests of non-BrahminsinMadras,MaharsinMaharashtra, andSikhsin Punjab. However, people like MahakaviSubramanya Bharathi,Vanchinathan, and Neelakanda Brahmachari played a major role from Tamil Nadu in both self-rule struggle and fighting for equality for all castes and communities.  Many women participated in the movement, including Kasturba Gandhi (Gandhi's wife),Rajkumari Amrit Kaur,Muthulaxmi Reddy,Aruna Asaf Ali, and many others.
The mass movements sparked nationalist sentiment with the Indian populace and figures like Mahatma Gandhi united a nation behind his non-violence movement; philosophy and undoubtedly put crucial pressure on the British occupation. The movements failed in their primary objective, achieving independence for India, as they were often called off before they naturally concluded due to laws and punishment. While in the later years of the Raj economic factors like the reversing trade fortunes between Britain and India and the cost of fielding the Indian armed forces abroad lumped on the British taxpayer by the 1935 Government of India act, had mounting implications for British administration, united resistance further drew light on the growing disparity of the British failures to achieve solidarity over India.
On 14 July 1942 theCongress Working Committee(executive committee of Indian National Congress), whose presidentAbul Kalam Azadsupported Gandhi, passed a resolution demanding complete independence from the British government, and proposed massive civil disobedience if the British did not accede to the demands. On 8 August 1942 theQuit India Movement(Bharat Chhodo Andolan) began, a civil disobedience movement in India in response to Mahatma Gandhi's call for immediate self-rule by Indians and against sendingIndians to World War II. Other major parties rejected the Quit India plan, and most cooperated closely with the British, as did the princely states, the civil service, and the police. The Muslim League supported the Raj and grew rapidly in membership, and in influence with the British.
The British swiftly responded to theQuit India Movementwith mass arrests. Over 100,000 arrests were made, massive fines were levied, and demonstrators were subjected to public flogging. Hundreds of civilians were killed in violence many shot by the police army. Tens of thousands of leaders were also arrested and imprisoned until 1945. Ultimately, the British government realised that India was ungovernable in the long run, and the question for the postwar era became how to exit gracefully and peacefully.[100][101]
Congress leader and famous poetHasrat MohaniandCommunist Party of IndialeaderSwami Kumaranandhad demanded complete independence (Purna Swaraj) from the British in 1921 and put the resolution during an All-India Congress Forum at theAhmedabadSession ofAICC.[102]Maghfoor Ahmad Ajazisupported the 'Purna Swaraj' motion demanded by Hasrat Mohani.[103]
In 1928,India Leaguewas established byV. K. Krishna Menonin London to demand total independence from the British rule.[104][105]This organisation has been described as "the principal organisation promoting Indian nationalism in pre-war Britain".[106]
Following the rejection of theSimon Commission'srejections, an all-party conference was held atMumbaiin May 1928 to instill a sense of camaraderie. The conference appointed a committee underMotilal Nehruto create a constitution for India. The Kolkata session of the Indian National Congress asked the British government to accord India dominion status by December 1929, or face a countrywide civil disobedience movement.
Amid rising discontent and increasingly violent regional movements, a call for complete sovereignty and an end to British rule found greater support from the people.  At theLahoresession in December 1929, the Indian National Congress adopted the aim of complete self-rule. It authorised the Working Committee to launch a civil disobedience movement throughout the country.  It was decided that 26 January 1930 should be observed all over India as thePurna Swaraj(complete self-rule) Day.
TheGandhi–Irwin Pactwas signed in March 1931, and the government agreed to release political prisoners. Mahatma Gandhi managed to have over 90,000 political prisoners released under this pact.[107]Though his appeal to terminate the death sentences ofBhagat Singh,Sukhdev ThaparandShivaram Rajguruwas not accepted by the British. For the next few years, Congress and the government negotiated until theGovernment of India Act 1935emerged. The Muslim League disputed the claim of the Congress to represent all people of India, while the Congress disputed the Muslim League's claim to voice the aspirations of all Muslims.
The Civil Disobedience Movement launched a new chapter in the Indian independence movement. It did not succeed by itself, but it brought the Indian population together, under the Indian National Congress's leadership. The movement resulted in self rule being a talking point once again, and recruited more Indians to the idea. The movement allowed the Indian independence community to revive their inner confidence and strength against the British Government. In addition, the movement weakened the authority of the British and aided in the end of the British Empire in India. Overall, the civil disobedience Movement was an essential achievement in the history of Indian self-rule because it persuaded New Delhi of the role of the masses inself-determination.[108]
TheGovernment of India Act 1935, the voluminous and final constitutional effort at governingBritish India, articulated three major goals: establishing a loose federal structure, achieving provincial autonomy, and safeguarding minority interests through separate electorates. The federal provisions, intended to uniteprincely statesand British India at the centre, were not implemented because of ambiguities in safeguarding the existing privileges of princes. In February 1937, however, provincial autonomy became a reality when elections were held; the Congress emerged as the dominant party with a clear majority in five provinces and held an upper hand in two, while the Muslim League performed poorly.
In 1939, the ViceroyLinlithgowdeclared India's entrance into the Second World War without consulting provincial governments. In protest, the Congress asked all of its elected representatives to resign from the government.Muhammad Ali Jinnah, the president of theAll-India Muslim League, persuaded participants at the annual Muslim League session at Lahore in 1940 to adopt what later came to be known as theLahore Resolution, demanding the division of India into two separate sovereign states, one Muslim, the other Hindu; sometimes referred to asTwo Nation Theory. Although the idea of Pakistan had been introduced as early as 1930, very few had responded to it.
Inoppositionto the Lahore Resolution, theAll India Azad Muslim Conferencegathered in Delhi in April 1940 to voice its support for a united India.[109]Its members included several Islamic organisations in India, as well as 1400 nationalist Muslim delegates;[110][111]the "attendance at the Nationalist meeting was about five times than the attendance at the League meeting."[112]
The All-India Muslim League worked to try to silence those Muslims who stood against the partition of India, often using "intimidation and coercion".[112][111]The murder of the All India Azad Muslim Conference leaderAllah Bakhsh Soomroalso made it easier for the All-India Muslim League to demand the creation of Pakistan.[112]
There is no real connection between these two unrests, labour, and Congress opposition. But their very existence and coexistence, explains and fully justifies the attention, which Lord Irwin gave to the labour problems.[113]
Apart from a few stray struggles, revolutions against the British rulers did not occur before the beginning of the 20th century. The Indian revolutionary underground began gathering momentum through the first decade of the 20th century, with groups arising inBengal,Maharashtra,Odisha, Bihar,Uttar Pradesh,Punjab, and theMadras Presidencyincluding what is now calledSouth India. More groups were scattered around India. Particularly notable movements arose in Bengal, especially around thePartition of Bengalin 1905, and inPunjab after 1907.[114]In the former case, it was the educated, intelligent and dedicated youth of the urban middle classBhadralokcommunity that came to form the "classic" Indian revolutionary,[114]while the latter had an immense support base in the rural and military society of Punjab.
In Bengal, theAnushilan Samitiemergedfrom conglomerationsof local youth groups and gyms (Akhra) in Bengal in 1902, forming two prominent and somewhat independent arms inEastandWest Bengalidentified asDhaka Anushilan SamitiinDhaka(modern-dayBangladesh), and theJugantargroup (centred atCalcutta) respectively. Led by nationalists of the likes ofAurobindo Ghoshand his brotherBarindra Ghosh, theSamitiwas influenced by philosophies as diverse asHinduShaktaphilosophypropounded by Bengali literatureBankimandVivekananda,Italian Nationalism, andPan-AsianismofKakuzo Okakura. TheSamitiwas involved in a number of noted incidences of revolutionary terrorism against British interests and administration in India within the decade of its founding, includingearly attemptsto assassinate Raj officials whilst led by Ghosh brothers. In the meantime, in Maharashtra and Punjab arose similarly militant nationalist feelings. The District Magistrate ofNasik,A.M.T. Jacksonwas shot dead byAnant Kanherein December 1909, followed by the death ofRobert D'Escourt Asheat the hands ofVanchi Iyer.[115]
Indian nationalism made headway through Indian societies as far as Paris and London. In LondonIndia Houseunder the patronage ofShyamji Krishna Vermacame under increasing scrutiny for championing and justifying violence in the cause of Indian nationalism, which found in Indian students in Britain and from Indian expatriates inParis Indian Societyavid followers. By 1907, through Indian nationalistMadame Bhikaji Rustom Cama's links to Russian revolutionary Nicholas Safranski, Indian groups including Bengal revolutionaries as well as India House underV.D. Savarkarwere able to obtain manuals for manufacturing bombs. India House was also a source of arms and seditious literature that was rapidly distributed in India. In addition toThe Indian Sociologist, pamphlets likeBande MataramandOh Martyrs!by Savarkar extolled revolutionary violence. Direct influences and incitement from India House were noted in several incidents of political violence, including assassinations, in India at the time.[115][116][117]One of the two charges against Savarkar during his trial in Bombay was for abetting the murder of the District Magistrate of Nasik, A.M.T. Jackson, byAnant Kanherein December 1909. The arms used were directly traced through an Italian courier to India House. Ex-India House residents M.P.T. Acharya and V.V.S. Aiyar were noted in theRowlatt reportto have aided and influenced political assassinations, including the murder of Robert D'Escourt Ashe.[115]The Paris-Safranski link was strongly suggested by French police to be involved in a 1907 attempt in Bengal to derail the train carrying the Lieutenant-GovernorSir Andrew Fraser.[118]
The activities of nationalists abroad is believed to have shaken the loyalty of a number of native regiments of theBritish Indian Army.[119]The assassination ofWilliam Hutt Curzon Wylliein the hands ofMadanlal Dhingrawas highly publicised and saw increasing surveillance and suppression of Indian nationalism.[120]These were followed by the1912 attempton the life of Viceroy of India. Following this, the nucleus of networks formed inIndia House, theAnushilan Samiti, nationalists in Punjab, and the nationalism that arose among Indian expatriates and labourers in North America, a different movement began to emerge in the North AmericanGhadar Party, culminating in theSedetious conspiracyof World War I led byRash Behari BoseandLala Hardayal.
However, the emergence of the Gandhian movement slowly began to absorb the different revolutionary groups. The BengalSamitimoved away from its philosophy of violence in the 1920s, when a number of its members identified closely with theCongressand Gandhian non-violent movement. Revolutionary nationalist violence saw a resurgence after the collapse of Gandhian non-cooperation movement in 1922. In Bengal, this saw reorganisation of groups linked to theSamitiunder the leadership ofSurya SenandHem Chandra Kanungo. A spate of violence led up to the enactment of theBengal Criminal Law Amendmentin the early 1920s, which recalled the powers of incarceration and detention of the Defence of India Act. In north India, remnants of Punjab and Bengalee revolutionary organisations reorganised, notably underSachindranath Sanyal, founding theHindustan Republican AssociationwithChandrashekhar Azadin north India.
The HSRA had strong influences from leftist ideologies.Hindustan Socialist Republican Association(HSRA) was formed under the leadership ofChandrasekhar Azad.Kakori train robberywas done largely by the members of HSRA. A number of Congress leaders from Bengal, especiallySubhash Chandra Bose, were accused by the British Government of having links with and allowing patronage to the revolutionary organisations during this time. The violence and radical philosophy revived in the 1930s, when revolutionaries of theSamitiand the HSRA were involved in theChittagong armoury raidand theKakori conspiracyand other attempts against the administration in British India and Raj officials.Sachindra Nath Sanyalmentored revolutionaries in theHindustan Socialist Republican Army(HSRA), including Bhagat Singh andJatindra Nath Das, among others; including arms training and how to make bombs.[121]Bhagat SinghandBatukeshwar Duttthrew a bomb inside theCentral Legislative Assemblyon 8 April 1929 protesting against the passage of the Public Safety Bill and the Trade Disputes Bill while raising slogans of "Inquilab Zindabad", though no one was killed or injured in the bomb incident. Bhagat Singh surrendered after the bombing incident and a trial was conducted. Sukhdev and Rajguru were also arrested by police during search operations after the bombing incident. Following the trial (Central Assembly Bomb Case), Bhagat Singh,SukhdevandRajguruwere hanged in 1931.Allama MashriqifoundedKhaksar Tehreekin order to direct particularly the Muslims towards the self-rule movement.[122]Some of its members left for the Indian National Congress then led by Subhas Chandra Bose, while others identified more closely withCommunism. TheJugantarbranch formally dissolved in 1938. On 13 March 1940,Udham SinghshotMichael O'Dwyer(the last political murder outside India), generally held responsible for theAmritsar Massacre, in London. However, the revolutionary movement gradually disseminated into the Gandhian movement. As the political scenario changed in the late 1930s — with the mainstream leaders considering several options offered by the British and with religious politics coming into play — revolutionary activities gradually declined. Many past revolutionaries joined mainstream politics by joiningCongressand other parties, especiallycommunist ones, while many of the activists were kept under hold in different jails across the country. Indians who were based in the UK, joinedthe India Leagueand theIndian Workers Association, partaking in revolutionary activities in Britain.[123]
Within a short time of its inception, these organisations became the focus of an extensive police and intelligence operations. Operations againstAnushilan Samitisaw founding of theSpecial BranchofCalcutta Police. The intelligence operations against India House saw the founding of theIndian Political Intelligence Officewhich later grew to be the Intelligence Bureau in independent India. Heading the intelligence and missions against Ghadarite movement and India revolutionaries was theMI5(g)section, and at one point involved thePinkerton'sdetective agency. Notable officers who led the police and intelligence operations against Indian revolutionaries, or were involved in it, at various time includedJohn Arnold Wallinger,Sir Robert Nathan,Sir Harold Stuart,Vernon Kell,Sir Charles Stevenson-MooreandSir Charles Tegart, as well asW. Somerset Maugham. The threat posed by the activities of theSamitiin Bengal duringWorld War I, along with the threat of aGhadarite uprising in Punjab, saw the passage ofDefence of India Act 1915. These measures saw the arrest, internment, transportations, and execution of a number of revolutionaries linked to the organisation, and was successful in crushing the East Bengal Branch. In the aftermath of the war, theRowlatt committeerecommended extending the Defence of India Act (as theRowlatt act) to thwart any possible revival of theSamitiin Bengal and the Ghadarite movement in Punjab.
In the 1920s,Alluri Sitarama Rajuled the ill-fatedRampa Rebellion of 1922–24, during which a band of tribal leaders and other sympathisers fought against the British Raj.  Local people referred to him as "Manyam Veerudu" ("Hero of the Jungles").  After the passage of the 1882 Madras Forest Act, its restrictions on the free movement of tribal peoples in the forest prevented them from engaging in their traditionalpodu(Slash-and-burn) agricultural system, which involvedshifting cultivation. Raju started a protest movement in the border areas of the Godavari Agency part ofMadras Presidency(present-dayAndhra Pradesh).  Inspired by the patriotic zeal of revolutionaries in Bengal, Raju raided police stations in and aroundChintapalle,Rampachodavaram,Dammanapalli, Krishna Devi Peta,Rajavommangi,Addateegala,NarsipatnamandAnnavaram. Raju and his followers stole guns and ammunition and killed severalBritish Indian Armyofficers, including Scott Coward nearDammanapalli.[124]The British campaign lasted for nearly a year from December 1922. Raju was eventually trapped by the British in the forests of Chintapalli then tied to a tree and shot dead with a rifle.[124]
TheKallara-Pangode Strugglewas one of some 39 agitations against the Government of India.  The Home department has later notified about 38 movements/struggles across Indian territories as the ones that culminated in self-rule ended theBritish Raj.
Vanchinathan, in a letter found in his pocket, stated the following:
Themlechasof England having captured our country, tread over theSanatana Dharmaof the Hindus and destroy them. Every Indian is trying to drive out the English and getswarajyamand restore Sanatana Dharma. Our Raman, Sivaji, Krishnan, Guru Govindan, Arjuna ruled our land protecting all dharmas, but in this land, they are making arrangements to crown George V, amlecha, and one who eats the flesh of cows.Three thousandMadraseeshave taken a vow to kill George V as soon as he lands in our country. In order to make others know our intention, I who am the least in the company, have done this deed this day. This is what everyone in Hindustan should consider it as his duty.I will kill Ashe, whose arrival here is to celebrate the crowning of cow-eater King George V in this glorious land which was once ruled by greatSamrats. This I do to make them understand the fate of those who cherish the thought of enslaving this sacred land.
I, as the least of them, wish to warn George by killing Ashe.Vande Mataram. Vande Mataram. Vande Mataram
In 1937,provincial electionswere held and the Congress came to power in seven of the eleven provinces. This was a strong indicator of the Indian people's support for complete self-rule.
When the Second World War started,Viceroy Linlithgowunilaterally declared India a belligerent state on the side of Britain, without consulting the elected Indian representatives. In opposition to Linlithgow's action, the entire Congress leadership resigned from the provincial and local governments. The Muslims and Sikhs, by contrast, strongly supported the war effort and gained enormous stature in London. Defying Congress, millions of Indians supported the war effort, and indeed theBritish Indian Armybecame the largest volunteer force, numbering 2,500,000 men during the war.[125]
Especially during theBattle of Britainin 1940, Gandhi resisted calls for massive civil disobedience movements that came from within as well as outside his party, stating he did not seek India's self-rule out of the ashes of a destroyed Britain. In 1942, the Congress launched theQuit Indiamovement. There was some violence but the Raj cracked down and arrested tens of thousands of Congress leaders, including all the main national and provincial figures. They were not released until the end of the war was in sight in 1945.
The self-rule movement included theKakori conspiracy(9 August 1925) led by Indian youth under the leadership ofPandit Ram Prasad Bismiland masterminded byRajendra Lahiri; and theAzad Hindmovement, whose main protagonistNetaji Subhas Chandra Bosewas a former leader of Congress. From its earliest wartime inception, Bose joined theAxis Powersto fight Britain.
The Quit India Movement (also known asBharat Chhodo Andolan) was acivil disobediencemovement in India which commenced on 8 August 1942 in response toGandhi's call for immediate self-rule by Indians and against sending Indians to World War II. He asked all teachers to leave their schools, and other Indians to leave their respective jobs and take part in this movement. Due to Gandhi's political influence, his request was followed by a significant proportion of the population. In addition, Congress-led the Quit India Movement to demand the British to leave India and transfer the political power to a representative government.
During the movement, Gandhi and his followers continued to use non-violence against British rule. This movement was where Gandhi gave his famous message, "Do or Die!", and this message spread towards the Indian community. In addition, this movement was addressed directly to women as "disciplined soldiers of Indian freedom" and they had to keep the war for independence to go on (against British rule).
At the outbreak of war, the Congress Party had during the Wardha meeting of the working-committee in September 1939, passed a resolution conditionally supporting the fight against fascism,[126]but were rebuffed when they asked for self-rule in return. In March 1942, faced with an increasingly dissatisfied sub-continent only reluctantly participating in the war, and deteriorations in the war situation in Europe andSouth East Asia, and with growing dissatisfactions among Indian troops- especially in Europe- and among the civilian population in the sub-continent, the British government sent a delegation to India underStafford Cripps, in what came to be known as theCripps' Mission. The purpose of the mission was to negotiate with the Indian National Congress a deal to obtain total co-operation during the war, in return of progressive devolution and distribution of power from the crown and theViceroyto elected Indian legislature. However, the talks failed, having failed to address the key demand of a timeframe towards self-government, and of the definition of the powers to be relinquished, essentially portraying an offer of limited dominion-status that was wholly unacceptable to the Indian movement.[127]To force the British Raj to meet its demands and to obtain definitive word on total self-rule, the Congress took the decision to launch the Quit India Movement.
The aim of the movement was to force the British Government to the negotiating table by holding the Allied war effort hostage. The call for determined butpassive resistancethat signified the certitude that Gandhi foresaw for the movement is best described by his call toDo or Die, issued on 8 August at theGowalia TankMaidan in Bombay, since renamedAugust Kranti Maidan(August Revolution Ground). However, almost the entire Congress leadership, and not merely at the national level, was put into confinement less than 24 hours after Gandhi's speech, and the greater number of the Congress were to spend the rest of the war in jail.
On 8 August 1942, the Quit India resolution was passed at the Mumbai session of the All India Congress Committee (AICC). The draft proposed that if the British did not accede to the demands, a massive Civil Disobedience would be launched. However, it was an extremely controversial decision. At Gowalia Tank,Mumbai, Gandhi urged Indians to follow non-violent civil disobedience. Gandhi told the masses to act as citizens of a sovereign nation and not to follow the orders of the British. The British, already alarmed by the advance of the Japanese army to the India–Burma border, responded the next day by imprisoning Gandhi at theAga Khan PalaceinPune. The Congress Party's Working Committee, or national leadership was arrested all together and imprisoned at the Ahmednagar Fort. They also banned the party altogether. All the major leaders of the INC were arrested and detained. As the masses were leaderless the protest took a violent turn. Large-scale protests and demonstrations were held all over the country. Workers remained absent en masse and strikes were called. The movement also saw widespread acts of sabotage, Indian under-ground organisation carried out bomb attacks on allied supply convoys, government buildings were set on fire, electricity lines were disconnected and transport and communication lines were severed. The disruptions were under control in a few weeks and had little impact on the war effort. The movement soon became a leaderless act of defiance, with a number of acts that deviated from Gandhi's principle of non-violence. In large parts of the country, the local underground organisations took over the movement.
All the other major parties rejected the Quit India plan, and most cooperated closely with the British, as did the princely states, the civil service, and the police. TheMuslim Leaguesupported the Raj and grew rapidly in membership, and in influence with the British.[128][129]
There was opposition to the Quit India Movement from several political quarters who were fighting for Indian self-rule. Hindu nationalist parties like theHindu Mahasabhaopenly opposed the call and boycotted the Quit India Movement.[130]Vinayak Damodar Savarkar, the president of the Hindu Mahasabha at that time, even went to the extent of writing a letter titled "Stick to your Posts", in which he instructed Hindu Sabhaites who happened to be "members of municipalities, local bodies, legislatures or those serving in the army...to stick to their posts" across the country, and not to join the Quit India Movement at any cost.[130]
The other Hindu nationalist organisation, and Mahasabha affiliateRashtriya Swayamsevak Sangh(RSS) had a tradition of keeping aloof from the anti-British Indian self-rule movement since its founding byK.B. Hedgewarin 1925. In 1942, the RSS, underM.S. Golwalkarcompletely abstained from joining in the Quit India Movement as well. The Bombay government (British) appreciated the RSS as such, by noting that,
The Sangh has scrupulously kept itself within the law, and in particular, has refrained from taking part in the disturbances that broke out in August 1942.[131]
The British Government stated that the RSS was not at all supporting any civil disobedience against them, and as such their other political activities(even if objectionable) can be overlooked.[132]Further, the British Government also asserted that at Sangh meetings organised during the times of anti-British movements started and fought by the Indian National Congress,
Speakers urged the Sangh members to keep aloof from the congress movement and these instructions were generally observed.[132]
As such, the British government did not crackdown on the RSS and Hindu Mahasabha at all.
The RSS head (sarsanghchalak) during that time,M.S. Golwalkarlater openly admitted to the fact that the RSS did not participate in the Quit India Movement. However, such an attitude during the Indian independence movement also led to the Sangh being viewed with distrust and anger, both by the general Indian public, as well as certain members of the organisation itself. In Golwalkar's own words,
In 1942 also, there was a strong sentiment in the hearts of many. At that time too, the routine work of the Sangh continued. Sangh decided not to do anything directly. 'Sangh is the organisation of inactive people, their talks have no substance' was the opinion uttered not only by outsiders but also our ownswayamsevaks.[133][134]
A number of violent incidents against British officials also took place during the Quit India movement around the country. The British arrested tens of thousands of leaders, keeping them imprisoned until 1945. Ultimately, the British government realised that India was ungovernable in the long run, and the question for the postwar era became how to exit gracefully and peacefully.
India's entry into the world war was strongly opposed bySubhas Chandra Bose. Bose had been elected President of the Congress in 1938 and 1939 but later resigned owing to differences of opinion with the Congress, however he remained emotionally attached to Congress for the remainder of his life. After his resignation he formed his own wing separated from the mainstream Congress leadership known asForward blocwhich was alocifocus for ex-congress leaders holding socialist views.[135]Bose then founded theAll India Forward Bloc. In 1940 the British authorities in Calcutta placed Bose under house arrest.  However, he escaped and made his way throughAfghanistantoNazi Germanyto seekHitler and Mussolini'shelp for raising an army to fight the British.  TheFree India LegioncomprisingErwin Rommel's IndianPOWswas formed.  After a dramatic decline in Germany's military fortunes, a German land invasion of India became untenable.  Hitler advised Bose to go to Japan where a submarine was arranged to transport Bose, who was ferried to Japanese Southeast Asia, where he formed theAzad Hind Government.  The Provisional Free Indian Government in exile reorganised the Japanese collaborationit unitIndian National Armycomposed of Indian POWs and volunteer Indianexpatriatesin South-East Asia, with the help of the Japanese. Its aim was to reach India as a fighting force that would build on public resentment to inspire revolt among Indian soldiers of the Raj.
The INA was to see action against the Allies, including theBritish Indian Army, in the forests of Arakan,Burma, and inAssam, layingsiege to Imphal and Kohimawith theJapanese 15th Army. During the war, theAndaman and Nicobarislandswere captured by the Japaneseand handed over by them to the INA.
While a number of Japanese officers, even those likeFujiwara, who were devoted to the Indian cause, observed Bose as a military incompetent as well as an unrealistic and stubborn man who sees only his own needs and problems and could not observe the larger picture of the war as the Japanese had to.[136]
The INA failed owing to disrupted logistics, poor supplies from the Japanese, and lack of training.[137]The Azad Hind Fauj surrendered unconditionally to the British in Singapore in 1945.Subhas Chandra Bose's deathoccurred from third-degree burns on 18 August 1945 after his overloaded Japanese plane crashed in Japanese-ruled Formosa (now Taiwan).
Trials against members of the INAbegan in late 1945, and included the infamous joint court-martial of key figuresShah Nawaz KhanandPrem Sahgal. A number of Congress members includingTej Bahadur Sapru,Aruna Asaf AliandJawaharlal Nehruplayed a significant role in getting INA members released.[138]
TheRoyal Indian Navy Mutinywas a failed insurrection which encompassed atotal strikeand subsequentmutinyby Indian sailors of the Royal Indian Navy on board ship and shore establishments at Bombay (Mumbai) harbour on 18 February 1946. From the initial flashpoint in Bombay, the mutiny spread and found support throughoutBritish India, fromKarachitoCalcuttaand ultimately came to involve 78 ships, 20 shore establishments and 10,000 sailors.[139][140]
The agitations, mass strikes, demonstrations and consequently support for the mutineers, therefore continued several days even after the mutiny had been called off. Along with this, the assessment may be made that it described in crystal clear terms to the government that theBritish Indian Armed forcescould no longer be universally relied upon for support in crisis, and even more it was more likely itself to be the source of the sparks that would ignite trouble in a country fast slipping out of the scenario of political settlement.[141]
The mutiny ended with the surrender of revolting the sailors to British officials. Congress and the Muslim League had convinced Indian sailors to surrender. They condemned the mutiny due to the political and military risks of unrest.
On 3 June 1947, ViscountLouis Mountbatten, the last BritishGovernor-General of India, announced the partitioning of British India intoIndiaandPakistan. With the speedy passage of theIndian Independence Act 1947, at 11:57 on14 August 1947Pakistan was declared a separate nation. Then at 12:02 A.M., on15 August 1947India became a sovereign and democratic nation. Eventually, 15 August became Independence Day for India marking the end of British India.  Also on 15 August, both Pakistan and India had the right to remain in or remove themselves from theBritish Commonwealth.
Violent clashes between Hindus, Sikhs, and Muslims followed. Prime MinisterJawaharlal Nehruand deputy prime ministerVallabhbhai Patelhad invited Mountbatten to continue asGovernor General of Indiaduring the period of transition. He was replaced in June 1948 byRajagopalachari. In May 1947, Nehru declared that anyprincely statewhich refused to join theConstituent Assemblywould be treated as an enemy state. Patel took on the responsibility for bringing princely states into the Union of India, steering efforts by his "iron fist in a velvet glove" policies. India used military force to integrateJunagadh,Hyderabad State(Operation Polo) andKashmir(Instrument of Accession) to India.[142]
TheConstituent Assembly, headed by the prominent lawyer, reformer and Dalit leader,B.R. Ambedkarwas tasked heading the creation of the constitution of independent India. The Constituent Assembly completed the work of drafting the constitution on 26 November 1949; on 26 January 1950, theRepublic of Indiawas officially proclaimed. The Constituent Assembly electedRajendra Prasadwas the firstPresident of India, taking over from Governor General Rajgopalachari.  Subsequently, the French cededChandernagorein 1951, andPondichéryand its remaining Indian colonies by 1954. Indian troopsannexed Goaand Portugal's otherIndian enclavesin 1961, andSikkimvotedto join the Indian Union in 1975 after theIndian victory over Chinain Nathu La and Cho La.
Following self-rule in 1947, India remained in theCommonwealth of Nations, andrelations between the UK and Indiahave since become friendly. There are many areas in which the two countries seek stronger ties for mutual benefit, and the two nations also have strong cultural and social ties. The UK has an ethnic Indian population of over 1.6 million. In 2010, Prime MinisterDavid Camerondescribed Indian – British relations as a "NewSpecial Relationship".[143]
TheByzantine Empire, also known as theEastern Roman Empire, was the continuation of theRoman Empirecentred onConstantinopleduringlate antiquityand theMiddle Ages. Having survivedthe eventsthat caused thefall of the Western Roman Empirein the 5th centuryAD, it endured until thefall of Constantinopleto theOttoman Empirein 1453. The term 'Byzantine Empire' was coined only after its demise; its citizens used the term 'Roman Empire' and called themselves 'Romans'.[a]
During the early centuries of the Roman Empire, the western provinces wereLatinised, but the eastern parts kept theirHellenistic culture.Constantine I(r.324–337) legalisedChristianityand moved the capital to Constantinople.Theodosius I(r. 379–395) made Christianity thestate religionand Greek gradually replaced Latin for official use. The empire adopted a defensive strategy and, throughout its remaining history, experienced recurring cycles of decline and recovery.
It reached its greatest extent during the reign ofJustinian I(r. 527–565), who briefly reconquered much of Italy and the westernMediterranean coast. Aplague beganaround 541, and adevastating war with Persiadrained the empire's resources. TheArab conquestsled to the loss of the empire's richest provinces—EgyptandSyria—to theRashidun Caliphate. In 698, Africawas lostto theUmayyad Caliphate, but the empire stabilised under theIsaurian dynasty. It expanded once more under theMacedonian dynasty, experiencinga two-century-long renaissance. Thereafter, periods of civil war and Seljuk incursion resulted in the loss of most ofAsia Minor. The empire recovered during theKomnenian restoration, and Constantinople remained the largest and wealthiest city in Europe until the 13th century.
The empire was largely dismantled in 1204, following thesack of Constantinopleduring theFourth Crusade; its former territorieswere then dividedinto competing Greekrump statesandLatin realms. Despite the eventualrecovery of Constantinoplein 1261, the reconstituted empire wielded only regional power during its final two centuries. Its remaining territories were progressively annexed by the Ottomans ina series of warsfought in the 14th and 15th centuries. The fall of Constantinople to the Ottomans in 1453 brought the empire to an end, but its history and legacy remain topics of debate to this day.
The empire's inhabitants, now generally termed "Byzantines", regarded themselves asRomans(in Greek,ῬωμαῖοιorRomaioi).[6]Similarly, their Islamic contemporaries called their empire the "land of the Romans" (Bilād al-Rūm).[7]However, Western Europe since 800 AD called them "Greeks" (Graeci), as the Papacy and medieval German emperors regarded themselves as the true inheritors of Roman identity.[8]The adjective "Byzantine", derived fromByzantion(Byzantiumin Latin), the name of the Greek settlementConstantinoplewas established on, was only used to describe the inhabitants of the city; it did not refer to the empire, calledRomanía(Ῥωμανίαor "Romanland") by its citizens.[9]
Following the empire's fall,early modernscholars referred to it by many names, including the "Eastern Empire",  the "Low Empire",  the "Late Empire",  the "Empire of the Greeks",  "Empire of Constantinople", and "Roman Empire".[10]The increasing use of "Byzantine" and "Byzantine Empire" started with the 15th-century historianLaonikos Chalkokondyles, whose works were widely propagated byHieronymus Wolf.[11]"Byzantine" was used adjectivally alongside terms such as "Empire of the Greeks" until the 19th century.[12]It is now the primary term, used to refer to all aspects of the empire; some modern historians believe it should not be used because it was originally a prejudicial and inaccurate term.[13]
Given the significant overlap in historiographicalperiodisationsof "Late Roman history", "late antiquity", and "Byzantine history", there is no consensus on a "foundation date" for the Byzantine Empire, or even if one existed. Scholarship with links to Greece orEastern Orthodoxyhas customarily placed it in the early 300s.[14]The growth of the study of "late antiquity" has led to some historians setting a start date in the seventh or eighth centuries.[15]Others believe a "new empire" began during changesc.300AD.[16]Geoffrey Greatrex believes that it is impossible to precisely date the foundation of the Byzantine Empire.[17]
Between the 3rd and 1st centuriesBC, theRoman Republicestablishedhegemonyover theeastern Mediterranean, whileits governmentdeveloped into the one-person rule ofan emperor.[18]TheRoman Empireenjoyed a period ofrelative stabilityuntilthe 3rd century AD, when external threats and internal crises caused it to splinter, as regional armies acclaimed their generals as "soldier-emperors".[19]One of these,Diocletian(r.284–305), recognised that the state was too big to be ruled by a single person.[18]He instituted theTetrarchy, a system which divided the empire into eastern and western halves.[20]Although the Tetrarchy quickly failed, the division of the empire proved an enduring concept.[21]
Constantine I(r. 306–337) secured absolute power in 324.[22]Over the next six years, he rebuilt the city of Byzantium as a newcapitalthat he called “New Rome” (later namedConstantinople).[23]The old capitalRomewas further from the prosperous eastern provinces and in a less strategically important location; it was not esteemed by the "soldier-emperors", who ruled from the frontiers, or by the empire's population.[24]Having been granted citizenship, they considered themselves just as Roman as those in the city of Rome.[25]He continued reforms of the empire's military and civil administration and instituted thegold solidusas a stable currency.[26]Hefavoured Christianityand became an opponent of paganism.[27]Constantine's dynasty prioritiseda lengthy conflictagainst the comparably powerfulSasanid Persiaand ended in 363 with the death of his nephewJulian.[28]The reign of the shortValentinianic dynasty, marked bywars against the Goths, religious debates, and anti-corruption campaigns, ended in the East with the death ofValensat theBattle of Adrianoplein 378.[29]
Valens's successor,Theodosius I(r. 379–395), secured peace in the east by allowing theGothsto settle in Roman territory;[30]he also twice intervened in the western half, defeating the usurpersMagnus MaximusandEugeniusin 388 and 394, respectively.[31]Heactively condemned paganism, confirmed the primacy ofNicene OrthodoxyoverArianisminthe East, and establishedChristianity as the Roman state religion.[32]He was the last emperor to rule both the western and eastern halves of the empire.[33]After his death, the West was destabilised but the East thrived due to its civilian administrators that continued to hold power.[34]Theodosius II(r. 408–450) largely left the rule of the East to officials such asAnthemius, who constructed theTheodosian Walls.[35]Constantinople had now firmly entrenched itself as the empire's capital.[36]
Aside from Constantinople’s walls, Theodosius' reign was also marked by the compilation of theCodex Theodosianus[37]and the theological dispute overNestorianism(a doctrine eventually deemedheretical).[38]His reign also saw the arrival ofAttila'sHuns, who ravaged theBalkans, leading to a largetributebeing exacted from the eastern empire.[39]Attila switched his attention to therapidly-deteriorating western empire,[40]and his people fractured after his death in 453.[41]Later,Leo I(r. 457–474) failed in his468 attempt to reconquertheWest.[42]The warlordOdoacerdeposedRomulus Augustulusin 476, killed his titular successorJulius Neposin 480, and abolished the office of western emperor.[43]
Through a combination of fortune and good political decisions, the Eastern Empire never experienced rebellious barbarian vassals or rule by barbarian warlords—the problems which ensured the downfall of the West.[44]Zeno(r. 474–491) convinced the problematicOstrogothkingTheodoricto take control of Italy from Odoacer;[45]dying when the empire was at peace, he was succeeded byAnastasius I(r. 491–518).[46]His belief inmonophysitismbrought occasional issues, but Anastasius was a capable administrator and instituted successful financial reforms including the abolition of thechrysargyron tax.[47]He was the first emperor since Diocletian who did not face any serious problems affecting the empire during his reign.[48]
The reign ofJustinian Iwas a watershed in Byzantine history.[49]Following his accession in 527, the legal code was rewritten as theCorpus Juris Civilisand Justinian produced extensive legislation on provincial administration;[50]he reasserted imperial control over religion and morality through purges of non-Christians and "deviants";[51]and having ruthlessly subduedthe 532 Nika revolthe rebuilt much of Constantinople, including the originalHagia Sophia.[52]Justinian I took advantage of political instability in Italy to attempt the reconquest of lost western territories. TheVandal Kingdomin North Africawas subjugated in 534by the generalBelisarius, whothen invaded Italy; theOstrogothic Kingdomwas destroyed in 554.[53]
In the 540s, Justinian began to suffer reversals on multiple fronts. Capitalising on Constantinople's preoccupation with the West,Khosrow Iof the Sasanian Empire invaded Byzantine territory and sackedAntiochin 540.[54]The emperor's internal reforms and policies began to falter, anda devastating plaguekilled a large proportion of the population and severely reduced the empire's social and financial stability.[55]The most difficult period of the Ostrogothic war, against their kingTotila, came during this decade, while divisions among Justinian's advisors undercut the administration's response.[56]He also did not fully heal the divisions inChalcedonian Christianity, as theSecond Council of Constantinoplefailed to make a real difference.[57]Justinian died in 565; his reign was more successful than any other Byzantine emperor, yet he left his empire under serious strain.[58]
Financially and territorially overextended,Justin II(r.565–578) was soon at war on many fronts. Fearing the aggressiveAvars, theLombardsconquered much of northern Italy by 572.[59]TheSasanian wars restartedin the same year, and continued until the emperorMauriceemerged victorious in 591; by this time, the Avars andSlavs had repeatedly invaded the Balkans, causing great instability.[60]Mauricecampaigned extensively in the regionduring the 590s, and although he re-established Byzantine control up to theDanube, he pushed his troops too far in 602—they mutinied, proclaimed an officer namedPhocasas emperor, and executed Maurice.[61]The Sasanians seized their moment andreopened hostilities; Phocas was unable to cope and soon faceda major rebellionled byHeraclius. Phocas lost Constantinople in 610 and was executed; this destructive civil war accelerated the empire's decline.[62]
UnderKhosrow II, the Sassanids occupied theLevantand Egypt and advanced into Asia Minor, while Byzantine control of Italy weakened, and the Avars and Slavs raided in the Balkans.[63]Although Heraclius repelleda siege of Constantinoplein 626 anddefeated the Sassanidsin 627, this was apyrrhic victory.[64]TheArab conquestssoon saw theconquest of the Levant,Egypt, andthe Sassanid Empireby the newly-formed ArabicRashidun Caliphate.[65]By Heraclius' death in 641, the empire had been severely reduced economically and territorially—the loss of the wealthy eastern provinces had deprived Constantinople of three-quarters of its revenue.[66]
The next seventy-five years are poorly documented.[67]Arab raids into Asia Minorbegan almost immediately, and the Byzantines responded by holding fortified centres and avoiding battle wherever possible; although Anatolia was invaded annually, it avoided permanent Arab occupation.[68]The outbreak of theFirst Fitnain 656 gave Byzantium breathing space, which it used wisely: some order was restored in the Balkans byConstans II(r. 641–668)[69]following his administrative reorganisation known as the "theme system", which allocated troops to defend specific provinces.[70]Constantine IV(r. 668–685) repelled the Arab efforts tocapture Constantinople in the 670susingGreek fire,[71]but suffereda reversalagainst theBulgars, who soon establishedan empire in the northern Balkans.[72]Nevertheless, he and Constans had done enough to secure the empire's position, especially as theUmayyad Caliphatewas undergoinganother civil war.[73]
Justinian IIsought to build on the stability established by his father Constantine, but was overthrown in 695 after attempting to exact too much from his subjects; over the next twenty-two years, there were six more rebellions duringan era of political instability.[74]The reconstituted caliphate sought to break ByzantiThe reign ofJustinian Iwas a watershed in Byzantine history.[49]Following his accession in 527, the legal code was rewritten as theCorpus Juris Civilisand Justinian produced extensive legislation on provincial administration;[50]he reasserted imperial control over religion and morality through purges of non-Christians and "deviants";[51]and having ruthlessly subduedthe 532 Nika revolthe rebuilt much of Constantinople, including the originalHagia Sophia.[52]Justinian I took advantage of political instability in Italy to attempt the reconquest of lost western territories. TheVandal Kingdomin North Africawas subjugated in 534by the generalBelisarius, whothen invaded Italy; theOstrogothic Kingdomwas destroyed in 554.[53]
In the 540s, Justinian began to suffer reversals on multiple fronts. Capitalising on Constantinople's preoccupation with the West,Khosrow Iof the Sasanian Empire invaded Byzantine territory and sackedAntiochin 540.[54]The emperor's internal reforms and policies began to falter, anda devastating plaguekilled a large proportion of the population and severely reduced the empire's social and financial stability.[55]The most difficult period of the Ostrogothic war, against their kingTotila, came during this decade, while divisions among Justinian's advisors undercut the administration's response.[56]He also did not fully heal the divisions inChalcedonian Christianity, as theSecond Council of Constantinoplefailed to make a real difference.[57]Justinian died in 565; his reign was more successful than any other Byzantine emperor, yet he left his empire under serious strain.[58]
Financially and territorially overextended,Justin II(r.565–578) was soon at war on many fronts. Fearing the aggressiveAvars, theLombardsconquered much of northern Italy by 572.[59]TheSasanian wars restartedin the same year, and continued until the emperorMauriceemerged victorious in 591; by this time, the Avars andSlavs had repeatedly invaded the Balkans, causing great instability.[60]Mauricecampaigned extensively in the regionduring the 590s, and although he re-established Byzantine control up to theDanube, he pushed his troops too far in 602—they mutinied, proclaimed an officer namedPhocasas emperor, and executed Maurice.[61]The Sasanians seized their moment andreopened hostilities; Phocas was unable to cope and soon faceda major rebellionled byHeraclius. Phocas lost Constantinople in 610 and was executed; this destructive civil war accelerated the empire's decline.[62]
UnderKhosrow II, the Sassanids occupied theLevantand Egypt and advanced into Asia Minor, while Byzantine control of Italy weakened, and the Avars and Slavs raided in the Balkans.[63]Although Heraclius repelleda siege of Constantinoplein 626 anddefeated the Sassanidsin 627, this was apyrrhic victory.[64]TheArab conquestssoon saw theconquest of the Levant,Egypt, andthe Sassanid Empireby the newly-formed ArabicRashidun Caliphate.[65]By Heraclius' death in 641, the empire had been severely reduced economically and territorially—the loss of the wealthy eastern provinces had deprived Constantinople of three-quarters of its revenue.[66]
The next seventy-five years are poorly documented.[67]Arab raids into Asia Minorbegan almost immediately, and the Byzantines responded by holding fortified centres and avoiding battle wherever possible; although Anatolia was invaded annually, it avoided permanent Arab occupation.[68]The outbreak of theFirst Fitnain 656 gave Byzantium breathing space, which it used wisely: some order was restored in the Balkans byConstans II(r. 641–668)[69]following his administrative reorganisation known as the "theme system", which allocated troops to defend specific provinces.[70]Constantine IV(r. 668–685) repelled the Arab efforts tocapture Constantinople in the 670susingGreek fire,[71]but suffereda reversalagainst theBulgars, who soon establishedan empire in the northern Balkans.[72]Nevertheless, he and Constans had done enough to secure the empire's position, especially as theUmayyad Caliphatewas undergoinganother civil war.[73]
Juum by taking Constantinople, but the newly crownedLeo IIIrepelled the 717–718 siege, the first major setback of the Muslim conquests.[75]
Leo and his sonConstantine V(r.741–775), two of the most capable Byzantine emperors, withstood continued Arab attacks, civil unrest, and natural disasters, and reestablished the state as a major regional power.[76]Leo's reign produced theEcloga, a new code of law to succeed that of Justinian I.[77]He also continued to reform the theme system in order to lead offensive campaigns against the Muslims, culminating ina decisive victory in 740.[78]Constantine overcame an early civil war against his brother-in-lawArtabasdos, made peace with the newAbbasid Caliphate,campaigned successfullyagainst the Bulgars, and continued to make administrative and military reforms.[79]Due to both emperors' support for theByzantine Iconoclasm, where the use ofreligious iconswas banned, they were later vilified by Byzantine historians;[80]Constantine's reign also saw the loss ofRavennato theLombards, and the beginning of a split from theRoman papacy.[81]
In 780, EmpressIreneassumed power as regent for her sonConstantine VI.[82]Although she was a capable administrator who temporarily resolved the iconoclasm controversy,[83]the empire was destabilised by her conflict with her son. The Bulgars and Abbasids inflicted numerous defeats on the Byzantine armies, and the papacy crownedCharlemagneas Roman emperor in 800.[84]In 802, the unpopular Irene was overthrown byNikephoros I; he reformed the empire's administration but diedin battle against the Bulgarsin 811.[85]Military defeats and societal disorder, especially the resurgence of iconoclasm, characterised the next eighteen years.[86]
Stability was somewhat restored during the reign ofTheophilos(r. 829–842). He capitalised on economic growth to complete construction programmes, including rebuilding thesea walls of Constantinople, overhaul provincial governance, and wage inconclusive campaigns against the Abbasids.[87]After his death, his empressTheodora, ruling on behalf of her sonMichael III, permanently extinguished the iconoclastic movement;[88]the empire prospered under their sometimes-fraught rule. Michael was posthumously vilified by historians loyal to the dynasty of his successorBasil I, who had him assassinated in 867 and was credited with his predecessor's achievements.[89]
Basil I (r.867–886) continued Michael's policies.[90]His armies campaigned with mixed results in Italy butdefeatedthePaulicians of Tephrike.[91]His successorLeo VI(r. 886–912)[b]compiled and propagated a huge number of written works. These included theBasilika, a Greek translation of Justinian I's legal code incorporating over 100 new laws created by Leo; theTactica, a military treatise; and theBook of the Eparch, a manual on Constantinople's trading regulations.[93]In non-literary contexts Leo was less successful: the empirelost in Sicilyandagainst the Bulgarians,[94]and he provoked theological scandal by marrying four times in an attempt to father a legitimate heir.[95]
The early reign of this heir,Constantine VII, was tumultuous, as his motherZoe, his uncleAlexander, the patriarchNicholas, the powerfulSimeon I of Bulgaria, and other influential figures jockeyed for power.[96]In 920, the admiralRomanos Iused his fleet to secure power, crowning himself and demoting Constantine to the position of junior co-emperor.[97]His reign, marked bythe end of the war against Bulgariaand successes in the east under the generalJohn Kourkouas, ended in 944 due to the machinations of his sons, whom Constantine then usurped.[98]Constantine's ineffectual sole rule has often been construed asthe zenith of Byzantine learning, but the works compiled were largely intended to legitimise and glorify the emperor'sMacedonian dynasty.[99]His son and successordied young; under two soldier-emperors,Nikephoros II(r. 963–969) andJohn I Tzimiskes(r. 969–976), the army claimed numerous military successes, including theconquest of CiliciaandAntioch, and asensational victory against Bulgaria and the Kievan Rus'in 971. John in particular was an astute administrator who reformed military structures and implemented effective fiscal policies.[100]
After John's death, Constantine VII's grandsonsBasil IIandConstantine VIIIruled jointly for half a century, although the latter exercised no real power.[101]Their early reign was occupied by conflicts against two prominent generals,Bardas SklerosandBardas Phokas, which ended in 989 after the former's death and the latter's submission, and a power struggle against the eunuchBasileios, who was dismissed in 985.[102]Basil, who never married or had children, subsequently refused to delegate any authority: he sidelined the military establishment by taking personal command of the army and promoting officers loyal to him.[103]His reign witnessedthe decades-long campaign against Bulgaria, which ended in total Byzantine victory at theBattle of Kleidionin 1014.[104]Diplomatic efforts, critical for this success,[105]also contributed to theannexation of several Georgian provincesin the 1020s and coexistence with the newFatimid Caliphate.[106]When he died in 1025, Basil's empire stretched from the Danube and Sicily in the west to theEuphratesin the east; his swift expansion was unaccompanied by administrative reforms.[107]
After Constantine VIII's death in 1028, his daughters, the empressesZoe(r. 1028–1052) andTheodora(r. 1042–1056), held the keys to power: four emperors (Romanos III,Michael IV,Michael V, andConstantine IX) ruled only because of their connection to Zoe, whileMichael VI(r. 1056–1057) was selected by Theodora.[108]This political instability, regular budget deficits, a series of expensive military failures, and other problems connected to over-extension led to substantial issues in the empire;[109]its strategic focus moved from maintaining its hegemony to prioritising defence.[110]
The empire soon came under sustained assault on three fronts, from theSeljuk Turks in the east, thePecheneg nomadsin the north, and theNormans in the west. The Byzantine army struggled to confront these enemies, who did not organise themselves as traditional states, and were thus untroubled by defeats in set-piece battles.[111]In 1071Bari, the last remaining Byzantine settlement in Italy, wascaptured by the Normans, while the Seljuks won a decisive victory at theBattle of Manzikert, taking the emperorRomanos IV Diogenesprisoner.[112]The latter event sparked a decade-long civil war, and as a result the Seljuks took possession of Anatolia up to theSea of Marmara.[113]
One prominent general,Alexios I, usurped the throne in 1081. In contrast to the prior turmoil, the three reigns of Alexios (r.1081–1118), his sonJohn II(r. 1118–1143), and his grandsonManuel I(r. 1143–1180) lasted a century andrestored the empire's regional authorityfor the final time.[114]Alexios immediately faced the Normans underRobert Guiscardandrepelled them through warfare and diplomacy.[115]He then targeted the Pechenegs anddecisively defeated them in 1091with help from theCumans, who were in turn defeated three years later.[116]Finally, looking to recover Asia Minor from the Seljuks, he approachedPope Urban IIfor helpc.1095. He did not anticipate the scale of western Christendom's response—theFirst Crusadeled to the recapture of western Anatolia, although Alexios and its leaders soon fell out.[117]The rest of his reign was spentdealing with the Normansand Seljuks, establishing a new, loyal aristocracy to ensure stability, and carrying out fiscal and ecclesiastical reforms.[118]
Alexios' concentration of power in the hands of hisKomnenos dynastymeant the most serious political threats came from within the imperial family—before his coronation, John II had to overcomehis mother Ireneandhis sister Anna, and the primary threat during his reign washis brother Isaac.[119]John campaigned annually and extensively—he fought the Pechenegs in 1122, theHungarians in the late 1120s, and the Seljuks throughout his reign, waginglarge campaigns in Syriain his final years—but he did not achieve large territorial gains.[120]In 1138, John raised the imperial standard over the CrusaderPrincipality of Antiochto intimidate the city into allying with the Byzantines, but did not attack, fearing that it would provoke western Christendom to respond.[121]
Manuel I used his father's overflowing imperial treasury in pursuit of his ambitions, and also to secure the empire's position in an increasingly multilateral geopolitical landscape.[122]Through a combination ofdiplomacy and bribery, he cultivated a ring of allies and clients around the empire: the Turks of theSultanate of Rum, theKingdom of Hungary, theCilician Armenians, Balkan princes, Italian and Dalmatian cities, and most importantly Antioch and theCrusader States, marryingone of their princessesin 1161.[123]Manuel averted the threat of war during the tumultuous passage of theSecond Crusadethrough Byzantine territories in 1147, but the campaign's failure was blamed on the Byzantines by western contemporaries.[124]He was less successful militarily: an invasion ofSicilywas decisively defeated byKing William Iin 1156, leading to tensions withFrederick Barbarossa, the Holy Roman Emperor;[125]two decades later, an invasion of Anatolia was resoundingly defeated at theBattle of Myriokephalon.[126]
Manuel's death left the empire rudderless and it soon came under intense pressure.[127]His sonAlexios IIwas too young to rule, and his troubled regency was overthrown by his uncleAndronikos I Komnenos: he was replaced byIsaac IIin 1185.[128]Centrifugal forces swirled at the borders as ambitious rulers seized their chance: Hungary and the Turks captured Byzantine territories,an exiled Komnenian princeseized Cyprus; and most injuriously,a revolt in 1185caused the foundation of aresurrected Bulgarian state.[129]Relations with the West deteriorated further after Constantinople allied withSaladin, the vanquisher of theThird Crusade, whose leaders also fought against Byzantium as they passed through its territory.[130]In 1195, Isaac II was deposed by his brotherAlexios III; this quarrel proved fatal.[131]
TheFourth Crusadewas originally intended to targetEgypt, but amid strategic difficulties, Isaac II's sonAlexios Angelosconvinced the crusaders to restore his father to the throne in exchange for a huge tribute.[132]Theyattacked Constantinople in 1203, reinstating Isaac II and his son to the throne. The new rulers swiftly grew unpopular and were deposed byAlexios V, an event used by the crusaders as a pretext tosack the city in April 1204, ransacking the wealth it had accumulated over nine centuries.[133]
Byzantine territories fragmented into competing political entities. The crusaders crownedBaldwin Ias the ruler of a newLatin Empirein Constantinople; it soon suffereda crushing defeatagainst the Bulgarians in 1205. It also failed to expand west or east, where three Greek successor states had formed: theEmpire of Nicaeaand theEmpire of Trebizondin Asia Minor, and theDespotate of Epiruson the Adriatic. The Venetians acquired many ports and islands, and thePrincipality of Achaeaemerged in southern Greece.[134]Trebizondlostthe key port ofSinopein 1214 and thereafter was unable to affect matters away from the southeastern Black Sea.[135]For a time, it seemed that Epirus was the one most likely to reclaim Constantinople from the Latins, and its rulerTheodore Doukascrowned himself emperor, but he suffered a critical defeat at theBattle of Klokotnitsain 1230, and Epirote power waned.[136]
Nicaea, ruled by theLaskarid dynastyand composed of a mixture of Byzantine refugees and native Greeks, blocked the Latins and the Seljuks of Rum from expanding east and west respectively.[137]John III(r.1221–1254) was a very capable emperor.[138]Hisprotectionisteconomic policies strongly encouraged Nicaeanself-sufficiency,[139]and he made many diplomatic treaties, especially afterMongolarmiesravaged Bulgariaanddefeated Rumbetween 1237 and 1243. This chaos was an opportunity for John, and he fought many successful campaigns against the states disrupted by theMongol invasions.[140]Soon after his death,his grandsonwas usurped byMichael VIII, founder of thePalaiologos dynasty, whorecaptured Constantinoplein 1261.[141]
Michael desired to restore the empire's glory through a rebuilding programme in Constantinople, clever diplomatic alliances, and expansionist wars in Europe.[142]He staved off the threateningCharles I of Anjoufirst by recognising papal primacy and certain Catholic doctrines at the 1274Second Council of Lyon, and then by aiding theSicilian Vespersagainst Charles in 1282.[143]However, his religious concessions were despised by most of the populace, and were repudiated by his successorAndronikos II(r.1282–1328).[144]He and his grandsonAndronikos III(r.1328–1341) led several campaigns to restore imperial influence, succeeding in Epirus and Thessaly. They also made several critical mistakes, including dismissing the fleet in 1285, hiring the mercenaryCatalan Company, who turned on the Byzantines, in the 1300s, and fighting each other between 1320 and 1328.[145]A disastrous civil war between 1341 and 1354 caused long-term economic difficulties, while theOttoman Turksgradually expanded.[146]
The diminished and weak Byzantine state only survived for another century through effective diplomacy and fortunately-timed external events.[147]The Ottomans gradually subjugated Anatolia and simultaneously expanded into Europe from 1354, takingPhilippopolisin 1363,Adrianopolisin 1369, andThessalonicain 1387.[148]Emperors were crowned and deposed at the whim of the Venetians, Genoese, and Ottomans.[149]AfterManuel II(r.1391–1425) refused to pay homage to SultanBayezid Iin 1394,Constantinople was besiegeduntil the rampaging warlordTimurdecisivelydefeated Bayezidin 1402, with the city perilously close to surrender.[150]
Manuel II oversaw two decades of peace while theOttomans convulsed in civil war.[151]In 1421, his unsuccessful backing of the claimantMustafa Çelebiled to arenewed Turkish assault.[152]AlthoughJohn VIII(r.1425–1448) reconciled with the Catholic West at theCouncil of Florence, his empire steadily diminished.[153]In 1452, SultanMehmed IIresolved to capture Constantinople, and laid siege early the following year. On 29 May 1453,the city was captured, the last emperor,Constantine XI, died in battle, and the Byzantine Empire ended.[154]
Diocletian and Constantine's 4th-century reforms reorganised the empire's provinces into overarchingDiocesesand then intoPraetorian prefecture's, separating the army from the civil administration.[155]The central government, led by theemperorfrom the time of the earlierpax romanaand into the latePalaiologan era, typically focused on the military, foreign relations, administering the law, and collecting taxes.[156]Thesenateevolved into a ceremonial body within the imperial court.[157]
Cities had been a collection of self-governing communities with central government and church representatives from the 5th century.[158]However, constant warfare significantly altered this, as regular raids and ongoing conflict led to power centralising due to the empire's fight for survival.[159]After the 7th century, the prefectures were abandoned, and in the 9th century, the provinces were divided into administrative units calledthemes(orthemata), governed solely by a military commander (strategos).[160]
Theodosius II (r. 402–450) formalisedRoman lawby appointing five jurists as principal authorities and compiling legislation issued since Constantine's reign into theCodex Theodosianus.[161]This process culminated in theCorpus Juris Civilisunder Justinian I (r. 527–565), who commissioned a complete standardisation of imperial decrees since Hadrian's time and resolved conflicting legal opinions of the jurists.[162]The result became the definitive legal authority. This body of law coveredcivil mattersand alsopublic law, including imperial power and administrative organisation.[163]After 534, Justinian issued theNovellae (New Laws)in Greek, which marked a transition from Roman to Byzantine law. Legal historian Bernard Stolte distinguishes Roman law as this because Western Europe inherited law through the Latin texts of theCorpus Juris Civilisonly.[164]
Zachary Chitwood argues that theCorpus Juris Civiliswas largely inaccessible in Latin, particularly in the provinces.[165]Following the 7th-century Arab conquests, people began questioning the development and application of law, leading to stronger ties between law and Christianity.[166]This context influenced Leo III (r. 717–741) to develop theEcloga, which placed an emphasis on humanity.[167]The Ecloga inspired practical legal texts like theFarmers' Law, Seamen's Law,andSoldiers' Law, which Chitwood suggests were used daily in the provinces as companions to theCorpus Juris Civilis.[168]During the Macedonian dynasty, efforts to reform law began with the publication of theProcheironand theEisagoge, which aimed to define the emperor's power under prevailing laws, and to replace theEclogadue to its association withiconoclasm.[169]Leo VI (r. 886–912) completed acomplete codificationof Roman law in Greek through theBasilika, a work of 60 books which became the foundation of Byzantine law.[170]In 1345, Constantine Harmenopoulos compiled theHexabiblos, a six-volume law book derived from various Byzantine legal sources.[171]
Christianity, bolstered byConstantine's support, began shaping all aspects of life in the early Byzantine Empire.[172]Despite the transition, the historian Anthony Kaldellis views Christianity as "bringing no economic, social, or political changes to the state other than being more deeply integrated into it".[173]When the Roman state in the West collapsed politically, cultural differences began to divide the Christian churches of the East and West.[174]Internal disputes within the Eastern churches led to the migration of monastic communities to Rome, exacerbating tensions between Rome and Constantinople.[175]These disputes,[c]particularly in Egypt and the eastern Mediterranean, eventually split the church into three branches:Chalcedonian, Monophysite (Coptic), andNestorian.[178]The Chalcedonian group maintained dominance within the empire's territories, while the Monophysite and Nestorian branches largely fell under Muslim rule in the 7th century.[179]
Eastern patriarchs frequently sought the Papacy's mediation in doctrinal and practical matters, but the pope's authority was not universally acknowledged, even in nearby regions like Northern Italy.[180]By 600, the Slavic settlement of the Balkans disrupted communication between Rome and Constantinople, further widening the divide.[181]The Arab and Lombard invasions, and the increasedFrankishpresence, deepened this estrangement and intensified disputes over jurisdiction and authority between the two spiritual centres.[182]Differences in ritual and theology, such as the use ofunleavened breadand theFilioque clause, as well as divergences in ecclesiology—plenitudo potestatisversus the authority ofEcumenical Councils—and issues of mutual respect, contributed to the separation of Western Christianity from Eastern Christianity.[183]This separation began by 597 and culminated in 1054 during theEast–West Schism.[184]
In the late 6th century, following Justinian I's wars, seven mobilefield armiescalledcomitatenses, numbering around 150,000 troops, were deployed around the empire; they remained the finest armies in Europe.[185]They were aided by twenty-five frontier garrisons of approximately 195,000 lower-qualitylimitaneitroops.[186]Additional troops includedsubsidised allied forcesandimperial guard unitslike theScholae Palatinae.[187]Naval forces were limited:flotillaswere based at key locations, while 30,000 oarsmen were assembled to row 500, mostly requisitioned,transportsto support theVandalic Warin Africa in 533.[188]
The losses suffered in the 7th-centuryArab conquestsled to fundamental changes.[189]The field armies were withdrawn into the core Anatolian territories and assigned to settle in specific districts, which became known asthemataand eventually replaced theold provinces.[190]The thematic armies, supported by the proceeds of their districts, came to resemble a provincialmilitiawith a small professional core, aided by foreign mercenaries and imperial regiments at Constantinople.[191]To defend againstits new Muslim enemy, the navy was similarly reorganised into several provincialised fleets.[192]It became the dominant power in the eastern Mediterranean, withdromonsequipped withGreek fireproving crucial on several occasions.[193]
As the 8th-century empire stabilised, the thematic militias proved rebellious and only suitable for defensive operations.[194]The professionaltagmataregiments, first introduced in the mid-700s and consisting of native Byzantine units alongside foreign forces such as theVarangian Guard, had completely replaced them by the 11th century.[195]The mobiletagmata, suitable for offensive warfare, evolved new tactical and strategic structures;[196]the late 10th-century army, perhaps the highest-quality force the empire produced, numbered approximately 140,000, up from below 100,000 in the late 700s.[197]However, its defensive capacities were neglected, especially during the 11th-century civil wars, leading to the loss of Anatolia to the Seljuks.[198]The navy had also been reduced, as the empire increasingly relied on potentially hostile powers such asVenice.[199]
Post-1081 reforms re-established an effective army; the institution offeudal-likepronoiagrants provided revenue to individuals in exchange for soldiers.[200]The new army heavily relied on foreign mercenaries alongside indigenous Byzantine troops, but the financial demands of a standing army proved too much for the Byzantine state, which succumbed to theFourth Crusadein 1204.[201]Thearmy of the Palaiologan dynasty, which retook Constantinople in 1261, was generally composed of a similar mix of mercenaries and indigenous troops, but it had lost all offensive capability by the late 1200s.[202]The empire's continued survival depended on foreign armies; attempts in the 1340s to rebuild the fleet, unwisely disbanded in 1284, were forcibly halted byGenoa.[203]No post-1204 Byzantine field army fielded more than 5,000 troops, and less than 8,000 defendedthe final siege of Constantinoplein 1453.[204]
Byzantine strategy was primarily defensive, aside from the brief period of aggression between the ninth and eleventh centuries, because of the empire's habitual lack of resources.[205]To avoid risky and expensive military campaigns, the Byzantinesengaged in extensive diplomatic efforts.[206]These took various forms, including: formal embassies, client management, alliance or peace negotiations, political marriages,propagandaand bribery, or evenespionageand assassination.[207]
Defensively-oriented Byzantine diplomacy was intended to protect theoikoumenē, the civilised Christian world which the empire rightfully ruled.[208]The decline of the keylimitrophesystem, wherein client states along the borders served as intermediaries between the empire and other large enemies, exposed the empire to attack. By the eleventh century, Byzantine diplomacy was more bilateral and balanced.[209]Although it lost some important advantages post-1204, diplomacy, including the still-influential Orthodox church, was nevertheless a central element in the empire's lengthy survival until 1453.[210]
Scholars associate the Roman, Hellenic, and Christian imperial identities with the general population, but there is ongoing debate about how these and other regional identities blended together.[211]
As many as 27 million people lived in the empire at its peak in 540, but this fell to 12 million by 800.[212]Although plague and territorial losses to Arab Muslim invaders weakened the empire, it eventually recovered and by the near end of theMacedonian dynastyin 1025, the population is estimated to have been as high as 18 million.[213]A few decades after the recapture of Constantinople in 1282, the empire's population was in the range of 3–5 million; by 1312, the number had dropped to 2 million.[214]By the time the Ottoman Turks captured Constantinople, there were only 50,000 people in the city, one-tenth of its population in its prime.[215]
Education was voluntary and required financial means, so the most literate people were often those associated with the church.[216]Primary education focused on teaching foundational subjects like reading, writing, and arithmetic whereas secondary school focused on thetriviumandquadriviumas their curriculum.[217]TheImperial University of Constantinoplewas formed in 425, and refounded in 1046 as a centre for law.[218][219][220]
During the 3rd century, 10–15% of the population was enslaved (numbering around 3 million in the east).[221]Youval Rotman calls the changes to slavery during this period "different degrees of unfreedom".[222]Previous roles fulfilled by slaves became high-demand free market professions (like tutors), and the state encouraged thecoloni, tenants bound to the land, as a new legal category between freemen and slaves.[223]From 294 the enslavement of children was progressively forbidden;Honorius(r.393–423) began freeing enslaved prisoners of war, and from the 9th century, emperors freed the slaves of conquered people.[224]Christianity as an institution had no direct impact, but by the 6th century it was a bishop's duty to ransom Christians, there were established limits on trading them, and state policies prohibited the enslavement of Christians; these changes shaped Byzantine slave-holding from the 8th century onwards.[225]Non-Christians could still be enslaved, and prices remained stable until 1300, when prices for adult slaves, particularly women, started rising.[226][227]
Agriculture was the main basis of taxation and the state sought to bind everyone to land for productivity.[228]Most land holdings were small and medium-sized lots around villages, and family farms were the primary source of agriculture.[229]Thecoloni,sometimes called proto-serfs, were free citizens, though historians continue to debate their exact status.[230]
TheEklogelaws of 741 made marriage a Christian institution and no longer a private contract, where it evolved alongside the increased rights of slaves and the change in power relations.[231]Marriage was considered an institution required to sustain the population, transfer property rights, and support the elderly of the family; the EmpressTheodorahad also said it was needed to restrict sexualhedonism.[232]Women usually married between the ages of 15 and 20, and the average family had two children.[233]Divorce could be done by mutual consent but was restricted over time, for example, only being allowed if a married person was joining a convent.[234]
Inheritance rights were well developed, including for all women.[235]The historian Anthony Kaldellis suggests that these rights may have been what prevented the emergence of large properties and a hereditary nobility capable of intimidating the state.[236]The prevalence of widows (estimated at 20%) meant women often controlled family assets as heads of households and businesses, contributing to the rise of some empresses to power.[237]Women played significant roles as taxpayers, landowners, and petitioners, often seeking the resolution of property disputes in court.[238]
Women had the same socio-economic status as men, but faced legal discrimination and limitations in economic opportunities and vocations.[239]Prohibited from serving as soldiers or holding political office, and restricted from serving asdeaconessesin the Church from the 7th century onwards, women were mostly assigned labour-intensive household responsibilities.[240]They worked in the food and textile industries, as medical staff, in public baths, in retail, and were practising members of artisan guilds.[241]They also worked in entertainment, tavern keeping, and prostitution, a class where some saints and empresses may have originated from.[242]Prostitution was widespread, and attempts were made to limit it, especially during Justinian's reign under the influence of Theodora.[243]Women participated in public life, engaging in social events and protests.[244]Women's rights were better in the empire than in comparable societies. Western European and American women took until the 19th century to surpass them.[245]
LatinandGreekwere the primary languages of the late Roman Empire, with the former prevalent in the west and the latter in the east.[246]Although Latin was historically important in the military, legal system, and government, its use declined in Byzantine territories from 400 AD.[247]Greek had begun to replace it even in those functions by the time ofJustinian I(r. 527–565), who may have tried to arrest Latin's decline. Its extinction in the east was thereafter inevitable.[248]A similar process of linguisticHellenizationoccurred in Asia Minor, whose inhabitants had mostly abandoned their indigenous languages for Greek by early Byzantine times.[249]Still, much of the population of the empire would have known neither Latin nor Greek, especially in rural areas—their languages includedArmenianinthat people's homelands,Aramaicdialects such asSyriacin Mesopotamia and the Levant,Copticin Egypt,Phoenicianon the Levant coast and inCarthage, andBerberin rural North Africa.[250]
The empire lost its linguistic diversity in the wars of the 7th and 8th centuries, becoming overwhelmingly Greek-speaking.[251]During this troubled period, classicalAttic Greek—one of thelinguistic registersthe Byzantine Greeks inherited—fell out of use, while theeveryday vernacularregisters were still used.[252]As the empire gained some stability from the 9th century onwards, and especially after theKomnenian restoration, Attic Greek came back into fashion for written works. In a phenomenon calleddiglossia, the gap between vernacular spoken Greek, which was rarely written in published works, and literary registers only spoken in formal contexts, became very wide.[253]
During thePalaiologan period, although classically-written works remained the normal style, Western-inspired writers began to use more vernacular elements, especially forromancesor near-contemporary histories. One example is theChronicle of the Morea, probably written by a French immigrant who was ignorant of formal Greek literature and who incorporated spoken Greek into his work.[254]All such written vernacular was in verse form, becoming the ancestor ofmodern Greekpoetry, while prose remained classically-written.[255]
The empire's geographic and maritime advantages reduced the costs of transporting goods and facilitated trade, making it a key driver of economic growth from antiquity and through the post-classical period.[256]Infrastructure, including roads, public buildings, and the legal system, supported trade and other economic activities.[257]Regions like Asia Minor, the Aegean islands, Egypt, the Levant, and Africa thrived as mature economic centres despite political challenges and military insecurities.[258]From the mid-6th century onward, plagues, invasions, and wars caused populations and economies to decline, leading to the collapse of the ancient economy.[259]Major cities like Constantinople, Antioch,Alexandria, andThessalonikicontinued to support substantial populations exceeding 100,000, while the countryside transitioned into fortified settlements.[260]These rural areas developed into hamlets and villages, reflecting an economic shift between historical periods towards more efficient land use.[261]
Low population density prompted emperors to encourage migration and resettlement, stimulating agriculture and demographic growth.[262]By the 9th century, the economy began to revive, marked by increased agricultural production and urban expansion.[263]Advances in science, technical knowledge, and literacy gave the empire a competitive edge over its neighbours.[264]The 11th and 12th centuries witnessed consistent and rapid population growth, marking the peak of this revival.[265]Italian merchants, particularly the Venetians, Genoese, and Pisans, took control of international trade, thus reducing the influence of native merchants.[266]The political system grew increasingly extractive and authoritarian, contributing to the empire's collapse in 1204.[267]
The fall of Constantinople during the Fourth Crusade in 1204 destroyed centuries of its wealth.[268]Large landholdings were confiscated, and the empire fragmented into smaller rump states ruled by competing factions, making governance inefficient and increasing the costs of doing business.[269]The state gradually lost control over trade practices, price regulations, the outflow of precious metals, and possibly even the minting of coins.[270]Italian merchants further dominated trade as the events of 1204 opened the Black Sea to Western merchants, permanently altering the empire's fortunes.[271]Farmers and manufacturers increasingly produced goods for local use and were affected by the insecurity of constant warfare.[272]Despite these challenges, the empire's mixed economy (characterised by state interventions, public works, and market liberalisation)[273]remained a model of medieval economic adaptability, even as it deteriorated under external pressures.[274]
Historical evidence of Byzantine dress is scant. It is known that the court had a distinguishable dress, and that ordinary men and women observed certain conventions of clothing.[275]Fashion trends started in the provinces, and not in the capital, which was more conservative.[276]The imperial dress was centred around theloros,tzangiaand crown, which represented the empire and the court.[277]Thelorosderived from thetrabea triumphalis, a ceremonial toga worn by consuls. It was more prominent in the early empire, indicating a continuation of the traditions of the Roman Empire.[278]Historian Jennifer Ball suggests that thechlamyscloak, which originated in the military, was similar to a modern-day business suit and an evolution of thepaludamentumcloak worn by aristocratic men, including the emperor during the early empire.[279]In the middle empire, dresses replaced the tunic for women.[280]The late empire saw the larger influence on Byzantine dress of non-Greek cultures like the Italians (Genoese and Venetians), Turks (Ottomans), and the Bulgarians.[281]
Feasting was central to the culture.[282]By the 10th century, dining shifted fromrecliningto tables with clean linen.[283]The introduction of the fork and salad dressing (with oil and vinegar) further shaped Italian and Western traditions[284]Classical Greco-Roman era foods were common such as the condimentgaros(similar to fermentedfish saucestoday) as well as the still popularbaklava.[285]Fruits likeaubergineandorange, unknown during classical times, were added to diets.[286]Foods that have continued into the modern era include the cured meatpaston,Fetacheese, salt roe similar to the modernboutargue, black sea caviar,tiropita,dolmades, and the souptrachanas.[287]There were famed medieval sweet wines such as theMalvasiafromMonemvasia, theCommandaria, and the eponymousRumney winewhich were drunk, as weremillet beer(known asboza) andretsina.[288]
Chariot raceswere held from the early era until 1204, becoming one of the world's longest continuous sporting events.[290]Mimes, thepantomimeand some wild animal shows were prominent until the 6th century.[291]Because Christian bishops and pagan philosophers did not like these activities, the state's funding for them ceased, leading to their decline and a move to private entertainment and sporting.[292]A Persian version of polo introduced by the Crusaders calledTzykanionwas played by the nobility and urban aristocracy in major cities during the middle and late eras, as was the sport of jousting introduced from the West.[293]Over time,game boardsliketavlibecame increasingly popular.[294]
Subjects inByzantine artwere primarilyChristianand typically non-naturalistic in their representation.[297]Emerging from both theearliest ChristianandLate Antique art,[298]many early examples were lost amid theRoman Persecution; the fragmented mosaics of the 3rd-centuryDura-Europos churchare a unique exception.[299]SuchByzantine mosaics, known for theirgold groundstyle, became a hallmark of the empire, displaying both secular and sacred themes in diverse places, including churches (Basilica of San Vitale), the circus (Hippodrome of Constantinople), and theGreat Palace of Constantinople.[300]The early 6th-century reign of Justinian I saw systemic developments:religious artcame to dominate, and once-popular public marble and bronzemonumental sculpturefell out of favour due topaganassociations.[301]Justinian commissioned the monumentalHagia Sophiachurch, and its influential elements became architectural hallmarks for the empire: the immense size, largedome, innovative use ofpendentivesand highly decorative interior were imitated as far north as theCathedral of Saint SophiainNovgorodand theSaint Sophia CathedralinKyiv.[302]The Hagia Sophia's creators, the engineer-architectsIsidore of MiletusandAnthemius of Tralles, are uniquely esteemed;[303]most Byzantine artists were unrecorded and typically deemed to have little importance.[304]
Smaller-scale art flourished throughout the entire Byzantine period: costlyivory carvings—often as diptychs (Barberini ivory) or triptychs (Harbaville Triptych)—featured imperial commemorations or religious scenes and were particularly valued, as weremetalworkandenamels.[305]Other costly objects includedilluminated manuscripts, which were lavishly illustrated for a wide range of texts, andsilks, often dyed in the prizedimperial purple; both became highly popular in Western Europe.[306]The rise of small, portableiconpaintings, used for both public and private religious worship, grew increasingly controversial.[307]During two periods ofByzantine Iconoclasm(726–843), possibly influenced byIslamic prohibitions on religious images,[308]icons were suppressed and enormous amounts of figurative religious art was destroyed.[309]Iconoclastscondemned their use, likening them to paganidolatryand ascribing recentUmayyad defeatsasdivine retributionfor their use.Iconophileseventually prevailed, maintaining their essential use forveneration, considered distinct fromworship, and found precedent ingospelreferences.[310]
Post-iconoclast Macedonian art(867–1056) saw acultural renaissance, and many artworks from this period survive.[311]Subjects and styles became standardised, particularlycross-in-squarechurches, and already-existing frontality andsymmetryevolved into a dominant artistic aesthetic, observable in the smallPala d'Oroenamel and the large mosaics of theHosios Loukas,Daphni, andNea Monimonasteries.[312]The subsequent Komnenos-Angelos periods (1081–1204) saw increased imperial patronage, alongside figurative artwork of increased emotional expression (Dead Christ and Mourners,c.1164).[296]Byzantine artistic influence spread widely toNorman Sicily(theMadrid Skylitzes) and Venice (the mosaics ofSt Mark's Basilica).[296]Serbian churches flourished, as three successiveschools of architecture—Raška(1170–1282),Byzantine Serbia(1282–1355), andMorava(1355–1489)—combined aRomanesqueaesthetic with increasingly voluminous decorations and domes.[313]As smallerPalaeologan artworks(1261–1453) gainedrelicstatus in Western Europe—many looted in the 1204 Fourth Crusade—they greatly influenced theItalo-Byzantinestyle ofCimabue,Duccio, and laterGiotto; the latter is traditionally regarded by art historians as the inaugurator ofItalian Renaissance painting.[314]
Byzantine literatureconcerns allGreek literaturefrom theMiddle Ages.[315]Although the empire waslinguistically diverse, the vast majority of extant texts are inmedieval Greek,[316]in twodiglossicvariants: a scholarly form based onAttic Greek, and avernacularbased onKoine Greek.[317]Most contemporary scholars consider all medieval Greek texts to be literature,[318]but some offer varying constraints.[319]The literature's early period (c.330–650) was dominated by the competing cultures ofHellenism,ChristianityandPaganism.[320]TheGreek Church Fathers—educated in an Ancient Greekrhetorictradition—sought to synthesise these influences.[315]Important early writers includeJohn Chrysostom,Pseudo-Dionysius the AreopagiteandProcopius, all of whom aimed to reinvent older forms to fit the empire.[321]Theologicalmiraclestories were particularly innovative and popular;[321]theSayings of the Desert Fathers(Apophthegmata Patrum) were copied in nearly every Byzantine monastery.[322]During theByzantine Dark Ages(c.650–800), production of literature mostly stopped, though some important theologians were active, such asMaximus the Confessor,Germanus I of ConstantinopleandJohn of Damascus.[321]
The subsequent cultural Macedonian Renaissance (c.800–1000; the "Encyclopedism period") saw a renewed proliferation of literature and revived the earlier Hellenic-Christian synthesis.[315]Works byHomer,Ancient Greek philosophersandtragedianswere translated, andhagiographywas heavily reorganised.[321]After this early flowering of monastic literature, there was a dearth untilSymeon the New Theologianin the late 10th century.[321]A new generation (c.1000–1250), including Symeon,Michael PsellosandTheodore Prodromos, rejected the Encyclopedist emphasis on order, and were interested in individual-focused ideals variously concerningmysticism,authorial voice, heroism, humour and love.[323]This included the Hellenistic-inspiredByzantine romanceandchivalricapproaches in rhetoric, historiography and the influential epicDigenes Akritas.[321]The empire's final centuries saw a renewal of hagiography and increased Western influence, leading to mass Greek to Latin translations.[324]Authors such asGemistos PlethonandBessarionexemplified a new focus on humanvicesalongside the preservation of classical traditions, the latter greatly influenced theItalian Renaissance.[324]
Byzantine musicis eclectically descended from earlyChristian plainsong,Jewish music, and a variety ofancient music; its exact connections toancient Greek musicremain uncertain.[326]It included bothsacredandseculartraditions, but the latter is little known, whereas the former remains the central music ofEastern Orthodoxliturgy into the 21st century.[327]The empire's church music, known as Byzantinechant, was exclusively unaccompaniedmonodicvocal music, sung in Greek.[328]From the 8th century, chant melodies were governed by theOktōēchosframework, a set of eightmodes—echos(ἦχος;lit.'sound')—each of these provide predeterminedmotivicformulae for composition.[329]These formulae were chosen for propertext stressand occasionally fortext painting, then collated throughcentonisationintohymnsorpsalms.[330]
Byzantine chant was central to theByzantine Rite; the earliest music was notnotated,[331]including early monostrophicshort hymns like thetroparion.[332]Proto-Ekphonetic notation(9th century onwards) marked simplerecitationpatterns. TheneumaticPalaeo-Byzantine notation system emerged in the 10th century, and the Middle Byzantine "Round Notation" from the mid-12th century onwards is the first fullydiastematicscheme.[333]Several major forms developed alongsidewell-known composers: the longkontakion(5th century onwards), popularised byRomanos the Melodist; the also-extensivekanōn(late 7th century onwards), developed byAndrew of Crete; and the shortersticheron(at least 8th century onwards), championed byKassia.[334]By the Palaiologan period, the dominance of strict compositional rules lessened andJohn Koukouzelesled a new school favouring a moreornamental"kalophonic" style which deeply informed post-empire Neo-Byzantine music.[335]
Secular music, often state-sponsored, was ubiquitous in daily life and featured in a variety of ceremonies, festivals, and theatre.[336]Secular vocal music was rarely notated, and extant manuscripts date much later, suggesting the tradition was passed throughoral traditionand likelyimprovised.[337]Prohibited for liturgical use, a wide variety of Byzantineinstrumentsflourished in secular contexts, although no notated instrumental music survives.[338]It is uncertain to what extent instrumentalists improvised or if they doubled vocalists monophonically orheterophonically.[339]Among the best known instruments are thehydraulic organ, used for circus and imperial court events; theancient Greek-descendedaulos, awind instrument; thetambouras, a pluckedstring instrument; and mostly popularly, theByzantine lyra.[339]Prominent genres includedacclamationchants of laudation or salutation; the celebratoryAcritic songs; symposia instrumentalbanquets, based on ancientsymposiums; anddance music.[340]
The scholars of the empire played a principal role in transmitting classical knowledge to theIslamic worldandRenaissance Italy, as well as producing commentaries that helped expand scientific knowledge.[341]This medieval Greek scholarship was not only based on scientific treatises from antiquity but also drew from Islamic, Latin, and Hebrew works, which helped spearhead new developments as late as the 11th and 12th centuries.[342]
Key people passed on important traditions that underpinned this scholarship, especially in the realms of philosophy, geometry, astronomy, and grammar.[343]For example, the Hagia Sophia architectIsidore of Miletus(c.530), compiledArchimedes' works whichLeo the Mathematician(c.850) incorporated into formal courses, and is why theArchimedes Palimpsestis known today.[344]John Philoponusand his critiques ofAristotelian physics, the pharmacologistPedanius Dioscorides, and Ptolematic geography and astronomy had an important influence on western science, as seen withPtolemy's influence onCopernicusand Philoponus onBonaventure,Gersonides,Buridan,OresmeandGalileo.[345]
Military innovations included theriding stirrupwhich provided stability for mounted archers and dramatically transformed the army; a specialised type ofhorseshoe; thelateen sail, which improved a ship's responsiveness to wind; andGreek fire—an incendiary weapon capable of burning even when doused with water, first appearing around the time of theSiege of Constantinople (674–678).[346]Inhealthcare, the empire pioneered the concept of the hospital as an institution offering medical care and the possibility of a cure for the patients, rather than merely being a place to die.[347]
After Constantinople fell, the Ottomans quickly absorbed the remaining independent territories, including Morea in 1460, Trebizond in 1461,AcciaiuoliAthens in 1456, andGattilusiLesvos in 1462.[348]They dismantled the Empire's political and secular institutions, leaving the impoverished Church to manage what would be later called theRum Millet, primarily as a tool for taxing its followers.[349]As the sole sovereign Orthodox state, Russia developed theThird Rome doctrine, emphasising its cultural heritage as distinct from Western Europe, because the latter had inherited much of the empire's secular learning.[350]TheDanubian Principalitiesbecame a haven for Orthodox Christians andPhanariotGreeks who sought to recreate a Byzantine Greek Empire.[351]In modern Greece, members of theRum Milletincreasingly identified as Greeks, eventually leading to asuccessful war of independencein the 19th century.[352]The modern Greek state nearly doubled its territory through the pursuit of theMegali Idea—a colonialist vision of reclaiming the former lands of the eastern empire—achieving limited success during theCrimean warbut making significant gains during theBalkan wars.[353]
Since the 15th century, Byzantine history has been deeply politicised, woven into nationalist, colonialist, and imperialist narratives.[354]This politicisation appears not only in Greece but also in Bulgarian, Romanian, Serbian, Hungarian, and Turkish nationalism, as well as in former French and Russian imperialist agendas.[355]In the English-speaking world, interpretations of Byzantine history frequently surface in political debates, alongside the growing appreciation for its legacy.[356]The complexity of this history makes it a sensitive topic, especially regarding Greece's role in Europe's evolving sense of identity and the origin stories of many European nations.[357]
The Byzantine Empire distinctively blended Roman political traditions, Greek literary heritage, and Christianity, creating the civilisational framework that laid the foundation for medieval Europe.[358]The Empire preserved European civilisation by acting as a shield against forces from Eurasian Steppe people such as the Avars, Bulgars, Cumans, Huns, Pechenegs, and Turks.[359]
The empire's legal codes significantly influenced the civil law traditions of continental Europe, Russia, Latin America, Ethiopia, and even the English-speaking common law countries; and possibly influenced Islamic legal traditions as well.[360][361]It also preserved and transmitted classical learning and manuscripts, making important contributions to the intellectual revival which fuelledItalian humanism.[362]
The Byzantine Empire played a pivotal role in shaping Christianity by supporting early Church fathers and the decisions of Church councils; developing the institution ofmonasticism; and fostering theOrthodox traditionwhich continues to define much of Eastern European identity.[363]It was also instrumental in preserving the Greek language and is credited with developing theGlagolitic alphabet, which later evolved into theCyrillic scriptandOld Church Slavonic.[364]These innovations provided the first literary language for the Slavs and formed the educational foundation for all Slavic nations.[365]
TheMongol Empireof the 13th and 14th centuries was thelargest contiguous empireinhistory.[4]Originating in present-dayMongoliainEast Asia, the Mongol Empire at its height stretched from theSea of Japanto parts ofEastern Europe, extending northward into parts of theArctic;[5]eastward and southward into parts of theIndian subcontinent, mounted invasions ofSoutheast Asia, and conquered theIranian plateau; and reached westward as far as theLevantand theCarpathian Mountains.
The Mongol Empire emerged from the unification of severalnomadic tribesin theMongol heartlandunder theleadershipof Temüjin, known by the title ofGenghis Khan(c.1162–1227), whom a council proclaimed as the ruler of allMongolsin 1206. The empire grew rapidly under his rule and that of his descendants, who sent outinvading armiesin every direction. The vast transcontinental empire connected theEastwith theWest, and thePacificto theMediterranean, in an enforcedPax Mongolica, allowing the exchange of trade, technologies, commodities, and ideologies acrossEurasia.
The empire began to split due to wars over succession, as the grandchildren of Genghis Khan disputed whether the royal line should follow from his son and initial heirÖgedeior from one of his other sons, such asTolui,Chagatai, orJochi. The Toluids prevailed after a bloody purge of Ögedeid and Chagatayid factions, but disputes continued among the descendants of Tolui. The conflict over whether the Mongol Empire would adopt a sedentary, cosmopolitan lifestyle or continue its nomadic, steppe-based way of life was a major factor in the breakup.
AfterMöngke Khandied in 1259, rivalkurultaicouncils simultaneously elected different successors, the brothersAriq BökeandKublai Khan, who fought each other in theToluid Civil War(1260–1264) and also dealt with challenges from the descendants of other sons of Genghis.[6][7]Kublai successfully took power, but war ensued as he sought unsuccessfully to regain control of theChagatayidandÖgedeidfamilies. By the time of Kublai's death in 1294, the Mongol Empire hadfractured into four separate khanates or empires, each pursuing its own interests and objectives: theGolden Hordekhanate in the northwest, theChagatai Khanatein Central Asia, theIlkhanatein Iran, and theYuan dynasty[a]in China, based in modern-dayBeijing.[12]In 1304, during the reign ofTemür, the three western khanates accepted thesuzeraintyof the Yuan dynasty.[13][14]
The Ilkhanate was the first to be lost, which disintegrated in the period between 1335 and 1353. Next, the Yuan dynasty lost control of theTibetan PlateauandChina properin 1354 and 1368, respectively, and collapsed after its capital ofDaduwas taken over byMing forces. The Genghisid rulers of the Yuan then retreated north and continued to rule theMongolian Plateau. The regime is thereafter known as theNorthern Yuandynasty in historiography, surviving as arump stateuntil the conquest by theQing dynastyin the 1630s. The Golden Horde had broken into competing khanates by the end of the 15th century, while the Chagatai Khanate lasted in one form or another until 1687, or, in the Yarkent Khanate's case, until 1705.
The Mongol Empire is also referred to as the "Mongolian Empire" or the "Mongol World Empire" in some English sources.[15][16]
The empire referred to itself asᠶᠡᠬᠡᠮᠣᠩᠭᠣᠯᠤᠯᠤᠰ(yeke mongɣol ulus,lit.'nation of the great Mongols' or the 'great Mongol nation') in Mongol orkür uluγ ulus(lit.the 'whole great nation') in Turkic.[17]
After the 1260 to 1264succession warbetweenKublai Khanand his brotherAriq Böke, Kublai's power became limited to the eastern part of the empire, centered on China. Kublai officially issued an imperialedicton 18 December 1271 to give the empire theHan-style dynastic name of "Great Yuan" (Dai Yuan, orDai Ön Ulus;Chinese:大元;pinyin:Dà Yuán) and to establish theYuan dynasty. Some sources give the full Mongol name asDai Ön Yehe Monggul Ulus.[18]
The area aroundMongolia,Manchuria, and parts ofNorth Chinahad been controlled by theKhitan-ledLiao dynastysince the 10th century. In 1125, theJin dynastyfounded by theJurchensoverthrew the Liao dynasty and attempted to gain control over former Liao territory in Mongolia. In the 1130s the Jin dynasty rulers, known as the Golden Kings, successfully resisted theKhamag Mongolconfederation, ruled at the time byKhabul Khan, great-grandfather of Genghis Khan.[19]
TheMongolian Plateauwas occupied mainly by five powerful tribal confederations (khanlig):Keraites,Khamag Mongol,Naiman,Mergid, andTatar. The Jin emperors, following a policy ofdivide and rule, encouraged disputes among the tribes, especially between the Tatars and the Mongols, in order to keep the nomadic tribes distracted by their own battles and thereby away from the Jin. Khabul's successor wasAmbaghai Khan, who was betrayed by the Tatars, handed over to the Jurchen, and executed. The Mongols retaliated by raiding the frontier, resulting in a failed Jurchen counter-attack in 1143.[19]
In 1147, the Jin somewhat changed their policy, signing a peace treaty with the Mongols and withdrawing from a score of forts. The Mongols then resumed attacks on the Tatars to avenge the death of their late khan, opening a long period of active hostilities. The Jin and Tatar armies defeated the Mongols in 1161.[19]
During the rise of the Mongol Empire in the 13th century, the usually cold, parched steppes of Central Asia enjoyed their mildest, wettest conditions in more than a millennium. It is thought that this resulted in a rapid increase in the number of war horses and other livestock, which significantly enhanced Mongol military strength.[20]
Known during his childhood as Temüjin, Genghis Khan was a son of a Mongol chieftain and rose very rapidly as a young man by working with Toghrul Khan of the Kerait. After Temujin went to war against Kurtait (also known as Wang Khan; given the Chinese title "Wang" for its meaning of King[21]), who was the most powerful Mongol leader at the time, he gave himself the name Genghis Khan. He then enlarged[how?]his Mongol state under himself and his kin, with the term Mongol coming to be used in reference to all Mongolic speaking tribes under the control of Genghis Khan. His most powerful allies were his father's friend,KhereidchieftainToghrul, and Temujin's childhoodanda(i.e.blood brother)Jamukhaof the Jadran clan. With their help, Temujin defeated the Merkit tribe, rescued his wifeBörte, and went on to defeat the Naimans and the Tatars.[22]
Temujin forbade the looting of his enemies without permission, and he implemented a policy of sharing spoils with his warriors and their families instead of giving them all to the aristocrats.[23]These policies brought him into conflict with his uncles, who were also legitimate heirs to the throne; they regarded Temujin not as a leader but as an insolent usurper. This dissatisfaction spread to his generals and other associates, and some Mongols who had previously been allies broke their allegiance.[22]War ensued, and Temujin and the forces still loyal to him prevailed, defeating the remaining rival tribes between 1203 and 1205 and bringing them under his sway. In 1206, Temujin was crowned as thekhagan(Emperor) of theYekhe Mongol Ulus(Great Mongol State) at aKurultai(general assembly/council). It was there that he assumed the title of Genghis Khan (universal leader) instead of one of the old tribal titles such as Gur Khan or Tayang Khan, marking the start of the Mongol Empire.[22]
Genghis Khan introduced many innovative ways of organizing his army: for example dividing it into decimal subsections of arbans (10 soldiers), zuuns (100),Mingghans(1000), andtumens(10,000). TheKheshig, theimperial guard, was founded and divided into day (khorchintorghuds) and night (khevtuul) guards.[24]Genghis rewarded those who had been loyal to him and placed them in high positions, as heads of army units and households, even though many of them came from very low-ranking clans.[25]
Compared to the units he gave to his loyal companions, those assigned to his own family members were relatively few. He proclaimed a new code of law of the empire,Ikh ZasagorYassa; later he expanded it to cover much of the everyday life and political affairs of the nomads. He forbade the selling of women, theft, fighting among the Mongols, and the hunting of animals during the breeding season.[25]
He appointed his stepbrotherShikhikhutugas supreme judge (jarughachi), ordering him to keep records of the empire. In addition to laws regarding family, food, and the army, Genghis also decreed religious freedom[citation needed]and supported domestic and international trade. He exempted the poor and the clergy from taxation.[26]He also encouraged literacy and the adaptation of theUyghur scriptinto what would become theMongolian scriptof the empire, ordering theUyghurTata-tonga, who had previously served the khan ofNaimans, to instruct his sons.[27]
Genghis quickly came into conflict with the Jin dynasty of the Jurchens and theWestern Xiaof theTangutsin northern China. He also had to deal with two other powers,TibetandQara Khitai.[28]
Before his death, Genghis Khan divided his empire among his sons and immediate family, making the Empire the joint property of the entire imperial family who, along with the Mongol aristocracy, constituted the ruling class.[29]
Genghis Khan arranged for the Chinese Taoist masterQiu Chujito visit him in Afghanistan, and also gave his subjects the right to religious freedom,[citation needed]despite his own shamanistic beliefs.
Genghis Khan died on 18 August 1227, by which time the Mongol Empire ruled from the Pacific Ocean to theCaspian Sea, an empire twice the size of theRoman Empireor the MuslimCaliphateat their height.[citation needed]Genghis named his third son, the charismaticÖgedei, as his heir. According to Mongol tradition, Genghis Khan wasburied in a secret location. The regency was originally held by Ögedei's younger brotherToluiuntil Ögedei's formal election at the kurultai in 1229.[30]
Among his first actions Ögedei sent troops to subjugate theBashkirs,Bulgars, and other nations in the Kipchak-controlled steppes.[31]In the east, Ögedei's armies re-established Mongol authority in Manchuria, crushing theEastern Xiaregime and theWater Tatars. In 1230, the great Khan personally led his army in the campaign against theJin dynastyof China. Ögedei's generalSubutaicaptured the capital of EmperorWanyan Shouxuin theSiege of Kaifengin 1232.[32]The Jin dynasty collapsed in 1234 when theMongols captured Caizhou, the town to which Wanyan Shouxu had fled. In 1234, three armies commanded by Ögedei's sons Kochu and Koten and theTangutgeneral Chagan invaded southern China. With the assistance of theSong dynastythe Mongols finished off the Jin in 1234.[33][34]
Many Han Chinese and Khitan defected to the Mongols to fight against the Jin. Two Han Chinese leaders,Shi Tianze, Liu Heima (劉黑馬, Liu Ni),[35]and the Khitan Xiao Zhala defected and commanded the 3 Tumens in the Mongol army.[36]Liu Heima and Shi Tianze served Ogödei Khan.[37]Liu Heima and Shi Tianxiang led armies against Western Xia for the Mongols.[38]There were four Han Tumens and three Khitan Tumens, with each Tumen consisting of 10,000 troops. The Yuan dynasty created a Han army 漢軍 from Jin defectors, and another of ex-Song troops called the Newly Submitted Army 新附軍.[39]
In the West, Ögedei's generalChormaqandestroyedJalal al-Din Mangburni, the lastshahof theKhwarizmian Empire. The small kingdoms in southern Persia voluntarily accepted Mongol supremacy.[40][41]In East Asia, there were a number ofMongol campaigns into Goryeo Korea, but Ögedei's attempt to annex theKorean Peninsulamet with little success.Gojong, the king ofGoryeo, surrendered but later revolted and massacred Mongoldarughachis(overseers); he then moved his imperial court fromGaeseongtoGanghwa Island.[42]
In 1235, the Mongols establishedKarakorumas their capital lasting until 1260. During that period,Ogedei Khanordered the construction of a palace within the surrounding of its walls.
Meanwhile, in an offensive action against theSong dynasty, Mongol armies captured Siyang-yang, theYangtzeandSichuan, but did not secure their control over the conquered areas. The Song generals were able to recapture Siyang-yang from the Mongols in 1239. After the sudden death of Ögedei's son Kochu in Chinese territory the Mongols withdrew from southern China, although Kochu's brother Prince Koten invadedTibetimmediately after their withdrawal.[22]
Batu Khan, another grandson of Genghis Khan, overran the territories of theBulgars, theAlans, the Kypchaks, Bashkirs,Mordvins,Chuvash, and other nations of the southern Russian steppe. By 1237 the Mongols were encroaching uponRyazan, the first Kievan Rus' principality they were to attack. After a three-day siege involving fierce fighting, the Mongols captured the city and massacred its inhabitants. They then proceeded to destroy the army of theGrand Principality of Vladimirat theBattle of the Sit River.[43]
The Mongols captured theAlaniacapitalMaghasin 1238. By 1240, all Kievan Rus'had fallen to the Asian invadersexcept for a few northern cities. Mongol troops under Chormaqan in Persia, connecting his invasionofTranscaucasiawith the invasion of Batu and Subutai, forced theGeorgianandArmeniannobles to surrender as well.[43]
Giovanni de Plano Carpini, the pope's envoy to the Mongol great khan, travelled throughKievin February 1246 and wrote:
They [the Mongols] attacked Russia, where they made great havoc, destroying cities and fortresses and slaughtering men; and they laid siege to Kiev, the capital of Russia; after they had besieged the city for a long time, they took it and put the inhabitants to death. When we were journeying through that land we came across countless skulls and bones of dead men lying about on the ground. Kiev had been a very large and thickly populated town, but now it has been reduced almost to nothing, for there are at the present time scarce two hundred houses there and the inhabitants are kept in complete slavery.[44]
Despite the military successes, strife continued within the Mongol ranks. Batu's relations withGüyük, Ögedei's eldest son, andBüri, the beloved grandson ofChagatai Khan, remained tense and worsened during Batu's victory banquet in southern Kievan Rus'. Nevertheless, Güyük and Buri could not do anything to harm Batu's position as long as his uncle Ögedei was still alive. Ögedei continued withoffensives into the Indian subcontinent, temporarily investingUchch,Lahore, andMultanof theDelhi Sultanateand stationing a Mongol overseer inKashmir,[45]though the invasions into India eventually failed and were forced to retreat. In northeastern Asia, Ögedei agreed to end the conflict withGoryeoby making it a client state and sent Mongol princesses to wed Goryeo princes. He then reinforced hiskheshigwith the Koreans through both diplomacy and military force.[46][47][48]
Theadvance into Europecontinued with Mongol invasions of Poland and Hungary. When the western flank of the Mongols plundered Polish cities, a European alliance among thePoles, theMoravians, and the Christian military orders of theHospitallers,Teutonic Knightsand theTemplarsassembled sufficient forces to halt, albeit briefly, the Mongol advanceat Legnica.
TheHungarianarmy, theirCroatian alliesand the Knights Templar were beaten by the Mongols at the banks of theSajo Riveron 11 April 1241. Before Batu's forces could continue on toViennaand northernAlbania, news of Ögedei's death in December 1241 brought a halt to the invasion.[49][50]As was customary in Mongol military tradition, all princes of Genghis's line had to attend the kurultai to elect a successor.
Batu and his western Mongol army withdrew from Central Europe the next year.[51]Today researchers doubt that Ögedei's death was the sole reason for the Mongols withdrawal. Batu did not return to Mongolia, so a new khan was not elected until 1246. Climatic and environmental factors, as well as the strong fortifications and castles of Europe, played an important role in the Mongols' decision to withdraw.[52][53]
Following the Great Khan Ögedei's death in 1241, and before the next kurultai, Ögedei's widowTöregenetook over the empire. She persecuted her husband's Khitan and Muslim officials and gave high positions to her own allies. She built palaces, cathedrals, and social structures on an imperial scale, supporting religion and education. She was able to win over most Mongol aristocrats to support Ögedei's sonGüyük. But Batu, ruler of theGolden Horde, refused to come to the kurultai, claiming that he was ill and that the climate was too harsh for him. The resulting stalemate lasted more than four years and further destabilized the unity of the empire.[54]
When Genghis Khan's youngest brotherTemügethreatened to seize the throne, Güyük came to Karakorum to try to secure his position.[55]Batu eventually agreed to send his brothers and generals to the kurultai convened by Töregene in 1246. Güyük by this time was ill and alcoholic, but his campaigns in Manchuria and Europe gave him the kind of stature necessary for a great khan. He was duly elected at a ceremony attended by Mongols and foreign dignitaries from both within and without the empire — leaders of vassal nations, representatives from Rome, and other entities who came to the kurultai to show their respects and conduct diplomacy.[56][57]
Güyük took steps to reduce corruption, announcing that he would continue the policies of his father Ögedei, not those of Töregene. He punished Töregene's supporters, except for governorArghun the Elder. He also replaced youngQara Hülëgü, the khan of theChagatai Khanate, with his favorite cousinYesü Möngke, to assert his newly conferred powers.[58]He restored his father's officials to their former positions and was surrounded by Uyghur,Naimanand Central Asian officials, favoringHan Chinesecommanders who had helped his father conquer Northern China. He continued military operations in Korea, advanced into Song China in the south, and into Iraq in the west, and ordered an empire-wide census. Güyük also divided the Sultanate of Rum betweenIzz-ad-Din KaykawusandRukn ad-Din Kilij Arslan, though Kaykawus disagreed with this decision.[58]
Not all parts of the empire respected Güyük's election. TheHashshashins, former Mongol allies whose Grand Master Hasan Jalalud-Din had offered his submission to Genghis Khan in 1221, angered Güyük by refusing to submit. Instead he murdered the Mongol generals in Persia. Güyük appointed his best friend's fatherEljigideias chief commander of the troops in Persia and gave them the task of both reducing the strongholds of theNizari Ismailisand conquering theAbbasidsat the center of the Islamic world, Iran andIraq.[58][59]
In 1248, Güyük raised more troops and suddenly marched westward from the Mongol capital of Karakorum. The reasoning was unclear. Some sources wrote that he sought to recuperate at his personal estate, Emyl; others suggested that he might have been moving to join Eljigidei to conduct a full-scale conquest of the Middle East, or possibly to make a surprise attack on his rival cousin Batu Khan inKievan Rus.[60]
Suspicious of Güyük's motives,Sorghaghtani Beki, the widow of Genghis's son Tolui, secretly warned her nephew Batu of Güyük's approach. Batu had himself been traveling eastward at the time, possibly to pay homage, or perhaps with other plans in mind. Before the forces of Batu and Güyük met, Güyük, sick and worn out by travel, died en route at Qum-Senggir (Hong-siang-yi-eulh) inXinjiang, possibly a victim of poison.[60]
Güyük's widowOghul Qaimishstepped forward to take control of the empire, but she lacked the skills of her mother-in-law Töregene, and her young sons Khoja and Naku and other princes challenged her authority. To decide on a new great khan, Batu called a kurultai on his own territory in 1250. As it was far from theMongol heartland, members of the Ögedeid and Chagataid families refused to attend. The kurultai offered the throne to Batu, but he rejected it, claiming he had no interest in the position.[61]Batu instead nominatedMöngke, a grandson of Genghis from his son Tolui's lineage. Möngke was leading a Mongol army in Rus, the northern Caucasus and Hungary. The pro-Tolui faction supported Batu's choice, and Möngke was elected; though given the kurultai's limited attendance and location, it was of questionable validity.[61]
Batu sent Möngke, under the protection of his brothers,Berkeand Tukhtemur, and his sonSartaqto assemble a more formal kurultai at Kodoe Aral in the heartland. The supporters of Möngke repeatedly invited Oghul Qaimish and the other major Ögedeid and Chagataid princes to attend the kurultai, but they refused each time. The Ögedeid and Chagataid princes refused to accept a descendant of Genghis's son Tolui as leader, demanding that only descendants of Genghis's son Ögedei could be great khan.[61]
When Möngke's mother Sorghaghtani and their cousinBerkeorganized a second kurultai on 1 July 1251, the assembled throng proclaimed Möngke great khan of the Mongol Empire. This marked a major shift in the leadership of the empire, transferring power from the descendants of Genghis's son Ögedei to the descendants of Genghis's son Tolui. The decision was acknowledged by a few of the Ögedeid and Chagataid princes, such as Möngke's cousinKadanand the deposed khan Qara Hülëgü, but one of the other legitimate heirs, Ögedei's grandson Shiremun, sought to topple Möngke.[62]
Shiremun moved with his own forces toward the emperor's nomadic palace with a plan for an armed attack, but Möngke was alerted by his falconer of the plan. Möngke ordered an investigation of the plot, which led to a series of major trials all across the empire. Many members of the Mongol elite were found guilty and put to death, with estimates ranging from 77 to 300, though princes of Genghis's royal line were often exiled rather than executed.[62]
Möngke confiscated the estates of the Ögedeid and the Chagatai families and shared the western part of the empire with his ally Batu Khan. After the bloody purge, Möngke ordered a general amnesty for prisoners and captives, but thereafter the power of the great khan's throne remained firmly with the descendants of Tolui.[62]
Möngke was a serious man who followed the laws of his ancestors and avoided alcoholism. He was tolerant of outside religions and artistic styles, leading to the building of foreign merchants' quarters,Buddhist monasteries,mosques, andChristian churchesin the Mongol capital. As construction projects continued, Karakorum was adorned with Chinese, European, andPersian architecture. One famous example was a large silver tree with cleverly designed pipes that dispensed various drinks. The tree, topped by a triumphant angel, was crafted byGuillaume Boucher, a Parisian goldsmith.[63]
Although he had a strong Chinese contingent, Möngke relied heavily on Muslim and Mongol administrators and launched a series of economic reforms to make government expenses more predictable. His court limited government spending and prohibited nobles and troops from abusing civilians or issuing edicts without authorization. He commuted the contribution system to a fixed poll tax which was collected by imperial agents and forwarded to units in need.[64]His court also tried to lighten thetax burdenon commoners by reducing tax rates. He also centralized control of monetary affairs and reinforced the guards at the postal relays. Möngke ordered an empire-wide census in 1252 that took several years to complete and was not finished untilNovgorodin the far northwest was counted in 1258.[64]
In another move to consolidate his power, Möngke assigned his brothersHulaguandKublaito rule Persia and Mongol-held China respectively. In the southern part of the empire he continued his predecessors' struggle against the Song dynasty. In order to outflank the Song from three directions, Möngke dispatched Mongol armies under his brother Kublai toYunnan, and under his uncle Iyeku to subdue Korea and pressure the Song from that direction as well.[58]
Kublai conquered theDali Kingdomin 1253 after the Dali King Duan Xingzhi defected to the Mongols and helped them conquer the rest ofYunnan. Möngke's general Qoridai stabilized his control over Tibet, inducing leading monasteries to submit to Mongol rule. Subutai's son Uryankhadai reduced the neighboring peoples of Yunnan to submission and went to war with the kingdom ofĐại Việtunder theTrần dynastyin northern Vietnam in 1258, but they had to draw back.[58]The Mongol Empire tried to invade Đại Việt again in 1285 and 1287 but were defeated both times.
After stabilizing the empire's finances, Möngke once again sought to expand its borders. At kurultais in Karakorum in 1253 and 1258 he approved new invasions of the Middle East andsouth China. Möngke put Hulagu in overall charge of military and civil affairs in Persia, and appointed Chagataids and Jochids to join Hulagu's army.[65]
The Muslims fromQazvindenounced the menace of theNizari Ismailis, a well-known sect ofShiites. The Mongol Naiman commanderKitbuqabegan to assault several Ismaili fortresses in 1253, before Hulagu advanced in 1256. Ismaili Grand MasterRukn al-Din Khurshahsurrendered in 1257 and was executed. All of theIsmaili strongholdsin Persia were destroyed by Hulagu's army in 1257, except forGirdkuhwhich held out until 1271.[65]
The center of the Islamic Empire at the time was Baghdad, which had held power for 500 years but was suffering internal divisions. When its caliphal-Mustasimrefused to submit to the Mongols,Baghdad was besieged and capturedby the Mongols in 1258 and subjected to a merciless sack, an event considered one of the most catastrophic events in the history of Islam, and sometimes compared to therupture of the Kaaba. With the destruction of the Abbasid Caliphate, Hulagu had an open route to Syria and moved against the other Muslim powers in the region.[66]
His army advanced towardAyyubid-ruled Syria, capturing small local states en route. The sultanAl-Nasir Yusufof the Ayyubids refused to show himself before Hulagu; however, he had accepted Mongol supremacy two decades earlier. When Hulagu headed further west, theArmeniansfromCilicia, theSeljuksfromRumand the Christian realms ofAntiochandTripolisubmitted to Mongol authority, joining them in their assault against the Muslims. While some cities surrendered without resisting, others, such as Mayafarriqin fought back; their populations were massacred and the cities were sacked.[66]
Meanwhile, in the northwestern portion of the empire, Batu's successor and younger brother Berke sent punitive expeditions toUkraine,Belarus,LithuaniaandPoland. Dissension began brewing between the northwestern and southwestern sections of the Mongol Empire as Batu suspected that Hulagu's invasion of Western Asia would result in the elimination of Batu's own dominance there.[67]
In the southern part of the empire, Möngke Khan himself led his army, but did not complete the conquest of China. Military operations were generally successful, but prolonged, so the forces did not withdraw to the north as was customary when the weather turned hot. Disease ravaged the Mongol forces with bloody epidemics, and Möngke died there on 11 August 1259. This event began a new chapter in the history of the Mongols, as again a decision needed to be made on a new great khan. Mongol armies across the empire withdrew from their campaigns to convene a new kurultai.[68]
Möngke's brother Hulagu broke off his successful military advance into Syria, withdrawing the bulk of his forces toMughanand leaving only a small contingent under his generalKitbuqa. The opposing forces in the region, the Christian Crusaders and Muslim Mamluks, both recognizing that the Mongols were the greater threat, took advantage of the weakened state of the Mongol army and engaged in an unusual passive truce with each other.[69]
In 1260, the Mamluks advanced from Egypt, being allowed to camp and resupply near the Christian stronghold ofAcre, and engaged Kitbuqa's forces just north of Galilee at theBattle of Ain Jalut. The Mongols were defeated, and Kitbuqa executed. This pivotal battle marked the western limit for Mongol expansion in the Middle East, and the Mongols were never again able to make serious military advances farther than Syria.[69]
In a separate part of the empire,Kublai Khan, another brother of Hulagu and Möngke, heard of the great khan's death at the Huai River in China. Rather than returning to the capital, he continued his advance into theWuchangarea of China, near theYangtze River. Their younger brotherAriqboketook advantage of the absence of Hulagu and Kublai, and used his position at the capital to win the title of great khan for himself, with representatives of all the family branches proclaiming him as the leader at the kurultai in Karakorum. When Kublai learned of this, he summoned his own kurultai atKaiping, and nearly all the senior princes and greatnoyansin North China and Manchuria supported his own candidacy over that of Ariqboke.[51]
Battles ensued between the armies of Kublai and those of his brother Ariqboke, which included forces still loyal to Möngke's previous administration. Kublai's army easily eliminated Ariqboke's supporters and seized control of the civil administration in southern Mongolia. Further challenges took place from their cousins, the Chagataids.[70][71][72]Kublai sent Abishka, a Chagataid prince loyal to him, to take charge of Chagatai's realm. But Ariqboke captured and then executed Abishka, having his own manAlghucrowned there instead. Kublai's new administration blockaded Ariqboke in Mongolia to cut off food supplies, causing a famine. Karakorum fell quickly to Kublai, but Ariqboke rallied and re-took the capital in 1261.[70][71][72]
In southwestern Ilkhanate, Hulagu was loyal to his brother Kublai, but clashes with their cousin Berke, a Muslim and the ruler of the Golden Horde, began in 1262. The suspicious deaths of Jochid princes in Hulagu's service, unequal distribution of war booty, and Hulagu's massacres of Muslims increased the anger of Berke, who considered supporting a rebellion of the Georgian Kingdom against Hulagu's rule in 1259–1260.[73]Berke also forged an alliance with the Egyptian Mamluks against Hulagu and supported Kublai's rival claimant, Ariqboke.[74]
Hulagu died on 8 February 1264. Berke sought to take advantage and invade Hulagu's realm, but he died along the way, and a few months later Alghu Khan of the Chagatai Khanate died as well. Kublai named Hulagu's sonAbaqaas new Ilkhan, and nominated Batu's grandsonMöngke Temürto lead the Golden Horde. Abaqa sought foreign alliances, such as attempting to form aFranco-Mongol allianceagainst the Egyptian Mamluks.[75]Ariqboqe surrendered to Kublai atShangduon 21 August 1264.[76]
In the south, afterthe fall of Xiangyangin 1273, the Mongols sought the final conquest of the Song dynasty in South China. In 1271, Kublai renamed the new Mongol regime in China as theYuan dynastyand sought tosinicizehis image asEmperor of Chinato win the control of the Chinese people. Kublai moved his headquarters to Khanbaliq, the genesis for what later became the modern city ofBeijing. His establishment of a capital there was a controversial move to many Mongols who accused him of being too closely tied toChinese culture.[77][78]
The Mongols were eventually successful in their campaigns against (Song) China, and the ChineseSong imperial familysurrendered to the Yuan in 1276, making the Mongols the first non-Chinese people to conquer all of China. Kublai used his base to build a powerful empire, creating an academy, offices, trade ports and canals, and sponsoring arts and science. Mongol records list 20,166 public schools created during his reign.[79]
After achieving actual or nominal dominion over much of Eurasia and successfully conquering China, Kublai pursued further expansion. Hisinvasions of BurmaandSakhalinwere costly, and hisattempted invasionsofĐại Việt(northern Vietnam) andChampa(southern Vietnam) ended in devastating defeat, but secured vassal statuses of those countries. The Mongol armies were repeatedly beaten in Đại Việt and were crushed at theBattle of Bạch Đằng (1288).
Nogai andKonchi, the khan of theWhite Horde, established friendly relations with the Yuan dynasty and the Ilkhanate. Political disagreement among contending branches of the family over the office of great khan continued, but the economic and commercial success of the Mongol Empire continued despite the squabbling.[80][81][82]
In 1274 and again in 1281, Kublai Khaninvaded Japanon two separate occasions. However, he was not able to conquer Japan. In the period of the Mongol invasion, thebattles of Bun'eiandKōanwere fought along the coast ofHakata Baynear modern-dayFukuoka.
Major changes occurred in the Mongol Empire in the late 1200s. Kublai Khan, after having conquered all of China and established the Yuan dynasty, died in 1294. He was succeeded by his grandsonTemür Khan, who continued Kublai's policies. At the same time theToluid Civil War, along with theBerke–Hulagu warand the subsequentKaidu–Kublai war, greatly weakened the authority of the great khan over the entirety of the Mongol Empire and the empire fractured into autonomous khanates, the Yuan dynasty and the three western khanates: the Golden Horde, the Chagatai Khanate and the Ilkhanate. Only the Ilkhanate remained loyal to the Yuan court but endured its own power struggle, in part because of a dispute with the growing Islamic factions within the southwestern part of the empire.[83]
After the death ofKaidu, the Chatagai ruler Duwa initiated a peace proposal and persuaded the Ögedeids to submit to Temür Khan.[84][85]In 1304, all of the khanates approved a peace treaty and accepted Yuan emperor Temür's supremacy.[86][87][88][89]This established the nominal supremacy of the Yuan dynasty over the western khanates, which was to last for several decades. This supremacy was based on weaker foundations than that of the earlier Khagans and each of the four khanates continued to develop separately and function as independent states.
Nearly a century of conquest and civil war was followed by relative stability, thePax Mongolica, and international trade and cultural exchanges flourished between Asia and Europe. Communication between the Yuan dynasty in China and the Ilkhanate in Persia further encouraged trade and commerce between east and west. Patterns of Yuan royal textiles could be found on the opposite side of the empire adorning Armenian decorations; trees and vegetables were transplanted across the empire; and technological innovations spread from Mongol dominions toward the West.[90]Pope John XXIIwas presented a memorandum from the eastern church describing the Pax Mongolica: "... Khagan is one of the greatest monarchs and all lords of the state, e.g., the king of Almaligh (Chagatai Khanate), emperor Abu Said and Uzbek Khan, are his subjects, saluting his holiness to pay their respects."[91]However, while the four khanates continued to interact with one another well into the 14th century, they did so assovereign statesand never again pooled their resources in a cooperative military endeavor.[12]
In spite of his conflicts with Kaidu and Duwa, Yuan emperor Temür established a tributary relationship with the war-likeShan peopleafter his series of military operations againstThailandfrom 1297 to 1303. This was to mark the end of the southern expansion of the Mongols.
WhenGhazantook the throne of the Ilkhanate in 1295, he formally accepted Islam as his own religion, marking a turning point in Mongol history after which Mongol Persia became more and more Islamic. Despite this, Ghazan continued to strengthen ties with Temür Khan and the Yuan dynasty in the east. It was politically useful to advertise the great khan's authority in the Ilkhanate, because theGolden Hordein Rus had long made claims on nearby Georgia.[83]Within four years, Ghazan began sending tribute to the Yuan court and appealing to other khans to accept Temür Khan as their overlord. He oversaw an extensive program of cultural and scientific interaction between the Ilkhanate and the Yuan dynasty in the following decades.[93]
Ghazan's faith may have been Islamic, but he continued his ancestors' war with the Egyptian Mamluks, and consulted with his old Mongol advisers in his native tongue. He defeated the Mamluk army at theBattle of Wadi al-Khazandarin 1299, but he was only briefly able to occupy Syria, due to distracting raids from theChagatai Khanateunder itsde factorulerKaidu, who wasat warwith both the Ilkhans and the Yuan dynasty.[citation needed]
Struggling for influence within the Golden Horde, Kaidu sponsored his own candidate Kobeleg againstBayan(r. 1299–1304), the khan of the White Horde. Bayan, after receiving military support from the Mongols in Rus, requested assistance from both Temür Khan and the Ilkhanate to organize a unified attack against Kaidu's forces. Temür was amenable and attacked Kaidu a year later. After a bloody battle with Temür's armies near theZawkhan Riverin 1301, Kaidu died and was succeeded byDuwa.[94][95]
Duwa was challenged by Kaidu's son Chapar, but with the assistance of Temür, Duwa defeated the Ögedeids.Tokhtaof the Golden Horde, also seeking a general peace, sent 20,000 men to buttress the Yuan frontier.[96]Tokhta died in 1312, though, and was succeeded byOzbeg(r. 1313–41), who seized the throne of the Golden Horde and persecuted non-Muslim Mongols. The Yuan's influence on the Horde was largely reversed and border clashes between Mongol states resumed.Ayurbarwada Buyantu Khan's envoys backed Tokhta's son against Ozbeg.[citation needed]
In the Chagatai Khanate,Esen Buqa I(r. 1309–1318) was enthroned as khan after suppressing a sudden rebellion by Ögedei's descendants and driving Chapar into exile. The Yuan and Ilkhanid armies eventually attacked the Chagatai Khanate. Recognising the potential economic benefits and the Genghisid legacy, Ozbeg reopened friendly relations with the Yuan in 1326. He strengthened ties with the Muslim world as well, building mosques and other elaborate structures such as baths.[citation needed]By the second decade of the 14th century, Mongol invasions had further decreased. In 1323,Abu Said Khan(r. 1316–35) of the Ilkhanate signed a peace treaty with Egypt. At his request, the Yuan court awarded his custodianChupanthe title of commander-in-chief of all Mongol khanates, but Chupan died in late 1327.[97]
Civil warerupted in the Yuan dynasty in 1328–29. After the death of Yesün Temür in 1328,Tugh Temürbecame the new leader in Khanbaliq, while Yesün Temür's sonRagibaghsucceeded to the throne in Shangdu, leading to the civil war known as theWar of the Two Capitals. Tugh Temür defeated Ragibagh, but the Chagatai khanEljigidey(r. 1326–29) supportedKusala, elder brother of Tugh Temür, as great khan. He invaded with a commanding force, and Tugh Temür abdicated. Kusala was elected khan on 30 August 1329. Kusala was then poisoned by aKypchakcommander under Tugh Temür, who returned to power.
Tugh Temür (1304–32) was knowledgeable about Chinese language and history and was also a creditable poet, calligrapher, and painter. In order to be accepted by other khanates as the sovereign of the Mongol world, he sent Genghisid princes and descendants of notable Mongol generals to the Chagatai Khanate, Ilkhan Abu Said, and Ozbeg. In response to the emissaries, they all agreed to send tribute each year.[98]Furthermore, Tugh Temür gave lavish presents and an imperial seal to Eljigidey to mollify his anger.
With the death of IlkhanAbu Said Bahaturin 1335, Mongol rule faltered and Persia fell into political anarchy. A year later his successor was killed by an Oirat governor, and the Ilkhanate was divided between the Suldus, theJalayir,QasaridTogha Temür(d. 1353), and Persian warlords. Taking advantage of the chaos, theGeorgianspushed the Mongols out of their territory, and the Uyghur commanderEretnaestablished an independent state (Eretnids) inAnatoliain 1336. Following the downfall of their Mongol masters, the loyal vassal, theArmenian Kingdom of Cilicia, received escalating threats from the Mamluks and were eventually overrun in 1375.[99]Along with the dissolution of the Ilkhanate in Persia, Mongol rulers in China and theChagatai Khanatewere also in turmoil. The plague known as theBlack Death, which started in the Mongol dominions and spread to Europe, added to the confusion.[100]Disease devastated all the khanates, cutting off commercial ties and killing millions.[101]The plague may have taken 50 million lives in Europe alone in the 14th century.[102]
As the power of the Mongols declined, chaos erupted throughout the empire as non-Mongol leaders expanded their own influence. The Golden Horde lost all of its western dominions (including modernBelarusandUkraine) to Poland andLithuaniabetween 1342 and 1369. Muslim and non-Muslim princes in the Chagatai Khanate warred with each other from 1331 to 1343, and the Chagatai Khanate disintegrated when non-Genghisid warlords set up their own puppet khans inTransoxianaandMoghulistan.JanibegKhan (r. 1342–1357) briefly reasserted Jochid dominance over the Chaghataids. Demanding submission from an offshoot of the Ilkhanate inAzerbaijan, he boasted that "today three uluses are under my control".[103]
However, rival families of the Jochids began fighting for the throne of the Golden Horde (Great Troubles, 1359–1381) after the assassination of his successorBerdibekKhan in 1359. The last Yuan rulerToghan Temür(r. 1333–70) was powerless to regulate those troubles, a sign that the empire had nearly reached its end. His court's unbacked currency had entered ahyperinflationary spiralandthe Han-Chinese people revolteddue to the Yuan's harsh impositions. In the 1350s,Gongmin of Goryeosuccessfully pushed Mongol garrisons back and exterminated the family of Toghan Temür Khan's empress whileTai Situ Changchub Gyaltsenmanaged to eliminate the Mongol influence in Tibet.[103]
Increasingly isolated from their subjects, the Mongols quickly lost most of China to the rebellious Ming forces and in 1368 fled to their heartland in Mongolia. After the overthrow of the Yuan dynasty the Golden Horde lost touch with Mongolia and China, while the two main parts of the Chagatai Khanate were defeated byTimur(Tamerlane) (1336–1405), who founded theTimurid Empire. However, remnants of the Chagatai Khanate survived; the last Chagataid state to survive was theYarkent Khanate, until its defeat by the OiratDzungar Khanatein theDzungar conquest of Altishahrin 1680. The Golden Horde broke into smaller Turkic-hordes that declined steadily in power over four centuries. Among them, the khanate's shadow, theGreat Horde, survived until 1502, when one of its successors, theCrimean Khanate, sacked Sarai.[104]The Crimean Khanate lasted until 1783, whereas khanates such as theKhanate of Bukharaand theKazakh Khanatelasted even longer.
The number of troops mustered by the Mongols is the subject of some scholarly debate,[105]but was at least 105,000 in 1206.[106]The Mongol military organization was simple but effective, based on thedecimalsystem. The army was built up from squads of ten men each, arbans (10 people), zuuns (100),Mingghans(1000), andtumens(10,000).[107]
The Mongols were most famous for theirhorse archers, but troops armed with lances were equally skilled, and the Mongols recruited other military specialists from the lands they conquered. With experiencedChineseengineers and a bombardier corps which was expert at buildingtrebuchets,catapultsand other machines, the Mongols could lay siege to fortified positions, sometimes building machinery on the spot using available local resources.[107]
Forces under the command of the Mongol Empire were trained, organized, and equipped for mobility and speed. Mongol soldiers were more lightly armored than many of the armies they faced but were able to make up for it with maneuverability. Each Mongol warrior would usually travel with multiple horses, allowing him to quickly switch to a fresh mount as needed. In addition, soldiers of the Mongol army functioned independently ofsupply lines, considerably speeding up army movement.[108]Skillful use of couriers enabled the leaders of these armies to maintain contact with each other.
Discipline was inculcated during anerge(traditional hunt), as reported byJuvayni. These hunts were distinctive fromhuntsin other cultures, being the equivalent to small unit actions. Mongol forces would spread out in a line, surround an entire region, and then drive all of thegamewithin that area together. The goal was to let none of the animals escape and to slaughter them all.[108]
Another advantage of the Mongols was their ability to traverse large distances, even in unusually cold winters; for instance, frozen rivers led them like highways to large urban centers on their banks. The Mongols were adept at river-work, crossing the riverSajóin spring flood conditions with thirty thousand cavalry soldiers in a single night during theBattle of Mohi(April 1241) to defeat the Hungarian kingBéla IV. Similarly, in the attack against the MuslimKhwarezmshaha flotilla of barges was used to prevent escape on the river.[citation needed]
Traditionally known for their prowess with ground forces, the Mongols rarely usednavalpower. In the 1260s and 1270s they used seapower while conquering theSong dynastyof China, though their attempts to mount seabornecampaigns against Japanwere unsuccessful. Around theEastern Mediterranean, their campaigns were almost exclusively land-based, with the seas controlled by theCrusaderandMamlukforces.[109]
All military campaigns were preceded by careful planning, reconnaissance, and the gathering of sensitive information relating to enemy territories and forces. The success, organization, and mobility of the Mongol armies permitted them to fight on several fronts at once. All adult males up to the age of 60 were eligible for conscription into the army, a source of honor in their tribal warrior tradition.[110]
The Mongol Empire was governed by a code of law devised by Genghis, calledYassa, meaning "order" or "decree". A particular canon of this code was that those of rank shared much the same hardship as the common man. It also imposed severe penalties,e.g., thedeath penaltyif one mounted soldier following another did not pick up something dropped from the mount in front. Penalties were also decreed for rape and to some extent for murder. Any resistance to Mongol rule was met with massive collective punishment. Cities weredestroyedand their inhabitants slaughtered if they defied Mongol orders.[citation needed]UnderYassa, chiefs and generals were selected based onmerit. The empire was governed by a non-democratic,parliamentary-style central assembly, calledkurultai, in which the Mongol chiefs met with the great khan to discuss domestic and foreign policies. Kurultais were also convened for the selection of each new great khan.[111]
The Mongols imported Central Asian Muslims to serve as administrators in China and sent Han Chinese and Khitans from China to serve as administrators over the Muslim population in Bukhara in Central Asia, thus using foreigners to curtail the power of the local peoples of both lands.[112]
At the time of Genghis Khan, virtually every religion had found Mongol converts, fromBuddhismtoChristianity, fromManichaeismtoIslam. To avoid strife, Genghis Khan set up an institution that ensured complete religious freedom, though he himself was ashamanist. Under his administration, all religious leaders were exempt from taxation and from public service.[113]
Initially there were few formal places of worship because of the nomadic lifestyle. However, under Ögedei (1186–1241), several building projects were undertaken in the Mongol capital. Along with palaces, Ögedei built houses of worship for the Buddhist, Muslim, Christian, andTaoistfollowers. The dominant religions at that time wereTengrismand Buddhism, although Ögedei's wife was a Nestorian Christian.[114]
Eventually, each of the successor states adopted the dominant religion of the local populations: the Mongol-ruled Chinese Yuan dynasty in the East (originally the Great Khan's domain) embraced Buddhism and Shamanism, while the three Western khanates adopted Islam.[115][116][117]
The oldest surviving literary work in theMongolian languageisThe Secret History of the Mongols, which was written for the royal family some time after Genghis Khan's death in 1227. It is the most significant native account of Genghis's life and genealogy, covering his origins and childhood through to the establishment of the Mongol Empire and the reign of his son, Ögedei.
Another classic from the empire is theJami' al-tawarikh, or "Universal History". It was commissioned in the early 14th century by the IlkhanGhazan Khanas a way of documenting the entire world's history, to help establish the Mongols' own cultural legacy.
Mongol scribes in the 14th century used a mixture of resin and vegetable pigments as a primitive form ofcorrection fluid;[118]this might be its first known usage.
The Mongols also appreciated the visual arts, though their taste in portraiture was strictly focused on portraits of their horses, rather than of people.[citation needed]
The Mongol Empire saw some significant developments in science due to the patronage of the Khans. Roger Bacon attributed the success of the Mongols as world conquerors principally to their devotion to mathematics.[119]Astronomy was one branch of science that the Khans took a personal interest in. According to theYuanshi, Ögedei Khan twice ordered the armillary sphere of Zhongdu to be repaired (in 1233 and 1236) and also ordered in 1234 the revision and adoption of the Damingli calendar.[120]He built a Confucian temple for Yelü Chucai in Karakorum around 1236 where Yelü Chucai created and regulated a calendar on the Chinese model.Möngke Khanwas noted by Rashid al-Din as having solved some of the difficult problems of Euclidean geometry on his own and written to his brother Hulagu Khan to send him the astronomerTusi.[121]Möngke Khan's desire to have Tusi build him an observatory in Karakorum did not reach fruition as the Khan died on campaign in southern China.Hulagu Khaninstead gave Tusi a grant to build the Maragheh Observatory in Persia in 1259 and ordered him to prepare astronomical tables for him in 12 years, despite Tusi asking for 30 years. Tusi successfully produced theIlkhanic Tablesin 12 years, produced a revised edition of Euclid's elements and taught the innovative mathematical device called theTusi couple. TheMaragheh observatoryheld around 400,000 books salvaged by Tusi from the siege of Baghdad and other cities. Chinese astronomers brought by Hulagu Khan worked there as well.
Kublai Khanbuilt a number of large observatories in China and his libraries included theWu-hu-lie-ti(Euclid) brought by Muslim mathematicians.[122]Zhu ShijieandGuo Shoujingwere notable mathematicians in Yuan China. The Mongol physicianHu Sihuidescribed the importance of a healthy diet in a 1330 medical treatise.
Ghazan Khan, able to understand four languages including Latin, built the Tabriz Observatory in 1295. The Byzantine Greek astronomerGregory Chioniadesstudied there underAjall Shams al-Din Omarwho had worked at Maragheh under Tusi. Chioniades played an important role in transmitting several innovations from the Islamic world to Europe. These include the introduction of the universal latitude-independent astrolabe to Europe and a Greek description of the Tusi-couple, which would later have an influence on Copernican heliocentrism. Choniades also translated several Zij treatises into Greek, including the Persian Zij-i Ilkhani by al-Tusi and the Maragheh observatory. TheByzantine-Mongol allianceand the fact that theEmpire of Trebizondwas anIlkhanatevassal facilitated Choniades' movements between Constantinople, Trebizond and Tabriz. Prince Radna, the Mongol viceroy of Tibet based in Gansu province, patronized the Samarkandi astronomer al-Sanjufini. The Arabic astronomical handbook dedicated by al-Sanjufini to Prince Radna, a descendant of Kublai Khan, was completed in 1363. It is notable for havingMiddle Mongolianglosses on its margins.[123]
The Mongol Empire had an ingenious and efficient mail system for the time, often referred to by scholars as theYam. It had lavishly furnished and well-guarded relay posts known asörtööset up throughout the Empire.[124]A messenger would typically travel 40 kilometres (25 miles) from one station to the next, either receiving a fresh, rested horse, or relaying the mail to the next rider to ensure the speediest possible delivery. The Mongol riders regularly covered 200 km (125 mi) per day, better than the fastest record set by thePony Expresssome 600 years later.[citation needed]The relay stations had attached households to service them. Anyone with apaizawas allowed to stop there for re-mounts and specified rations, while those carrying military identities used the Yam even without a paiza. Many merchants, messengers, and travelers from China, the Middle East, and Europe used the system. When the great khan died in Karakorum, news reached the Mongol forces underBatu Khanin Central Europe within 4–6 weeks thanks to the Yam.[49]
Genghis and his successorÖgedeibuilt a wide system of roads, one of which carved through theAltai mountains. After his enthronement, Ögedei further expanded the road system, ordering the Chagatai Khanate and Golden Horde to link up roads in western parts of the Mongol Empire.[125]
Kublai Khan, founder of theYuan dynasty, built special relays for high officials, as well as ordinary relays, that had hostels. During Kublai's reign, the Yuan communication system consisted of some 1,400 postal stations, which used 50,000 horses, 8,400 oxen, 6,700 mules, 4,000 carts, and 6,000 boats.[citation needed]
InManchuriaand southernSiberia, the Mongols still useddogsledrelays for the Yam. In the Ilkhanate, Ghazan restored the declining relay system in the Middle East on a restricted scale. He constructed some hostels and decreed that only imperial envoys could receive a stipend. The Jochids of the Golden Horde financed their relay system by a special Yam tax.[citation needed]
The Mongol unification in 1206 and subsequent peace in the region allowed for trade routes to form through the Mongol Empire.[126]The Mongols had a history of supporting merchants and trade. Genghis Khan had encouraged foreign merchants early in his career, even before uniting the Mongols. Merchants provided information about neighboring cultures, served as diplomats and official traders for the Mongols, and were essential for many goods, since the Mongols produced little of their own.
Mongol government and elites provided capital for merchants and sent them far afield, in anortoq(merchant partner) arrangement. In Mongol times, the contractual features of a Mongol-ortoqpartnership closely resembled that ofqiradandcommendaarrangements, however, Mongol investors were not constrained using uncoined precious metals and tradable goods for partnership investments and primarily financed money-lending and trade activities.[127]Moreover, Mongol elites formed trade partnerships with merchants from Italian cities, includingMarco Polo's family.[128]As the empire grew, any merchants or ambassadors with proper documentation and authorization received protection and sanctuary as they traveled through Mongol realms. Well-traveled and relatively well-maintained roads linked lands from the Mediterranean basin to China, greatly increasing overland trade and resulting in some dramatic stories of those who travelled through what would become known as theSilk Road.
Western explorerMarco Polotraveled east along the Silk Road, and the Chinese Mongol monkRabban Bar Saumamade a comparably epic journey along the route, venturing from his home ofKhanbaliq(Beijing) as far as Europe. European missionaries, such asWilliam of Rubruck, also traveled to the Mongol court to convert believers to their cause, or went as papal envoys to correspond with Mongol rulers in an attempt to secure aFranco-Mongol alliance. It was rare, however, for anyone to journey the full length of Silk Road. Instead, merchants moved products like a bucket brigade, goods being traded from one middleman to another, moving from China all the way to the West; the goods moved over such long distances fetched extravagant prices.[citation needed]
After Genghis, the merchant partner business continued to flourish under his successors Ögedei and Güyük. Merchants brought clothing, food, information, and other provisions to the imperial palaces, and in return the great khans gave the merchants tax exemptions and allowed them to use the official relay stations of the Mongol Empire. Merchants also served as tax farmers in China, Rus and Iran. If the merchants were attacked by bandits, losses were made up from local residents who were obliged to find the stolen goods.[129][130]
Policies changed under the Great KhanMöngke. Because of money laundering and overtaxing, he attempted to limit abuses and sent imperial investigators to supervise theortoqbusinesses. He decreed that all merchants must pay commercial and property taxes, and he paid off all drafts drawn by high-ranking Mongol elites from the merchants. This policy continued under the Yuan dynasty.[129]
The fall of the Mongol Empire in the 14th century led to the collapse of the political, cultural, and economic unity along the Silk Road. Turkic tribes seized the western end of the route from theByzantine Empire, sowing the seeds of a Turkic culture that would later crystallize into theOttoman Empireunder theSunnifaith. In the East, the Han Chinese overthrew the Yuan dynasty in 1368, launching their ownMing dynastyand pursuing a policy of economic isolationism.[131]
The Mongol Empire, at its height of the largest contiguous empire in history, had a lasting impact, unifying large regions. Some of these (such as eastern and western Russia, and the western parts of China) remain unified today.[132]Mongols might have been assimilated into local populations after the fall of the empire, and some of their descendants adopted localreligions. For example, the eastern khanate largely adoptedBuddhism, and the three western khanates adoptedIslam, largely underSufiinfluence.[115]
According to some[specify]interpretations, Genghis Khan's conquests causedwholesale destruction on an unprecedented scalein certain geographic regions, leading to changes in the demographics of Asia.
The non-military achievements of the Mongol Empire include the introduction of a writing system, a Mongol alphabet based on the characters of theOld Uyghur, which is still used inMongoliatoday.[133]
Some of the other long-term consequences of the Mongol Empire include:
[I]n this region [Mesopotamia] nomadism really did attempt, and really did to a very considerable degree succeed in its attempt, to stamp a settled civilized system out of existence. When Jengis Khan first invaded China, we are told that there was a serious discussion among the Mongol chiefs whether all the towns and settled populations should not be destroyed. To these simple practitioners of the open-air life the settled populations seemed corrupt, crowded, vicious, effeminate, dangerous, and incomprehensible; a detestable human efflorescence upon what would otherwise have been good pasture. They had no use whatever for the towns. **** But it was only under Hulagu in Mesopotamia that these ideas seem to have been embodied in a deliberate policy. The Mongols here did not only burn and massacre; they destroyed the irrigation system that had endured for at least eight thousand years, and with that the mother civilization of all the Western world came to an end.[137]
Thehistory of Chinaspans several millennia across a wide geographical area. Each region now considered part of the Chinese world has experienced periods of unity, fracture, prosperity, and strife. Chinese civilization first emerged in theYellow Rivervalley, which along with theYangtzebasin constitutes the geographic core of theChinese cultural sphere. China maintains a rich diversity of ethnic and linguistic people groups. Thetraditional lensfor viewing Chinese history is thedynastic cycle: imperial dynasties rise and fall, and are ascribed certain achievements. Throughout pervades the narrative that Chinese civilization can be traced as an unbroken threadmany thousands of years into the past, making it one of thecradles of civilization. At various times, states representative of a dominant Chinese culture have directly controlled areas stretching as far west as theTian Shan, theTarim Basin, and theHimalayas, as far north as theSayan Mountains, and as far south as thedelta of the Red River.
TheNeolithicperiod saw increasingly complex polities begin to emerge along theYellowandYangtzerivers. TheErlitou culturein thecentral plains of Chinais sometimes identified with theXia dynasty(3rd millennium BC) of traditionalChinese historiography. The earliest survivingwritten Chinesedates to roughly 1250 BC, consisting of divinations inscribed onoracle bones.Chinese bronze inscriptions, ritual texts dedicated to ancestors, form another large corpus of early Chinese writing. The earliest strata of received literature in Chinese includepoetry,divination, andrecords of official speeches. China is believed to be one of a very few loci of independent invention of writing, and the earliest surviving records display an already-mature written language. Thecultureremembered by the earliestextant literatureis that of theZhou dynasty(c.1046– 256 BC), China'sAxial Age, during which theMandate of Heavenwas introduced, and foundations laid for philosophies such asConfucianism,Taoism,Legalism, andWuxing.
China wasfirst unitedunder a single imperial state byQin Shi Huangin 221 BC.Orthography, weights, measures, and law were all standardized. Shortly thereafter, China entered its classical era with theHan dynasty(202 BC – 220 AD), marking a critical period. A term for the Chinese language is still "Han language", and the dominant Chinese ethnic group is known asHan Chinese. The Chinese empire reached some of its farthest geographical extents during this period. Confucianism was officially sanctioned and itscore textswere edited into their received forms. Wealthy landholding families independent of the ancient aristocracy began to wield significant power. Han technology can be considered on par with that of the contemporaneousRoman Empire: mass production of paper aided the proliferation of written documents, and the written language of this period was employed for millennia afterwards. China became known internationally for itssericulture. When the Han imperial order finally collapsed after four centuries, China entered an equally lengthy period of disunity, during whichBuddhismbegan to have a significant impact on Chinese culture, whilecalligraphy, art, historiography, and storytelling flourished. Wealthy families in some cases became more powerful than the central government. The Yangtze River valley was incorporated into the dominant cultural sphere.
A period of unity began in 581 with theSui dynasty, which soon gave way to the long-livedTang dynasty(608–907), regarded as another Chinese golden age. The Tang dynasty saw flourishing developments in science, technology, poetry, economics, and geographical influence. China's only officially recognized empress,Wu Zetian, reigned during the dynasty's first century. Buddhism was adopted by Tang emperors. "Tang people" is the other common demonym for the Han ethnic group. After the Tang fractured, theSong dynasty(960–1279) saw the maximal extent of imperial Chinese cosmopolitan development.Mechanical printingwas introduced, and many of the earliest surviving witnesses of certain texts arewood-block printsfrom this era. Song scientific advancement led the world, and theimperial examination systemgave ideological structure to the political bureaucracy. Confucianism and Taoism were fully knit together inNeo-Confucianism.
Eventually, theMongol Empireconquered all of China, establishing theYuan dynastyin 1271. Contact with Europe began to increase during this time. Achievements under the subsequentMing dynasty(1368–1644) includeglobal exploration, fineporcelain, and many extant public works projects, such as those restoring theGrand CanalandGreat Wall. Three of the fourClassic Chinese Novelswere written during the Ming. TheQing dynastythat succeeded the Ming was ruled by ethnicManchupeople. TheQianlongemperor (r.1735–1796) commissioneda complete encyclopaediaof imperial libraries, totaling nearly a billion words. Imperial China reached its greatest territorial extent of during the Qing, but China came into increasing conflict with European powers, culminating in theOpium Warsand subsequentunequal treaties.
The 1911Xinhai Revolution, led bySun Yat-senand others, created theRepublic of China. From 1927 to 1949, acostly civil warroiled between the Republican government underChiang Kai-shekand the Communist-alignedChinese Red Army, interrupted by the industrializedEmpire of Japaninvading the divided country until its defeat in the Second World War.
After theCommunistvictory,Mao Zedongproclaimed the establishment of thePeople's Republic of China(PRC) in 1949, with the ROC retreating to Taiwan. Both governments still claim sole legitimacy of the entire mainland area. The PRC has slowly accumulated the majority of diplomatic recognition, and Taiwan's status remains disputed to this day. From 1966 to 1976, theCultural Revolutionin mainland China helped consolidate Mao's power towards the end of his life. After his death, the government beganeconomic reformsunderDeng Xiaoping, and became the world'sfastest-growing major economy.[when?]China had been the most populous nation in the world for decades since its unification, until it was surpassed byIndiain 2023.
Thearchaic humanspecies ofHomo erectusarrived inEurasiasometime between 1.3 and 1.8million years ago(Ma) and numerous remains of its subspecies have been found in what is now China.[1]The oldest of these is the southwesternYuanmou Man(元谋人; inYunnan), dated toc.1.7 Ma, which lived in a mixedbushland-forest environment alongsidechalicotheres,deer, the elephantStegodon,rhinos, cattle, pigs, and thegiant short-faced hyena.[2]The better-knownPeking Man(北京猿人; near Beijing) of 700,000–400,000BP,[1]was discovered in theZhoukoudiancave alongsidescrapers,choppers, and, dated slightly later, points,burins, and awls.[3]OtherHomo erectusfossils have been found widely throughout the region, including the northwesternLantian ManinShaanxi, as well minor specimens in northeasternLiaoningand southernGuangdong.[1]The dates of mostPaleolithic siteswere long debated but have been more reliably established based on modernmagnetostratigraphy: Majuangou at 1.66–1.55 Ma, Lanpo at 1.6 Ma,Xiaochangliangat 1.36 Ma, Xiantai at 1.36 Ma,Banshanat 1.32 Ma, Feiliang at 1.2 Ma and Donggutuo at 1.1 Ma.[4]Evidence of fire use byHomo erectusoccurred between 1–1.8 million years BP at the archaeological site ofXihoudu, Shanxi Province.[5]
The circumstances surrounding theevolutionofHomo erectusto contemporaryH. sapiensis debated; the three main theories include the dominant"Out of Africa" theory(OOA), theregional continuity modeland the admixture variant of the OOA hypothesis.[1]Regardless, the earliest modern humans have been dated to China at 120,000–80,000 BP based on fossilized teeth discovered inFuyan CaveofDao County, Hunan.[6]The larger animals which lived alongside these humans include the extinctAiluropoda baconipanda, theCrocuta ultimahyena, theStegodon, and thegiant tapir.[6]Evidence ofMiddle PalaeolithicLevalloistechnology has been found in the lithic assemblage ofGuanyindongCave site in southwest China, dated to approximately 170,000–80,000 years ago.[7]
TheNeolithic Agein China is considered to have begun about 10,000 years ago.[8]Because the Neolithic is conventionally defined by the presence of agriculture, it follows that the Neolithic began at different times in the various regions of what is now China. Agriculture in China developed gradually, with initial domestication of a few grains and animals gradually expanding with the addition of many others over subsequent millennia.[9]The earliest evidence of cultivated rice, found by the Yangtze River, was carbon-dated to 8,000 years ago.[10]Early evidence formilletagriculture in the Yellow River valley wasradiocarbon-datedto about 7000 BC.[11]TheJiahusite is one of the best preserved early agricultural villages (7000 to 5800 BC). AtDamaidiin Ningxia, 3,172cliff carvingsdating to 6000–5000 BC have been discovered, "featuring 8,453 individual characters such as the sun, moon, stars, gods and scenes of hunting or grazing", according to researcher Li Xiangshi. Written symbols, sometimes calledproto-writing, were found at the site of Jiahu, which is dated around 7000 BC,[12]Damaidi around 6000 BC,Dadiwanfrom 5800 BC to 5400 BC,[13]andBanpodating from the 5th millennium BC. With agriculture came increased population, the ability to store and redistribute crops, and the potential to support specialist craftsmen and administrators, which may have existed at late Neolithic sites likeTaosiand theLiangzhu culturein the Yangtze delta.[10]The cultures of the middle and late Neolithic in the central Yellow River valley are known, respectively, as theYangshao culture(5000 BC to 3000 BC) and theLongshan culture(3000 BC to 2000 BC). Pigs and dogs were the earliest-domesticated animals in the region, and after about 3000 BC domesticated cattle and sheep arrived from Western Asia. Wheat also arrived at this time but remained a minor crop. Fruit such aspeaches,cherriesandoranges, as well as chickens and various vegetables, were also domesticated in Neolithic China.[9]
Bronze artifacts have been found at theMajiayao culturesite (between 3100 and 2700 BC).[14][15]The Bronze Age is also represented at theLower Xiajiadian culture(2200–1600 BC)[16]site in northeast China.Sanxingduilocated in what is nowSichuanis believed to be the site of a major ancient city, of a previously unknown Bronze Age culture (between 2000 and 1200 BC). The site was first discovered in 1929 and then re-discovered in 1986. Chinese archaeologists have identified the Sanxingdui culture to be part of thestate of Shu, linking the artifacts found at the site to its early legendary kings.[17][18]
Ferrous metallurgybegins to appear in the late 6th century in theYangtzevalley.[19]A bronze hatchet with a blade ofmeteoric ironexcavated near the city ofGaochenginShijiazhuang(nowHebei) has been dated to the 14th century BC. An Iron Age culture of theTibetan Plateauhas tentatively been associated with theZhang Zhung culturedescribed in early Tibetan writings.
Chinese historians in later periods were accustomed to the notion of one dynasty succeeding another, but the political situation in early China was much more complicated. Hence, as some scholars of China suggest, the Xia and the Shang can refer to political entities that existed concurrently, just as the early Zhou existed at the same time as the Shang.[20]This bears similarities to how China, both contemporaneously and later, has been divided into states that were not one region, legally or culturally.[21]
The earliest period once considered historical was the legendary era of the sage-emperorsYao,Shun, andYu. Traditionally, theabdication systemwas prominent in this period,[22]with Yao yielding his throne to Shun, who abdicated to Yu, who founded the Xia dynasty.
TheXia dynasty(c.2070– c.1600 BC) is the earliest of the three dynasties described in much later traditional historiography, which includes theBamboo AnnalsandSima Qian'sShiji(c.91 BC). The Xia is generally considered mythical by Western scholars, but in China it is usually associated with the early Bronze Age site atErlitou(1900–1500 BC) in Henan that was excavated in 1959. Since no writing was excavated at Erlitou or any other contemporaneous site, there is not enough evidence to prove whether the Xia dynasty ever existed. Some archaeologists claim that the Erlitou site was the capital of the Xia.[23]In any case, the site of Erlitou had a level of political organization that would not be incompatible with the legends of Xia recorded in later texts.[24]More importantly, the Erlitou site has the earliest evidence for an elite who conducted rituals using cast bronze vessels, which would later be adopted by the Shang and Zhou.[25]
Both archaeological evidence like oracle bones and bronzes, as well as transmitted texts attest the historical existence of the Shang dynasty (c.1600– c.1046 BC). Findings from the earlier Shang period come from excavations atErligang(modernZhengzhou). Findings have been found atYinxu(near modernAnyang, Henan), the site of the final Shang capital during theLate Shangperiod (c.1250–1050 BC).[26]The findings at Anyang include the earliest written record of the Chinese so far discovered: inscriptions of divination records in ancient Chinese writing on the bones or shells of animals—theoracle bones, dating fromc.1250– c.1046 BC.[27]
A series of at least twenty-nine kings reigned over the Shang dynasty.[28]Throughout their reigns, according to theShiji, the capital city was moved six times.[29]The final and most important move was toYinduring the reign ofWu Dingc.1250 BC.[30]The term Yin dynasty has been synonymous with the Shang dynasty in history, although it has lately been used to refer specifically to the latter half of the Shang dynasty.[28]
Although written records found at Anyang confirm the existence of the Shang dynasty,[31]Western scholars are often hesitant to associate settlements that are contemporaneous with the Anyang settlement with the Shang dynasty. For example, archaeological findings atSanxingduisuggest a technologically advanced civilization culturally unlike Anyang. The evidence is inconclusive in proving how far the Shang realm extended from Anyang. The leading hypothesis is that Anyang, ruled by the same Shang in the official history, coexisted and traded with numerous other culturally diverse settlements in the area that is now referred to asChina proper.[32]
The Zhou dynasty (1046 BC to about 256 BC) is the longest-lasting dynasty in Chinese history, though its power declined steadily over the almost eight centuries of its existence. In the late 2nd millennium BC, the Zhou dynasty arose in the Wei River valley of modern western Shaanxi Province, where they were appointed Western Protectors by theShang. A coalition led by the ruler of the Zhou,King Wu, defeated the Shang at theBattle of Muye. They took over most of the central and lower Yellow River valley and enfeoffed their relatives and allies in semi-independent states across the region.[33]Several of these states eventually became more powerful than the Zhou kings.
The kings of Zhou invoked the concept of theMandate of Heavento legitimize their rule, a concept that was influential for almost every succeeding dynasty.[34]Like Shangdi, Heaven (tian) ruled over all the other gods, and it decided who would rule China.[35]It was believed that a ruler lost the Mandate of Heaven when natural disasters occurred in great number, and when, more realistically, the sovereign had apparently lost his concern for the people. In response, the royal house would be overthrown, and a new house would rule, having been granted the Mandate of Heaven.
The Zhou established two capitalsZongzhou(near modernXi'an) andChengzhou(Luoyang), with the king's court moving between them regularly. The Zhou alliance gradually expanded eastward into Shandong, southeastward into the Huai River valley, and southward into theYangtze Rivervalley.[33]
In 771 BC,King Youand his forces were defeated in theBattle of Mount Liby rebel states andQuanrongbarbarians. The rebel aristocrats established a new ruler,King Ping, inLuoyang,[36]: 4beginning the second major phase of the Zhou dynasty: the Eastern Zhou period, which is divided into the Spring and Autumn and Warring States periods. The former period is named after the famousSpring and Autumn Annals. The sharply reduced political authority of the royal house left a power vacuum at the center of the Zhou culture sphere. The Zhou kings had delegated local political authority to hundreds ofsettlement states, some of them only as large as a walled town and surrounding land. These states began to fight against one another and vie forhegemony. The more powerful states tended to conquer and incorporate the weaker ones, so the number of states declined over time.[37]By the 6th century BC most small states had disappeared by being annexed and just a few large and powerful principalities remained. Some southern states, such as Chu and Wu, claimed independence from the Zhou, who undertook wars against some of them (Wu and Yue). Many new cities were established in this period and society gradually became more urbanized and commercialized. Many famous individuals such asLaozi,ConfuciusandSun Tzulived during this chaotic period.
Conflict in this period occurred both between and within states. Warfare between states forced the surviving states to develop better administrations to mobilize more soldiers and resources. Within states there was constant jockeying between elite families. For example, the three most powerful families in the Jin state—Zhao, Wei and Han—eventually overthrew the ruling family andpartitioned the state between them.
TheHundred Schools of Thoughtofclassical Chinese philosophybegan blossoming during this period and the subsequent Warring States period. Such influential intellectual movements asConfucianism,Taoism,LegalismandMohismwere founded, partly in response to the changing political world. The first two philosophical thoughts would have an enormous influence on Chinese culture.
After further political consolidations, seven prominent states remained during the 5th centuryBC. The years in which these states battled each other is known as theWarring Statesperiod. Though theZhouking nominally remained as such until 256BC, he was largely a figurehead that held little real power.
Numerous developments were made during this period in the areas of culture and mathematics—including theZuo Zhuanwithin theSpring and Autumn Annals(a literary work summarizing the preceding Spring and Autumn period), and the bundle of 21 bamboo slips from theTsinghuacollection, dated to 305BC—being the world's earliest known example of a two-digit, base-10 multiplication table. The Tsinghua collection indicates that sophisticated commercial arithmetic was already established during this period.[38]
As neighboring territories of the seven states were annexed (including areas of modernSichuanandLiaoning), they were now to be governed under an administrative system ofcommanderiesandprefectures. This system had been in use elsewhere since the Spring and Autumn period, and its influence on administration would prove resilient—its terminology can still be seen in the contemporaneousshengandxian("provinces" and "counties") of contemporary China.
The state ofQinbecame dominant in the waning decades of the Warring States period, conquering theShucapital ofJinshaon the Chengdu Plain; and then eventually drivingChufrom its place in the Han River valley. Qin imitated the administrative reforms of the other states, thereby becoming a powerhouse.[9]Its final expansion began during the reign ofYing Zheng, ultimately unifying the other six regional powers, and enabling him to proclaim himself as China's firstemperor—known to history asQin Shi Huang.
Ying Zheng's establishment of the Qin dynasty (秦朝) in 221 BC effectively formalised the region as a true empire for the first time in Chinese history, rather than a state, and its pivotal status probably led to "Qin" (秦) later evolving into the Western term "China".[39]To emphasise his sole rule, Zheng proclaimed himselfShi Huangdi(始皇帝; "First Emperor"); theHuangdititle, derived fromChinese mythology, became the standard for subsequent rulers.[40][a]Based inXianyang, the empire was a centralized bureaucratic monarchy, a governing scheme which dominated the future of Imperial China.[42][43]In an effort to improve the Zhou's perceived failures, this system consisted of more than 36commanderies(郡;jun),[b]made up ofcounties(县;xian) and progressively smaller divisions, each with a local leader.[46]
Many aspects of society were informed byLegalism, a state ideology promoted by the emperor and hischancellorLi Sithat was introduced at an earlier time byShang Yang.[47]In legal matters this philosophy emphasised mutual responsibility in disputes and severe punishments for crime, while economic practices included the general encouragement of agriculture and repression of trade.[47]Reforms occurred in weights and measures, writing styles (seal script) and metal currency (Ban Liang), all of which were standardized.[48][49]Traditionally, Qin Shi Huang is regarded as ordering amass burning of books and the live burial of scholarsunder the guise of Legalism, though contemporary scholars express considerable doubt on thehistoricity of this event.[47]Despite its importance, Legalism was probably supplemented in non-political matters byConfucianismfor social and moral beliefs and the five-elementWuxing(五行) theories forcosmologicalthought.[50]
The Qin administration kept exhaustive records on their population, collecting information on their sex, age, social status and residence.[51]Commoners, who made up over 90% of the population,[52]"suffered harsh treatment" according to the historianPatricia Buckley Ebrey, as they were often conscripted into forced labor for the empire's construction projects.[53]This included a massive system of imperial highways in 220 BC, which ranged around 4,250 miles (6,840 km) altogether.[54]Other major construction projects were assigned to the generalMeng Tian, who concurrentlyled a successful campaignagainst the northernXiongnupeoples (210s BC), reportedly with 300,000 troops.[54][c]Under Qin Shi Huang's orders, Meng supervised the combining of numerous ancient walls into what came to be known as theGreat Wall of Chinaand oversaw the building of a 500 miles (800 km) straight highway between northern and southern China.[56]The emperor also oversaw the construction of hismonumental mausoleum, which includes the well knownTerracotta Army.[57]
After Qin Shi Huang's death the Qin government drastically deteriorated and eventually capitulated in 207 BC after the Qin capital was captured and sacked by rebels, which would ultimately lead to the establishment of the Han Empire.[58][59]
The Han dynasty was founded byLiu Bang, who emerged victorious in theChu–Han Contentionthat followed the fall of the Qin dynasty. Agolden agein Chinese history, the Han dynasty's long period of stability and prosperity consolidated the foundation of China as a unified state under a central imperial bureaucracy, which was to last intermittently for most of the next two millennia. During the Han dynasty, territory of China was extended to most of theChina properand to areas far west.Confucianismwas officially elevated to orthodox status and was to shape the subsequent Chinese civilization. Art, culture and science all advanced to unprecedented heights. With the profound and lasting impacts of this period of Chinese history, the dynasty name "Han" had been taken as the name of the Chinese people, now thedominant ethnic groupin modern China, and had been commonly used to refer to Chinese language andwritten characters.
After theinitial laissez-faire policiesof EmperorsWenandJing, the ambitiousEmperor Wubrought the empire to its zenith. To consolidate his power, he disenfranchised the majority of imperial relatives, appointing military governors to control their former lands.[60]As a further step, he extended patronage to Confucianism, which emphasizes stability and order in a well-structured society.Imperial Universitieswere established to support its study. At the urging of his Legalist advisors, however, he also strengthened the fiscal structure of the dynastywith government monopolies.
Major military campaignswere launched to weaken the nomadicXiongnu Empire, limiting their influence north of the Great Wall. Along with the diplomatic efforts led byZhang Qian, the sphere of influence of the Han Empire extended to thestates in the Tarim Basin, opened up theSilk Roadthat connected China to the west, stimulating bilateral trade and cultural exchange. To the south, various small kingdoms far beyond the Yangtze River Valley were formally incorporated into the empire.
Emperor Wu also dispatched aseries of military campaignsagainst theBaiyuetribes. The Han annexedMinyue in 135 BCand 111 BC,Nanyue in 111 BC, andDian in 109 BC.[61]Migration and military expeditions led to the cultural assimilation of the south.[62]It also brought the Han into contact with kingdoms in Southeast Asia, introducing diplomacy and trade.[63]
After Emperor Wu the empire slipped into gradual stagnation and decline. Economically, the state treasury was strained by excessive campaigns and projects, while land acquisitions by elite families gradually drained the tax base. Variousconsort clansexerted increasing control over strings of incompetent emperors and eventually the dynasty was briefly interrupted by the usurpation ofWang Mang.
In AD 9 the usurperWang Mangclaimed that theMandate of Heavencalled for the end of the Han dynasty and the rise of his own, and he founded the short-lived Xin dynasty. Wang Mang started an extensive program of land and other economic reforms, including the outlawing of slavery and land nationalization and redistribution. These programs, however, were never supported by the landholding families, because they favored the peasants. The instability of power brought about chaos, uprisings, and loss of territories. This was compounded by mass flooding of theYellow River; silt buildup caused it to split into two channels and displaced large numbers of farmers. Wang Mang was eventually killed inWeiyang Palaceby an enraged peasant mob in AD 23.
Emperor Guangwureinstated the Han dynasty with the support of landholding and merchant families atLuoyang,eastof the former capital Xi'an. Thus, this new era is termed theEastern Han dynasty. With the capable administrations of EmperorsMingandZhang, former glories of the dynasty were reclaimed, with brilliant military and cultural achievements. TheXiongnu Empirewasdecisively defeated. The diplomat and generalBan Chaofurther expanded the conquests across thePamirsto the shores of theCaspian Sea,[64]: 175thus reopening theSilk Road, and bringing trade, foreign cultures, along with thearrival of Buddhism. With extensive connections with the west, the first of severalRoman embassies to Chinawere recorded in Chinese sources, coming from the sea route in AD 166, and a second one in AD 284.
The Eastern Han dynasty was one of themost prolific eras of science and technologyin ancient China, notably the historic invention ofpapermakingbyCai Lun, and the numerous scientific and mathematical contributions by the famouspolymathZhang Heng.
By the 2nd century, the empire declined amidst land acquisitions, invasions, and feuding betweenconsort clansandeunuchs. TheYellow Turban Rebellionbroke out in AD 184, ushering in an era ofwarlords. In the ensuing turmoil, three states emerged, trying to gain predominance and reunify the land, giving this historical period its name. The classic historical novelRomance of the Three Kingdomsdramatizes events of this period.
The warlordCao Caoreunified the north in 208, and in 220 his son accepted the abdication ofEmperor Xian of Han, thus initiating theWeidynasty. Soon, Wei's rivalsShuandWuproclaimed their independence. This period was characterized by a gradual decentralization of the state that had existed during the Qin and Han dynasties, and an increase in the power of great families.
In 266, theJin dynastyoverthrew the Wei and later unified the country in 280, but this union was short-lived.
TheJin dynastyreunited China proper for the first time since the end of theHan dynasty, ending theThree Kingdomsera. However, the Jin dynasty was severely weakened by theWar of the Eight Princesand lost control of northern China afternon-Han Chinese settlers rebelledand capturedLuoyangandChang'an. In 317, the Jin princeSima Rui, based in modern-dayNanjing, became emperor and continued the dynasty, now known as the Eastern Jin, which held southern China for another century. Prior to this move, historians refer to the Jin dynasty as the Western Jin.
Northern China fragmented into a series of independent states known as theSixteen Kingdoms, most of which were founded byXiongnu,Xianbei,Jie,DiandQiangrulers. These non-Han peoples were ancestors of theTurks,Mongols, andTibetans. Many had, to some extent, been "sinicized" long before their ascent to power. In fact, some of them, notably theQiangand the Xiongnu, had already been allowed to live in the frontier regions within theGreat Wallsince late Han times. During this period, warfare ravaged the north and prompted large-scale Han Chinese migration south to the Yangtze River Basin and Delta.
In the early 5th century China entered a period known as the Northern and Southern dynasties, in which parallel regimes ruled the northern and southern halves of the country. In the south, the Eastern Jin gave way to theLiu Song,Southern Qi,Liangand finallyChen. Each of these Southern dynasties were led by Han Chinese ruling families and usedJiankang(modern Nanjing) as the capital. They held off attacks from the north and preserved many aspects of Chinese civilization, while northern barbarian regimes began tosinify.
In the north the last of the Sixteen Kingdoms was extinguished in 439 by theNorthern Wei, a kingdom founded by theXianbei, a nomadic people who unified northern China. The Northern Wei eventually split into theEasternandWestern Wei, which then became theNorthern QiandNorthern Zhou. These regimes were dominated by Xianbei or Han Chinese who had married into Xianbei families. During this period most Xianbei people adopted Han surnames, eventually leading to complete assimilation into the Han.
Despite the division of the country, Buddhism spread throughout the land. In southern China, fierce debates about whetherBuddhismshould be allowed were held frequently by the royal court and nobles. By the end of the era, Buddhists andTaoistshad become much more tolerant of each other.[65]
The short-lived Sui dynasty was a pivotal period in Chinese history. Founded byEmperor Wenin 581 in succession of theNorthern Zhou, the Sui went on to conquer theSouthern Chenin 589 to reunify China, ending three centuries of political division. The Sui pioneered many new institutions, including the government system ofThree Departments and Six Ministries,imperial examinationsfor selecting officials from commoners, while improved on the systems offubing systemof the army conscription and theequal-field systemof land distributions. These policies, which were adopted by later dynasties, brought enormous population growth, and amassed excessive wealth to the state.Standardized coinagewas enforced throughout the unified empire. Buddhism took root as a prominent religion and was supported officially. Sui China was known for its numerous mega-construction projects. Intended for grains shipment and transporting troops, theGrand Canalwas constructed, linking the capitalsDaxing (Chang'an)andLuoyangto the wealthysoutheast region, and in another route, to the northeast border. TheGreat Wallwas also expanded, while series of military conquests and diplomatic maneuvers further pacified its borders. However, the massive invasions of theKorean Peninsuladuring theGoguryeo–Sui Warfailed disastrously, triggering widespread revolts that led tothe fall of the dynasty.
The Tang dynasty was agolden age of Chinese civilization, a prosperous, stable, and creative period with significant developments in culture, art, literature, particularlypoetry, and technology.Buddhismbecame the predominant religion for the common people.Chang'an(modernXi'an), the national capital, was thelargest city in the world during its time.[66]
The first emperor,Emperor Gaozu, came to the throne on 18 June 618, placed there by his son, Li Shimin, who became the second emperor,Taizong, one of the greatestemperors in Chinese history. Combined military conquests and diplomatic maneuvers reduced threats from Central Asian tribes, extended the border, and brought neighboring states intoa tributary system. Military victories in theTarim Basinkept the Silk Road open, connecting Chang'an to Central Asia and areas far to the west. In the south, lucrative maritime trade routes from port cities such asGuangzhouconnected with distant countries, and foreign merchants settled in China, encouraging acosmopolitanculture. The Tang culture and social systems were observed and adapted by neighboring countries, most notablyJapan. Internally theGrand Canallinked the political heartland in Chang'an to the agricultural and economic centers in the eastern and southern parts of the empire.Xuanzang, a ChineseBuddhist monk, scholar, traveller, and translator travelled to India on his own and returned with "over six hundred Mahayana and Hinayana texts, seven statues of the Buddha and more than a hundredsarirarelics."
The prosperity of the early Tang dynasty was abetted by a centralized bureaucracy. The government was organized as "Three Departments and Six Ministries" to separately draft, review, and implement policies. These departments were run by royal family members and landed aristocrats, but as the dynasty wore on, were joined or replaced byscholar officialsselected byimperial examinations, setting patterns for later dynasties.
Under the Tang "equal-field system" all land was owned by the Emperor and granted to each family according to household size. Men granted land were conscripted for military service for a fixed period each year, a military policy known as thefubingsystem. These policies stimulated a rapid growth in productivity and a significant army without much burden on the state treasury. By the dynasty's midpoint, however,standing armieshad replaced conscription, and land was continuously falling into the hands of private owners and religious institutions granted exemptions.
The dynasty continued to flourish under the rule of EmpressWu Zetian, the only officialempress regnantin Chinese history, and reached its zenith during the long reign ofEmperor Xuanzong, who oversaw an empire that stretched from thePacificto theAral Seawith at least50 millionpeople. There were vibrant artistic and cultural creations, including works of the greatest Chinesepoets,Li BaiandDu Fu.
At the zenith of prosperity of the empire, theAn Lushan Rebellionfrom 755 to 763 was a watershed event. War, disease, and economic disruptiondevastated the populationand drastically weakened the central imperial government. Upon suppression of the rebellion, regional military governors, known asjiedushi, gained increasingly autonomous status as the central government lost its ability to control them. With loss of revenue from land tax, the central imperial government came to rely heavily on itssalt monopoly. Externally, former submissive states raided the empire and the vast border territories were lost for centuries. Nevertheless, civil society recovered and thrived amidst the weakened imperial bureaucracy.
In late Tang period the empire was worn out by recurring revolts of the regional military governors, while scholar-officials engaged in fiercefactional strifeand corruptedeunuchsamassed immense power. Catastrophically, theHuang Chao Rebellion, from 874 to 884, devastated the entire empire for a decade. The sack of the southern portGuangzhouin 879 was followed by themassacreof most of its inhabitants, especially the large foreign merchant enclaves.[68][69]By 881, both capitals,LuoyangandChang'an, fell successively. The reliance on ethnicHanandTurkicwarlordsin suppressing the rebellion increased their power and influence. Consequently, the fall of the dynasty followingZhu Wen's usurpation led to anera of division.
In 808, 30,000 Shatuo under Zhuye Jinzhong defected from the Tibetans to Tang China and the Tibetans punished them by killing Zhuye Jinzhong as they were chasing them.[70]The Uyghurs also fought against an alliance of Shatuo and Tibetans at Beshbalik.[71]The Shatuo Turks under Zhuye Chixin (Li Guochang) served the Tang dynasty in fighting against their fellow Turkic people in theUyghur Khaganate. In 839, when the Uyghur khaganate (Huigu) general Jueluowu (掘羅勿) rose against the rule of then-reigningZhangxin Khan, he elicited the help from Zhuye Chixin by giving Zhuye 300 horses, and together, they defeated Zhangxin Khan, who then committed suicide, precipitating the subsequent collapse of the Uyghur Khaganate. In the next few years, when Uyghur Khaganate remnants tried to raid Tang borders, the Shatuo participated extensively in counterattacking the Uyghur Khaganate with other tribes loyal to Tang.[72]In 843, Zhuye Chixin, under the command of the Han Chinese officerShi Xiongwith Tuyuhun, Tangut and Han Chinese troops, participated in a raid against the Uyghur khaganate that led to the slaughter of Uyghur forces at Shahu mountain.[73]
The period of political disunity between the Tang and the Song, known as the Five Dynasties and Ten Kingdoms period, lasted from 907 to 960. During this half-century, China was in all respects a multi-state system. Five regimes, namely, (Later)Liang,Tang,Jin,HanandZhou, rapidly succeeded one another in control of the traditional Imperial heartland in northern China. Among the regimes, rulers of (Later)Tang,JinandHanweresinicizedShatuo Turks, which ruled over an ethnic majority ofHan Chinesein the north. More stable and smaller regimes of mostly ethnic Han rulers coexisted in south and western China over the period, cumulatively constituted the "Ten Kingdoms".
Amidst political chaos in the north, the strategicSixteen Prefectures(region along today'sGreat Wall) were ceded to the emergingKhitan Liao dynasty, which drastically weakened the defense ofChina properagainst northern nomadic empires. To the south, Vietnamgained lasting independenceafterbeing a Chinese prefectureformany centuries. With wars dominating in Northern China, there were mass southward migrations of population, which further enhanced the southward shift of cultural and economic centers in China. The era ended with the coup ofLater ZhougeneralZhao Kuangyin, and the establishment of theSong dynastyin 960, which eventually annihilated the remains of the "Ten Kingdoms" and reunified China.
In 960, the Song dynasty was founded byEmperor Taizu, with its capital established inKaifeng(then known asBianjing). In 979, the Song dynasty reunified most ofChina proper, while large swaths of the outer territories were occupied bysinicizednomadic empires. TheKhitanLiao dynasty, which lasted from 907 to 1125, ruled overManchuria,Mongolia, and parts ofNorthern China. Meanwhile, in what are now the north-western Chinese provinces ofGansu,Shaanxi, andNingxia, theTanguttribes founded theWestern Xia dynastyfrom 1032 to 1227.
Aiming to recover the strategicsixteen prefectureslost in theprevious dynasty,campaignswere launched against theLiao dynastyin theearly Song period, which all ended in failure. Then in 1004, the Liaocavalryswept over the exposedNorth China Plainand reached the outskirts of Kaifeng, forcing the Song's submission and then agreement to theChanyuan Treaty, which imposed heavy annual tributes from the Song treasury. The treaty was a significant reversal of Chinese dominance of the traditionaltributary system. Yet the annual outflow of Song's silver to the Liao was paid back through the purchase of Chinese goods and products, which expanded the Song economy, and replenished its treasury. This dampened the incentive for the Song to further campaign against the Liao. Meanwhile, this cross-border trade and contact induced further sinicization within theLiao Empire, at the expense of its military might which was derived from its nomadic lifestyle. Similar treaties and social-economical consequences occurred in Song's relations with theJin dynasty.
Within the Liao Empire theJurchentribes revolted against their overlords to establish the Jin dynasty in 1115. In 1125, the devastating Jincataphractannihilated the Liao dynasty, while remnants of Liao court members fled to Central Asia to found theQara KhitaiEmpire (Western Liao dynasty).Jin's invasion of the Song dynastyfollowed swiftly. In 1127, Kaifeng was sacked, a massive catastrophe known as theJingkang Incident, ending theNorthern Song dynasty. Later theentire north of China was conquered. The survived members of Song court regrouped in the new capital city ofHangzhou, and initiated theSouthern Song dynasty, which ruled territories south of theHuai River. In the ensuing years, the territory and population of China were divided between the Song dynasty, the Jin dynasty and the Western Xia dynasty. The era ended with theMongol conquest, as Western Xia fell in 1227, theJin dynasty in 1234, and finally theSouthern Song dynasty in 1279.
Despite its military weakness, the Song dynasty is widely considered to be the high point of classical Chinese civilization. TheSong economy, facilitated by technological advancement, had reached a level of sophistication probably unseen in world history before its time. The population soared to over100 millionand the living standards of common people improved tremendously due to improvements in rice cultivation and the wide availability of coal for production. The capital cities of Kaifeng and subsequently Hangzhou were both themost populous citiesin the world for their time, and encouraged vibrant civil societies unmatched by previous Chinese dynasties. Although land trading routes to the far west were blocked by nomadic empires, there was extensivemaritime tradewith neighbouring states, such as inSouth-east Asia, which facilitated the use of Song coinage as the de facto currency of exchange. Giant wooden vessels equipped withcompassestraveled throughout theChina Seasand northern Indian Ocean. The concept of insurance was practised by merchants to hedge the risks of such long-haul maritimeshipments. With prosperous economic activities, the historically first use ofpaper currencyemerged in the western city ofChengdu, as a cheaper supplement to the existing coppercoins.
The Song dynasty was considered to be the golden age of great advancements in science and technology of China, thanks to innovative scholar-officials such asSu Song(1020–1101) andShen Kuo(1031–1095). Inventions such as the hydro-mechanical astronomical clock, the first continuous and endless power-transmitting chain,woodblock printingandpaper moneywere all invented during the Song dynasty, further cementing its status.
There was court intrigue between the political reformers and conservatives, led by the chancellorsWang AnshiandSima Guang, respectively. By the mid-to-late 13th century, the Chinese had adopted the dogma ofNeo-Confucianphilosophy formulated byZhu Xi. Enormous literary works were compiled during the Song dynasty, such as the innovative historical narrativeZizhi Tongjian("Comprehensive Mirror to Aid in Government"). The invention ofmovable-type printingfurther facilitated the spread of knowledge. Culture and the arts flourished, with grandiose artworks such asAlong the River During the Qingming FestivalandEighteen Songs of a Nomad Flute, along with great Buddhist painters such as the prolificLin Tinggui.
The Song dynasty was also a period of major innovation in thehistory of warfare.Gunpowder, while invented in theTang dynasty, was first put into practical use on the battlefield by the Song army, inspiring a succession of newfirearmsandsiege enginesdesigns. During the Southern Song dynasty, as its survival hinged decisively on guarding theYangtzeandHuai Riveragainst the cavalry forces from the north, the first standing navy in China was assembled in 1132, with its admiral's headquarters established atDinghai.Paddle-wheelwarships equipped withtrebuchetscould launchincendiary bombsmade of gunpowder and lime to effect, as recorded in Song's victory over the invading Jin forces at theBattle of Tangdaoin theEast China Sea, and theBattle of Caishion the Yangtze River in 1161.
The advances in civilisation during the Song dynasty came to an abrupt end following the devastating Mongol conquest of the North and subsequently other areas of the empire, during which the population sharply dwindled, with a marked contraction in economy. Despite viciouslyhalting Mongol advancesfor more than three decades, the Southern Song capital Hangzhou fell in 1276, followed by the final annihilation of the Song standing navy at theBattle of Yamenin 1279.
TheYuan dynastywas formally proclaimed in 1271, when theGreat Khan of Mongol,Kublai Khan, one of the grandsons ofGenghis Khan, assumed the additional title ofEmperor of China, and consideredhis inherited partof the Mongol Empire as aChinese dynasty. In the preceding decades, the Mongols had conquered the Jin dynasty in Northern China, and the Southern Song dynasty fell in 1279 after a protracted and bloody war. TheMongolYuan dynasty became the firstconquest dynastyin Chinese history to rule the entirety ofChina properandits populationas anethnic minority. The dynasty also directly controlled theMongol heartlandand other regions, inheriting the largest share of territory of theeastern Mongol empire, which roughly coincided with the modern area of China and nearby regions in East Asia. Further expansion of the empire was halted after defeats in theinvasions of JapanandVietnam. Following the previous Jin dynasty, the capital of Yuan dynasty was established atKhanbaliq(also known as Dadu, modern-day Beijing). TheGrand Canalwas reconstructed to connect the remote capital city to lively economic hubs in southern part of China, setting the precedence and foundation for Beijing to largely remain as thecapitalof the successive regimes of the unified Chinese mainland.
A series ofMongol civil warsin the late 13th century led to thedivision of the Mongol Empire. In 1304 the emperors of the Yuan dynasty were upheld as the nominalKhaganover western khanates (theChagatai Khanate, theGolden Hordeand theIlkhanate), which nonetheless remainedde factoautonomous. The era was known asPax Mongolica, when much of the Asian continent was ruled by the Mongols. For the first and only time in history, theSilk Roadwas controlled entirely by a single state, facilitating the flow of people, trade, and cultural exchange. A network of roads and apostal systemwere established to connect the vast empire. Lucrative maritime trade, developed from the previous Song dynasty, continued to flourish, withQuanzhouandHangzhouemerging as the largest ports in the world. Adventurous travelers from the far west, most notably theVenetian,Marco Polo, would settle in China for decades. Upon his return, his detailtravel recordinspired generations ofmedieval Europeanswith the splendors of the far East. The Yuan dynasty was the first ancient economy, wherepaper currency, known at the time asJiaochao, was used as the predominant medium of exchange. Its unrestricted issuance in the late Yuan dynasty inflictedhyperinflation, which eventually brought the downfall of the dynasty.
While the Mongol rulers of the Yuan dynasty adopted substantially to Chinese culture, theirsinicizationwas of lesser extent compared to earlierconquest dynastiesin Chinese history. For preserving racial superiority as the conqueror and ruling class, traditional nomadic customs and heritage from theMongolian Steppewere held in high regard. On the other hand, the Mongol rulers also adopted flexibly to a variety of cultures from many advanced civilizations within the vast empire. Traditional social structure and culture in China underwent immense transform during the Mongol dominance. Large groups offoreign migrantssettled in China, who enjoyed elevated social status over the majority Han Chinese, while enriching Chinese culture with foreign elements. The class ofscholar officialsand intellectuals, traditional bearers of elite Chinese culture, lost substantial social status. This stimulated the development of culture of the common folks. There were prolific works inzajuvariety shows andliterary songs(sanqu), which were written in a distinctivepoetry styleknown asqu. Novels of vernacular style gained unprecedented status and popularity.
Before the Mongol invasion, Chinese dynasties reported approximately120 millioninhabitants; after the conquest had been completed in 1279, the 1300 census reported roughly60 millionpeople.[74]This major decline is not necessarily due only to Mongol killings. Scholars such as Frederick W. Mote argue that the wide drop in numbers reflects an administrative failure to record rather than an actual decrease; others such asTimothy Brookargue that the Mongols created a system ofenserfmentamong a huge portion of the Chinese populace, causing many to disappear from the census altogether; other historians including William McNeill and David Morgan consider thatplaguewas the main factor behind the demographic decline during this period. In the 14th century China suffered additional depredations from epidemics of plague, estimated to have killed around a quarter of the population of China.[75]: 348–351
Throughout the Yuan dynasty, there was some general sentiment among the populace against the Mongol dominance. Yet rather than the nationalist cause, it was mainly strings of natural disasters and incompetent, corrupt governance that triggered widespread peasant uprisings since the 1340s. After themassive naval engagementat Lake Poyang,Zhu Yuanzhangprevailed over other rebel forces in the south. He proclaimed himselfemperorand founded theMing dynastyin 1368. The same year his northern expedition army captured the capital Khanbaliq. The Yuan remnants fled back to Mongolia andsustained the regime, but the period of Yuan dominance was effectively over for good. Other Mongol Khanates in Central Asia continued to exist after the fall of Yuan dynasty in China.
TheMing dynastywas founded by Zhu Yuanzhang in 1368, who proclaimed himself as theHongwu Emperor. The capital was initially set atNanjing, and was later moved toBeijingfromYongle Emperor's reign onward.
Urbanization increased as the population grew and as the division of labor grew more complex. Large urban centers, such as Nanjing and Beijing, also contributed to the growth of private industry. In particular, small-scale industries grew up, often specializing in paper, silk, cotton, and porcelain goods. For the most part, however, relatively small urban centers with markets proliferated around the country. Town markets mainly traded food, with some necessary manufactures such as pins or oil.
Despite thexenophobiaand intellectual introspection characteristic of the increasingly popular new school ofneo-Confucianism, China under the early Ming dynasty was not isolated. Foreign trade and other contacts with the outside world, particularly Japan, increased considerably. Chinese merchants explored all of the Indian Ocean, reaching East Africa with thevoyages of Zheng He.
The Hongwu Emperor, being the only founder of aChinese dynastywho was also of peasant origin, had laid the foundation of a state that relied fundamentally in agriculture. Commerce and trade, which flourished in the previousSongandYuandynasties, were less emphasized. Neo-feudal landholdings of the Song and Mongol periods were expropriated by the Ming rulers. Land estates were confiscated by the government, fragmented, and rented out. Private slavery was forbidden. Consequently, after the death of the Yongle Emperor, independent peasant landholders predominated in Chinese agriculture. These laws might have paved the way to removing the worst of the poverty during the previous regimes. Towards later era of the Ming dynasty, with declining government control, commerce, trade and private industries revived.
The dynasty had a strong and complex central government that unified and controlled the empire. The emperor's role became more autocratic, although Hongwu Emperor necessarily continued to use what he called the "Grand Secretariat" to assist with the immense paperwork of the bureaucracy, includingmemorials(petitions and recommendations to the throne), imperial edicts in reply, reports of various kinds, and tax records. It was this same bureaucracy that later prevented the Ming government from being able to adapt to changes in society, and eventually led to its decline.
The Yongle Emperor strenuously tried to extend China's influence beyond its borders by demanding other rulers send ambassadors to China to present tribute. A large navy was built, including four-masted ships displacing 1,500 tons. A standing army of 1 million troops was created. The Chinese armiesconqueredandoccupied Vietnamfor around 20 years, while theChinese fleet sailedthe China seas and the Indian Ocean, cruising as far as the east coast of Africa. The Chinese gained influence in easternMoghulistan. Several maritime Asian nations sent envoys with tribute for the Chinese emperor. Domestically, the Grand Canal was expanded and became a stimulus to domestic trade. Over 100,000 tons of iron per year were produced. Many books were printed using movable type. The imperial palace in Beijing'sForbidden Cityreached its current splendor. It was also during these centuries that the potential of south China came to be fully exploited. New crops were widely cultivated and industries such as those producing porcelain and textiles flourished.
In 1449Esen Tayisiled anOiratMongol invasion of northern China which culminated in the capture of theZhengtong EmperoratTumu. Since then, the Ming became on the defensive on the northern frontier, which led to theMing Great Wallbeing built. Most of what remains of the Great Wall of China today was either built or repaired by the Ming. The brick and granite work was enlarged, the watchtowers were redesigned, and cannons were placed along its length.
At sea the Ming became increasingly isolationist after the death of the Yongle Emperor. The treasure voyages which sailed the Indian Ocean were discontinued, and themaritime prohibitionlaws were set in place banning the Chinese from sailing abroad. European traders who reached China in the midst of theAge of Discoverywere repeatedly rebuked in their requests for trade, with the Portuguese beingrepulsed by the Ming navyatTuen Munin 1521 and againin 1522. Domestic and foreign demands for overseas trade, deemed illegal by the state, led to widespreadwokoupiracy attacking the southeastern coastline during the rule of theJiajing Emperor(1507–1567), which only subsided after the opening of ports inGuangdongandFujianandmuch military suppression.[76]In addition to raids from Japan by thewokou, raids from Taiwan and thePhilippines by the Pisheyealso ravaged the southern coasts.[77]The Portuguese were allowed to settle inMacauin 1557 for trade, which remained in Portuguese hands until 1999. After the Spanish invasion of the Philippines, trade with theSpanish at Manilaimported large quantities ofMexican and Peruvian silverfrom theSpanish Americasto China.[78]: 144–145The Dutch entry into the Chinese seas was also met with fierce resistance, with the Dutch being chased off thePenghu islandsin theSino-Dutch conflictsof 1622–1624 and were forced to settle in Taiwan instead.The Dutch in Taiwanfought with the Ming in theBattle of Liaoluo Bayin 1633 and lost, and eventually surrendered to the Ming loyalistKoxingain 1662, after the fall of the Ming dynasty.
In 1556, during the rule of theJiajing Emperor, theShaanxi earthquakekilled about 830,000 people, the deadliest earthquake of all time.
The Ming dynasty intervened deeply in theJapanese invasions of Korea (1592–1598), which ended with the withdrawal of all invading Japanese forces in Korea, and the restoration of theJoseon dynasty, its traditional ally andtributary state. Theregional hegemonyof the Ming dynasty was preserved at a toll on its resources. Coincidentally, with Ming's control inManchuriain decline, theManchu(Jurchen) tribes, under their chieftainNurhaci, broke away from Ming's rule, and emerged as a powerful, unified state, which waslater proclaimedas theQing dynasty. It went on to subdue the much weakenedKoreaas itstributary, conqueredMongolia, and expanded its territory to the outskirt of the Great Wall. The most elite army of the Ming dynasty was to station at theShanhai Passto guard the last stronghold against the Manchus, which weakened its suppression of internalpeasants uprisings.
The Qing dynasty (1644–1912) was the last imperial dynasty in China. Founded by theManchus, it was the secondconquest dynastyto rule the entirety ofChina proper, and roughly doubled the territory controlled by the Ming. The Manchus were formerly known asJurchens, residing in the northeastern part of the Ming territory outside the Great Wall. They emerged as the major threat to the late Ming dynasty afterNurhaciunited all Jurchen tribes and his son,Hong Taiji, declared the founding of the Qing dynasty in 1636. The Qing dynasty set up theEight Bannerssystem that provided the basic framework for the Qing military conquest.Li Zicheng's peasant rebellion captured Beijing in 1644 and theChongzhen Emperor, the last Ming emperor, committed suicide. The Manchus allied with the Ming generalWu Sanguito seize Beijing, which was made the capital of the Qing dynasty, and then proceeded to subdue theMing remnants in the south. During theMing-Qing transition, when the Ming dynasty and later the Southern Ming, the emerging Qing dynasty, and several other factions like theShun dynastyandXi dynastyfounded by peasant revolt leaders fought against each another, which, along with innumerablenatural disastersat that time such as those caused by theLittle Ice Age[79]andepidemicslike theGreat Plague during the last decade of the Ming dynasty,[80]caused enormous loss of lives andsignificant harm to the economy. In total, these decades saw the loss of as many as25 millionlives, but the Qing appeared to have restored China's imperial power and inaugurate another flowering of the arts.[81]The early Manchu emperors combined traditions ofInner Asianrule with Confucian norms of traditional Chinese government and were considered a Chinese dynasty.
The Manchus enforced a 'queue order', forcing Han Chinese men to adopt the Manchuqueue hairstyle. Officials were required to wear Manchu-style clothingChangshan(bannermendress andTangzhuang), but ordinary Han civilians were allowed to weartraditional Han clothing. Bannermen could not undertake trade or manual labor; they had to petition to be removed from banner status. They were considered aristocracy and were given annual pensions, land, and allotments of cloth. TheKangxi Emperorordered the creation of theKangxi Dictionary, the most complete dictionary of Chinese characters that had been compiled.
Over the next half-century, all areas previously under the Ming dynasty were consolidated under the Qing.Conquests in Central Asiain the eighteenth century extended territorial control. Between 1673 and 1681, the Kangxi Emperor suppressed theRevolt of the Three Feudatories, an uprising of three generals in Southern China who had been denied hereditary rule of large fiefdoms granted by theprevious emperor. In 1683, the Qing staged an amphibious assault on southern Taiwan, bringing down the rebelKingdom of Tungning, which was founded by the Ming loyalistKoxinga(Zheng Chenggong) in 1662 after the fall of the Southern Ming, and had served as a base for continued Ming resistance in Southern China. The Qingdefeated the Russians at Albazin, resulting in theTreaty of Nerchinsk.
By the end ofQianlong Emperor's long reign in 1796, the Qing Empire was atits zenith. The Qing ruled more thanone-third of the world's population, and had the largest economy in the world. By area it wasone of the largest empires ever.
In the 19th century the empire was internally restive and externally threatened by western powers. The defeat by theBritish Empirein theFirst Opium War(1840) led to theTreaty of Nanking(1842), under whichHong Kongwas ceded to Britain and importation ofopium(produced by British Empire territories) was allowed. Opium usage continued to grow in China, adversely affecting societal stability. Subsequent military defeats andunequal treatieswith other western powers continued even after the fall of the Qing dynasty.
Internally theTaiping Rebellion(1851–1864), a Christian religious movement led by the "Heavenly King"Hong Xiuquanswept from the south to establish theTaiping Heavenly Kingdomand controlled roughly a third of China proper for over a decade. The court in desperation empowered Han Chinese officials such asZeng Guofanto raise local armies. After initial defeats, Zeng crushed the rebels in theThird Battle of Nankingin 1864.[82]This was one of the largest wars in the 19th century in troop involvement; there was massive loss of life, with a death toll of about 20 million.[83]A string of civil disturbances followed, including thePunti–Hakka Clan Wars,Nian Rebellion,Dungan Revolt, andPanthay Rebellion.[84]All rebellions were ultimately put down, but at enormous cost and with millions dead, seriously weakening the central imperial authority. China never rebuilt a strong central army, and many local officials used their military power to effectively rule independently in their provinces.[82]
Yet the dynasty appeared to recover in theTongzhi Restoration(1860–1872), led by Manchu royal family reformers and Han Chinese officials such as Zeng Guofan and his protegesLi HongzhangandZuo Zongtang. TheirSelf-Strengthening Movementmade effective institutional reforms, imported Western factories and communications technology, with prime emphasis on strengthening the military. However, the reform was undermined by official rivalries, cynicism, and quarrels within the imperial family. The defeat ofYuan Shikai's modernized "Beiyang Fleet" in theFirst Sino-Japanese War(1894–1895) led to the formation of theNew Army. TheGuangxu Emperor, advised byKang Youwei, then launched a comprehensive reform effort, theHundred Days' Reform(1898).Empress Dowager Cixi, however, feared that precipitous change would lead to bureaucratic opposition and foreign intervention and quickly suppressed it.
In the summer of 1900, theBoxer Uprisingopposed foreign influence and murdered Chinese Christians and foreign missionaries. When Boxers entered Beijing, the Qing government ordered all foreigners to leave, but they and many Chinese Christians werebesieged in the foreign legations quarter. AnEight-Nation Alliancesent theSeymour Expeditionof Japanese, Russian, British, Italian, German, French, American, and Austrian troops to relieve the siege, but they were routed and forced to retreat by Boxer and Qing troops at theBattle of Langfang. Afterthe Alliance's attack on the Dagu Forts, the court declared war on the Alliance and authorised the Boxers to join with imperial armies. Afterfierce fighting at Tianjin, the Alliance formed the second, much largerGaselee Expeditionandfinally reached Beijing; the Empress Dowager evacuated toXi'an. TheBoxer Protocolended the war, exacting a tremendousindemnity.
The Qing court then instituted administrative and legal reforms known as thelate Qing reforms, including abolition of theexamination system. But young officials, military officers, and students debated reform, perhaps aconstitutional monarchy, or the overthrow of the dynasty and the creation of a republic. They were inspired by an emerging public opinion formed by intellectuals such asLiang Qichaoand the revolutionary ideas ofSun Yat-sen. A localised military uprising, theWuchang uprising, began on 10 October 1911, inWuchang(today part ofWuhan), and soon spread. The Republic of China was proclaimed on 1 January 1912, ending 2,000 years of dynastic rule.
Theprovisional government of the Republic of Chinawas formed inNanjingon 12 March 1912. Sun Yat-sen becamePresident of the Republic of China, but he turned power over toYuan Shikai, who commanded theNew Army. Over the next few years, Yuan proceeded to abolish the national and provincial assemblies, and declared himself as the emperor ofEmpire of Chinain late 1915, in the style of anabsolute monarchy. Yuan's imperial ambitions were fiercely opposed by his subordinates; faced with the rapidly growing prospect of violent rebellion, he abdicated in March 1916 and died of natural causes in June.
Yuan's death in 1916 left a power vacuum; the republican government (that had been nearly brought to its knees by his policies) was all but shattered. This opened the way for theWarlord Era, during which much of China was ruled by shifting coalitions of competing provincial military leaders and theBeiyang government, ushering in a short-lived period of uncertainty. Intellectuals, disappointed in the failure of the Republic, launched theNew Culture Movement.
In 1919, theMay Fourth Movementbegan as a response to the pro-Japanese terms imposed on China by theTreaty of Versaillesfollowing World War I. It quickly became a nationwide protest movement. The protests were a moral success as the cabinet fell and China refused to sign the Treaty of Versailles, which had awarded German holdings ofShandongto Japan. Memory of the mistreatment at Versailles fuels resentment into the 21st century.[85]
Political and intellectual ferment waxed strong throughout the 1920s and 1930s. According to Patricia Ebrey:
In the 1920s Sun Yat-sen established a revolutionary base in Guangzhou and set out to unite the fragmented nation. He welcomed assistance from theSoviet Union(itself fresh from Lenin's Communist takeover) and he entered into an alliance with the fledglingChinese Communist Party(CCP). After Sun's death from cancer in 1925, one of his protégés,Chiang Kai-shek, seized control of theNationalist Party(KMT) and succeeded in bringing most of south and central China under its rule in theNorthern Expedition(1926–1927). Having defeated the warlords in the south and central China bymilitary force, Chiang was able to secure the nominal allegiance of the warlords in the North and establish theNationalist governmentin Nanjing. In 1927, Chiang turned on the CCP and relentlessly purged the Communists elements in hisNRA. In 1934, driven from their mountain bases such as theChinese Soviet Republic, the CCP forces embarked on theLong Marchacross China's most desolate terrain to the northwest, a feat transformed into legend, where they established a guerrilla base atYan'anin Shaanxi. During the Long March, the communists reorganised under a new leader,Mao Zedong(Mao Tse-tung).
The bitterChinese Civil Warbetween the Nationalists and the Communists continued, openly or clandestinely, through the 14-year-long Japanese occupation of various parts of the country (1931–1945). The two Chinese parties nominally formed a United Front to oppose the Japanese in 1937, during theSecond Sino-Japanese War(1937–1945), which became a part ofWorld War II, although this alliance was tenuous at best and disagreements, sometimes violent, between the forces were still common. Japanese forces committed numerouswar atrocitiesagainst the civilian population, including biological warfare (seeUnit 731) and theThree Alls Policy(Sankō Sakusen), namely being: "Kill All, Burn All and Loot All".[87]During the war, China was recognized as one of the Allied "Big Four" in theDeclaration by United Nations, as a tribute to its enduring struggle against the invading Japanese.[88]China was one of the four majorAllies of World War II, and was later considered one of the primary victors in the war.[89]
Following the defeat of Japan in 1945, the war between the Nationalist government forces and the CCP resumed, after failed attempts at reconciliation and a negotiated settlement. By 1949, the CCP had established control over most of the country.Odd Arne Westadsays the Communists won the Civil War because they made fewer military mistakes than Chiang, and because in his search for a powerful centralized government, Chiang antagonised too many interest groups in China. Furthermore, his party was weakened in the war against the Japanese. Meanwhile, the Communists told different groups, such as peasants, exactly what they wanted to hear, and cloaked themselves in the cover of Chinese Nationalism.[90]During the civil war both the Nationalists and Communists carried out mass atrocities, with millions of non-combatants killed by both sides.[91]These included deaths from forced conscription and massacres.[92]
The Nationalists were slowly routed towards the South. When the Nationalist government forces were defeated by CCP forces in mainland China in 1949, the Nationalist government fled toTaiwanwith its forces, along with Chiang and a large number of their supporters; the Nationalist government had taken effective control of Taiwan at the end of WWII as part of the overall Japanese surrender, when Japanese troops in Taiwan surrendered to the Republic of China troops there.[93]
Until the early 1970s the ROC was recognised as thesole legitimate government of Chinaby the United Nations, the United States and most Western nations, refusing to recognise the PRC on account of its status as a communist nation during the Cold War. This changed in 1971 when thePRC was seated in the United Nations, replacing the ROC. The KMT ruled Taiwan under martial law until 1987, with the stated goal of being vigilant against Communist infiltration and preparing to retake mainland China. Therefore, political dissent was not tolerated during that period, and crackdowns against dissidents were common.
In the 1990s the ROC underwent a major democratic reform, beginning with the 1991 resignation of the members of theLegislative YuanandNational Assemblyelected in 1947. These groups were originally created to represent mainland China constituencies. Also lifted were the restrictions on the use of Taiwanese languages in the broadcast media and in schools. In 1996, the ROC heldits first direct presidential election, and the incumbent president, KMT candidateLee Teng-hui, was elected. In 2000, the KMT status as the ruling party ended when the DPP took power, only to regain its status in the2008 electionbyMa Ying-jeou.
Due to the controversial nature ofTaiwan's political status, the ROC is currently recognised bymerely 12 UN member states and the Holy Seeas of 2024[update]as the legitimate government of "China".
Major combat in the Chinese Civil War ended in 1949 with the KMT pulling out of the mainland, with the government relocating toTaipeiand maintaining control only over a few islands. The CCP was left in control ofmainland China. On 1 October 1949, Mao Zedong proclaimed the People's Republic of China.[94]"Communist China" and "Red China" were two common names for the PRC.[95]
The PRC was shaped by aseries of campaignsandfive-year plans. TheGreat Leap Forward, a radical campaign that encompassed numerous attempted economic and social reforms, resulted in tens of millions of deaths.[96][better source needed]Mao's government carried out mass executions of landowners, institutedcollectivisationand implemented theLaogaicamp system. Execution, deaths from forced labor and other atrocities resulted in millions of deaths under Mao. In 1966 Mao and his allies launched theCultural Revolution, which continued until Mao's death a decade later. The Cultural Revolution, motivated by power struggles within the Party and a fear of theSoviet Union, led to a major upheaval in Chinese society.
Following theSino-Soviet splitand motivated by concerns of invasion by either the Soviet Union or the United States, China initiated theThird Front campaignto develop national defense and industrial infrastructure in its rugged interior.[97]: 44Through its distribution of infrastructure, industry, and human capital around the country, the Third Front created favorable conditions for subsequent market development and private enterprise.[97]: 177
In 1972, at the peak of the Sino-Soviet split, Mao andZhou Enlaimet U.S. presidentRichard Nixonin Beijing to establish relations with the US. In the same year, thePRC was admitted to the United Nationsin place of the Republic of China, with permanent membership of the Security Council.
A power struggle followed Mao's death in 1976. TheGang of Fourwere arrested and blamed for the excesses of the Cultural Revolution, marking the end of a turbulent political era in China.Deng Xiaopingoutmaneuvered Mao's anointed successor chairmanHua Guofeng, and gradually emerged as thede factoleader over the next few years.
Deng Xiaoping was theParamount Leaderof China from 1978 to 1992, although he never became the head of the party or state, and his influence within the Party led the country tosignificant economic reforms. The CCP subsequently loosened governmental control over citizens' personal lives and thecommuneswere disbanded with many peasants receiving multiple land leases, which greatly increased incentives and agricultural production. In addition, there were many free market areas opened. The most successful free market area was Shenzhen. It is located in Guangdong and the property tax free area still exists today. This turn of events marked China's transition from a planned economy to a mixed economy with an increasingly open market environment, a system termed by some[98]asmarket socialism, and officially by the CCP asSocialism with Chinese characteristics. The PRC adopted its currentconstitutionon 4 December 1982.
In 1989 the death of former general secretaryHu Yaobanghelped to spark theTiananmen Square protestsof that year, during which students and others campaigned for several months, speaking out against corruption and in favour of greater political reform, including democratic rights and freedom of speech. However, they were eventually put down on 4 June whenArmytroops and vehicles entered and forcibly cleared the square, resulting inconsiderable numbers of fatalities. This event was widely reported, and brought worldwide condemnation and sanctions against the communist government.[99][100]
CCP general secretary and PRC presidentJiang Zeminand PRC premierZhu Rongji, both former mayors of Shanghai, led post-Tiananmen PRC in the 1990s. Under Jiang and Zhu's ten years of administration, the PRC's economic performance pulled an estimated 150 million peasants out of poverty and sustained an average annual gross domestic product growth rate of 11.2%.[101][better source needed]The country formally joined theWorld Trade Organizationin 2001. By1997and1999, former European colonies ofBritish Hong KongandPortuguese Macaubecame the Hong Kong and Macauspecial administrative regions of the People's Republic of China, respectively.
Although the PRC needed economic growth to spur its development, the government began to worry that rapid economic growth was degrading the country's natural resources and environment. Another concern was that certain sectors of society were not sufficiently benefiting from the PRC's economic development; one example of this was the wide gap between urban and rural areas in terms of development and prevalence of updated infrastructure. As a result, under former CCP general secretary and PresidentHu Jintaoand PremierWen Jiabao, the PRC initiated policies to address issues of equitable distribution of resources, but the outcome was not known as of 2014[update].[102]More than 40 million farmers were displaced from their land,[103]usually for economic development, contributing to 87,000 demonstrations and riots across China in 2005.[104]For much of the PRC's population, living standards improved very substantially and freedom increased, but political controls remained tight and rural areas poor.[105]
According to theU.S. Department of Defense, as many as 3 millionUyghursand members of otherMuslimminority groups are being held in China'sinternment campswhich are located in theXinjiangregion and which Western news reports often label as "concentration camps".[106]The camps were established in late 2010s underXi Jinping'sadministration.[107][108]Human Rights Watchsays that they have been used to indoctrinate Uyghurs and other Muslims since 2017 as part of apeople's war on terror, a policy announced in 2014.[109][110][107]The use of these centers appears to have ended in 2019 following international pressure.[111]AcademicKerry Brownattributes their closures beginning in late 2019 to the expense required to operate them.[112]: 138China has repeatedly denied this, asserting that the West has never been able to produce reliably-sourced satellite footage of any such detainment or resulting detention of minority groups. Although no comprehensive independent surveys of such centres have been performed as of June 2024, spot checks by journalists have found such sites converted or abandoned.[111]In 2022, a Washington Post reporter checked a dozen sites previously identified as reeducation centres and found "[m]ost of them appeared to be empty or converted, with several sites labeled as coronavirus quarantine facilities, teachers' schools and vocational schools."[111]In 2023,Amnesty Internationalsaid that they were "witnessing more and more arbitrary detention", but that detained individuals were being moved from the camps into theformal prison system.[113]
The novel coronavirusSARS-CoV-2, which causes the diseaseCOVID-19, was first detected inWuhan, Hubei in 2019 and led to aglobal pandemic, causing the majority of the world to enter a period of lockdown for at least a year following.
Total military dead/missing:≈667,000–1,100,000Total military wounded:≈604,200(excludingGRUNK/Khmer RougeandPathet Lao)
1964–1965: Viet Cong offensive andAmerican intervention
TheVietnam War(1 November 1955[A 1]– 30 April 1975) was an armed conflict inVietnam,Laos, andCambodiafought betweenNorth Vietnam(Democratic Republic of Vietnam) andSouth Vietnam(Republic of Vietnam) and their allies. North Vietnam was supported by theSoviet UnionandChina, while South Vietnam was supported by the United States and otheranti-communistnations. The conflict was the second of theIndochina warsand a proxy war of theCold Warbetween the Soviet Union and the United States. In essence, the Vietnam War was a postcolonial war ofnational liberation, asignificant theaterin the global Cold War, and, simultaneously, acivil war, with civil warfare as a defining feature from the outset.[52]: 23DirectUS military involvementescalated from 1965 until its withdrawal in 1973. The fighting spilled into theLaotianandCambodian Civil Wars, which ended with all three countries becoming communist in 1975.
After the defeat of theFrench Unionin theFirst Indochina Warthat began in 1946, Vietnam gained independence in the1954 Geneva Conferencebut was divided into two parts at the17th parallel: theViet Minh, led byHo Chi Minh, took control of North Vietnam, while the US assumed financial and military support for South Vietnam, led byNgo Dinh Diem.[A 8]The North Vietnamese began supplying and directing theViet Cong(VC), acommon frontof dissidents in the south which intensified aguerrilla warfrom 1957. In 1958, North Vietnaminvaded Laos, establishing theHo Chi Minh trailto supply and reinforce the VC. By 1963, the north had covertly sent 40,000 soldiers of its ownPeople's Army of Vietnam(PAVN), armed with Soviet and Chinese weapons, to fight in the insurgency in the south. PresidentJohn F. Kennedyincreased US involvement from 900military advisorsin 1960 to 16,000 in 1963 and sent more aid to theArmy of the Republic of Vietnam(ARVN), which failed to produce results. In 1963, Diem was killed ina US-backed military coup, which added to the south's instability.
Following theGulf of Tonkin incidentin 1964, the US Congress passeda resolutionthat gave PresidentLyndon B. Johnsonauthority to increase military presence without a declaration of war. Johnson launcheda bombing campaign of the northand began sending combat troops, dramatically increasing deployment to 184,000 by the end of 1965, and to 536,000 by the end of 1968. US forces relied onair supremacyand overwhelming firepower to conductsearch and destroyoperations in rural areas. In 1968, North Vietnam launched theTet Offensive, which was a tactical defeat but convinced many in the US that the war could not be won. Johnson's successor,Richard Nixon, began a policy of "Vietnamization" from 1969, which saw the conflict fought by an expanded ARVN while US forces withdrew. A1970 coupin Cambodia resulted in a PAVN invasion and a US–ARVNcounter-invasion, escalating its civil war. US troops had mostly withdrawn from Vietnam by 1972, and the 1973Paris Peace Accordssaw the rest leave. The accords were broken and fighting continued until the1975 spring offensiveandfall of Saigonto the PAVN, marking the war's end. North and South Vietnam were reunified in 1976.
The war exacted anenormous cost: estimates of Vietnamese soldiers and civilians killed range from 970,000 to 3 million. Some 275,000–310,000Cambodians, 20,000–62,000Laotians, and 58,220 US service members died.[A 7]Its end would precipitate theVietnamese boat peopleand the largerIndochina refugee crisis, which saw millions leave Indochina, of which an estimated 250,000 perished at sea.[56][57]20% of South Vietnam's jungle was sprayed withtoxic herbicides, which led to health problems among people who were exposed.[58]: 144–145[59]TheKhmer Rougecarried out theCambodian genocide, while conflict between them and the unified Vietnam escalated into theCambodian–Vietnamese War. In response, Chinainvaded Vietnam, withborder conflictslasting until 1991. Within the US, the war gave rise toVietnam syndrome, a public aversion to American overseas military involvement,[60]which, with theWatergate scandal, contributed to the crisis of confidence that affected America throughout the 1970s.[61]
Various names have been applied and shifted over time, thoughVietnam Waris the most commonly used title in English. It has been called theSecond Indochina Warsince it spread toLaosandCambodia,[62]theVietnam Conflict,[63][64]andNam(colloquially 'Nam). In Vietnam it is commonly known asKháng chiến chống Mỹ(lit.'Resistance War against America').[65][66]TheGovernment of Vietnamofficially refers to it as theResistance War against America to Save the Nation.[67]
Vietnam had been under French control as part ofFrench Indochinasince the 1880s. Under French rule, Vietnamese independence movements were suppressed, so revolutionary groups conducted their activities abroad, particularly in France and China.Nguyen Sinh Cung, established theIndochinese Communist Party(ICP) in 1930; theMarxist–Leninistparty aimed to overthrow French rule and establish an independent communist state in Vietnam.[68]Besides communism, Vietnam’s struggle for independence was shaped by a variety of movements, ranging from reformist ideas to republicanism and revolutionary nationalism.[69]Prominent non-communist groups included theVietnamese Nationalist Party,Vietnamese Revolutionary League,Great Viet Nationalist Party, and religious factions such asCaodaism,Hòa Hảo, andCatholiccommunities.
Tensions between Vietnamese nationalists and communists emerged as early as the 1920s.[70][71]Vietnamese communists envisioned their revolution as part of a global proletarian revolution and not merely a nationalist liberation movement. They believed that class struggle and purges were essential to dismantle older social structures and pave the way for socialism.[72]The Vietnamese communist revolution’s pursuit of centralized control fueled a protracted civil conflict, characterized by intense violence, ideological purges, and the systematic suppression of competing nationalist movements.[73]The Vietnam War was simultaneously a postcolonial war of national liberation, a key theater of the global Cold War, and a civil war, in which civil warfare was a defining feature from the outset.[52]
In 1940,Japan invadedFrench Indochina, following France'scapitulationtoNazi Germany. French influence was suppressed by the Japanese, and in 1941 Cung, now known asHo Chi Minh, returned to Vietnam to establish theViet Minh, an anti-Japanese resistance movement that advocated for independence.[68]The Viet Minh received aid from theAllies, namely the US, Soviet Union, andChina. Beginning in 1944, the USOffice of Strategic Services(O.S.S.) provided the Viet Minh with weapons and training to fight the occupying Japanese andVichy Frenchforces.[74][75]Throughout the war, Vietnamese guerrilla resistance against the Japanese grew dramatically, and by the end of 1944 the Viet Minh had grown to over 500,000 members.[76]US PresidentFranklin D. Rooseveltcontinued to support Vietnamese resistance, and proposed that Vietnam's independence be granted under an international trusteeship after the war.[77]
In 1945, Japan, losing the war,overthrew the French governmentin Indochina, establishing theEmpire of Vietnamand installing Vietnamese EmperorBảo Đạias its figurehead leader.[78]Following thesurrender of Japanin August, the Viet Minh launched theAugust Revolution, overthrowing the Japanese-backed state and seizing weapons from the surrendering Japanese forces. On 2 September, Ho Chi Minh proclaimed theDeclaration of independence of the Democratic Republic of Vietnam(DRV). However, British and French forces swiftly arrived in Indochina to accept the Japanese surrender, and on 23 September they launched a coup which overthrew the DRV and reinstated French rule.[79][80][81]American support for the Viet Minh promptly ended, and O.S.S. forces left as the Frenchsought to reassert control of the country.
The Indochinese Communist Party was primarily responsible for starting widespread Vietnamese-on-Vietnamese violence.[82]: 515Particularly, from August 1945 to December 1946, the communist-ledViet Minhsought to consolidate power by terrorizing and purging rival Vietnamesenationalist groupsandTrotskyist activists.[83][84][85][86]: 383–441In 1946, the Viet Minh colluded with French forces to eliminate Vietnamese nationalists.[87][88]: 175–177[89]: 699–700
Tensions between the Viet Minh and French authorities had erupted intofull-scale warby 1946, a conflict which soon became entwined with the widerCold War. From 1948 onward, Vietnamese communists intensified violence and monopolized political power through campaigns such asland reform, class struggle, rectification (chỉnh huấn), and the suppression of the moderateNhân Văn–Giai Phẩm movement.[90][91]
On 12 March 1947, US presidentHarry S. Trumanannounced theTruman Doctrine, ananticommunistforeign policy which pledged US support to nations resisting "attempted subjugation by armed minorities or by outside pressures".[92]In Indochina, this doctrine was first put into practice in February 1950, when the United States recognized the French-backedState of VietnaminSaigon, led by former Emperor Bảo Đại, as the legitimate government of Vietnam, after thecommunist statesof theSoviet UnionandPeople's Republic of Chinarecognized theDemocratic Republic of Vietnam, led by Ho Chi Minh, as the legitimate Vietnamese government the previous month.[93]: 377–379[25]: 88The outbreak of theKorean Warin June convinced Washington policymakers that the war in Indochina was another example of communist expansionism, directed by the Soviet Union.[25]: 33–35
Military advisors from China began assisting the Viet Minh in July 1950.[94]: 14Chinese weapons, expertise, and laborers transformed the Viet Minh from a guerrilla force into a regular army.[25]: 26[95]In September 1950, the US further enforced the Truman Doctrine by creating aMilitary Assistance and Advisory Group(MAAG) to screen French requests for aid, advise on strategy, and train Vietnamese soldiers.[96]: 18By 1954, the US had spent $1 billion in support of the French military effort, shouldering 80% of the cost of the war.[25]: 35
During the Battle of Dien Bien Phu in 1954, UScarrierssailed to theGulf of Tonkinand the US conducted reconnaissance flights. France and the US discussed the use oftactical nuclear weapons, though reports of how seriously this was considered and by whom, are vague.[97][25]: 75According to then-Vice PresidentRichard Nixon, the Joint Chiefs of Staff drew up plans to use nuclear weapons to support the French.[97]Nixon, a so-called "hawk", suggested the US might have to "put American boys in".[8]: 76PresidentDwight D. Eisenhowermade American participation contingent on British support, but the British were opposed.[8]: 76Eisenhower, wary of involving the US in an Asian land war, decided against intervention.[25]: 75–76Throughout the conflict, US intelligence estimates remained skeptical of France's chance of success.[98]
On 7 May 1954, the French garrison at Dien Bien Phu surrendered. The defeat marked the end of French military involvement in Indochina. At theGeneva Conference, they negotiated a ceasefire with the Viet Minh, and independence was granted to Cambodia, Laos, and Vietnam.[99][100]
At the 1954 Geneva Conference, Vietnam was temporarily partitioned at the17th parallel. Ho Chi Minh wished to continue war in the south, but was restrained by Chinese allies who convinced him he could win control by electoral means.[101][25]: 87–88Under the Geneva Accords, civilians were allowed to move freely between the two provisional states for a 300-day period. Elections throughout the country were to be held in 1956 to establish a unified government.[25]: 88–90However, the US, represented at the conference by Secretary of StateJohn Foster Dulles, objected to the resolution; Dulles' objection was supported only by the representative of Bảo Đại.[75]John Foster's brother,Allen Dulles, who was director of theCentral Intelligence Agency, then initiated apsychological warfarecampaign which exaggerated anti-Catholic sentiment among the Viet Minh and distributed propaganda attributed to Viet Minh threatening an American attack on Hanoi with atomic bombs.[75][102][25]: 96–97
During the 300-day period, up to one million northerners, mainly minority Catholics, moved south, fearing persecution by the Communists.[25]: 96[103]The exodus was coordinated by a U.S.-funded $93 million relocation program, which involved theFrench Navyand the USSeventh Fleetto ferry refugees.[104]The northern refugees gave the laterNgô Đình Diệmregime a strong anti-communist constituency.[105]: 238Over 100,000 Viet Minh fighters went to the north for "regroupment", expecting to return south within two years.[58]: 98The Viet Minh left roughly 5,000 to 10,000cadresin the south as a base for future insurgency.[25]: 104The last French soldiers left South Vietnam in April 1956[25]: 116and the PRC also completed its withdrawal from North Vietnam.[94]: 14
Between 1953 and 1956, the North Vietnamese government instituted agrarian reforms, including "rent reduction" and "land reform", which resulted in political oppression. During land reform, North Vietnamese witnesses suggested a ratio of one execution for every 160 village residents, which extrapolates to 100,000 executions. Because the campaign was mainly in the Red River Delta area, 50,000 executions became accepted by scholars.[106]: 143[107][108]: 569[109]However, declassified documents from Vietnamese and Hungarian archives indicate executions were much lower, though likely greater than 13,500.[110]In 1956, leaders in Hanoi admitted to "excesses" in implementing this program and restored much of the land to the original owners.[25]: 99–100
The south, meanwhile, constituted the State of Vietnam, with Bảo Đại as Emperor, and Ngô Đình Diệm as prime minister. Neither the US, nor Diệm's State of Vietnam, signed anything at the Geneva Conference. The non-communist Vietnamese delegation objected strenuously to any division of Vietnam, but lost when the French accepted the proposal of Viet Minh delegatePhạm Văn Đồng,[111]: 134who proposed Vietnam eventually be united by elections under the supervision of "local commissions".[111]: 119The US countered with what became known as the "American Plan", with the support of South Vietnam and the UK.[111]: 140It provided for unification elections under the supervision of the UN, but was rejected by the Soviet delegation.[111]: 140The US said, "With respect to the statement made by the representative of the State of Vietnam, the United States reiterates its traditional position that peoples are entitled to determine their own future and that it will not join in any arrangement which would hinder this".[111]: 570–571US President Eisenhower wrote in 1954:
I have never talked or corresponded with a person knowledgeable in Indochinese affairs who did not agree that had elections been held as of the time of the fighting, possibly 80% of the population would have voted for the Communist Ho Chi Minh as their leader rather than Chief of State Bảo Đại. Indeed, the lack of leadership and drive on the part of Bảo Đại was a factor in the feeling prevalent among Vietnamese that they had nothing to fight for.[112]
According to thePentagon Papers, which commented on Eisenhower's observation, Diệm would have been a more popular candidate than Bảo Đại against Hồ, stating that "It is almost certain that by 1956 the proportion which might have voted for Ho - in a free election against Diem - would have been much smaller than 80%."[113]In 1957, independent observers from India, Poland, and Canada representing theInternational Control Commission(ICC) stated that fair elections were impossible, with the ICC reporting that neither South nor North Vietnam had honored the armistice agreement.[114]
From April to June 1955, Diệm eliminated political opposition in the south by launching operations against religious groups: theCao ĐàiandHòa HảoofBa Cụt. The campaign also attacked theBình Xuyênorganized crimegroup, which was allied with members of the communist party secret police and had military elements. The group was defeated in April following abattle in Saigon. As broad-based opposition to his harsh tactics mounted, Diệm increasingly sought to blame the communists.[8]
In areferendum on the future of the State of Vietnamin October 1955, Diệmriggedthe poll supervised by his brotherNgô Đình Nhuand was credited with 98% of the vote, including 133% in Saigon. His American advisors had recommended a more "modest" winning margin of "60 to 70 percent." Diệm, however, viewed the election as a test of authority.[105]: 224He declared South Vietnam to be an independent state under the name Republic of Vietnam (ROV), with him as president.[25]Likewise, Ho Chi Minh and other communists won at least 99% of the vote in North Vietnamese "elections".[106]: 193–194, 202–203, 215–217
Thedomino theory, which argued that if a country fell to communism, all surrounding countries would follow, was first proposed by theEisenhower administration.[93]: 19John F. Kennedy, then asenator, said in a speech to theAmerican Friends of Vietnam: "Burma, Thailand, India, Japan, the Philippines and obviously Laos and Cambodia are among those whose security would be threatened if the Red Tide of Communism overflowed into Vietnam."[115]
A devout Catholic, Diệm was fervently anti-communist, nationalist, and socially conservative. Historian Luu Doan Huynh notes "Diệm represented narrow and extremist nationalism coupled with autocracy andnepotism."[93]: 200–201Most Vietnamese wereBuddhist, and alarmed by Diệm's actions, like his dedication of the country to theVirgin Mary.
On 11 April 1955,[116]Diệm launched the "Denounce the Communists" campaign, during which suspected communists and other anti-government elements were arrested, imprisoned, tortured, or executed. He instituted the death penalty in August 1956 against activity deemed communist.[51]The North Vietnamese government claimed that, by November 1957, over 65,000 individuals were imprisoned and 2,148 killed in the process.[117]According toGabriel Kolko, 40,000political prisonershad been jailed by the end of 1958.[58]: 89In October 1956, Diệm launched aland reform programlimiting the size of rice farms per owner. 1.8m acres of farm land became available for purchase by landless people. By 1960, the process had stalled because many of Diem's biggest supporters were large landowners.[118]: 14–16
In May 1957, Diệm undertook a10-day state visit to the US. President Eisenhower pledged his continued support, and a parade was held in Diệm's honor. But Secretary of State Dulles privately conceded Diệm had to be backed because they could find no better alternative.[105]: 230
Between 1954-57, the Diệm government succeeded in preventing large-scale unrest in the countryside. In April 1957, insurgents launched an assassination campaign, referred to as "extermination of traitors".[119]17 people were killed in theChâu Đốc massacreat a bar in July, and in September a district chief was killed with his family.[51]By early 1959, Diệm had come to regard the violence as an organized campaign and implemented Law 10/59, which made political violence punishable by death and property confiscation.[120]There had been division among former Viet Minh, whose main goal was to hold elections promised in the Geneva Accords, leading to "wildcat" activities separate from the other communists and anti-GVN (Government of the Republic of Vietnam) activists.Douglas Pikeestimated that insurgents carried out 2,000 abductions, and 1,700 assassinations of government officials, village chiefs, hospital workers and teachers from 1957 to 1960.[25]: 106[51]Violence between insurgents and government forces increased drastically from 180 clashes in January 1960, to 545 in September.[121]
In September 1960,COSVN, North Vietnam's southern headquarters, ordered a coordinated uprising in South Vietnam against the government and a third of the population was soon living in areas of communist control.[25]: 106–107In December 1960, North Vietnam formally created theViet Cong(VC) with the intent of uniting all anti-GVN insurgents, including non-communists. It was formed inMemot, Cambodia, and directed through COSVN.[94]: 55–58The VC "placed heavy emphasis on the withdrawal of American advisors and influence, on land reform and liberalization of the GVN, oncoalition governmentand the neutralization of Vietnam." The identities of the leaders of the organization were often kept secret.[51]
Support for the VC was driven by resentment of Diem's reversal of Viet Minh land reforms in the countryside. The Viet Minh had confiscated large private landholdings, reduced rents and debts, and leased communal lands, mostly to poorer peasants. Diem brought the landlords back, people who had been farming land for years had to return it to landlords and pay years of back rent.Marilyn B. Youngwrote that "The divisions within villages reproduced those that had existed against the French: 75% support for the NLF, 20% trying to remain neutral and 5% firmly pro-government".[122]: 73
In March 1956, southern communist leaderLê Duẩnpresented a plan to revive the insurgency entitled "The Road to the South", to the Politburo in Hanoi. However, as China and the Soviets opposed confrontation, his plan was rejected.[94]: 58Despite this, the North Vietnamese leadership approved tentative measures to revive southern insurgency in December.[50]Communist forces were under a single command structure set up in 1958.[123]In May 1958, North Vietnamese forces seized the transportation hub atTcheponein Southern Laos near the demilitarized zone, between North and South Vietnam.[124]: 24
The North Vietnamese Communist Party approved a "people's war" on the South at a session in January 1959,[25]: 119–120and, in May,Group 559was established to upgrade theHo Chi Minh trail, at this time a six-month mountain trek through Laos. On 28 July, North Vietnamese andPathet Laoforces invaded Laos, fighting theRoyal Lao Armyalong the border.[125]: 26About 500 of the "regroupees" of 1954 were sent south on the trail during its first year of operation.[126]The first arms delivery via the trail was completed in August 1959.[127]In April 1960, North Vietnam imposed military conscription for men. About 40,000 communist soldiers infiltrated the south from 1961 to 1963.[94]: 76
In the1960 U.S. presidential election, Senator John F. Kennedy defeated incumbent Vice President Richard Nixon. Although Eisenhower warned Kennedy about Laos and Vietnam, Europe and Latin America "loomed larger than Asia on his sights."[105]: 264
The Kennedy administration remained committed to the Cold War foreign policy inherited from the Truman and Eisenhower administrations. In 1961, the US had 50,000 troops based in South Korea, and Kennedy faced four crisis situations: the failure of theBay of Pigs Invasionhe had approved in April,[128]settlement negotiations between the pro-Western government of Laos and the Pathet Lao communist movement in May,[105]: 265construction of theBerlin Wallin August, and the Cuban Missile Crisis in October. Kennedy believed another failure to stop communist expansion would irreparably damage US credibility. He was determined to "draw a line in the sand" and prevent a communist victory in Vietnam. He told James Reston ofThe New York Timesafter the Vienna summit with Khrushchev, "Now we have a problem making our power credible and Vietnam looks like the place."[129][130]
Kennedy's policy toward South Vietnam assumed Diệm and his forces had to defeat the guerrillas on their own. He was against the deployment of American combat troops and observed "to introduce U.S. forces in large numbers there today, while it might have an initially favorable military impact, would almost certainly lead to adverse political and, in the long run, adverse military consequences."[131]The quality of the South Vietnamese military, however, remained poor. Poor leadership, corruption, and political promotions weakened the ARVN. The frequency of guerrilla attacks rose as the insurgency gathered steam. While Hanoi's support for the VC played a role, South Vietnamese governmental incompetence was at the core of the crisis.[93]: 369
One major issue Kennedy raised was whether the Soviet space and missile programs had surpassed those of the US. Although Kennedy stressed long-range missile parity with the Soviets, he was interested in usingspecial forcesforcounterinsurgencywarfare inThird Worldcountries threatened by communist insurgencies. Although they were intended for use behind front lines after a conventional Soviet invasion of Europe, Kennedy believed guerrilla tactics employed by special forces, such as theGreen Berets, would be effective in a "brush fire" war in Vietnam.
Kennedy advisorsMaxwell TaylorandWalt RostowrecommendedUS troops be sent to South Vietnam disguised as flood relief workers.[132]Kennedy rejected the idea but increased military assistance. In April 1962,John Kenneth Galbraithwarned Kennedy of the "danger we shall replace the French as a colonial force in the area and bleed as the French did."[133]Eisenhower put 900 advisors in Vietnam, and by November 1963, Kennedy had put 16,000 military personnel there.[25]: 131
TheStrategic Hamlet Programwas initiated in late 1961. This joint U.S.–South Vietnamese program attempted to resettle the rural population into fortified villages. It was implemented in early 1962 and involved some forced relocation and segregation of rural South Vietnamese, into new communities where the peasantry would be isolated from the VC. It was hoped these new communities would provide security for the peasants and strengthen the tie between them and the central government. However, by November 1963 the program had waned, and it ended in 1964.[8]: 1070In July 1962, 14 nations, including China, South Vietnam, the Soviet Union, North Vietnam, and the US, signed anagreementpromising to respect Laos' neutrality.
The inept performance of the ARVN was exemplified by failed actions such as theBattle of Ấp Bắcon 2 January 1963, in which the VC won a battle against a much larger and better-equipped South Vietnamese force, many of whose officers seemed reluctant even to engage in combat.[134]: 201–206The ARVN lost 83 soldiers and 5 US helicopters, serving to ferry troops shot down by VC forces, while the VC lost only 18 soldiers. The ARVN forces were led by Diệm's most trusted general,Huỳnh Văn Cao. Cao was a Catholic, promoted due to religion and fidelity rather than skill, and his main job was to preserve his forces to stave off coups. Policymakers in Washington began to conclude Diệm was incapable of defeating the communists and might even make a deal with Ho Chi Minh. He seemed concerned only with fending off coups and had become paranoid after attempts in1960and1962, which he partly attributed to US encouragement. AsRobert F. Kennedynoted, "Diệm wouldn't make even the slightest concessions. He was difficult to reason with..."[135]Historian James Gibson summed up the situation:
Strategic hamlets had failed... The South Vietnamese regime was incapable of winning the peasantry because of its class base among landlords. Indeed, there was no longer a 'regime' in the sense of a relatively stable political alliance and functioning bureaucracy. Instead, civil government and military operations had virtually ceased. The National Liberation Front had made great progress and was close to declaring provisional revolutionary governments in large areas.[136]
Discontent with Diệm's policies exploded in May 1963, following theHuế Phật Đản shootingsof nine Buddhists protesting the ban on displaying theBuddhist flagonVesak, Buddha's birthday. This resulted in mass protests—theBuddhist crisis—against discriminatory policies that gave privileges to Catholics over the Buddhist majority. Diệm's elder brotherNgô Đình Thụcwas the Archbishop of Huế and aggressively blurred the separation between church and state. Thuc's anniversary celebrations occurred shortly before Vesak had been bankrolled by the government, and Vatican flags were displayed prominently. There had been reports of Catholic paramilitaries demolishing Buddhist pagodas throughout Diệm's rule. Diệm refused to make concessions to the Buddhist majority or take responsibility for the deaths. On 21 August 1963, theARVN Special Forcesof ColonelLê Quang Tung, loyal to Diệm's younger brother Ngô Đình Nhu,raided pagodas, causing widespread destruction and leaving a death toll into the hundreds.
US officials began discussingregime changeduring the middle of 1963. TheUnited States Department of Statewanted to encourage a coup, while the Pentagon favored Diệm. Chief among the proposed changes was removal of Diệm's younger brother Nhu, who controlled the secret police and special forces, and was seen as being behind the Buddhist repression and the architect of the Ngô family's rule. This proposal was conveyed to the US embassy in Saigon inCable 243. The CIA contacted generals planning to remove Diệm, and told them the US would not oppose such a move, nor punish them by cutting off aid. Diệm was overthrown and then executed, along with his brother, on 2 November 1963. When Kennedy was informed, Maxwell Taylor remembered he "rushed from the room with a look of shock and dismay on his face."[105]: 326Kennedy had not anticipated Diệm's murder. The U.S. ambassadorHenry Cabot Lodge, invited the coup leaders to the embassy and congratulated them. Lodge informed Kennedy that "the prospects now are for a shorter war".[105]: 327Kennedy wrote Lodge a letter congratulating him for "a fine job".[137]
Following the coup, chaos ensued. Hanoi took advantage and increased its support for the VC. South Vietnam entered extreme political instability, as one military government toppled another in quick succession. Increasingly, each new regime was viewed by the communists as a puppet of the Americans; whatever the failings of Diệm, his credentials as a nationalist had been impeccable.[93]: 328US advisors were embedded at every level of the South Vietnamese armed forces. They were however criticized for ignoring the political nature of the insurgency.[138]The Kennedy administration sought to refocus US efforts on pacification – which in this case was defined as countering the growing threat of insurgency[139][140]– and"winning the hearts and minds"of the population. Military leadership in Washington, however, was hostile to any role for U.S. advisors other than troop training.[141]GeneralPaul Harkins, thecommander of U.S. forces in South Vietnam, confidently predicted victory by Christmas 1963.[96]: 103The CIA was less optimistic, however, warning that "the Viet Cong by and large retain de facto control of much of the countryside and have steadily increased the overall intensity of the effort".[142]
Paramilitary officers from the CIA'sSpecial Activities Divisiontrained and ledHmongtribesmen in Laos and into Vietnam. The indigenous forces were in the tens of thousands and conducted direct action missions, led by paramilitary officers, against the Communist Pathet Lao forces and their North Vietnamese supporters.[143]The CIA ran thePhoenix Programand participated in theMilitary Assistance Command, Vietnam – Studies and Observations Group(MAC-V SOG).[144]
Kennedywas assassinatedon 22 November 1963. Vice PresidentLyndon B. Johnsonhad not been heavily involved with policy toward Vietnam;[145][A 9]however, upon becoming president, he immediately focused on it. On 24 November, he said, "the battle against communism... must be joined... with strength and determination."[147]Johnson knew he had inherited a deteriorating situation,[148]but adhered to the widely accepted domino argument for defending the South: Should they retreat or appease, either action would imperil other nations.[149]Findings from RAND'sViet Cong Motivation and Morale Projectbolstered his confidence that an air war would weaken the insurgency. Some argue the policy of North Vietnam was not to topple other non-communist governments in South East Asia.[93]: 48
The military revolutionary council, meeting in lieu of a strong South Vietnamese leader, had 12 members. It was headed by GeneralDương Văn Minh, whom journalistStanley Karnow, recalled as "a model of lethargy".[105]: 340Lodge cabled home about Minh: "Will he be strong enough to get on top of things?" Minh's regime was overthrown in January 1964 by GeneralNguyễn Khánh.[105]: 341There was persistent instability in the military: several coups—not all successful—occurred in a short period of time.
On 2 August 1964,USSMaddox, on an intelligence mission along North Vietnam's coast, fired upon and damaged torpedo boats approaching it in the Gulf of Tonkin.[58]: 124A second attack was reported two days later onUSSTurner JoyandMaddox. The circumstances were murky.[25]: 218–219Johnson commented to Undersecretary of State George Ball that "those sailors out there may have been shooting at flying fish."[150]AnNSApublication declassified in 2005 revealed there was no attack on 4 August.[151]
The second "attack" led toretaliatory airstrikes, and prompted Congress to approve theGulf of Tonkin Resolutionon 7 August.[152]: 78The resolution granted the president power "to take all necessary measures to repel any armed attack against the forces of the United States and to prevent further aggression" and Johnson relied on this as giving him authority to expand the war.[25]: 221Johnson pledged he was not "committing American boys to fighting a war that I think ought to be fought by the boys of Asia to help protect their own land".[25]: 227
TheNational Security Councilrecommended an escalation of the bombing of North Vietnam. Following anattack on a U.S. Army baseon 7 February 1965,[153]airstrikes were initiated, while Soviet PremierAlexei Kosyginwas on astate visitto North Vietnam.Operation Rolling ThunderandOperation Arc Lightexpanded aerial bombardment and ground support operations.[154]The bombing campaign, which lasted three years, was intended to force North Vietnam to cease its support for the VC by threatening to destroy North Vietnamese air defenses and infrastructure. It was additionally aimed at bolstering South Vietnamese morale.[155]Between March 1965 and November 1968,Rolling Thunderdeluged the north with a million tons of missiles, rockets and bombs.[105]: 468
Bombing was not restricted to North Vietnam. Other aerial campaigns, targeted different parts of the VC and PAVN infrastructure. These included the Ho Chi Minh Trail through Laos and Cambodia. The ostensibly neutral Laos had becomethe scene of a civil war, pitting theLaotian governmentbacked by the US, against the Pathet Lao and its North Vietnamese allies.
Aerial bombardment against the Pathet Lao and PAVN forces was carried out by the US to prevent the collapse of the Royal central government, and deny use of the Ho Chi Minh Trail. Between 1964-73, the U.S. dropped two million tons of bombs on Laos, nearly equal to the 2.1 million tons of bombs it dropped on Europe and Asia during World War II, making Laos the most heavily bombed country in history.[156]
The objective of stopping North Vietnam and the VC was never reached. TheChief of Staff of the United States Air ForceCurtis LeMay, however, had long advocated saturation bombing in Vietnam and wrote of the communists that "we're going to bomb them back into the Stone Age".[25]: 328
Following the Tonkin Resolution, Hanoi anticipated the arrival of US troops and began expanding the VC, as well as sending increasing numbers of PAVN personnel southwards. They were outfitting the VC forces and standardizing their equipment withAK-47rifles and other supplies, as well as forming the9th Division.[25]: 223[157]"From a strength of approximately 5,000 at the start of 1959 the Viet Cong's ranks grew to about 100,000 at the end of 1964... Between 1961-64 the Army's strength rose from about 850,000 to nearly a million men."[138]U.S. troop numbers deployed to Vietnam during the same period were much lower: 2,000 in 1961, rising to 16,500 in 1964.[158]The use of captured equipment decreased, while more ammunition and supplies were required to maintain regular units. Group 559 was tasked with expanding the Ho Chi Minh Trail, in light of the bombardment by US warplanes. The war had shifted into the final, conventional phase of Hanoi'sthree-stage protracted warfare model. The VC was now tasked with destroying the ARVN and capturing and holding areas; however, it was not yet strong enough to assault towns and cities.
In December 1964, ARVN forces suffered heavy losses at theBattle of Bình Giã,[159]in a battle both sides viewed as a watershed. Previously, the VC had utilized hit-and-run guerrilla tactics. At Binh Gia, however, they defeated a strong ARVN force in a conventional battle and remained in the field for four days.[160]: 58Tellingly, South Vietnamese forces were again defeated in June 1965 at theBattle of Đồng Xoài.[160]: 94
On 8 March 1965, 3,500U.S. Marineswere landed nearDa Nang, South Vietnam.[25]: 246–247This marked the beginning of the American ground war. U.S. public opinion overwhelmingly supported the deployment.[161]The Marines' initial assignment was defense ofDa Nang Air Base. The first deployment was increased to nearly 200,000 by December.[93]: 349–351U.S. military had long been schooled in offensive warfare. Regardless of political policies, U.S. commanders were institutionally and psychologically unsuited to a defensive mission.[93]: 349–351
GeneralWilliam Westmorelandinformed AdmiralU. S. Grant Sharp Jr., commander of U.S. Pacific forces, that the situation was critical,[93]: 349–351"I am convinced that U.S. troops with their energy, mobility, and firepower can successfully take the fight to the NLF (Viet Cong)".[162]With this recommendation, Westmoreland advocated an aggressive departure from America's defensive posture and the sidelining of the South Vietnamese. By ignoring ARVN units, the U.S. commitment became open-ended.[93]: 353Westmoreland outlined a three-point plan to win:
The plan was approved by Johnson and marked a profound departure from the insistence that South Vietnam was responsible for defeating the VC. Westmoreland predicted victory by December 1967.[164]Johnson did not communicate this change to the media, instead he emphasized continuity.[165]The change in policy depended on matching the North Vietnamese and VC in a contest ofattritionandmorale. The opponents were locked in a cycle ofescalation.[93]: 353–354However the Johnson administration ruled out invasion of North Vietnam due to fears of Chinese or Soviet intervention.[166]Westmoreland and McNamara touted thebody countsystem for gauging victory, a metric that proved flawed.[167]
The American buildup transformed the South Vietnamese economy and had a profound effect on society. South Vietnam was inundated with manufactured goods. Washington encouraged itsSEATOallies to contribute troops; Australia, New Zealand, Thailand and the Philippines[105]: 556agreed. South Korea asked to join theMany Flagsprogram in return for economic compensation. Major allies, however, notably Canada and the UK, declined troop requests.[168]
The U.S. and its allies mounted complexsearch and destroyoperations. In November 1965, the U.S. engaged in its first major battle with the PAVN, theBattle of Ia Drang.[169]The operation was the first large scale helicopter air assault by the U.S., and first to employBoeing B-52 Stratofortressbombers in support.[25]: 284–285These tactics continued in 1966–67, however, the PAVN/VC insurgents remained elusive and demonstrated tactical flexibility. By 1967, the war had generated large-scale internal refugees, 2 million in South Vietnam, with 125,000 people evacuated and rendered homeless duringOperation Masheralone,[170]the largest search and destroy operation to that point. Operation Masher had negligible impact, however, as the PAVN/VC returned to the province just four months after it ended.[171]: 153–156Despite major operations, which the VC and PAVN would typically evade, the war was characterized by smaller-unit engagements.[172]The VC and PAVN would initiate 90% of large firefights, and thus the PAVN/VC would retain strategic initiative despite overwhelming US force and fire-power deployment.[172]The PAVN and Viet Cong had developed strategies capable of countering US military doctrines and tactics: seeNLF and PAVN battle tactics.
Meanwhile, the political situation in South Vietnam began to stabilize with the arrival of prime minister Air MarshalNguyễn Cao Kỳand figurehead chief of state, GeneralNguyễn Văn Thiệu, in mid-1965 at the head of a junta. In 1967, Thieu became president with Ky as his deputy, after rigged elections. Though they were nominally a civilian government, Kỳ was supposed to maintain real power through a behind-the-scenes military body. However, Thiệu outmanoeuvred and sidelined Kỳ. Thiệu was accused of murdering Kỳ loyalists through contrived military accidents. Thiệu remained president until 1975, having won aone-candidate election in 1971.[105]: 706
Johnson employed a "policy of minimum candor"[105]: 18with the media. Military information officers sought to manage coverage by emphasizing stories that portrayed progress. This policy damaged public trust in official pronouncements. As coverage of the war and the Pentagon diverged, a so-calledcredibility gapdeveloped.[105]: 18Despite Johnson and Westmoreland publicly proclaiming victory and Westmoreland stating the "end is coming into view",[173]internal reports in thePentagon Papersindicate that VC forces retained strategic initiative and controlled their losses. VC attacks against static US positions accounted for 30% of engagements, VC/PAVN ambushes and encirclements for 23%, American ambushes against VC/PAVN forces for 9%, and American forces attacking Viet Cong emplacements only 5%.[172]
In late 1967, the PAVN lured American forces into the hinterlands atĐắk Tôand at the MarineKhe Sanh combat base, where the U.S. foughtThe Hill Fights. These were part of a diversionary strategy meant to draw US forces towards the Central Highlands.[174]Preparations were underway for theTet Offensive, with the intention ofVăn Tiến Dũngforces to launch "direct attacks on the American and puppet nerve centers—Saigon,Huế, Danang, all the cities, towns and main bases..."[175]Le Duan sought to placate critics of the stalemate by planning a decisive victory.[176]: 90–94He reasoned this could be achieved through sparking an uprising within the towns and cities,[176]: 148along with mass defections among ARVN units, who were on leave during the truce period.[177]
The Tet Offensive began on 30 January 1968, as over 100 cities were attacked by over 85,000 VC/PAVN troops, including assaults on military installations, headquarters, and government buildings, including theU.S. Embassy in Saigon.[93]: 363–365U.S. and South Vietnamese forces were shocked by the scale, intensity and deliberative planning, as infiltration of personnel and weapons into the cities was accomplished covertly;[175]the offensive constituted anintelligence failureon the scale ofPearl Harbor.[105]: 556Most cities were recaptured within weeks,except the former imperial capital Huế, which PAVN/VC troops held on for 26 days.[178]: 495Theyexecuted approximately 2,800 unarmed Huếcivilians and foreigners they considered to be spies.[179][178]: 495In the following Battle of Huế American forces employed massive firepower that left 80% of the city in ruins.[58]: 308–309AtQuảng Trị City, theARVN Airborne Division, the 1st Division and a regiment of the US 1st Cavalry Division managed to hold out and overcome an assault intended to capture the city.[180][181]: 104In Saigon, VC/PAVN fighters had captured areas in and around the city, attacking key installations before US and ARVN forces dislodged them after three weeks.[25]: 479During one battle,Peter Arnettreported an infantry commander saying of theBattle of Bến Trethat "it became necessary to destroy the village in order to save it."[182][183]
During the first month of the offensive, 1,100 Americans and other allied troops, 2,100 ARVN and 14,000 civilians were killed.[184]After two months, nearly 5,000 ARVN and over 4,000 U.S. forces had been killed and 45,820 wounded.[184]The U.S. claimed 17,000 PAVN/VC had been killed and 15,000 wounded.[181]: 104[180]: 82A month later a second offensive known as theMay Offensivewas launched; it demonstrated the VC were still capable of carrying out orchestrated nationwide offensives.[25]: 488–489Two months later a third offensive was launched,Phase III Offensive. PAVN records of their losses across all three offensives was 45,267 killed and 111,179 total casualties.[185][186]It had become the bloodiest year up to then. The failure to spark a general uprising and lack of defections among the ARVN units meant both war goals of Hanoi had fallen flat at enormous cost.[176]: 148–149
Prior to Tet, in November 1967, Westmoreland had spearheaded a public relations drive for the Johnson administration to bolster flagging public support.[187]In a speech to theNational Press Clubhe said a point had been reached "where the end comes into view."[188]Thus, the public was shocked and confused when Westmoreland's predictions were trumped by the Tet Offensive.[187]Public approval of his performance dropped from 48% to 36%, and endorsement for the war fell from 40% to 26%."[105]: 546The public and media began to turn against Johnson as the offensives contradicted claims of progress.[187]
At one point in 1968, Westmoreland considered the use ofnuclear weaponsin a contingency plan codenamedFracture Jaw, which was abandoned when it became known to the White House.[189]Westmoreland requested 200,000 additional troops, which was leaked to the media, and the fallout combined with intelligence failures caused him to be removed from command in March 1968, succeeded by his deputyCreighton Abrams.[190]
On 10 May 1968,peace talksbegan between the US and North Vietnam in Paris. Negotiations stagnated for five months, until Johnson gave orders to halt the bombing of North Vietnam. Hanoi realized it could not achieve a "total victory" and employed a strategy known as "talking while fighting, fighting while talking", in which offensives would occur concurrently with negotiations.[191]
Johnson declined to run for re-election as his approval rating slumped from 48% to 36%.[25]: 486His escalation of the war divided Americans, cost 30,000 American lives by that point and was regarded to have destroyed his presidency.[25]: 486Refusal to send more troops was seen as Johnson's admission that the war was lost.[192]As McNamara said, "the dangerous illusion of victory by the United States was therefore dead."[93]: 367
Vietnam was a major political issue during theUnited States presidential election in 1968. The election was won by Republican Richard Nixon who claimed to have a secret plan to end the war.[25]: 515[193]
Nixon began troop withdrawals in 1969. His plan to build up the ARVN so it could take over the defense of South Vietnam became known as "Vietnamization". As the PAVN/VC recovered from their 1968 losses and avoided contact, Abrams conducted operations aimed at disrupting logistics, with better use of firepower and more cooperation with the ARVN.[25]: 517In October 1969, Nixon had ordered B-52s loaded with nuclear weaponsto race to the border of Soviet airspaceto convince the Soviets, in accord with themadman theory, he was capable of anything to end the war.[194][195]Nixon had soughtdétentewith the Soviet Union andrapprochement with China, which decreased tensions and led to nuclear arms reductions. However, the Soviets continued to supply the North Vietnamese.[196][197]
On 2 September 1969, Ho Chi Minh died.[198]The failure of the Tet Offensive to spark an uprising in the south caused a shift in Hanoi's war strategy, and theGiáp-Chinh"Northern-First" faction regained control over military affairs from the Lê Duẩn-Hoàng Văn Thái"Southern-First" faction.[199]: 272–274An unconventional victory was sidelined in favor of a conventional victory through conquest.[176]: 196–205Large-scale offensives were rolled back in favor ofsmall-unitandsapperattacks as well as targeting the pacification and Vietnamization strategy.[199]Following Tet, the PAVN had transformed from alight-infantry, limited mobility force into ahigh-mobileand mechanizedcombined armsforce.[199]: 189By 1970, over 70% of communist troops in the south were northerners, and southern-dominated VC units no longer existed.[200]
Theanti-war movementwas gaining strength in the US. Nixon appealed to the "silent majority" who he said supported the war. But revelations of the 1968My Lai massacre,[25]: 518–521in which a US Army unit raped and killed civilians, and the 1969 "Green Beret Affair", where eightSpecial Forcessoldiers, were arrested for the murder[201]of a suspected double agent,[202]provoked outrage.
In 1971, thePentagon Paperswere leaked toThe New York Times. The top-secret history of US involvement in Vietnam, commissioned by the Department of Defense, detailed public deceptions by the government. TheSupreme Courtruled its publication was legal.[203]
Following the Tet Offensive and decreasing support among the public, US forces began a period of morale collapse, and disobedience.[204]: 349–350[205]: 166–175At home, desertion rates quadrupled from 1966 levels.[206]Among the enlisted, only 2.5% chose infantry combat positions in 1969–70.[206]ROTCenrollment decreased from 191,749 in 1966 to 72,459 by 1971,[207]and reached a low of 33,220 in 1974,[208]depriving US forces of much-needed military leadership.
Open refusal to engage in patrols or carry out orders emerged, with a case of an entire company refusing orders.[209]Unit cohesion began to dissipate and focused on minimizing contact with the PAVN/VC.[205]A practice known as "sand-bagging" started, where units ordered to patrol would go into the country-side, find a site out of view from superiors and radio in false coordinates and reports.[171]: 407–411Drug usage increased among US forces, 30% regularly used marijuana,[171]: 407while a House subcommittee found 10% regularly used high-grade heroin.[206][25]: 526From 1969 on, search-and-destroy operations became referred to as "search and avoid" operations, falsifying battle reports while avoiding guerrillas.[210]900fraggingand suspected fragging incidents were investigated, most occurring between 1969-71.[211]: 331[171]: 407In 1969, field-performance was characterized by low morale and poor leadership.[211]: 331The decline in US morale was demonstrated by theBattle of FSB Mary Annin 1971, in which a sapper attack inflicted serious losses on the U.S. defenders.[211]: 357Westmoreland, no longer in command but tasked with investigation of the failure, cited a dereliction of duty, lax defensive postures and lack of officers in charge.[211]: 357
On the collapse of morale, historian Shelby Stanton wrote:
In the last years of the Army's retreat, its remaining forces were relegated to static security. The American Army's decline was readily apparent in this final stage. Racial incidents, drug abuse, combat disobedience, and crime reflected growing idleness, resentment, and frustration... the fatal handicaps of faulty campaign strategy, incomplete wartime preparation, and the tardy, superficial attempts at Vietnamization. An entire American army was sacrificed on the battlefield of Vietnam.[211]: 366–368
Beginning in 1969, American troops were withdrawn from border areas where most of the fighting took place and redeployed along the coast and interior. US casualties in 1970 were less than half of 1969, after being relegated to less active combat.[212]While US forces were redeployed, the ARVN took over combat operations, with casualties double US ones in 1969, and more than triple US ones in 1970.[213]In the post-Tet environment, membership in theSouth Vietnamese Regional ForceandPopular Forcemilitias grew, and they were now capable of providing village security, which the Americans had not accomplished.[213]
In 1970, Nixon announced the withdrawal of an additional 150,000 American troops, reducing US numbers to 265,500.[212]By 1970, VC forces were no longer southern-majority, nearly 70% of units were northerners.[214]Between 1969-71 the VC and some PAVN units had reverted tosmall unit tacticstypical of 1967 and prior, instead of nationwide offensives.[176]In 1971, Australia and New Zealand withdrew their soldiers and US troops were reduced to 196,700, with a deadline to remove another 45,000 troops by February 1972. The US reduced support troops, and in March 1971 the5th Special Forces Group, the first American unit deployed to South Vietnam, withdrew.[215]: 240[A 10]
PrinceNorodom Sihanoukhad proclaimed Cambodia neutral since 1955,[218]but permitted the PAVN/VC to use the port ofSihanoukvilleand theSihanouk Trail. In March 1969 Nixon launched a secret bombing campaign, calledOperation Menu, against communist sanctuaries along the Cambodia/Vietnam border. Only five congressional officials were informed.[A 11]
In March 1970,Sihanouk was deposedby hispro-Americanprime ministerLon Nol, who demanded North Vietnamese troops leave Cambodia or face military action.[219]Nol began rounding up Vietnamese civilians in Cambodia into internment camps and massacring them, provoking reactions from the North and South Vietnamese governments.[220]In April–May 1970, North Vietnam invaded Cambodia at the request of theKhmer Rouge, following negotiations with deputy leaderNuon Chea. Nguyen Co Thach recalls: "Nuon Chea has asked for help and we have liberated five provinces of Cambodia in ten days."[221]US and ARVN forces launched theCambodian Campaignin May to attack PAVN/VC bases. A counter-offensive in 1971, as part ofOperation Chenla IIby the PAVN, would recapture most border areas and decimate Nol's forces.
The US incursion into Cambodia sparkednationwide U.S. protestsas Nixon had promised to deescalate American involvement.Students were killed by National Guardsmenin May 1970 during a protest atKent State University, which provoked further outrage. The reaction by the administration was seen as callous, reinvigorating the declining anti-war movement.[205]: 128–129The US Air Force continued to bomb Cambodia as part ofOperation Freedom Deal.
Building on the success of ARVN units in Cambodia, and further testing the Vietnamization program, the ARVN was tasked withOperation Lam Son 719in February 1971, the first major ground operation to attack the Ho Chi Minh Trail. This was the first time the PAVN would field-test its combined arms force.[176]The first few days were a success, but momentum slowed after fierce resistance. Thiệu had halted the general advance, leaving PAVN armored divisions able to surround them.[222]
Thieu orderedair assaulttroops to capture the Tchepone crossroad and withdraw, despite facing four-times larger numbers. During the withdrawal, the PAVN counterattack had forced a panicked rout. Half of the ARVN troops were either captured or killed, half of the ARVN/US support helicopters were downed and the operation was considered a fiasco, demonstrating operational deficiencies within the ARVN.[105]: 644–645Nixon and Thieu had sought a showcase victory simply by capturing Tchepone, and it was spun off as an "operational success".[223][25]: 576–582
Vietnamization was again tested by theEaster Offensiveof 1972, a conventional PAVN invasion of South Vietnam. The PAVN overran the northern provinces and attacked from Cambodia, threatening to cut the country in half. US troop withdrawals continued, but American airpower responded, beginningOperation Linebacker, and the offensive was halted.[25]: 606–637The US Navy initiatedOperation Pocket Moneyin May, an aerial mining campaign inHaiphongHarbor that prevented North Vietnam's allies from resupplying it with weapons.[224]
The war was central to the1972 U.S. presidential electionas Nixon's opponent,George McGovern, campaigned on immediate withdrawal. Nixon's Security Advisor,Henry Kissinger, had continued secret negotiations with North Vietnam'sLê Đức Thọand in October 1972 reached an agreement. Thiệu demanded changes to the peace accord upon its discovery, and when North Vietnam went public with the details, the Nixon administration claimed they were attempting to embarrass the president. The negotiations became deadlocked when Hanoi demanded changes. To show his support for South Vietnam and force Hanoi back to the negotiating table, Nixon orderedOperation Linebacker II, a bombing of Hanoi and Haiphong in December 1972.[25]: 649–663Nixon pressured Thiệu to accept the agreement or face military action.[225]
On 15 January 1973, all US combat activities were suspended. Lê Đức Thọ and Henry Kissinger, along with the PRG Foreign MinisterNguyễn Thị Bìnhand a reluctant Thiệu, signed theParis Peace Accordson 27 January 1973.[171]: 508–513This ended direct U.S. involvement in the war, created a ceasefire between North Vietnam/PRG and South Vietnam, guaranteed the territorial integrity of Vietnam under the Geneva Conference of 1954, called for elections or a political settlement between the PRG and South Vietnam, allowed 200,000 communist troops to remain in the south, and agreed to a POW exchange. There was a 60-day period for the withdrawal of US forces. "This article", noted Peter Church, "proved... to be the only one of the Paris Agreements which was fully carried out."[226]All US forces personnel were withdrawn by March 1973.[96]: 260
In the lead-up to the ceasefire on 28 January, both sides attempted to maximize land and population under their control in a campaign known as theWar of the flags. Fighting continued after the ceasefire, without US participation, and throughout the year.[171]: 508–513North Vietnam was allowed to continue supplying troops in the South but only to replace expended material. TheNobel Peace Prizewas awarded to Kissinger and Thọ, but Thọ declined it saying true peace did not yet exist.
On 15 March 1973, Nixon implied the US would intervene militarily if the North launched a full offensive, and Secretary of DefenseSchlesingerre-affirmed this during his June confirmation hearings. Public and congressional reaction to Nixon's statement was unfavorable, prompting the Senate to pass theCase–Church Amendmentto prohibit any intervention.[105]: 670–672
Northern leaders expected the ceasefire terms would favor their side, but Saigon, bolstered by a surge of US aid just before the ceasefire went into effect, began to roll them back. The North responded with a new strategy hammered out in meetings in Hanoi in March 1973, according to the memoirs ofTrần Văn Trà.[105]: 672–674With US bombings suspended, work on the Ho Chi Minh Trail and other logistical structures could proceed. Logistics would be upgraded until the North was in a position to launch a massive invasion of the South, projected for the 1975–76 dry season. Trà calculated this date would be Hanoi's last opportunity to strike, before Saigon's army could be fully trained.[105]: 672–674The PAVN resumed offensive operations when the dry season began in 1973, and by January 1974 had recaptured territory it lost during the previous dry season.
Within South Vietnam, the departure of the US and the global recession after the1973 oil crisishurt an economy partly dependent on US financial support and troop presence. After clashes that left 55 ARVN soldiers dead, Thiệu announced on 4 January 1974, that the war had restarted and the Peace Accords were no longer in effect. There were over 25,000 South Vietnamese casualties during the ceasefire period.[227][25]: 683Gerald Fordtook over as US president in August 1974, and Congress cut financial aid to South Vietnam from $1 billion a year to $700 million. Congress voted in restrictions on funding to be phased in through 1975 and then total cutoff in 1976.[25]: 686
The success of the 1973–1974 dry season offensive inspired Trà to return to Hanoi in October 1974 and plead for a larger offensive the next dry season. This time, Trà could travel on a drivable highway with fueling stops, a vast change from when the Ho Chi Minh Trail was a dangerous mountain trek.[105]: 676Giáp, the North Vietnamese defense minister, was reluctant to approve Trà's plan since a larger offensive might provoke US reaction and interfere with the big push planned for 1976. Trà appealed to Giáp's superior, Lê Duẩn, who approved it. Trà's plan called for a limited offensive from Cambodia intoPhước Long Province. The strike was designed to solve logistical problems, gauge the reaction of South Vietnamese forces, and determine whether the US would return.[25]: 685–690On 13 December 1974, PAVN forcesattacked Phước Long. Phuoc Binh fell on 6 January 1975. Ford desperately asked Congress for funds to assist and re-supply the South before it was overrun.[228]Congress refused.[228]The fall of Phuoc Binh and lack of American response left the South Vietnamese elite demoralized.
The speed of this success led the Politburo to reassess its strategy. It decided operations in the Central Highlands would be turned over to General Văn Tiến Dũng and thatPleikushould be seized, if possible. Dũng said to Lê Duẩn: "Never have we had military and political conditions so perfect or a strategic advantage as great as we have now."[229]At the start of 1975, the South Vietnamese had three times as much artillery and twice as many tanks and armored vehicles as the PAVN. However, heightened oil prices meant many assets could not be leveraged. Moreover, the rushed nature of Vietnamization, intended to cover the US retreat, resulted in a lack of spare parts, ground-crew, and maintenance personnel, which rendered most of it inoperable.[204]: 362–366
On 10 March 1975, Dũng launched Campaign 275, a limited offensive into the Central Highlands, supported by tanks and heavy artillery. The target wasBan Ma Thuột; if the town could be taken, the provincial capital Pleiku and the road to the coast, would be exposed for a campaign in 1976. The ARVN proved incapable of resisting the onslaught, and its forces collapsed. Again, Hanoi was surprised by the speed of their success. Dung urged the Politburo to allow him to seize Pleiku immediately and turn his attention toKon Tum. He argued that with two months of good weather until onset of the monsoon, it would be irresponsible not to take advantage.[8]
Thiệu, a former general, ordered the abandonment of the Central Highlands and less defensible positions in a rushed policy described as "light at the top, heavy at the bottom". While the bulk of ARVN forces attempted to flee, isolated units fought desperately. ARVN general Phu abandoned Pleiku and Kon Tum and retreated toward the coast, in what became known as the "convoy of tears".[25]: 693–694On 20 March, Thiệu reversed himself and ordered Huế, Vietnam's third-largest city, be held at all costs, and then changed policy several times. As the PAVN launched their attack, panic set in, and ARVN resistance withered. On 22 March, the PAVNattacked Huế. Civilians flooded the airport and docks hoping for escape. As resistance in Huế collapsed, PAVN rockets rained down on Da Nang and its airport. By 28 March 35,000 PAVN troops were poised to attack the suburbs. By 30 March 100,000 leaderless ARVN troops surrendered as the PAVN marched through Da Nang. With the fall of the city, the defense of the Central Highlands and Northern provinces ended.[25]: 699–700
With the north half of the country under their control, the Politburo ordered Dũng to launch the final offensive against Saigon. The operational plan for theHo Chi Minh Campaigncalled for Saigon's capture before 1 May. Hanoi wished to avoid the coming monsoon and prevent redeployment of ARVN forces defending the capital. PAVN forces, their morale boosted by their recent victories, rolled on, takingNha Trang,Cam RanhandDa Lat.[25]: 702–704
On 7 April, three PAVN divisions attackedXuân Lộc, 40 miles (64 km) northeast of Saigon. For two weeks, fighting raged as the ARVN defenders made alast standto try to block PAVN advance. On 21 April, however, the exhausted garrison was ordered to withdraw towards Saigon.[25]: 704–707An embittered and tearful Thiệu resigned, declaring that the US had betrayed South Vietnam. In a scathing attack, he suggested Kissinger had tricked him into signing the Paris peace agreement, promising military aid that failed to materialize. Having transferred power toTrần Văn Hươngon 21 April, he left forTaiwan.[25]: 714After having appealed unsuccessfully to Congress for $722 million in emergency aid for South Vietnam, Ford gave a televised speech on 23 April, declaring an end to the War and US aid.[230][231]
By the end of April, the ARVN had collapsed except in theMekong Delta. Refugees streamed southward, ahead of the main PAVN onslaught. By 27 April, 100,000 PAVN troops encircled Saigon. The city was defended by about 30,000 ARVN troops. To hasten a collapse and foment panic, the PAVN shelledTan Son Nhut Airportand forced its closure. With the runways closed, large numbers of civilians had no way out.[25]: 716
Chaos and panic broke out as South Vietnamese officials and civilians scrambled to leave.Martial lawwas declared. American helicopters began evacuating South Vietnamese, US and foreign nationals from Tan Son Nhut and the U.S. embassy compound.Operation Frequent Windhad been delayed until the last possible moment, because of AmbassadorGraham Martin's belief Saigon could be held and a political settlement reached. Frequent Wind was the largest helicopter evacuation in history. It began on 29 April, in an atmosphere of desperation, as hysterical crowds of Vietnamese vied for limited space. Frequent Wind continued around the clock, as PAVN tanks breached defenses near Saigon. In the early morning of 30 April, the last US Marines evacuated the embassy by helicopter, as civilians swamped the perimeter and poured into the grounds.[25]: 718–720
On 30 April 1975, PAVN troops entered Saigon and overcame all resistance, capturing key buildings and installations.[232]Tanks from the2nd Corpscrashed through the gates of theIndependence Palaceand the VC flag was raised above it.[233]President Dương Văn Minh, who had succeeded Huong two days earlier, surrendered to Lieutenant colonel Bùi Văn Tùng, political commissar of the 203rd Tank Brigade.[234][235][236]: 95–96Minh was then escorted toRadio Saigonto announce the surrender declaration.[237]: 85The statement was on air at 2:30 pm.[236]
During the war a large segment of Americans became opposed to U.S. involvement. In January 1967, only 32% of Americans thought the US had made a mistake in sending troops.[238]Public opinion steadily turned against the war following 1967 and by 1970 only a third believed the U.S. had not made a mistake by sending troops.[239][240]
Early opposition to US involvement drew its inspiration from the Geneva Conference of 1954. American support of Diệm in refusing elections was seen as thwarting the democracy America claimed to support. Kennedy, while senator, opposed involvement.[158]Many young people protested because they were beingdrafted, others because the anti-war movement grew popular among thecounterculture. Some advocates within the peace movement advocated aunilateralwithdrawal. Opposition to the war tended to unite groups opposed to U.S. anti-communism andimperialism,[241]and for those involved with theNew Left. Others, such asStephen Spiro, opposed the war based on the theory ofJust War. Some wanted to show solidarity with the Vietnamese, such asNorman MorrisonemulatingThích Quảng Đức.
High-profile opposition increasingly turned to mass protests to shift public opinion. Riots broke out at the1968 Democratic National Convention.[25]: 514After reports of American military abuses, such as the My Lai massacre, brought attention and support to the anti-war movement, some veterans joinedVietnam Veterans Against the War. In October 1969, theVietnam Moratoriumattracted millions of Americans.[242]The fatal shooting of four students at Kent State University in 1970 led to nationwide university protests.[243]Anti-war protests declined after the Paris Peace Accords and theend of the draftin 1973, and the withdrawal of troops.
China provided significant support for North Vietnam when the US started to intervene, including financial aid and the deployment of hundreds of thousands of military personnel in support roles. China said its military and economic aid to North Vietnam totaled $20 billion ($160 billion adjusted for 2022 prices) during the Vietnam War;[7]included were 5 million tons of food to North Vietnam (equivalent to a year's food production), accounting for 10–15% of their food supply by the 1970s.[7]
In the summer of 1962,Mao Zedongagreed to supply Hanoi with 90,000 rifles and guns free of charge, and starting in 1965, China began sendinganti-aircraftunits and engineering battalions, to repair the damage caused by American bombing. They helped man anti-aircraft batteries, rebuild roads and railroads, transport supplies, and perform other engineering works. This freed PAVN units for combat. China sent 320,000 troops and annual arms shipments worth $180 million.[244]: 135China claims to have caused 38% of American air losses in the war.[7]China also began financing the Khmer Rouge as a counterweight to North Vietnam. China "armed and trained" the Khmer Rouge during the civil war, and continued to aid them afterward.[245]
The Soviet Union supplied North Vietnam with medical supplies, arms, tanks, planes, helicopters, artillery, anti-aircraft missiles and other military equipment. Soviet crews fired Soviet-madesurface-to-air missilesat US aircraft in 1965.[246]Following thedissolution of the Soviet Unionin 1991,Russianofficials acknowledged that the USSR had stationed up to 3,000 troops in Vietnam.[247]16 Soviet military personnel were killed in action during the war according to official Soviet military sources.[248]
According to Russian sources, between 1953 and 1991, the hardware donated by the Soviet Union included: 2,000 tanks; 1,700APCs; 7,000 artillery guns; over 5,000 anti-aircraft guns; 158 surface-to-air missile launchers; and 120 helicopters. In total, the Soviets sent North Vietnam annual arms shipments worth $450 million.[249][25]: 364–371From July 1965 to the end of 1974, fighting in Vietnam was observed by some 6,500 officers and generals, as well as more than 4,500 soldiers and sergeants of theSoviet Armed Forces, amounting to 11,000 military personnel.[250]TheKGBhelped develop thesignals intelligencecapabilities of the North Vietnamese.[251]
As South Vietnam was formally part of a military alliance with the US, Australia, New Zealand, France, the UK, Pakistan, Thailand and the Philippines, the alliance was invoked during the war. The UK, France and Pakistan declined to participate, and South Korea, Taiwan, and Spain were non-treaty participants.
The ethnic minority peoples of South Vietnam, like theMontagnardsin the Central Highlands, the Hindu and MuslimCham, and the BuddhistKhmer Krom, were actively recruited in the war. There was a strategy of recruitment and favorable treatment of Montagnard tribes for the VC, as they were pivotal for control of infiltration routes.[252]Some groups split off and formed theUnited Front for the Liberation of Oppressed Races(FULRO) to fight for autonomy or independence. FULRO fought against the South Vietnamese and VC, later fighting against the unifiedSocialist Republic of Vietnam, after the fall of South Vietnam.
During the war, South Vietnamese president Diem began a program to settle ethnic Vietnamese Kinh on Montagnard lands in the Central Highlands region. This provoked a backlash from the Montagnards, some joining the VC as a result. The Cambodians under pro-China Sihanouk and pro-American Lon Nol, supported their fellow co-ethnic Khmer Krom in South Vietnam, following an anti-ethnic Vietnamese policy. Following Vietnamization, many Montagnard groups and fighters were incorporated into theSouth Vietnamese Rangersas border sentries.
War crimestook place, by both sides, including: rape, massacres of civilians, bombings of civilian targets,terrorism, torture, and murder ofprisoners of war. Common crimes included theft, arson, and the destruction of property not warranted bymilitary necessity.[253]
In 1966, theRussell Tribunalwas organized by public figures opposed to the war led byBertrand Russellin an effort to apply the precepts ofinternational law. The tribunal found the US and its allies guilty ofacts of aggression, use of weapons forbidden by the laws of war, bombardment of targets of a purely civilian character, mistreatment of prisoners, andgenocide. Though the tribunal's lack of juridical authority meant its findings were largely ignored by the US and other governments, the hearings contributed to a growing body of evidence which established the factual basis for a counter-narrative to the United States' justifications for the war and inspired hearings, tribunals and legal investigations.[254]
In 1968, theVietnam War Crimes Working Group(VWCWG) was established bythe Pentagontask forceset up in the wake of the My Lai massacre, to ascertain the veracity of emerging claims ofUS war crimes. Of the crimes reported to military authorities, sworn statements by witnesses and status reports indicated 320 incidents had a factual basis.[255]The substantiated cases included seven massacres between 1967 and 1971 in which at least 137 civilians were killed; 78 further attacks targeting non-combatants resulting in at least 57 deaths and 15 sexually assaulted; and 141 cases of US soldiers torturing civilian detainees, or prisoners of war with fists, sticks, bats, water or electric shock. Journalists have documented overlooked and uninvestigated war crimes, involving every active army division,[255]including atrocities committed byTiger Force.[256]R. J. Rummelestimated that American forces committed around 5,500democidalkillings between 1960-72.[30]
US forces establishedfree-fire zonesto prevent VC fighters from sheltering in South Vietnamese villages.[257]Such practice, which involved the assumption that anyone appearing in the designated zones was an enemy that could be freely targeted by weapons, was regarded by journalist Lewis Simons as "a severe violation of the laws of war".[258]Nick Turseargues that a relentless drive toward higherbody counts, widespread use of free-fire zones, rules of engagement where civilians who ran from soldiers or helicopters could be viewed as VC and disdain for Vietnamese civilians, led to massive civilian casualties and war crimes.[259]: 251One example cited by Turse isOperation Speedy Express, which was described byJohn Paul Vannas, in effect, "many Mỹ Lais".[259]: 251A report byNewsweeksuggested at least 5,000 civilians may have been killed during the operation, and an official US military body count of 10,889 enemy combatants killed.[260]
Rummel estimated 39,000 were killed by South Vietnam during the Diem-era in democide; for 1964–75, Rummel estimated 50,000 people were killed in democide. Thus, the total for 1954 to 1975 is about 80,000 deaths caused by South Vietnam.[30]Benjamin Valentinoestimates 110,000–310,000 deaths as a "possible case" of "counter-guerrilla mass killings" by US and South Vietnamese forces.[261]ThePhoenix Program, coordinated by the CIA and involving US and South Vietnamese security forces, was aimed at destroying the political infrastructure of the VC. The program killed 26,000 to 41,000 people, with an unknown number being innocent civilians.[171]: 341–343[262][263][264]
Torture and ill-treatment were frequently applied by the South Vietnamese to POWs, as well as civilian prisoners.[265]: 77During their visit to theCon Son Prisonin 1970, US congressmenAugustus HawkinsandWilliam R. Andersonwitnessed detainees either confined in minute "tiger cages" or chained to their cells, and provided with poor-quality food. American doctors inspecting the prison found inmates suffering symptoms resulting from forced immobility and torture.[265]: 77During their visits to US detention facilities, theInternational Red Crossrecorded many cases of torture and inhumane treatment.[265]: 78Torture was conducted by the South Vietnamese government in collusion with the CIA.[266][267]Unlike massacres such as My Lai, media reports of the torture of POWs by South Vietnamese and US forces did not generate significant public outcry in the United States.[268]
South Korean forces were accused of war crimes. One documented event was thePhong Nhị and Phong Nhất massacrewhere the2nd Marine Brigadereportedly killed between 69 and 79 civilians in February 1968 in Phong Nhị and Phong Nhất villages,Điện Bàn District.[269]South Korean forces are accused of perpetrating theBình Hòa massacre,Binh Tai MassacreandHà My massacre.
Ami Pedahzur has written that "the overall volume and lethality of Viet Cong terrorism rivals or exceeds all but a handful of terrorist campaigns waged over the last third of the twentieth century", based on the definition of terrorists as a non-state actor, and examining targeted killings and civilian deaths which are estimated at over 18,000 from 1966 to 1969.[270]The US Department of Defense estimates the VC/PAVN conducted 36,000 murders and 58,000 kidnappings from 1967 to 1972,c.1973.[271]Benjamin Valentino attributes 45,000–80,000 "terrorist mass killings" to the VC.[261]Statistics for 1968–1972 suggest "about 80 percent of the terrorist victims were ordinary civilians and only about 20 percent were government officials, policemen, members of the self-defence forces or pacification cadres."[19]: 273VC tactics included frequent mortaring of civilians in refugee camps, and placing of mines on highways frequented by villagers taking goods to urban markets. Some mines were set only to go off after heavy vehicle passage, causing slaughter aboard packed buses.[19]: 270–279
Notable VC atrocities include the massacre of over 3,000 unarmed civilians at Huế[272]during the Tet Offensive and killing of 252 civilians during theĐắk Sơn massacre.[273]155,000 refugees fleeing the North Vietnamese Spring Offensive were reported to have been killed, or abducted, on the road toTuy Hòain 1975.[274]PAVN/VC troops killed 164,000 civilians in democide between 1954-75 in South Vietnam.[30]North Vietnam was known for its abusive treatment of American POWs, most notably inHỏa Lò Prison(theHanoi Hilton), where torture was employed toextract confessions.[105]: 655
Women were active in a large variety of roles, making significant impacts and the war having significant impacts on them.[275][276][277]Several million Vietnamese women served in the military and in militias, particularly in the VC, with the slogan "when war comes, even the women must fight" being widely used.[278]These women made vital contributions on the Ho Chi Minh Trail, espionage, medical care, logistical and administrative work, and sometimes direct combat.[279][280]Women workers took on more roles in the economy and Vietnam saw an increase in women's rights.[281]In Vietnam and elsewhere, women emerged as leaders of anti-war peace campaigns and made significant contributions towar journalism.[282]
However, women still faced significant levels of discrimination during and were often targets ofsexual violenceandwar crimes.[283]Post-war, some Vietnamese women veterans faced difficulty reintegrating into society and having their contributions recognised, as well as advances in women's rights failing to be sustained.[284][285]Portrayals of the war have been criticised for their depictions of women, both for overlooking the role women played and reducing Vietnamese women to racist stereotypes.[286][287]Women are at the forefront of campaigns to deal with the war's aftermath, such as the long-terms effect ofAgent Orangeuse and theLai Đại Hàn.[288][289][290]
The experience of African-American military personnel has received significant attention. The site "African-American Involvement in the Vietnam War" compiles examples,[291]as does the work of journalistWallace Terrywhose bookBloods: An Oral History of the Vietnam War by Black Veterans, includes observations about the impact on the black community and black servicemen. He notes: the higher proportion of combat casualties among African-American servicemen than other races, the shift toward and different attitudes of black military volunteers and conscripts, the discrimination encountered by black servicemen "on the battlefield in decorations, promotion and duty assignments", as well as having to endure "the racial insults, cross-burnings and Confederate flags of their white comrades"—and the experiences faced by black soldiers stateside, during the war and after withdrawal.[292]
Civil rights leaders protested the disproportionate casualties and overrepresentation in hazardous duty, experienced by African American servicemen, prompting reforms that were implemented beginning in 1967. As a result, by the war's completion in 1975, black casualties had declined to 13% of US combat deaths, approximately equal to percentage of draft-eligible black men, though still slightly higher than the 10% who served in the military.[293]
Nearly all US-allied forces were armed with US weapons including theM1 Garand,M1 carbine,M14 rifle, andM16 rifle. The Australian and New Zealand forces employed the 7.62 mmL1A1 Self-Loading Rifle, with occasional use of the M16 rifle.
The PAVN/VC, although having inherited US, French, and Japanese weapons from World War II and theFirst Indochina War, were largely armed and supplied by China, the Soviet Union, and itsWarsaw Pactallies. Some weapons—notably anti-personnel explosives, theK-50M, and "home-made" versions of theRPG-2—were manufactured in North Vietnam. By 1969 the US Army had identified 40 rifle/carbine types, 22 machine gun types, 17 types of mortar, 20 recoilless rifle or rocket launcher types, nine types of antitank weapons, and 14 anti-aircraft artillery weapons used by ground troops on all sides. Also in use, mostly by anti-communist forces, were 24 types of armored vehicles and self-propelled artillery, and 26 types of field artillery and rocket launchers.
The US dropped over 7 million tons of bombs on Indochina during the war, more than triple the 2.1 million tons it dropped on Europe and Asia during World War II, and more than ten times the amount during the Korean War. 500 thousand tons were dropped on Cambodia, 1 million tons on North Vietnam, and 4 million tons on South Vietnam. On a per person basis, the 2 million tons dropped on Laos make it the most heavily bombed country in history;The New York Timesnoted this was "nearly a ton for every person in Laos."[156]Due to the particularly heavy impact of cluster bombs, Laos was a strong advocate of theConvention on Cluster Munitionsto ban the weapons, and was host to its first meeting in 2010.[294]
Former US Air Force official Earl Tilford recounted "repeated bombing runs of a lake in central Cambodia. The B-52s literally dropped their payloads in the lake." The Air Force ran many missions like this to secure additional funding during budget negotiations, so the tonnage expended does not directly correlate with the resulting damage.[295]
Casualty estimates vary, with one source suggesting up to 3.8 million violent war deaths in Vietnam for 1955 to 2002.[297][298][299][5]A demographic study calculated 791,000–1,141,000 war-related deaths for all of Vietnam, for military and civilians.[18]Between 195,000 and 430,000 South Vietnamese civilians died.[19]: 450–453[29]Guenter Lewy estimated 65,000 North Vietnamese civilians died.[19]: 450–453Estimates of civilian deaths caused by American bombing of North Vietnam range from 30,000[8]: 176, 617to 182,000.[20]A 1975 US Senate subcommittee estimated 1.4 million South Vietnamese civilians casualties during the war, including 415,000 deaths.[259]: 12The military of South Vietnam suffered an estimated 254,256 killed between 1960-74, and additional deaths from 1954 to 1959 and in 1975.[31]: 275Other estimates point to higher figures of 313,000 casualties.[98][44][18][45][46][47]
The US Department of Defense figure for PAVN/VC killed in Vietnam from 1965 to 1974 was 950,765. Officials believed these body count figures need to be deflated by 30 percent. Lewy asserts that one-third of the reported "enemy" killed may have been civilians, concluding that the figure was closer to 444,000.[19]: 450–453
According to figures released by the Vietnamese government there were 849,018 confirmed military deaths on the PAVN/VC side.[22][23]The Vietnamese government released its estimate of war deaths for the more lengthy period of 1955 to 1975. This includes battle deaths of Vietnamese soldiers in the Laotian and Cambodian Civil Wars, in which the PAVN was a participant. Non-combat deaths account for 30-40% of these.[22]However, the figures do not include deaths of South Vietnamese and allied soldiers.[43]These do not include the estimated 300,000–500,000 PAVN/VC missing in action. Vietnamese government figures estimate 1.1 million dead and 300,000 missing from 1945 to 1979, with approximately 849,000 dead and 232,000 missing from 1960 to 1975.[21]
US reports of "enemy KIA", referred to as body count, were thought to have been subject to "falsification and glorification", and a true estimate of PAVN/VC combat deaths is difficult to assess, as US victories were assessed by having a "greater kill ratio".[300][301]It was difficult to distinguish between civilians and military personnel in the VC, as many were part-time guerrillas or impressed laborers who did not wear uniforms[302][303]and civilians killed were sometimes written off as enemy killed, because high enemy casualties was directly tied to promotions and commendation.[199]: 649–650[304][305]
Between 275,000[46]and 310,000[47]Cambodians died, including 50,000–150,000 combatants and civilians from US bombings.[306]20,000–62,000 Laotians died,[44]and 58,281 U.S. military personnel were killed,[33]of which 1,584 are still listed missing as of 2021[update].[307]
In July 1976, North and South Vietnam were merged to form the Socialist Republic of Vietnam.[308]Despite speculation that the victorious North Vietnamese would, in Nixon's words, "massacre the civilians there [South Vietnam] by the millions," no mass executions took place.[309][A 12]
However many South Vietnamese were sent tore-education campswhere they endured torture, starvation, and disease while being forced to perform hard labor.[312][313]According to Amnesty International, this figure varied depending on different observers: "..."50,000 to 80,000" (Le Monde, 1978), "150,000 to 200,000" (The Washington Post, 1978), and "300,000" (Agence France Presse from Hanoi, 1978)."[314]Such variations are because "Some estimates may include not only detainees but also people sent from the cities to the countryside." According to a native observer, 443,360 people had to register for a period in re-education camps in Saigon alone, and while some were released after a few days, others stayed for more than a decade.[315]Between 1975-80, more than 1 million northerners migrated south, to regions formerly in the Republic of Vietnam, while, as part of theNew Economic Zones program, around 750,000 to over 1 million southerners were moved mostly to mountainous forested areas.[316][317]Gabriel García Márquezdescribed South Vietnam as a "False paradise" when he visited in 1980:
The cost of this delirium was stupefying: 360,000 people mutilated, a million widows, 500,000 prostitutes, 500,000 drug addicts, a million tuberculous and more than a million soldiers of the old regime, impossible to rehabilitate into a new society. Ten percent of the population of Ho Chi Minh City was suffering from serious venereal diseases when the war ended, and there were 4 million illiterates throughout the South.[318]
The US used itssecurity council vetoto block Vietnam's UN recognition three times, an obstacle to it receiving aid.[319]
By 1975, the North Vietnamese had lost influence over the Khmer Rouge.[25]: 708Phnom Penh, Cambodia's capital, fell to the Khmer Rouge in April. UnderPol Pot, the Khmer Rouge wouldkill 1–3 million Cambodiansfrom a population of 8 million, in one of thebloodiest genocides ever.[45][320][321][322]
The relationship between Vietnam andDemocratic Kampuchea(Cambodia) escalated after the war. In response to the Khmer Rouge taking overPhu QuocandTho Chu, and the belief they were responsible for the disappearance of 500 Vietnamese natives on Tho Chu, Vietnam launched a counterattack to take back the islands.[323]After failed attempts to negotiate, Vietnam invaded Democratic Kampuchea in 1978 and ousted the Khmer Rouge, in the Cambodian–Vietnamese War. In response, China invaded Vietnam in 1979. The two countries fought a border war: theSino-Vietnamese War. From 1978 to 1979, some 450,000 ethnicChineseleft Vietnam by boat as refugees or were deported.
The Pathet Lao overthrew the monarchy of Laos in 1975, establishing theLao People's Democratic Republic. The change in regime was "quite peaceful, a sort of Asiatic 'velvet revolution'"—although 30,000 former officials were sent to reeducation camps, often enduring harsh conditions.[108]: 575–576
Unexploded ordnance, mostly from US bombing, continues to kill people, and has rendered much land hazardous and impossible to cultivate. Ordnance has killed 42,000 people since the war.[324][325]In Laos, 80 million bombs failed to explode and still remain. Unexploded ordnance has killed or injured over 20,000 Laotians and about 50 people are killed or maimed annually.[326][327]It is estimated the explosives will not be removed entirely for centuries.[176]: 317
Over 3 million people left Vietnam, Laos, and Cambodia in theIndochina refugee crisisafter 1975. Most Asian countries were unwilling to accept them, many led by boat and were known asboat people.[328]Between 1975-98, an estimated 1.2 millionrefugeesfrom Vietnam and other Southeast Asian countries resettled in the US, while Canada, Australia, and France resettled over 500,000, China accepted 250,000.[329]Laos experienced the largest refugee flight proportionally, 300,000 out of a population of 3 million crossed the border into Thailand. Included among them were "about 90%" of Laos' "intellectuals, technicians, and officials."[108]: 575An estimated 200,000 to 400,000 boat people died at sea, according to theUnited Nations High Commissioner for Refugees.[330]
Failure of US goals is often placed at different institutions and levels. Some have suggested it was due to failure of leadership.[331]Others point to military doctrine. Secretary of Defense Robert McNamara stated that "the achievement of a military victory by U.S. forces in Vietnam was indeed a dangerous illusion."[93]: 368The inability to bring Hanoi to the bargaining table by bombing illustrated another US miscalculation, and the limitations of military abilities in achieving political goals.[105]: 17Army Chief of StaffHarold Keith Johnsonnoted, "if anything came out of Vietnam, it was that air power couldn't do the job."[332]General William Westmoreland admitted bombing had been ineffective, saying he doubted "that the North Vietnamese would have relented."[332]Kissinger wrote to President Ford that "in terms of military tactics ... our armed forces are not suited to this kind of war. Even the Special Forces who had been designed for it could not prevail."[333]Hanoi had persistently sought unification, and the effects of US bombing had negligible impact on North Vietnam's goals.[176]: 1–10US bombing mobilized people throughout North Vietnam and internationally, due to a superpower attempting to bomb a small society into submission.[176]: 48–52
Americans struggled to absorb the lessons of the military intervention. PresidentRonald Reagancoined the term "Vietnam Syndrome" to describe the reluctance of the public and politicians to support military interventions abroad. US polling in 1978 revealed nearly 72% of Americans believed the war was "fundamentally wrong and immoral."[240]: 10Six months after the beginning of Operation Rolling Thunder,Gallup, Inc.found 60% of Americans did not believe sending troops was a mistake in September 1965, and only 24% believed it was. Subsequent polling did not find a plurality believed sending troops was a mistake until October 1967, and did not find a majority believing it was until August 1968, during the third phase of the Tet Offensive. Thereafter, Gallup found majorities believing it was a mistake through the signing of the Peace Accords in January 1973, when 60% believed it was a mistake, and retrospective polls by Gallup between 1990 and 2000, found 69-74% of Americans believed it was a mistake.[334]TheVietnam War POW/MIA issue, concerning the fate of US service personnel listed asmissing in action, persisted. The costs loom large in American consciousness; a 1990 poll showed the public incorrectly believed more Americans died in Vietnam than World War II.[335]
Between 1953-75, the US was estimated to have spent $168 billion on the war (equivalent to $1.7 trillion in 2024).[337]This resulted in a largebudget deficit. Other figures point to $139 billion from 1965 to 1974 (not inflation-adjusted), 10 times education spending, and 50 times more than housing and community development.[338]It was stated that war-spending could have paid every mortgage in the US, with money leftover.[338]As of 2013[update], the US government pays Vietnam veterans and their families more than $22 billion annually in war-related claims.[339][340]
More than 3 million Americans served, 1.5 million saw combat.[341]"At the height of American involvement in 1968, for example, 543,000 American military personnel were stationed in Vietnam, but only 80,000 were considered combat troops."[342]Conscription in the US existed since World War II, but ended in 1973.[343][344]
58,220 American soldiers were killed,[A 7]more than 150,000 wounded, and at least 21,000 permanently disabled.[345]The average age of troops killed was 23.[346]According to Dale Kueter, "Of those killed in combat, 86% were white, 13% were black..."[347]Approximately 830,000 veterans, 15%, sufferedposttraumatic stress disorder.[345]This unprecedented number was because the military had provided heavy psychoactive drugs to servicemen, which left them unable to process trauma.[348]Drug use, racial tensions, and the growing incidence of fragging—attempting to kill unpopular officers—created problems for the military and impacted its capability.[349]: 44–47125,000 Americans left for Canada to avoid the draft,[350]and approximately 50,000 servicemen deserted.[351]In 1977, PresidentJimmy Cartergranted an unconditional pardon to all Vietnam-eradraft evaderswithProclamation 4483.[352]
The war called into question army doctrine. Marine generalVictor H. Krulakcriticized Westmoreland's attrition strategy, calling it "wasteful of American lives... with small likelihood of a successful outcome."[332]Doubts surfaced about military's ability to train foreign forces. There was found to be considerable flaws and dishonesty by commanders, due to promotions being tied to the body count system touted by Westmoreland and McNamara.[167]Secretary of Defense McNamara wrote to President Johnson: "The picture of the world's greatest superpower killing or seriously injuring 1,000 noncombatants a week, while trying to pound a tiny backward nation into submission on an issue whose merits are hotly disputed, is not a pretty one."[353]
One of the most controversial aspects of the US military effort, was widespread use of chemicaldefoliantsbetween 1961-71. 20 million gallons of toxic herbicides (likeAgent Orange) were sprayed on 6 million acres of forests and crops.[59]They were used todefoliateparts of the countryside to prevent the Viet Cong from being able to hide weaponry and encampments under the foliage, and deprive them of food. Defoliation was used to clear sensitive areas, including base perimeters and ambush sites along roads and canals. More than 20% of South Vietnam's forests and 3% of its cultivated land was sprayed. 90% was directed at forest defoliation.[19]: 263The chemicals used continue to change the landscape, cause diseases and birth defects, and poison the food chain.[354][355]US records have listed figures including the destruction of 20% of the jungles of South Vietnam and 20-36% of themangroveforests.[356]The environmental destruction caused was described by Swedish Prime MinisterOlof Palme, lawyers, and academics as anecocide.[357][358][56][359][57][360]
Agent Orange and similar substances used by the US have caused many deaths and injuries, including among the crews that handled them. Scientific reports have concluded that refugees exposed to sprays continued to experience pain in the eyes, skin and gastrointestinal upsets. In one study, 92% of participants suffered incessant fatigue; others reportedmonstrous births.[361]Analysis of studies on the association between Agent Orange and birth defects, have found a significant correlation such that having a parent who was exposed to Agent Orange, will increase one's likelihood of possessing or acting as a carrier of birth defects.[362]The most common deformity appears to bespina bifida. There is substantial evidence defects carry on for three generations or more.[363]In 2012, the US and Vietnam began a cooperative cleaning toxic chemicals onDanang International Airport, marking the first time Washington has been involved in cleaning up Agent Orange in Vietnam.[364]
Vietnamese victims affected by Agent Orange attempted a class action lawsuit againstDow Chemicaland other US chemical manufacturers, but aUS District Courtdismissed their case.[365]They appealed, but the dismissal was cemented in 2008 by anappeals court.[366]As of 2006[update], the Vietnamese government estimated there were over 4,000,000 victims ofdioxinpoisoning in Vietnam, although the US government denies any conclusive scientific links between Agent Orange and Vietnamese victims of dioxin poisoning. In some areas of southern Vietnam, dioxin levels remain at over 100 times the accepted international standard.[367]
The U.S. Veterans Administration has listedprostate cancer,respiratory cancers,multiple myeloma,type 2 diabetes,B-cell lymphomas,soft-tissue sarcoma,chloracne,porphyria cutanea tarda,peripheral neuropathyas, "presumptive diseases associated with exposure to Agent Orange or other herbicides during military service."[368]Spina bifida is the sole birth defect in children of veterans recognized as being caused by exposure to Agent Orange.[369]
The war has featured extensively in television, film, video games, music and literature. In Vietnam, a film set during Operation Linebacker II wasGirl from Hanoi(1974) depicting war-time life. Another notable work was the diary of Đặng Thùy Trâm, a North Vietnamese doctor who enlisted in the Southern battlefield, and was killed aged 27 by US forces. Her diaries were published in Vietnam asĐặng Thùy Trâm's Diary(Last Night I Dreamed of Peace), where it became a bestseller and was made into a filmDon't Burn. In Vietnam, the diary has been compared toThe Diary of Anne Frank, and both are used in literary education.[370]
One of the first major films based on the war wasJohn Wayne's pro-warThe Green Berets(1968). Further cinematic representations were released during the 1970s and 80s, the most noteworthy examples beingMichael Cimino'sThe Deer Hunter(1978),Francis Ford Coppola'sApocalypse Now(1979),Oliver Stone'sPlatoon(1986) andStanley Kubrick'sFull Metal Jacket(1987). Other films includeGood Morning, Vietnam(1987),Casualties of War(1989),Born on the Fourth of July(1989).[8]
The war influenced a generation of musicians and songwriters, both pro/anti-war and pro/anti-communist, with theVietnam War Song Projecthaving identified 5,000+ songs referencing the conflict.[371]The bandCountry Joe and the FishrecordedThe "Fish" Cheer/I-Feel-Like-I'm-Fixin'-to-Die Ragin 1965, and it became one of the most influential protest anthems.[8]
Myths play a role in thehistoriographyof the war, and have become part of theculture of the United States. Discussion of myth has focused on US experiences, but changing myths of war have played a role in Vietnamese and Australian historiography. Scholarship has focused on "myth-busting",[372]: 373attacking orthodox and revisionist schools of American historiography, and challenging myths about American society and soldiery in the war.[372]: 373
Kuzmarov inThe Myth of the Addicted Army: Vietnam and the Modern War on Drugschallenges the popular and Hollywood narrative that US soldiers were heavy drug users,[373]in particular the notion that the My Lai massacre was caused by drug use.[372]: 373According to Kuzmarov, Nixon is primarily responsible for creating the drug myth.[372]: 374Michael Allen accuses Nixon of mythmaking, by exploiting the plight of theNational League of POW/MIA Familiesto allow the government to appear caring, as the war was increasingly considered lost.[372]: 376Allen's analysis ties the position of potential missing Americans, or prisoners into post-war politics and presidential elections, including theSwift boatcontroversy.[372]: 376–377
The references for this article are grouped in three sections.
Algebrais a branch ofmathematicsthat deals with abstractsystems, known asalgebraic structures, and the manipulation ofexpressionswithin those systems. It is a generalization ofarithmeticthat introducesvariablesandalgebraic operationsother than the standard arithmetic operations, such asadditionandmultiplication.
Elementary algebrais the main form of algebra taught in schools. It examines mathematical statements using variables for unspecified values and seeks to determine for which values the statements are true. To do so, it uses different methods of transforming equations to isolate variables.Linear algebrais a closely related field that investigateslinear equationsand combinations of them calledsystems of linear equations. It provides methods to find the values that solve all equations in the system at the same time, and to study the set of these solutions.
Abstract algebrastudies algebraic structures, which consist of asetofmathematical objectstogether with one or severaloperationsdefined on that set. It is a generalization of elementary and linear algebra since it allows mathematical objects other than numbers and non-arithmetic operations. It distinguishes between different types of algebraic structures, such asgroups,rings, andfields, based on the number of operations they use and the laws they follow, calledaxioms.Universal algebraandcategory theoryprovide general frameworks to investigate abstract patterns that characterize different classes of algebraic structures.
Algebraic methods were first studied in theancient periodto solve specific problems in fields likegeometry. Subsequent mathematicians examined general techniques to solve equations independent of their specific applications. They described equations and their solutions using words and abbreviations until the 16th and 17th centuries when a rigorous symbolic formalism was developed. In the mid-19th century, the scope of algebra broadened beyond atheory of equationsto cover diverse types of algebraic operations and structures. Algebra is relevant to many branches of mathematics, such as geometry,topology,number theory, andcalculus, and other fields of inquiry, likelogicand theempirical sciences.
Algebra is the branch of mathematics that studiesalgebraic structuresand theoperationsthey use.[1]An algebraic structure is a non-emptysetofmathematical objects, such as theintegers, together with algebraic operations defined on that set, likeadditionandmultiplication.[2][a]Algebra explores the laws, general characteristics, and types of algebraic structures. Within certain algebraic structures, it examines the use ofvariablesinequationsand how to manipulate these equations.[4][b]
Algebra is often understood as a generalization ofarithmetic.[8]Arithmetic studies operations like addition,subtraction, multiplication, anddivision, in a particular domain of numbers, such as the real numbers.[9]Elementary algebraconstitutes the first level of abstraction. Like arithmetic, it restricts itself to specific types of numbers and operations. It generalizes these operations by allowing indefinite quantities in the form of variables in addition to numbers.[10]A higher level of abstraction is found inabstract algebra, which is not limited to a particular domain and examines algebraic structures such asgroupsandrings. It extends beyond typical arithmetic operations by also covering other types of operations.[11]Universal algebra is still more abstract in that it is not interested in specific algebraic structures but investigates the characteristics of algebraic structures in general.[12]
The term "algebra" is sometimes used in a more narrow sense to refer only to elementary algebra or only to abstract algebra.[14]When used as acountable noun, an algebra isa specific type of algebraic structurethat involves avector spaceequipped witha certain type of binary operation.[15]Depending on the context, "algebra" can also refer to other algebraic structures, like aLie algebraor anassociative algebra.[16]
The wordalgebracomes from theArabictermالجبر(al-jabr), which originally referred to the surgical treatment ofbonesetting. In the 9th century, the term received a mathematical meaning when the Persian mathematicianMuhammad ibn Musa al-Khwarizmiemployed it to describe a method of solving equations and used it in the title of a treatise on algebra,al-Kitāb al-Mukhtaṣar fī Ḥisāb al-Jabr wal-Muqābalah[The Compendious Book on Calculation by Completion and Balancing] which was translated into Latin asLiber Algebrae et Almucabola.[c]The word entered the English language in the 16th century fromItalian,Spanish, and medievalLatin.[18]Initially, its meaning was restricted to thetheory of equations, that is, to the art of manipulatingpolynomial equationsin view of solving them. This changed in the 19th century[d]when the scope of algebra broadened to cover the study of diverse types of algebraic operations and structures together with their underlyingaxioms, the laws they follow.[21]
Elementary algebra, also called school algebra, college algebra, and classical algebra,[22]is the oldest and most basic form of algebra. It is a generalization ofarithmeticthat relies onvariablesand examines how mathematicalstatementsmay be transformed.[23]
Arithmetic is the study of numerical operations and investigates how numbers are combined and transformed using the arithmetic operations ofaddition,subtraction,multiplication,division,exponentiation, extraction ofroots, andlogarithm. For example, the operation of addition combines two numbers, called the addends, into a third number, called the sum, as in2+5=7{\displaystyle 2+5=7}.[9]
Elementary algebra relies on the same operations while allowing variables in addition to regular numbers. Variables aresymbolsfor unspecified or unknown quantities. They make it possible to state relationships for which one does not know the exact values and to express general laws that are true, independent of which numbers are used. For example, theequation2×3=3×2{\displaystyle 2\times 3=3\times 2}belongs to arithmetic and expresses an equality only for these specific numbers. By replacing the numbers with variables, it is possible to express a general law that applies to any possible combination of numbers, like thecommutative property of multiplication, which is expressed in the equationa×b=b×a{\displaystyle a\times b=b\times a}.[23]
Algebraic expressionsare formed by using arithmetic operations to combine variables and numbers. By convention, the lowercase lettersx{\displaystyle x},y{\displaystyle y}, andz{\displaystyle z}represent variables. In some cases, subscripts are added to distinguish variables, as inx1{\displaystyle x_{1}},x2{\displaystyle x_{2}}, andx3{\displaystyle x_{3}}. The lowercase lettersa{\displaystyle a},b{\displaystyle b}, andc{\displaystyle c}are usually used forconstantsandcoefficients.[e]The expression5x+3{\displaystyle 5x+3}is an algebraic expression created by multiplying the number 5 with the variablex{\displaystyle x}and adding the number 3 to the result. Other examples of algebraic expressions are32xyz{\displaystyle 32xyz}and64x12+7x2−c{\displaystyle 64x_{1}^{2}+7x_{2}-c}.[25]
Some algebraic expressions take the form of statements that relate two expressions to one another. An equation is a statement formed by comparing two expressions, saying that they are equal. This can be expressed using theequals sign(={\displaystyle =}), as in5x2+6x=3y+4{\displaystyle 5x^{2}+6x=3y+4}.Inequationsinvolve a different type of comparison, saying that the two sides are different. This can be expressed using symbols such as theless-than sign(<{\displaystyle <}), thegreater-than sign(>{\displaystyle >}), and the inequality sign (≠{\displaystyle \neq }). Unlike other expressions, statements can be true or false, and theirtruth valueusually depends on the values of the variables. For example, the statementx2=4{\displaystyle x^{2}=4}is true ifx{\displaystyle x}is either 2 or −2 and false otherwise.[26]Equations with variables can be divided into identity equations and conditional equations. Identity equations are true for all values that can be assigned to the variables, such as the equation2x+5x=7x{\displaystyle 2x+5x=7x}.Conditional equations are only true for some values. For example, the equationx+4=9{\displaystyle x+4=9}is only true ifx{\displaystyle x}is 5.[27]
The main goal of elementary algebra is to determine the values for which a statement is true. This can be achieved by transforming and manipulating statements according to certain rules. A key principle guiding this process is that whatever operation is applied to one side of an equation also needs to be done to the other side. For example, if one subtracts 5 from the left side of an equation one also needs to subtract 5 from the right side to balance both sides. The goal of these steps is usually to isolate the variable one is interested in on one side, a process known assolving the equationfor that variable. For example, the equationx−7=4{\displaystyle x-7=4}can be solved forx{\displaystyle x}by adding 7 to both sides, which isolatesx{\displaystyle x}on the left side and results in the equationx=11{\displaystyle x=11}.[28]
There are many other techniques used to solve equations. Simplification is employed to replace a complicated expression with an equivalent simpler one. For example, the expression7x−3x{\displaystyle 7x-3x}can be replaced with the expression4x{\displaystyle 4x}since7x−3x=(7−3)x=4x{\displaystyle 7x-3x=(7-3)x=4x}by the distributive property.[29]For statements with several variables,substitutionis a common technique to replace one variable with an equivalent expression that does not use this variable. For example, if one knows thaty=3x{\displaystyle y=3x}then one can simplify the expression7xy{\displaystyle 7xy}to arrive at21x2{\displaystyle 21x^{2}}.In a similar way, if one knows the value of one variable one may be able to use it to determine the value of other variables.[30]
Algebraic equations can be interpretedgeometricallyto describe spatial figures in the form of agraph. To do so, the different variables in the equation are understood ascoordinatesand the values that solve the equation are interpreted as points of a graph. For example, ifx{\displaystyle x}is set to zero in the equationy=0.5x−1{\displaystyle y=0.5x-1}, theny{\displaystyle y}must be −1 for the equation to be true. This means that the(x,y){\displaystyle (x,y)}-pair(0,−1){\displaystyle (0,-1)}is part of the graph of the equation. The(x,y){\displaystyle (x,y)}-pair(0,7){\displaystyle (0,7)},by contrast, does not solve the equation and is therefore not part of the graph. The graph encompasses the totality of(x,y){\displaystyle (x,y)}-pairs that solve the equation.[31]
A polynomial is an expression consisting of one or more terms that are added or subtracted from each other, likex4+3xy2+5x3−1{\displaystyle x^{4}+3xy^{2}+5x^{3}-1}.Each term is either a constant, a variable, or a product of a constant and variables. Each variable can be raised to a positive integer power. A monomial is a polynomial with one term while two- and three-term polynomials are called binomials and trinomials. Thedegree of a polynomialis the maximal value (among its terms) of the sum of the exponents of the variables (4 in the above example).[32]Polynomials of degree one are calledlinear polynomials. Linear algebra studies systems of linear polynomials.[33]A polynomial is said to beunivariateormultivariate, depending on whether it uses one or more variables.[34]
Factorizationis a method used to simplify polynomials, making it easier to analyze them and determine the values for which theyevaluate to zero. Factorization consists of rewriting a polynomial as a product of several factors. For example, the polynomialx2−3x−10{\displaystyle x^{2}-3x-10}can be factorized as(x+2)(x−5){\displaystyle (x+2)(x-5)}.The polynomial as a whole is zero if and only if one of its factors is zero, i.e., ifx{\displaystyle x}is either −2 or 5.[35]Before the 19th century, much of algebra was devoted topolynomial equations, that isequationsobtained by equating a polynomial to zero. The first attempts for solving polynomial equations were to express the solutions in terms ofnth roots. The solution of a second-degree polynomial equation of the formax2+bx+c=0{\displaystyle ax^{2}+bx+c=0}is given by thequadratic formula[36]
x=−b±b2−4ac2a.{\displaystyle x={\frac {-b\pm {\sqrt {b^{2}-4ac\ }}}{2a}}.}
Solutions for the degrees 3 and 4 are given by thecubicandquarticformulas. There are no general solutions for higher degrees, as proven in the 19th century by theAbel–Ruffini theorem.[37]Even when general solutions do not exist, approximate solutions can be found by numerical tools like theNewton–Raphson method.[38]
Thefundamental theorem of algebraasserts that every univariate polynomial equation of positive degree withrealorcomplexcoefficients has at least one complex solution. Consequently, every polynomial of a positive degree can befactorizedinto linear polynomials. This theorem was proved at the beginning of the 19th century, but this does not close the problem since the theorem does not provide any way for computing the solutions.[39]
Linear algebra starts with the study ofsystems of linear equations.[40]Anequation is linearif it can be expressed in the forma1x1+a2x2+...+anxn=b{\displaystyle a_{1}x_{1}+a_{2}x_{2}+...+a_{n}x_{n}=b}wherea1{\displaystyle a_{1}},a2{\displaystyle a_{2}}, ...,an{\displaystyle a_{n}}andb{\displaystyle b}are constants. Examples arex1−7x2+3x3=0{\displaystyle x_{1}-7x_{2}+3x_{3}=0}and14x−y=4{\textstyle {\frac {1}{4}}x-y=4}. Asystem of linear equationsis a set of linear equations for which one is interested in common solutions.[41]
Matricesare rectangular arrays of values that have been originally introduced for having a compact and synthetic notation for systems of linear equations.[42]For example, the system of equations
9x1+3x2−13x3=02.3x1+7x3=9−5x1−17x2=−3{\displaystyle {\begin{aligned}9x_{1}+3x_{2}-13x_{3}&=0\\2.3x_{1}+7x_{3}&=9\\-5x_{1}-17x_{2}&=-3\end{aligned}}}
whereA,X{\displaystyle A,X}andB{\displaystyle B}are the matrices
A=[93−132.307−5−170],X=[x1x2x3],B=[09−3].{\displaystyle A={\begin{bmatrix}9&3&-13\\2.3&0&7\\-5&-17&0\end{bmatrix}},\quad X={\begin{bmatrix}x_{1}\\x_{2}\\x_{3}\end{bmatrix}},\quad B={\begin{bmatrix}0\\9\\-3\end{bmatrix}}.}
Under some conditions on the number of rows and columns, matrices can beadded,multiplied, and sometimesinverted. All methods for solving linear systems may be expressed as matrix manipulations using these operations. For example, solving the above system consists of computing an inverted matrixA−1{\displaystyle A^{-1}}such thatA−1A=I,{\displaystyle A^{-1}A=I,}whereI{\displaystyle I}is theidentity matrix. Then, multiplying on the left both members of the above matrix equation byA−1,{\displaystyle A^{-1},}one gets the solution of the system of linear equations as[43]
Methods of solving systems of linear equations range from the introductory, like substitution[44]and elimination,[45]to more advanced techniques using matrices, such asCramer's rule, theGaussian elimination, andLU decomposition.[46]Some systems of equations areinconsistent, meaning that no solutions exist because the equations contradict each other.[47][f]Consistent systems have either one unique solution or an infinite number of solutions.[48][g]
The study ofvector spacesandlinear mapsform a large part of linear algebra. A vector space is an algebraic structure formed by a set with an addition that makes it anabelian groupand ascalar multiplicationthat is compatible with addition (seevector spacefor details). A linear map is a function between vector spaces that is compatible with addition and scalar multiplication. In the case offinite-dimensional vector spaces, vectors and linear maps can be represented by matrices. It follows that the theories of matrices and finite-dimensional vector spaces are essentially the same. In particular, vector spaces provide a third way for expressing and manipulating systems of linear equations.[49]From this perspective, a matrix is a representation of a linear map: if one chooses a particularbasisto describe the vectors being transformed, then the entries in the matrix give the results of applying the linear map to the basis vectors.[50]
Systems of equations can be interpreted as geometric figures. For systems with two variables, each equation represents alineintwo-dimensional space. The point where the two lines intersect is the solution of the full system because this is the only point that solves both the first and the second equation. For inconsistent systems, the two lines run parallel, meaning that there is no solution since they never intersect. If two equations are not independent then they describe the same line, meaning that every solution of one equation is also a solution of the other equation. These relations make it possible to seek solutions graphically by plotting the equations and determining where they intersect.[51]The same principles also apply to systems of equations with more variables, with the difference being that the equations do not describe lines but higher dimensional figures. For instance, equations with three variables correspond toplanesinthree-dimensional space, and the points where all planes intersect solve the system of equations.[52]
Abstract algebra, also called modern algebra,[53]is the study ofalgebraic structures. An algebraic structure is a framework for understandingoperationsonmathematical objects, like the addition of numbers. While elementary algebra and linear algebra work within the confines of particular algebraic structures, abstract algebra takes a more general approach that compares how algebraic structures differ from each other and what types of algebraic structures there are, such asgroups,rings, andfields.[54]The key difference between these types of algebraic structures lies in the number of operations they use and the laws they obey.[55]Inmathematics education, abstract algebra refers to an advancedundergraduatecourse that mathematics majors take after completing courses in linear algebra.[56]
On a formal level, an algebraic structure is aset[h]of mathematical objects, called the underlying set, together with one or several operations.[i]Abstract algebra is primarily interested inbinary operations,[j]which take any two objects from the underlying set as inputs and map them to another object from this set as output.[60]For example, the algebraic structure⟨N,+⟩{\displaystyle \langle \mathbb {N} ,+\rangle }has thenatural numbers(N{\displaystyle \mathbb {N} }) as the underlying set and addition (+{\displaystyle +}) as its binary operation.[58]The underlying set can contain mathematical objects other than numbers, and the operations are not restricted to regular arithmetic operations.[61]For instance, the underlying set of thesymmetry groupof a geometric object is made up ofgeometric transformations, such asrotations, under which the object remainsunchanged. Its binary operation isfunction composition, which takes two transformations as input and has the transformation resulting from applying the first transformation followed by the second as its output.[62]
Abstract algebra classifies algebraic structures based on the laws oraxiomsthat its operations obey and the number of operations it uses. One of the most basic types is a group, which has one operation and requires that this operation isassociativeand has anidentity elementandinverse elements. An operation is associative if the order of several applications does not matter, i.e., if(a∘b)∘c{\displaystyle (a\circ b)\circ c}[k]is the same asa∘(b∘c){\displaystyle a\circ (b\circ c)}for all elements. An operation has an identity element or a neutral element if one elementeexists that does not change the value of any other element, i.e., ifa∘e=e∘a=a{\displaystyle a\circ e=e\circ a=a}.An operation has inverse elements if for any elementa{\displaystyle a}there exists a reciprocal elementa−1{\displaystyle a^{-1}}that undoesa{\displaystyle a}. If an element operates on its inverse then the result is the neutral elemente, expressed formally asa∘a−1=a−1∘a=e{\displaystyle a\circ a^{-1}=a^{-1}\circ a=e}.Every algebraic structure that fulfills these requirements is a group.[64]For example,⟨Z,+⟩{\displaystyle \langle \mathbb {Z} ,+\rangle }is a group formed by the set ofintegerstogether with the operation of addition. The neutral element is 0 and the inverse element of any numbera{\displaystyle a}is−a{\displaystyle -a}.[65]The natural numbers with addition, by contrast, do not form a group since they contain only positive integers and therefore lack inverse elements.[66]
Group theoryexamines the nature of groups, with basic theorems such as thefundamental theorem of finite abelian groupsand theFeit–Thompson theorem.[67]The latter was a key early step in one of the most important mathematical achievements of the 20th century: the collaborative effort, taking up more than 10,000 journal pages and mostly published between 1960 and 2004, that culminated in a completeclassification of finite simple groups.[68]
A ring is an algebraic structure with two operations that work similarly to the addition and multiplication of numbers and are named and generally denoted similarly. A ring is acommutative groupunder addition: the addition of the ring is associative, commutative, and has an identity element and inverse elements. The multiplication is associative anddistributivewith respect to addition; that is,a(b+c)=ab+ac{\displaystyle a(b+c)=ab+ac}and(b+c)a=ba+ca.{\displaystyle (b+c)a=ba+ca.}Moreover, multiplication is associative and has anidentity elementgenerally denoted as1.[69][l]Multiplication needs not to be commutative; if it is commutative, one has acommutative ring.[71]Thering of integers(Z{\displaystyle \mathbb {Z} }) is one of the simplest commutative rings.[72]
Afieldis a commutative ring such that⁠1≠0{\displaystyle 1\neq 0}⁠and each nonzero element has amultiplicative inverse.[73]The ring of integers does not form a field because it lacks multiplicative inverses. For example, the multiplicative inverse of7{\displaystyle 7}is17{\displaystyle {\tfrac {1}{7}}},which is not an integer. Therational numbers, thereal numbers, and thecomplex numberseach form a field with the operations of addition and multiplication.[74]
Ring theoryis the study of rings, exploring concepts such assubrings,quotient rings,polynomial rings, andidealsas well as theorems such asHilbert's basis theorem.[75]Field theory is concerned with fields, examiningfield extensions,algebraic closures, andfinite fields.[76]Galois theoryexplores the relation between field theory and group theory, relying on thefundamental theorem of Galois theory.[77]
Besides groups, rings, and fields, there are many other algebraic structures studied by algebra. They includemagmas,semigroups,monoids,abelian groups,commutative rings,modules,lattices,vector spaces,algebras over a field, andassociativeandnon-associative algebras. They differ from each other regarding the types of objects they describe and the requirements that their operations fulfill. Many are related to each other in that a basic structure can be turned into a more advanced structure by adding additional requirements.[55]For example, a magma becomes a semigroup if its operation is associative.[78]
Homomorphismsare tools to examine structural features by comparing two algebraic structures.[79]A homomorphism is a function from the underlying set of one algebraic structure to the underlying set of another algebraic structure that preserves certain structural characteristics. If the two algebraic structures use binary operations and have the form⟨A,∘⟩{\displaystyle \langle A,\circ \rangle }and⟨B,⋆⟩{\displaystyle \langle B,\star \rangle }then the functionh:A→B{\displaystyle h:A\to B}is a homomorphism if it fulfills the following requirement:h(x∘y)=h(x)⋆h(y){\displaystyle h(x\circ y)=h(x)\star h(y)}.The existence of a homomorphism reveals that the operation⋆{\displaystyle \star }in the second algebraic structure plays the same role as the operation∘{\displaystyle \circ }does in the first algebraic structure.[80]Isomorphismsare a special type of homomorphism that indicates a high degree of similarity between two algebraic structures. An isomorphism is abijectivehomomorphism, meaning that it establishes a one-to-one relationship between the elements of the two algebraic structures. This implies that every element of the first algebraic structure is mapped to one unique element in the second structure without any unmapped elements in the second structure.[81]
Another tool of comparison is the relation between an algebraic structure and itssubalgebra.[82]The algebraic structure and its subalgebra use the same operations,[m]which follow the same axioms. The only difference is that the underlying set of the subalgebra is a subset of the underlying set of the algebraic structure.[n]All operations in the subalgebra are required to beclosedin its underlying set, meaning that they only produce elements that belong to this set.[82]For example, the set ofeven integerstogether with addition is a subalgebra of the full set of integers together with addition. This is the case because the sum of two even numbers is again an even number. But the set of odd integers together with addition is not a subalgebra because it is not closed: adding two odd numbers produces an even number, which is not part of the chosen subset.[83]
Universal algebrais the study of algebraic structures in general. As part of its general perspective, it is not concerned with the specific elements that make up the underlying sets and considers operations with more than two inputs, such asternary operations. It provides a framework for investigating what structural features different algebraic structures have in common.[85][o]One of those structural features concerns theidentitiesthat are true in different algebraic structures. In this context, an identity is auniversalequation or an equation that is true for all elements of the underlying set. For example, commutativity is a universal equation that states thata∘b{\displaystyle a\circ b}is identical tob∘a{\displaystyle b\circ a}for all elements.[87]Avarietyis a class of all algebraic structures that satisfy certain identities. For example, if two algebraic structures satisfy commutativity then they are both part of the corresponding variety.[88][p][q]
Category theoryexamines how mathematical objects are related to each other using the concept ofcategories. A category is a collection of objects together with a collection ofmorphismsor "arrows" between those objects. These two collections must satisfy certain conditions. For example, morphisms can be joined, orcomposed: if there exists a morphism from objecta{\displaystyle a}to objectb{\displaystyle b}, and another morphism from objectb{\displaystyle b}to objectc{\displaystyle c}, then there must also exist one from objecta{\displaystyle a}to objectc{\displaystyle c}. Composition of morphisms is required to be associative, and there must be an "identity morphism" for every object.[92]Categories are widely used in contemporary mathematics since they provide a unifying framework to describe and analyze many fundamental mathematical concepts. For example, sets can be described with thecategory of sets, and any group can be regarded as the morphisms of a category with just one object.[93]
The origin of algebra lies in attempts to solve mathematical problems involving arithmetic calculations and unknown quantities. These developments happened in the ancient period inBabylonia,Egypt,Greece,China, andIndia. One of the earliest documents on algebraic problems is theRhind Mathematical Papyrusfrom ancient Egypt, which was written around 1650 BCE.[r]It discusses solutions tolinear equations, as expressed in problems like "A quantity; its fourth is added to it. It becomes fifteen. What is the quantity?" Babylonian clay tablets from around the same time explain methods to solve linear andquadratic polynomial equations, such as the method ofcompleting the square.[95]
Many of these insights found their way to the ancient Greeks. Starting in the 6th century BCE, their main interest was geometry rather than algebra, but they employed algebraic methods to solve geometric problems. For example, they studied geometric figures while taking their lengths and areas as unknown quantities to be determined, as exemplified inPythagoras' formulation of thedifference of two squaresmethod and later inEuclid'sElements.[96]In the 3rd century CE,Diophantusprovided a detailed treatment of how to solve algebraic equations in a series of books calledArithmetica. He was the first to experiment with symbolic notation to express polynomials.[97]Diophantus's work influenced Arab development of algebra with many of his methods reflected in the concepts and techniques used in medieval Arabic algebra.[98]In ancient China,The Nine Chapters on the Mathematical Art, a book composed over the period spanning from the 10th century BCE to the 2nd century CE,[99]explored various techniques for solving algebraic equations, including the use of matrix-like constructs.[100]
There is no unanimity of opinion as to whether these early developments are part of algebra or only precursors. They offered solutions to algebraic problems but did not conceive them in an abstract and general manner, focusing instead on specific cases and applications.[101]This changed with the Persian mathematicianal-Khwarizmi,[s]who published hisThe Compendious Book on Calculation by Completion and Balancingin 825 CE. It presents the first detailed treatment of general methods that can be used to manipulate linear and quadratic equations by "reducing" and "balancing" both sides.[103]Other influential contributions to algebra came from the Arab mathematicianThābit ibn Qurraalso in the 9th century and the Persian mathematicianOmar Khayyamin the 11th and 12th centuries.[104]
In India,Brahmaguptainvestigated how to solve quadratic equations and systems of equations with several variables in the 7th century CE. Among his innovations were the use of zero and negative numbers in algebraic equations.[105]The Indian mathematiciansMahāvīrain the 9th century andBhāskara IIin the 12th century further refined Brahmagupta's methods and concepts.[106]In 1247, the Chinese mathematicianQin Jiushaowrote theMathematical Treatise in Nine Sections, which includesan algorithmfor thenumerical evaluation of polynomials, including polynomials of higher degrees.[107]
The Italian mathematicianFibonaccibrought al-Khwarizmi's ideas and techniques to Europe in books including hisLiber Abaci.[108]In 1545, the Italian polymathGerolamo Cardanopublished his bookArs Magna, which covered many topics in algebra, discussedimaginary numbers, and was the first to present general methods for solvingcubicandquartic equations.[109]In the 16th and 17th centuries, the French mathematiciansFrançois VièteandRené Descartesintroduced letters and symbols to denote variables and operations, making it possible to express equations in an concise and abstract manner. Their predecessors had relied on verbal descriptions of problems and solutions.[110]Some historians see this development as a key turning point in the history of algebra and consider what came before it as the prehistory of algebra because it lacked the abstract nature based on symbolic manipulation.[111]
In the 17th and 18th centuries, many attempts were made to find general solutions to polynomials of degree five and higher. All of them failed.[37]At the end of the 18th century, the German mathematicianCarl Friedrich Gaussproved thefundamental theorem of algebra, which describes the existence ofzerosof polynomials of any degree without providing a general solution.[19]At the beginning of the 19th century, the Italian mathematicianPaolo Ruffiniand the Norwegian mathematicianNiels Henrik Abelwereable to showthat no general solution exists for polynomials of degree five and higher.[37]In response to and shortly after their findings, the French mathematicianÉvariste Galoisdeveloped what came later to be known asGalois theory, which offered a more in-depth analysis of the solutions of polynomials while also laying the foundation ofgroup theory.[20]Mathematicians soon realized the relevance of group theory to other fields and applied it to disciplines like geometry and number theory.[112]
Starting in the mid-19th century, interest in algebra shifted from the study of polynomials associated with elementary algebra towards a more general inquiry into algebraic structures, marking the emergence ofabstract algebra. This approach explored the axiomatic basis of arbitrary algebraic operations.[113]The invention of new algebraic systems based on different operations and elements accompanied this development, such asBoolean algebra,vector algebra, andmatrix algebra.[114]Influential early developments in abstract algebra were made by the German mathematiciansDavid Hilbert,Ernst Steinitz, andEmmy Noetheras well as the Austrian mathematicianEmil Artin. They researched different forms of algebraic structures and categorized them based on their underlying axioms into types, like groups, rings, and fields.[115]
The idea of the even more general approach associated with universal algebra was conceived by the English mathematicianAlfred North Whiteheadin his 1898 bookA Treatise on Universal Algebra. Starting in the 1930s, the American mathematicianGarrett Birkhoffexpanded these ideas and developed many of the foundational concepts of this field.[116]The invention of universal algebra led to the emergence of various new areas focused on the algebraization of mathematics—that is, the application of algebraic methods to other branches of mathematics. Topological algebra arose in the early 20th century, studying algebraic structures such astopological groupsandLie groups.[117]In the 1940s and 50s,homological algebraemerged, employing algebraic techniques to studyhomology.[118]Around the same time,category theorywas developed and has since played a key role in thefoundations of mathematics.[119]Other developments were the formulation ofmodel theoryand the study offree algebras.[120]
The influence of algebra is wide-reaching, both within mathematics and in its applications to other fields.[121]The algebraization of mathematics is the process of applying algebraic methods and principles to otherbranches of mathematics, such asgeometry,topology,number theory, andcalculus. It happens by employing symbols in the form of variables to express mathematical insights on a more general level, allowing mathematicians to develop formal models describing how objects interact and relate to each other.[122]
One application, found in geometry, is the use of algebraic statements to describe geometric figures. For example, the equationy=3x−7{\displaystyle y=3x-7}describes a line in two-dimensional space while the equationx2+y2+z2=1{\displaystyle x^{2}+y^{2}+z^{2}=1}corresponds to aspherein three-dimensional space. Of special interest toalgebraic geometryarealgebraic varieties,[t]which are solutions tosystems of polynomial equationsthat can be used to describe more complex geometric figures.[124]Algebraic reasoning can also solve geometric problems. For example, one can determine whether and where the line described byy=x+1{\displaystyle y=x+1}intersects with the circle described byx2+y2=25{\displaystyle x^{2}+y^{2}=25}by solving the system of equations made up of these two equations.[125]Topology studies the properties of geometric figures ortopological spacesthat are preserved under operations ofcontinuous deformation.Algebraic topologyrelies on algebraic theories such asgroup theoryto classify topological spaces. For example,homotopy groupsclassify topological spaces based on the existence ofloopsorholesin them.[126]
Number theory is concerned with the properties of and relations between integers.Algebraic number theoryapplies algebraic methods and principles to this field of inquiry. Examples are the use of algebraic expressions to describe general laws, likeFermat's Last Theorem, and of algebraic structures to analyze the behavior of numbers, such as thering of integers.[127]The related field ofcombinatoricsuses algebraic techniques to solve problems related to counting, arrangement, and combination of discrete objects. An example inalgebraic combinatoricsis the application of group theory to analyzegraphsand symmetries.[128]The insights of algebra are also relevant to calculus, which uses mathematical expressions to examinerates of changeandaccumulation. It relies on algebra, for instance, to understand how these expressions can be transformed and what role variables play in them.[129]Algebraic logicemploys the methods of algebra to describe and analyze the structures and patterns that underlielogical reasoning,[130]exploring both the relevant mathematical structures themselves and their application to concrete problems of logic.[131]It includes the study ofBoolean algebrato describepropositional logic[132]as well as the formulation and analysis of algebraic structures corresponding to more complexsystems of logic.[133]
Algebraic methods are also commonly employed in other areas, like the natural sciences. For example, they are used to expressscientific lawsand solve equations inphysics,chemistry, andbiology.[135]Similar applications are found in fields likeeconomics,geography,engineering(includingelectronicsandrobotics), andcomputer scienceto express relationships, solve problems, and model systems.[136]Linear algebra plays a central role inartificial intelligenceandmachine learning, for instance, by enabling the efficient processing and analysis of largedatasets.[137]Various fields rely on algebraic structures investigated by abstract algebra. For example, physical sciences likecrystallographyandquantum mechanicsmake extensive use of group theory,[138]which is also employed to study puzzles such asSudokuandRubik's cubes,[139]andorigami.[140]Bothcoding theoryandcryptologyrely on abstract algebra to solve problems associated withdata transmission, like avoiding the effects ofnoiseand ensuringdata security.[141]
Algebra education mostly focuses on elementary algebra, which is one of the reasons why elementary algebra is also called school algebra. It is usually not introduced untilsecondary educationsince it requires mastery of the fundamentals of arithmetic while posing new cognitive challenges associated with abstract reasoning and generalization.[143]It aims to familiarize students with the formal side of mathematics by helping them understand mathematical symbolism, for example, how variables can be used to represent unknown quantities. An additional difficulty for students lies in the fact that, unlike arithmetic calculations, algebraic expressions are often difficult to solve directly. Instead, students need to learn how to transform them according to certain laws, often to determine an unknown quantity.[144]
Some tools to introduce students to the abstract side of algebra rely on concrete models and visualizations of equations, including geometric analogies, manipulatives including sticks or cups, and "function machines" representing equations asflow diagrams. One method usesbalance scalesas a pictorial approach to help students grasp basic problems of algebra. The mass of some objects on the scale is unknown and represents variables. Solving an equation corresponds to adding and removing objects on both sides in such a way that the sides stay in balance until the only object remaining on one side is the object of unknown mass.[145]Word problemsare another tool to show how algebra is applied to real-life situations. For example, students may be presented with a situation in which Naomi's brother has twice as many apples as Naomi. Given that both together have twelve apples, students are then asked to find an algebraic equation that describes this situation (2x+x=12{\displaystyle 2x+x=12}) and to determine how many apples Naomi has(x=4{\displaystyle x=4}).[146]
At the university level, mathematics students encounter advanced algebra topics from linear and abstract algebra. Initialundergraduatecourses in linear algebra focus on matrices, vector spaces, and linear maps. Upon completing them, students are usually introduced to abstract algebra, where they learn about algebraic structures like groups, rings, and fields, as well as the relations between them. The curriculum typically also covers specific instances of algebraic structures, such as the systems of rational numbers, the real numbers, and the polynomials.[147]
Geometry(fromAncient Greekγεωμετρία(geōmetría)'land measurement'; fromγῆ(gê)'earth, land'andμέτρον(métron)'a measure')[1]is a branch ofmathematicsconcerned with properties of space such as the distance, shape, size, and relative position of figures.[2]Geometry is, along witharithmetic, one of the oldest branches of mathematics. A mathematician who works in the field of geometry is called ageometer. Until the 19th century, geometry was almost exclusively devoted toEuclidean geometry,[a]which includes the notions ofpoint,line,plane,distance,angle,surface, andcurve, as fundamental concepts.[3]
Originally developed to model the physical world, geometry has applications in almost all sciences, and also in art,architecture, and other activities that are related to graphics.[4]Geometry also has applications in areas of mathematics that are apparently unrelated. For example, methods of algebraic geometry are fundamental inWiles's proofofFermat's Last Theorem, a problem that was stated in terms ofelementary arithmetic, and remained unsolved for several centuries.
During the 19th century several discoveries enlarged dramatically the scope of geometry. One of the oldest such discoveries isCarl Friedrich Gauss'sTheorema Egregium("remarkable theorem") that asserts roughly that theGaussian curvatureof a surface is independent from any specificembeddingin aEuclidean space. This implies that surfaces can be studiedintrinsically, that is, as stand-alone spaces, and has been expanded into the theory ofmanifoldsandRiemannian geometry. Later in the 19th century, it appeared that geometries without theparallel postulate(non-Euclidean geometries) can be developed without introducing any contradiction. The geometry that underliesgeneral relativityis a famous application of non-Euclidean geometry.
Since the late 19th century, the scope of geometry has been greatly expanded, and the field has been split in many subfields that depend on the underlying methods—differential geometry,algebraic geometry,computational geometry,algebraic topology,discrete geometry(also known ascombinatorial geometry), etc.—or on the properties of Euclidean spaces that are disregarded—projective geometrythat consider only alignment of points but not distance and parallelism,affine geometrythat omits the concept of angle and distance,finite geometrythat omitscontinuity, and others. This enlargement of the scope of geometry led to a change of meaning of the word "space", which originally referred to the three-dimensionalspaceof the physical world and itsmodelprovided by Euclidean geometry; presently ageometric space, or simply aspaceis amathematical structureon which some geometry is defined.
The earliest recorded beginnings of geometry can be traced to ancientMesopotamiaandEgyptin the 2nd millennium BC.[5][6]Early geometry was a collection of empirically discovered principles concerning lengths, angles, areas, and volumes, which were developed to meet some practical need insurveying,construction,astronomy, and various crafts. The earliest known texts on geometry are theEgyptianRhind Papyrus(2000–1800 BC) andMoscow Papyrus(c.1890 BC), and theBabylonian clay tablets, such asPlimpton 322(1900 BC). For example, the Moscow Papyrus gives a formula for calculating the volume of a truncated pyramid, orfrustum.[7]Later clay tablets (350–50 BC) demonstrate that Babylonian astronomers implementedtrapezoidprocedures for computing Jupiter's position andmotionwithin time-velocity space. These geometric procedures anticipated theOxford Calculators, including themean speed theorem, by 14 centuries.[8]South of Egypt theancient Nubiansestablished a system of geometry including early versions of sun clocks.[9][10]
In the 7th century BC, theGreekmathematicianThales of Miletusused geometry to solve problems such as calculating the height of pyramids and the distance of ships from the shore. He is credited with the first use of deductive reasoning applied to geometry, by deriving four corollaries toThales's theorem.[11]Pythagorasestablished thePythagorean School, which is credited with the first proof of thePythagorean theorem,[12]though the statement of the theorem has a long history.[13][14]Eudoxus(408–c.355 BC) developed themethod of exhaustion, which allowed the calculation of areas and volumes of curvilinear figures,[15]as well as a theory of ratios that avoided the problem ofincommensurable magnitudes, which enabled subsequent geometers to make significant advances. Around 300 BC, geometry was revolutionized by Euclid, whoseElements, widely considered the most successful and influential textbook of all time,[16]introducedmathematical rigorthrough theaxiomatic methodand is the earliest example of the format still used in mathematics today, that of definition, axiom, theorem, and proof. Although most of the contents of theElementswere already known, Euclid arranged them into a single, coherent logical framework.[17]TheElementswas known to all educated people in the West until the middle of the 20th century and its contents are still taught in geometry classes today.[18]Archimedes(c.287–212 BC) ofSyracuse, Italyused the method of exhaustion to calculate theareaunder the arc of aparabolawith thesummation of an infinite series, and gave remarkably accurate approximations ofpi.[19]He also studied thespiralbearing his name and obtained formulas for thevolumesofsurfaces of revolution.
Indianmathematicians also made many important contributions in geometry. TheShatapatha Brahmana(3rd century BC) contains rules for ritual geometric constructions that are similar to theSulba Sutras.[20]According to (Hayashi 2005, p. 363), theŚulba Sūtrascontain "the earliest extant verbal expression of the Pythagorean Theorem in the world, although it had already been known to the Old Babylonians. They contain lists ofPythagorean triples,[b]which are particular cases ofDiophantine equations.[21]In theBakhshali manuscript, there are a handful of geometric problems (including problems about volumes of irregular solids). The Bakhshali manuscript also "employs a decimal place value system with a dot for zero."[22]Aryabhata'sAryabhatiya(499) includes the computation of areas and volumes.Brahmaguptawrote his astronomical workBrāhmasphuṭasiddhāntain 628. Chapter 12, containing 66Sanskritverses, was divided into two sections: "basic operations" (including cube roots, fractions, ratio and proportion, and barter) and "practical mathematics" (including mixture, mathematical series, plane figures, stacking bricks, sawing of timber, and piling of grain).[23]In the latter section, he stated his famous theorem on the diagonals of acyclic quadrilateral. Chapter 12 also included a formula for the area of a cyclic quadrilateral (a generalization ofHeron's formula), as well as a complete description ofrational triangles(i.e.triangles with rational sides and rational areas).[23]
In theMiddle Ages,mathematics in medieval Islamcontributed to the development of geometry, especiallyalgebraic geometry.[24][25]Al-Mahani(b. 853) conceived the idea of reducing geometrical problems such as duplicating the cube to problems in algebra.[26]Thābit ibn Qurra(known as Thebit inLatin) (836–901) dealt witharithmeticoperations applied toratiosof geometrical quantities, and contributed to the development ofanalytic geometry.[27]Omar Khayyam(1048–1131) found geometric solutions tocubic equations.[28]The theorems ofIbn al-Haytham(Alhazen), Omar Khayyam andNasir al-Din al-Tusionquadrilaterals, including theLambert quadrilateralandSaccheri quadrilateral, were part of a line of research on theparallel postulatecontinued by later European geometers, includingVitello(c.1230– c.1314),Gersonides(1288–1344), Alfonso,John Wallis, andGiovanni Girolamo Saccheri, that by the 19th century led to the discovery ofhyperbolic geometry.[29]
In the early 17th century, there were two important developments in geometry. The first was the creation of analytic geometry, or geometry withcoordinatesandequations, byRené Descartes(1596–1650) andPierre de Fermat(1601–1665).[30]This was a necessary precursor to the development ofcalculusand a precise quantitative science ofphysics.[31]The second geometric development of this period was the systematic study ofprojective geometrybyGirard Desargues(1591–1661).[32]Projective geometry studies properties of shapes which are unchanged underprojectionsandsections, especially as they relate toartistic perspective.[33]
Two developments in geometry in the 19th century changed the way it had been studied previously.[34]These were the discovery ofnon-Euclidean geometriesby Nikolai Ivanovich Lobachevsky, János Bolyai and Carl Friedrich Gauss and of the formulation ofsymmetryas the central consideration in theErlangen programmeofFelix Klein(which generalized the Euclidean and non-Euclidean geometries). Two of the master geometers of the time wereBernhard Riemann(1826–1866), working primarily with tools frommathematical analysis, and introducing theRiemann surface, andHenri Poincaré, the founder ofalgebraic topologyand the geometric theory ofdynamical systems. As a consequence of these major changes in the conception of geometry, the concept of "space" became something rich and varied, and the natural background for theories as different ascomplex analysisandclassical mechanics.[35]
The following are some of the most important concepts in geometry.[3][36]
Euclidtook an abstract approach to geometry in hisElements,[37]one of the most influential books ever written.[38]Euclid introduced certainaxioms, orpostulates, expressing primary or self-evident properties of points, lines, and planes.[39]He proceeded to rigorously deduce other properties by mathematical reasoning. The characteristic feature of Euclid's approach to geometry was its rigor, and it has come to be known asaxiomaticorsyntheticgeometry.[40]At the start of the 19th century, the discovery ofnon-Euclidean geometriesbyNikolai Ivanovich Lobachevsky(1792–1856),János Bolyai(1802–1860),Carl Friedrich Gauss(1777–1855) and others[41]led to a revival of interest in this discipline, and in the 20th century,David Hilbert(1862–1943) employed axiomatic reasoning in an attempt to provide a modern foundation of geometry.[42]
Points are generally considered fundamental objects for building geometry. They may be defined by the properties that they must have, as in Euclid's definition as "that which has no part",[43]or insynthetic geometry. In modern mathematics, they are generally defined aselementsof asetcalledspace, which is itselfaxiomaticallydefined.
With these modern definitions, every geometric shape is defined as a set of points; this is not the case in synthetic geometry, where a line is another fundamental object that is not viewed as the set of the points through which it passes.
However, there are modern geometries in which points are not primitive objects, or even without points.[44][45]One of the oldest such geometries isWhitehead's point-free geometry, formulated byAlfred North Whiteheadin 1919–1920.
Eucliddescribed a line as "breadthless length" which "lies equally with respect to the points on itself".[43]In modern mathematics, given the multitude of geometries, the concept of a line is closely tied to the way the geometry is described. For instance, inanalytic geometry, a line in the plane is often defined as the set of points whose coordinates satisfy a givenlinear equation,[46]but in a more abstract setting, such asincidence geometry, a line may be an independent object, distinct from the set of points which lie on it.[47]In differential geometry, ageodesicis a generalization of the notion of a line tocurved spaces.[48]
In Euclidean geometry a plane is a flat, two-dimensional surface that extends infinitely;[43]the definitions for other types of geometries are generalizations of that. Planes are used in many areas of geometry. For instance, planes can be studied as atopological surfacewithout reference to distances or angles;[49]it can be studied as anaffine space, where collinearity and ratios can be studied but not distances;[50]it can be studied as thecomplex planeusing techniques ofcomplex analysis;[51]and so on.
Acurveis a 1-dimensional object that may be straight (like a line) or not; curves in 2-dimensional space are calledplane curvesand those in 3-dimensional space are calledspace curves.[52]
In topology, a curve is defined by a function from an interval of the real numbers to another space.[49]In differential geometry, the same definition is used, but the defining function is required to be differentiable.[53]Algebraic geometry studiesalgebraic curves, which are defined asalgebraic varietiesofdimensionone.[54]
Asurfaceis a two-dimensional object, such as a sphere or paraboloid.[55]Indifferential geometry[53]andtopology,[49]surfaces are described by two-dimensional 'patches' (orneighborhoods) that are assembled bydiffeomorphismsorhomeomorphisms, respectively. In algebraic geometry, surfaces are described bypolynomial equations.[54]
Asolidis a three-dimensional object bounded by a closed surface; for example, aballis the volume bounded by a sphere.
Amanifoldis a generalization of the concepts of curve and surface. Intopology, a manifold is atopological spacewhere every point has aneighborhoodthat ishomeomorphicto Euclidean space.[49]Indifferential geometry, adifferentiable manifoldis a space where each neighborhood isdiffeomorphicto Euclidean space.[53]
Manifolds are used extensively in physics, including ingeneral relativityandstring theory.[56]
Eucliddefines a plane angle as the inclination to each other, in a plane, of two lines which meet each other, and do not lie straight with respect to each other.[43]In modern terms, an angle is the figure formed by tworays, called thesidesof the angle, sharing a common endpoint, called thevertexof the angle.[57]The size of an angle is formalized as anangular measure.
InEuclidean geometry, angles are used to studypolygonsandtriangles, as well as forming an object of study in their own right.[43]The study of the angles of a triangle or of angles in aunit circleforms the basis oftrigonometry.[58]
Indifferential geometryandcalculus, the angles betweenplane curvesorspace curvesorsurfacescan be calculated using thederivative.[59][60]
Length,area, andvolumedescribe the size or extent of an object in one dimension, two dimension, and three dimensions respectively.[61]
InEuclidean geometryandanalytic geometry, the length of a line segment can often be calculated by thePythagorean theorem.[62]
Area and volume can be defined as fundamental quantities separate from length, or they can be described and calculated in terms of lengths in a plane or 3-dimensional space.[61]Mathematicians have found many explicitformulas for areaandformulas for volumeof various geometric objects. Incalculus, area and volume can be defined in terms ofintegrals, such as theRiemann integral[63]or theLebesgue integral.[64]
Other geometrical measures include thecurvatureandcompactness.
The concept of length or distance can be generalized, leading to the idea ofmetrics.[65]For instance, theEuclidean metricmeasures the distance between points in theEuclidean plane, while thehyperbolic metricmeasures the distance in thehyperbolic plane. Other important examples of metrics include theLorentz metricofspecial relativityand the semi-Riemannian metricsofgeneral relativity.[66]
In a different direction, the concepts of length, area and volume are extended bymeasure theory, which studies methods of assigning a size ormeasuretosets, where the measures follow rules similar to those of classical area and volume.[67]
Congruenceandsimilarityare concepts that describe when two shapes have similar characteristics.[68]In Euclidean geometry, similarity is used to describe objects that have the same shape, while congruence is used to describe objects that are the same in both size and shape.[69]Hilbert, in his work on creating a more rigorous foundation for geometry, treated congruence as an undefined term whose properties are defined byaxioms.
Congruence and similarity are generalized intransformation geometry, which studies the properties of geometric objects that are preserved by different kinds of transformations.[70]
Classical geometers paid special attention to constructing geometric objects that had been described in some other way. Classically, the only instruments used in most geometric constructions are thecompassandstraightedge.[c]Also, every construction had to be complete in a finite number of steps. However, some problems turned out to be difficult or impossible to solve by these means alone, and ingenious constructions usingneusis, parabolas and other curves, or mechanical devices, were found.
The geometrical concepts of rotation and orientation define part of the placement of objects embedded in the plane or in space.
Traditional geometry allowed dimensions 1 (alineor curve), 2 (aplaneor surface), and 3 (our ambient world conceived of asthree-dimensional space). Furthermore, mathematicians and physicists have usedhigher dimensionsfor nearly two centuries.[71]One example of a mathematical use for higher dimensions is theconfiguration spaceof a physical system, which has a dimension equal to the system'sdegrees of freedom. For instance, the configuration of a screw can be described by five coordinates.[72]
Ingeneral topology, the concept of dimension has been extended fromnatural numbers, to infinite dimension (Hilbert spaces, for example) and positivereal numbers(infractal geometry).[73]Inalgebraic geometry, thedimension of an algebraic varietyhas received a number of apparently different definitions, which are all equivalent in the most common cases.[74]
The theme ofsymmetryin geometry is nearly as old as the science of geometry itself.[75]Symmetric shapes such as thecircle,regular polygonsandplatonic solidsheld deep significance for many ancient philosophers[76]and were investigated in detail before the time of Euclid.[39]Symmetric patterns occur in nature and were artistically rendered in a multitude of forms, including the graphics ofLeonardo da Vinci,M. C. Escher, and others.[77]In the second half of the 19th century, the relationship between symmetry and geometry came under intense scrutiny.Felix Klein'sErlangen programproclaimed that, in a very precise sense, symmetry, expressed via the notion of a transformationgroup, determines what geometryis.[78]Symmetry in classicalEuclidean geometryis represented bycongruencesand rigid motions, whereas inprojective geometryan analogous role is played bycollineations,geometric transformationsthat take straight lines into straight lines.[79]However it was in the new geometries of Bolyai and Lobachevsky, Riemann,Cliffordand Klein, andSophus Liethat Klein's idea to 'define a geometry via itssymmetry group' found its inspiration.[80]Both discrete and continuous symmetries play prominent roles in geometry, the former intopologyandgeometric group theory,[81][82]the latter inLie theoryandRiemannian geometry.[83][84]
A different type of symmetry is the principle ofdualityinprojective geometry, among other fields. This meta-phenomenon can roughly be described as follows: in anytheorem, exchangepointwithplane,joinwithmeet,lies inwithcontains, and the result is an equally true theorem.[85]A similar and closely related form of duality exists between avector spaceand itsdual space.[86]
Euclidean geometryis geometry in its classical sense.[87]As it models the space of the physical world, it is used in many scientific areas, such asmechanics,astronomy,crystallography,[88]and many technical fields, such asengineering,[89]architecture,[90]geodesy,[91]aerodynamics,[92]andnavigation.[93]The mandatory educational curriculum of the majority of nations includes the study of Euclidean concepts such aspoints,lines,planes,angles,triangles,congruence,similarity,solid figures,circles, andanalytic geometry.[94]
Euclidean vectors are used for a myriad of applications in physics and engineering, such asposition,displacement,deformation,velocity,acceleration,force, etc.
Differential geometryuses techniques ofcalculusandlinear algebrato study problems in geometry.[95]It has applications inphysics,[96]econometrics,[97]andbioinformatics,[98]among others.
In particular, differential geometry is of importance tomathematical physicsdue toAlbert Einstein'sgeneral relativitypostulation that theuniverseiscurved.[99]Differential geometry can either beintrinsic(meaning that the spaces it considers aresmooth manifoldswhose geometric structure is governed by aRiemannian metric, which determines how distances are measured near each point) orextrinsic(where the object under study is a part of some ambient flat Euclidean space).[100]
Topology is the field concerned with the properties ofcontinuous mappings,[101]and can be considered a generalization of Euclidean geometry.[102]In practice, topology often means dealing with large-scale properties of spaces, such asconnectednessandcompactness.[49]
The field of topology, which saw massive development in the 20th century, is in a technical sense a type oftransformation geometry, in which transformations arehomeomorphisms.[103]This has often been expressed in the form of the saying 'topology is rubber-sheet geometry'. Subfields of topology includegeometric topology,differential topology,algebraic topologyandgeneral topology.[104]
Algebraic geometry is fundamentally the study by means ofalgebraicmethods of some geometrical shapes, calledalgebraic sets, and defined as commonzerosofmultivariate polynomials.[105]Algebraic geometry became an autonomous subfield of geometryc.1900, with a theorem calledHilbert's Nullstellensatzthat establishes a strong correspondence between algebraic sets andidealsofpolynomial rings. This led to a parallel development of algebraic geometry, and its algebraic counterpart, calledcommutative algebra.[106]From the late 1950s through the mid-1970s algebraic geometry had undergone major foundational development, with the introduction byAlexander Grothendieckofscheme theory, which allows usingtopological methods, includingcohomology theoriesin a purely algebraic context.[106]Scheme theory allowed to solve many difficult problems not only in geometry, but also innumber theory.Wiles' proof of Fermat's Last Theoremis a famous example of a long-standing problem ofnumber theorywhose solution uses scheme theory and its extensions such asstack theory. One of sevenMillennium Prize problems, theHodge conjecture, is a question in algebraic geometry.[107]
Algebraic geometry has applications in many areas, includingcryptography[108]andstring theory.[109]
Complex geometrystudies the nature of geometric structures modelled on, or arising out of, thecomplex plane.[110][111][112]Complex geometry lies at the intersection of differential geometry, algebraic geometry, and analysis ofseveral complex variables, and has found applications tostring theoryandmirror symmetry.[113]
Complex geometry first appeared as a distinct area of study in the work ofBernhard Riemannin his study ofRiemann surfaces.[114][115][116]Work in the spirit of Riemann was carried out by theItalian school of algebraic geometryin the early 1900s. Contemporary treatment of complex geometry began with the work ofJean-Pierre Serre, who introduced the concept ofsheavesto the subject, and illuminated the relations between complex geometry and algebraic geometry.[117][118]The primary objects of study in complex geometry arecomplex manifolds,complex algebraic varieties, andcomplex analytic varieties, andholomorphic vector bundlesandcoherent sheavesover these spaces. Special examples of spaces studied in complex geometry include Riemann surfaces, andCalabi–Yau manifolds, and these spaces find uses in string theory. In particular,worldsheetsof strings are modelled by Riemann surfaces, andsuperstring theorypredicts that the extra 6 dimensions of 10 dimensionalspacetimemay be modelled by Calabi–Yau manifolds.
Discrete geometryis a subject that has close connections withconvex geometry.[119][120][121]It is concerned mainly with questions of relative position of simple geometric objects, such as points, lines and circles. Examples include the study ofsphere packings,triangulations, the Kneser-Poulsen conjecture, etc.[122][123]It shares many methods and principles withcombinatorics.
Computational geometrydeals withalgorithmsand theirimplementationsfor manipulating geometrical objects. Important problems historically have included thetravelling salesman problem,minimum spanning trees,hidden-line removal, andlinear programming.[124]
Although being a young area of geometry, it has many applications incomputer vision,image processing,computer-aided design,medical imaging, etc.[125]
Geometric group theorystudiesgroup actionson objects that are regarded as geometric (significantly, isometric actions onmetric spaces) to studyfinitely generated groups, often involving large-scale geometric techniques.[126]It is closely connected tolow-dimensional topology, such as inGrigori Perelman's proof of theGeometrization conjecture, which included the proof of thePoincaré conjecture, aMillennium Prize Problem.[127]
Group actions on theirCayley graphsare foundational examples of isometric group actions. Other major topics includequasi-isometries,Gromov-hyperbolic groupsand their generalizations (relativelyandacylindrically hyperbolic groups),free groupsandtheir automorphisms,groups acting on trees, various notions of nonpositive curvature for groups (CAT(0) groups,Dehn functions,automaticity...),right angled Artin groups, and topics close tocombinatorial group theorysuch assmall cancellation theoryand algorithmic problems (e.g. theword,conjugacy, andisomorphism problems). Other group-theoretic topics likemapping class groups,property (T),solvability,amenabilityandlattices in Lie groupsare sometimes regarded as strongly geometric as well.[126][128][129][130]
Convex geometryinvestigatesconvexshapes in the Euclidean space and its more abstract analogues, often using techniques ofreal analysisanddiscrete mathematics.[131]It has close connections toconvex analysis,optimizationandfunctional analysisand important applications innumber theory.
Convex geometry dates back to antiquity.[131]Archimedesgave the first known precise definition of convexity. Theisoperimetric problem, a recurring concept in convex geometry, was studied by the Greeks as well, includingZenodorus. Archimedes,Plato,Euclid, and laterKeplerandCoxeterall studiedconvex polytopesand their properties. From the 19th century on, mathematicians have studied other areas of convex mathematics, including higher-dimensional polytopes, volume and surface area of convex bodies,Gaussian curvature,algorithms,tilingsandlattices.
Geometry has found applications in many fields, some of which are described below.
Mathematics and art are related in a variety of ways. For instance, the theory ofperspectiveshowed that there is more to geometry than just the metric properties of figures: perspective is the origin ofprojective geometry.[132]
Artists have long used concepts ofproportionin design.Vitruviusdeveloped a complicated theory ofideal proportionsfor the human figure.[133]These concepts have been used and adapted by artists fromMichelangeloto modern comic book artists.[134]
Thegolden ratiois a particular proportion that has had a controversial role in art. Often claimed to be the most aesthetically pleasing ratio of lengths, it is frequently stated to be incorporated into famous works of art, though the most reliable and unambiguous examples were made deliberately by artists aware of this legend.[135]
Tilings, or tessellations, have been used in art throughout history.Islamic artmakes frequent use of tessellations, as did the art ofM. C. Escher.[136]Escher's work also made use ofhyperbolic geometry.
Cézanneadvanced the theory that all images can be built up from thesphere, thecone, and thecylinder. This is still used in art theory today, although the exact list of shapes varies from author to author.[137][138]
Geometry has many applications in architecture. In fact, it has been said that geometry lies at the core of architectural design.[139][140]Applications of geometry to architecture include the use ofprojective geometryto createforced perspective,[141]the use ofconic sectionsin constructing domes and similar objects,[90]the use oftessellations,[90]and the use of symmetry.[90]
The field ofastronomy, especially as it relates to mapping the positions ofstarsandplanetson thecelestial sphereand describing the relationship between movements of celestial bodies, have served as an important source of geometric problems throughout history.[142]
Riemannian geometryandpseudo-Riemanniangeometry are used ingeneral relativity.[143]String theorymakes use of several variants of geometry,[144]as doesquantum information theory.[145]
Calculuswas strongly influenced by geometry.[30]For instance, the introduction ofcoordinatesbyRené Descartesand the concurrent developments ofalgebramarked a new stage for geometry, since geometric figures such asplane curvescould now be representedanalyticallyin the form of functions and equations. This played a key role in the emergence ofinfinitesimal calculusin the 17th century. Analytic geometry continues to be a mainstay of pre-calculus and calculus curriculum.[146][147]
Another important area of application isnumber theory.[148]Inancient GreecethePythagoreansconsidered the role of numbers in geometry. However, the discovery of incommensurable lengths contradicted their philosophical views.[149]Since the 19th century, geometry has been used for solving problems in number theory, for example through thegeometry of numbersor, more recently,scheme theory, which is used inWiles's proof of Fermat's Last Theorem.[150]
"Three scientists, Ibn al-Haytham, Khayyam, and al-Tusi, had made the most considerable contribution to this branch of geometry whose importance came to be completely recognized only in the 19th century. In essence, their propositions concerning the properties of quadrangles which they considered, assuming that some of the angles of these figures were acute of obtuse, embodied the first few theorems of the hyperbolic and the elliptic geometries. Their other proposals showed that various geometric statements were equivalent to the Euclidean postulate V. It is extremely important that these scholars established the mutual connection between this postulate and the sum of the angles of a triangle and a quadrangle. By their works on the theory of parallel lines Arab mathematicians directly influenced the relevant investigations of their European counterparts. The first European attempt to prove the postulate on parallel lines—made by Witelo, the Polish scientists of the 13th century, while revising Ibn al-Haytham'sBook of Optics(Kitab al-Manazir)—was undoubtedly prompted by Arabic sources. The proofs put forward in the 14th century by the Jewish scholar Levi ben Gerson, who lived in southern France, and by the above-mentioned Alfonso from Spain directly border on Ibn al-Haytham's demonstration. Above, we have demonstrated thatPseudo-Tusi's Exposition of Euclidhad stimulated both J. Wallis's and G. Saccheri's studies of the theory of parallel lines."
Trigonometry(fromAncient Greekτρίγωνον(trígōnon)'triangle'andμέτρον(métron)'measure')[1]is a branch ofmathematicsconcerned with relationships betweenanglesand side lengths of triangles. In particular, thetrigonometric functionsrelate the angles of aright trianglewithratiosof its side lengths. The field emerged in theHellenistic worldduring the 3rd century BC from applications ofgeometrytoastronomical studies.[2]The Greeks focused on thecalculation of chords, while mathematicians in India created the earliest-known tables of values for trigonometric ratios (also calledtrigonometric functions) such assine.[3]
Throughout history, trigonometry has been applied in areas such asgeodesy,surveying,celestial mechanics, andnavigation.[4]
Trigonometry is known for its manyidentities. Thesetrigonometric identities[5]are commonly used for rewriting trigonometricalexpressionswith the aim to simplify an expression, to find a more useful form of an expression, or tosolve an equation.[6]
Sumerianastronomers studied angle measure, using a division of circles into 360 degrees.[8]They, and later theBabylonians, studied the ratios of the sides ofsimilartriangles and discovered some properties of these ratios but did not turn that into a systematic method for finding sides and angles of triangles. Theancient Nubiansused a similar method.[9]
In the 3rd century BC,Hellenistic mathematicianssuch asEuclidandArchimedesstudied the properties ofchordsandinscribed anglesin circles, and they proved theorems that are equivalent to modern trigonometric formulae, although they presented them geometrically rather than algebraically. In 140 BC,Hipparchus(fromNicaea, Asia Minor) gave the first tables of chords, analogous to moderntables of sine values, and used them to solve problems in trigonometry andspherical trigonometry.[10]In the 2nd century AD, the Greco-Egyptian astronomerPtolemy(from Alexandria, Egypt) constructed detailed trigonometric tables (Ptolemy's table of chords) in Book 1, chapter 11 of hisAlmagest.[11]Ptolemy usedchordlength to define his trigonometric functions, a minor difference from thesineconvention we use today.[12](The value we call sin(θ) can be found by looking up the chord length for twice the angle of interest (2θ) in Ptolemy's table, and then dividing that value by two.) Centuries passed before more detailed tables were produced, and Ptolemy's treatise remained in use for performing trigonometric calculations in astronomy throughout the next 1200 years in the medievalByzantine,Islamic, and, later, Western European worlds.
The modern definition of the sine is first attested in theSurya Siddhanta, and its properties were further documented in the 5th century (AD) byIndian mathematicianand astronomerAryabhata.[13]These Greek and Indian works were translated and expanded bymedieval Islamic mathematicians. In 830 AD, Persian mathematicianHabash al-Hasib al-Marwaziproduced the first table of cotangents.[14][15]By the 10th century AD, in the work of Persian mathematicianAbū al-Wafā' al-Būzjānī, all sixtrigonometric functionswere used.[16]Abu al-Wafa had sine tables in 0.25° increments, to 8 decimal places of accuracy, and accurate tables of tangent values.[16]He also made important innovations inspherical trigonometry[17][18][19]ThePersianpolymathNasir al-Din al-Tusihas been described as the creator of trigonometry as a mathematical discipline in its own right.[20][21][22]He was the first to treat trigonometry as a mathematical discipline independent from astronomy, and he developed spherical trigonometry into its present form.[15]He listed the six distinct cases of a right-angled triangle in spherical trigonometry, and in hisOn the Sector Figure, he stated the law of sines for plane and spherical triangles, discovered thelaw of tangentsfor spherical triangles, and provided proofs for both these laws.[23]Knowledge of trigonometric functions and methods reachedWestern EuropeviaLatin translationsof Ptolemy's GreekAlmagestas well as the works ofPersian and Arab astronomerssuch asAl BattaniandNasir al-Din al-Tusi.[24]One of the earliest works on trigonometry by a northern European mathematician isDe Triangulisby the 15th century German mathematicianRegiomontanus, who was encouraged to write, and provided with a copy of theAlmagest, by theByzantine Greek scholarcardinalBasilios Bessarionwith whom he lived for several years.[25]At the same time, another translation of theAlmagestfrom Greek into Latin was completed by the CretanGeorge of Trebizond.[26]Trigonometry was still so little known in 16th-century northern Europe thatNicolaus Copernicusdevoted two chapters ofDe revolutionibus orbium coelestiumto explain its basic concepts.
Driven by the demands ofnavigationand the growing need for accurate maps of large geographic areas, trigonometry grew into a major branch of mathematics.[27]Bartholomaeus Pitiscuswas the first to use the word, publishing hisTrigonometriain 1595.[28]Gemma Frisiusdescribed for the first time the method oftriangulationstill used today in surveying. It wasLeonhard Eulerwho fully incorporatedcomplex numbersinto trigonometry. The works of the Scottish mathematiciansJames Gregoryin the 17th century andColin Maclaurinin the 18th century were influential in the development oftrigonometric series.[29]Also in the 18th century,Brook Taylordefined the generalTaylor series.[30]
Trigonometric ratios are the ratios between edges of a right triangle. These ratios depend only on one acute angle of the right triangle, since any two right triangles with the same acute angle aresimilar.[31]
So, these ratios definefunctionsof this angle that are calledtrigonometric functions. Explicitly, they are defined below as functions of the known angleA, wherea,bandhrefer to the lengths of the sides in the accompanying figure.
In the following definitions, thehypotenuseis the side opposite to the 90-degree angle in a right triangle; it is the longest side of the triangle and one of the two sides adjacent to angleA. Theadjacent legis the other side that is adjacent to angleA. Theopposite sideis the side that is opposite to angleA. The termsperpendicularandbaseare sometimes used for the opposite and adjacent sides respectively. See below underMnemonics.
Thereciprocalsof these ratios are named thecosecant(csc),secant(sec), andcotangent(cot), respectively:
The cosine, cotangent, and cosecant are so named because they are respectively the sine, tangent, and secant of the complementary angle abbreviated to "co-".[32]
With these functions, one can answer virtually all questions about arbitrary triangles by using thelaw of sinesand thelaw of cosines.[33]These laws can be used to compute the remaining angles and sides of any triangle as soon as two sides and their included angle or two angles and a side or three sides are known.
A common use ofmnemonicsis to remember facts and relationships in trigonometry. For example, thesine,cosine, andtangentratios in a right triangle can be remembered by representing them and their corresponding sides as strings of letters. For instance, a mnemonic is SOH-CAH-TOA:[34]
One way to remember the letters is to sound them out phonetically (i.e./ˌsoʊkəˈtoʊə/SOH-kə-TOH-ə, similar toKrakatoa).[35]Another method is to expand the letters into a sentence, such as "SomeOldHippieCaughtAnotherHippieTrippin'OnAcid".[36]
Trigonometric ratios can also be represented using theunit circle, which is the circle of radius 1 centered at the origin in the plane.[37]In this setting, theterminal sideof an angleAplaced instandard positionwill intersect the unit circle in a point (x,y), wherex=cos⁡A{\displaystyle x=\cos A}andy=sin⁡A{\displaystyle y=\sin A}.[37]This representation allows for the calculation of commonly found trigonometric values, such as those in the following table:[38]
Using theunit circle, one can extend the definitions of trigonometric ratios to all positive and negative arguments[39](seetrigonometric function).
The following table summarizes the properties of the graphs of the six main trigonometric functions:[40][41]
Because the six main trigonometric functions are periodic, they are notinjective(or, 1 to 1), and thus are not invertible. Byrestrictingthe domain of a trigonometric function, however, they can be made invertible.[42]: 48ff
The names of the inverse trigonometric functions, together with their domains and range, can be found in the following table:[42]: 48ff[43]: 521ff
When considered as functions of a real variable, the trigonometric ratios can be represented by aninfinite series. For instance, sine and cosine have the following representations:[44]
With these definitions the trigonometric functions can be defined forcomplex numbers.[45]When extended as functions of real or complex variables, the followingformulaholds for the complex exponential:
This complex exponential function, written in terms of trigonometric functions, is particularly useful.[46][47]
Trigonometric functions were among the earliest uses formathematical tables.[48]Such tables were incorporated into mathematics textbooks and students were taught to look up values and how tointerpolatebetween the values listed to get higher accuracy.[49]Slide ruleshad special scales for trigonometric functions.[50]
Scientific calculatorshave buttons for calculating the main trigonometric functions (sin, cos, tan, and sometimescisand their inverses).[51]Most allow a choice of angle measurement methods:degrees, radians, and sometimesgradians. Most computerprogramming languagesprovide function libraries that include the trigonometric functions.[52]Thefloating point unithardware incorporated into the microprocessor chips used in most personal computers has built-in instructions for calculating trigonometric functions.[53]
In addition to the six ratios listed earlier, there are additional trigonometric functions that were historically important, though seldom used today. These include thechord(crd(θ) = 2 sin(⁠θ/2⁠)), theversine(versin(θ) = 1 − cos(θ) = 2 sin2(⁠θ/2⁠)) (which appeared in the earliest tables[54]), thecoversine(coversin(θ) = 1 − sin(θ) = versin(⁠π/2⁠−θ)), thehaversine(haversin(θ) =⁠1/2⁠versin(θ) = sin2(⁠θ/2⁠)),[55]theexsecant(exsec(θ) = sec(θ) − 1), and theexcosecant(excsc(θ) = exsec(⁠π/2⁠−θ) = csc(θ) − 1). SeeList of trigonometric identitiesfor more relations between these functions.
For centuries, spherical trigonometry has been used for locating solar, lunar, and stellar positions,[56]predicting eclipses, and describing the orbits of the planets.[57]
In modern times, the technique oftriangulationis used inastronomyto measure the distance to nearby stars,[58]as well as insatellite navigation systems.[19]
Historically, trigonometry has been used for locating latitudes and longitudes of sailing vessels, plotting courses, and calculating distances during navigation.[59]
Trigonometry is still used in navigation through such means as theGlobal Positioning Systemandartificial intelligenceforautonomous vehicles.[60]
In landsurveying, trigonometry is used in the calculation of lengths, areas, and relative angles between objects.[61]
On a larger scale, trigonometry is used ingeographyto measure distances between landmarks.[62]
The sine and cosine functions are fundamental to the theory ofperiodic functions,[63]such as those that describesoundandlightwaves.Fourierdiscovered that everycontinuous,periodic functioncould be described as aninfinite sumof trigonometric functions.
Even non-periodic functions can be represented as anintegralof sines and cosines through theFourier transform. This has applications toquantum mechanics[64]andcommunications,[65]among other fields.
Trigonometry is useful in manyphysical sciences,[66]includingacoustics,[67]andoptics.[67]In these areas, they are used to describesoundandlight waves, and to solve boundary- and transmission-related problems.[68]
Other fields that use trigonometry or trigonometric functions includemusic theory,[69]geodesy,audio synthesis,[70]architecture,[71]electronics,[69]biology,[72]medical imaging(CT scansandultrasound),[73]chemistry,[74]number theory(and hencecryptology),[75]seismology,[67]meteorology,[76]oceanography,[77]image compression,[78]phonetics,[79]economics,[80]electrical engineering,mechanical engineering,civil engineering,[69]computer graphics,[81]cartography,[69]crystallography[82]andgame development.[81]
Trigonometry has been noted for its many identities, that is, equations that are true for all possible inputs.[83]
Identities involving only angles are known astrigonometric identities. Other equations, known astriangle identities,[84]relate both the sides and angles of a given triangle.
In the following identities,A,BandCare the angles of a triangle anda,bandcare the lengths of sides of the triangle opposite the respective angles (as shown in the diagram).
Thelaw of sines(also known as the "sine rule") for an arbitrary triangle states:[85]
whereΔ{\displaystyle \Delta }is the area of the triangle andRis the radius of thecircumscribed circleof the triangle:
Thelaw of cosines(known as the cosine formula, or the "cos rule") is an extension of the Pythagorean theorem to arbitrary triangles:[85]
Thelaw of tangents, developed byFrançois Viète, is an alternative to the Law of Cosines when solving for the unknown edges of a triangle, providing simpler computations when using trigonometric tables.[86]It is given by:
Given two sidesaandband the angle between the sidesC, thearea of the triangleis given by half the product of the lengths of two sides and the sine of the angle between the two sides:[85]
The following trigonometricidentitiesare related to thePythagorean theoremand hold for any value:[87]
The second and third equations are derived from dividing the first equation bycos2⁡A{\displaystyle \cos ^{2}{A}}andsin2⁡A{\displaystyle \sin ^{2}{A}}, respectively.
Euler's formula, which states thateix=cos⁡x+isin⁡x{\displaystyle e^{ix}=\cos x+i\sin x}, produces the followinganalyticalidentities for sine, cosine, and tangent in terms ofeand theimaginary uniti:
Other commonly used trigonometric identities include the half-angle identities, the angle sum and difference identities, and the product-to-sum identities.[31]
Number theoryis a branch ofpure mathematicsdevoted primarily to the study of theintegersandarithmetic functions. German mathematicianCarl Friedrich Gauss(1777–1855) said, "Mathematics is the queen of the sciences—and number theory is the queen of mathematics."[1]Number theorists studyprime numbersas well as the properties ofmathematical objectsconstructed from integers (for example,rational numbers), or defined as generalizations of the integers (for example,algebraic integers).
Integers can be considered either in themselves or as solutions to equations (Diophantine geometry). Questions in number theory can often be understood through the study ofanalyticalobjects, such as theRiemann zeta function, that encode properties of the integers, primes or other number-theoretic objects in some fashion (analytic number theory). One may also studyreal numbersin relation to rational numbers, as for instance how irrational numbers can be approximated by fractions (Diophantine approximation).
The older term for number theory isarithmetic, although this word today is used by the general public to meanelementary calculations. By the early twentieth century, the termnumber theoryhad been widely adopted.[note 1]However,arithmeticalis still commonly preferred as an adjective tonumber-theoretic.
The earliest historical find of an arithmetical nature is a fragment of a table:Plimpton 322(Larsa, Mesopotamia, c. 1800 BC), a broken clay tablet, contains a list of "Pythagorean triples", that is, integers(a,b,c){\displaystyle (a,b,c)}such thata2+b2=c2{\displaystyle a^{2}+b^{2}=c^{2}}. The triples are too many and too large to have been obtained bybrute force. The heading over the first column reads: "Thetakiltumof the diagonal which has been subtracted such that the width..."[2]
The table's layout suggests that it was constructed by means of what amounts, in modern language, to theidentity[3]
which is implicit in routineOld Babylonianexercises. If some other method was used, the triples were first constructed and then reordered byc/a{\displaystyle c/a}, presumably for actual use as a "table", for example, with a view to applications.[4]
It is not known what these applications may have been, or whether there could have been any;Babylonian astronomy, for example, truly came into its own many centuries later. It has been suggested instead that the table was a source of numerical examples for school problems.[5][note 2]Plimpton 322 tablet is the only surviving evidence of what today would be called number theory within Babylonian mathematics, though a kind ofBabylonian algebrawas much more developed.[6]
Although other civilizations probably influenced Greek mathematics at the beginning,[7]all evidence of such borrowings appear relatively late,[8][9]and it is likely that Greekarithmētikḗ(the theoretical or philosophical study of numbers) is an indigenous tradition. Aside from a few fragments, most of what is known about Greek mathematics in the 6th to 4th centuries BCE (theArchaicandClassicalperiods) comes through either the reports of contemporary non-mathematicians or references from mathematical works in the earlyHellenistic period.[10]In the case of number theory, this means largelyPlato,Aristotle, andEuclid.
Plato had a keen interest in mathematics, and distinguished clearly betweenarithmētikḗand calculation (logistikē). Plato reports in his dialogueTheaetetusthatTheodorushad proven that3,5,…,17{\displaystyle {\sqrt {3}},{\sqrt {5}},\dots ,{\sqrt {17}}}are irrational.Theaetetus, a disciple of Theodorus's, worked on distinguishing different kinds ofincommensurables, and was thus arguably a pioneer in the study ofnumber systems. Aristotle further claimed that the philosophy of Plato closely followed the teachings of thePythagoreans,[11]and Cicero repeats this claim:Platonem ferunt didicisse Pythagorea omnia("They say Plato learned all things Pythagorean").[12]
Euclid devoted part of hisElements(Books VII-IX) to topics that belong unambiguously to number theory, includingprime numbersanddivisibility. He gave an algorithm, theEuclidean algorithm, for computing thegreatest common divisorof two numbers (Prop. VII.2) and aproof implying the infinitude of primes(Prop. IX.20). There is also older material likely based on Pythagorean teachings (Prop. IX.21–34), such as "odd times even is even" and "if an odd number measures [= divides] an even number, then it also measures [= divides] half of it".[13]This is all that is needed to prove that2{\displaystyle {\sqrt {2}}}isirrational.[14]Pythagoreans apparently gave great importance to the odd and the even.[15]The discovery that2{\displaystyle {\sqrt {2}}}is irrational is credited to the early Pythagoreans, sometimes assigned toHippasus, who was expelled or split from the Pythagorean community as a result.[16][17]This forced a distinction betweennumbers(integers and the rationals—the subjects of arithmetic) andlengthsandproportions(which may be identified with real numbers, whether rational or not).
The Pythagorean tradition also spoke of so-calledpolygonalorfigurate numbers.[18]Whilesquare numbers,cubic numbers, etc., are seen now as more natural thantriangular numbers,pentagonal numbers, etc., the study of the sums of triangular and pentagonal numbers would prove fruitful in theearly modern period(17th to early 19th centuries).
Anepigrampublished byLessingin 1773 appears to be a letter sent byArchimedestoEratosthenes.[19][20]The epigram proposed what has become known asArchimedes's cattle problem; its solution (absent from the manuscript) requires solving an indeterminate quadratic equation (which reduces to what would later be misnamedPell's equation). As far as it is known, such equations were first successfully treated by Indian mathematicians. It is not known whether Archimedes himself had a method of solution.
Aside from the elementary work of Neopythagoreans such asNicomachusandTheon of Smyrna, the foremost authority inarithmētikḗin Late Antiquity wasDiophantus of Alexandria, who probably lived in the 3rd century AD, approximately five hundred years after Euclid. Little is known about his life, but he wrote two works that are extant:On Polygonal Numbers, a short treatise written in the Euclidean manner on the subject, and theArithmetica, a work on pre-modern algebra (namely, the use of algebra to solve numerical problems). Six out of the thirteen books of Diophantus'sArithmeticasurvive in the original Greek and four more survive in an Arabic translation. TheArithmeticais a collection of worked-out problems where the task is invariably to find rational solutions to a system of polynomial equations, usually of the formf(x,y)=z2{\displaystyle f(x,y)=z^{2}}orf(x,y,z)=w2{\displaystyle f(x,y,z)=w^{2}}. In modern parlance,Diophantine equationsarepolynomial equationsto which rational or integer solutions are sought.
TheChinese remainder theoremappears as an exercise[21]inSunzi Suanjing(between the third and fifth centuries).[22](There is one important step glossed over in Sunzi's solution:[note 3]it is the problem that was later solved byĀryabhaṭa'sKuṭṭaka– seebelow.) The result was later generalized with a complete solution calledDa-yan-shu(大衍術) inQin Jiushao's 1247Mathematical Treatise in Nine Sections[23]which was translated into English in early nineteenth century by British missionaryAlexander Wylie.[24]There is also some numerical mysticism in Chinese mathematics,[note 4]but, unlike that of the Pythagoreans, it seems to have led nowhere.
While Greek astronomy probably influenced Indian learning, to the point of introducingtrigonometry,[25]it seems to be the case that Indian mathematics is otherwise an autochthonous tradition;[26]in particular, there is no evidence that Euclid'sElementsreached India before the eighteenth century.[27]Āryabhaṭa (476–550 AD) showed that pairs of simultaneous congruencesn≡a1modm1{\displaystyle n\equiv a_{1}{\bmod {m}}_{1}},n≡a2modm2{\displaystyle n\equiv a_{2}{\bmod {m}}_{2}}could be solved by a method he calledkuṭṭaka, orpulveriser;[28]this is a procedure close to (a generalisation of) the Euclidean algorithm, which was probably discovered independently in India.[29]Āryabhaṭa seems to have had in mind applications to astronomical calculations.[25]
Brahmagupta (628 AD) started the systematic study of indefinite quadratic equations—in particular, the misnamedPell equation, in whichArchimedesmay have first been interested, and which did not start to be solved in the West until the time of Fermat and Euler. Later Sanskrit authors would follow, using Brahmagupta's technical terminology. A general procedure (thechakravala, or "cyclic method") for solving Pell's equation was finally found byJayadeva(cited in the eleventh century; his work is otherwise lost); the earliest surviving exposition appears inBhāskara II's Bīja-gaṇita (twelfth century).[30]
Indian mathematics remained largely unknown in Europe until the late eighteenth century;[31]Brahmagupta and Bhāskara's work was translated into English in 1817 byHenry Colebrooke.[32]
In the early ninth century, the caliphAl-Ma'munordered translations of many Greek mathematical works and at least one Sanskrit work (theSindhind, which may[33]or may not[34]be Brahmagupta'sBrāhmasphuṭasiddhānta).
Diophantus's main work, theArithmetica, was translated into Arabic byQusta ibn Luqa(820–912).
Part of the treatiseal-Fakhri(byal-Karajī, 953 – c. 1029) builds on it to some extent. According to Rashed Roshdi, Al-Karajī's contemporaryIbn al-Haythamknew[35]what would later be calledWilson's theorem.
Other than a treatise on squares in arithmetic progression byFibonacci—who traveled and studied in north Africa and Constantinople—no number theory to speak of was done in western Europe during the Middle Ages. Matters started to change in Europe in the lateRenaissance, thanks to a renewed study of the works of Greek antiquity. A catalyst was the textual emendation and translation into Latin of Diophantus'Arithmetica.[36]
Pierre de Fermat(1607–1665) never published his writings but communicated through correspondence instead. Accordingly, his work on number theory is contained almost entirely in letters to mathematicians and in private marginal notes.[37]Although he drew inspiration from classical sources, in his notes and letters Fermat scarcely wrote any proofs—he had no models in the area.[38]
Over his lifetime, Fermat made the following contributions to the field:
The interest ofLeonhard Euler(1707–1783) in number theory was first spurred in 1729, when a friend of his, the amateur[note 7]Goldbach, pointed him towards some of Fermat's work on the subject.[49][50]This has been called the "rebirth" of modern number theory,[51]after Fermat's relative lack of success in getting his contemporaries' attention for the subject.[52]Euler's work on number theory includes the following:[53]
Joseph-Louis Lagrange(1736–1813) was the first to give full proofs of some of Fermat's and Euler's work and observations; for instance, thefour-square theoremand the basic theory of the misnamed "Pell's equation" (for which an algorithmic solution was found by Fermat and his contemporaries, and also by Jayadeva and Bhaskara II before them.) He also studiedquadratic formsin full generality (as opposed tomX2+nY2{\displaystyle mX^{2}+nY^{2}}), including defining their equivalence relation, showing how to put them in reduced form, etc.
Adrien-Marie Legendre(1752–1833) was the first to state the law of quadratic reciprocity. He also conjectured what amounts to theprime number theoremandDirichlet's theorem on arithmetic progressions. He gave a full treatment of the equationax2+by2+cz2=0{\displaystyle ax^{2}+by^{2}+cz^{2}=0}[64]and worked on quadratic forms along the lines later developed fully by Gauss.[65]In his old age, he was the first to prove Fermat's Last Theorem forn=5{\displaystyle n=5}(completing work byPeter Gustav Lejeune Dirichlet, and crediting both him andSophie Germain).[66]
Carl Friedrich Gauss(1777–1855) worked in a wide variety of fields in both mathematics and physics including number theory, analysis, differential geometry, geodesy, magnetism, astronomy and optics. TheDisquisitiones Arithmeticae(1801), which he wrote three years earlier when he was 21, had an immense influence in the area of number theory and set its agenda for much of the 19th century. Gauss proved in this work the law ofquadratic reciprocityand developed the theory of quadratic forms (in particular, defining their composition). He also introduced some basic notation (congruences) and devoted a section to computational matters, including primality tests.[67]The last section of theDisquisitionesestablished a link betweenroots of unityand number theory:
The theory of the division of the circle...which is treated in sec. 7 does not belong by itself to arithmetic, but its principles can only be drawn from higher arithmetic.[68]
In this way, Gauss arguably made forays towardsÉvariste Galois's work and the areaalgebraic number theory.
Starting early in the nineteenth century, the following developments gradually took place:
Algebraic number theory may be said to start with the study of reciprocity andcyclotomy, but truly came into its own with the development ofabstract algebraand early ideal theory andvaluationtheory; see below. A conventional starting point for analytic number theory isDirichlet's theorem on arithmetic progressions(1837),[70][71]whose proof introducedL-functionsand involved some asymptotic analysis and a limiting process on a real variable.[72]The first use of analytic ideas in number theory actually goes back to Euler (1730s),[73][74]who used formal power series and non-rigorous (or implicit) limiting arguments. The use ofcomplexanalysis in number theory comes later: the work ofBernhard Riemann(1859) on thezeta functionis the canonical starting point;[75]Jacobi's four-square theorem(1839), which predates it, belongs to an initially different strand that has by now taken a leading role in analytic number theory (modular forms).[76]
The termelementaryusually denotes a method that does not usecomplex analysis. For example, theprime number theoremwas first proven using complex analysis in 1896, but an elementary proof was found only in 1949 byErdősandSelberg.[77]The term is somewhat ambiguous. For example, proofs based on complexTauberian theorems, such asWiener–Ikehara, are often seen as quite enlightening but not elementary despite usingFourier analysis, not complex analysis. Here as elsewhere, anelementaryproof may be longer and more difficult for most readers than a non-elementary one.
Number theory has the reputation of being a field many of whose results can be stated to the layperson. At the same time, the proofs of these results are not particularly accessible, in part because the range of tools they use is, if anything, unusually broad within mathematics.[78]
Some subjects generally considered to be part of analytic number theory (e.g.,sieve theory) are better covered by the second rather than the first definition.[note 8]Small sieves, for instance, use little analysis and yet still belong to analytic number theory.[note 9]
The following are examples of problems in analytic number theory: theprime number theorem, theGoldbach conjecture, thetwin prime conjecture, theHardy–Littlewood conjectures, theWaring problemand theRiemann hypothesis. Some of the most important tools of analytic number theory are thecircle method,sieve methodsandL-functions(or, rather, the study of their properties). The theory ofmodular forms(and, more generally,automorphic forms) also occupies an increasingly central place in the toolbox of analytic number theory.[80]
One may ask analytic questions aboutalgebraic numbers, and use analytic means to answer such questions; it is thus that algebraic and analytic number theory intersect. For example, one may defineprime ideals(generalizations ofprime numbersin the field of algebraic numbers) and ask how many prime ideals there are up to a certain size. This questioncan be answeredby means of an examination ofDedekind zeta functions, which are generalizations of theRiemann zeta function, a key analytic object at the roots of the subject.[81]This is an example of a general procedure in analytic number theory: deriving information about the distribution of asequence(here, prime ideals or prime numbers) from the analytic behavior of an appropriately constructed complex-valued function.[82]
Analgebraic numberis anycomplex numberthat is a solution to some polynomial equationf(x)=0{\displaystyle f(x)=0}with rational coefficients; for example, every solutionx{\displaystyle x}ofx5+(11/2)x3−7x2+9=0{\displaystyle x^{5}+(11/2)x^{3}-7x^{2}+9=0}is an algebraic number. Fields of algebraic numbers are also calledalgebraic number fields, or shortlynumber fields. Algebraic number theory studies algebraic number fields.[83]
It could be argued that the simplest kind of number fields, namelyquadratic fields, were already studied by Gauss, as the discussion of quadratic forms inDisquisitiones Arithmeticaecan be restated in terms ofidealsandnormsin quadratic fields. (Aquadratic fieldconsists of all
numbers of the forma+bd{\displaystyle a+b{\sqrt {d}}}, wherea{\displaystyle a}andb{\displaystyle b}are rational numbers andd{\displaystyle d}is a fixed rational number whose square root is not rational.)
For that matter, the eleventh-centurychakravala methodamounts—in modern terms—to an algorithm for finding the units of a real quadratic number field. However, neither Bhāskara nor Gauss knew of number fields as such.
The grounds of the subject were set in the late nineteenth century, whenideal numbers, thetheory of idealsandvaluation theorywere introduced; these are three complementary ways of dealing with the lack of unique factorisation in algebraic number fields. (For example, in the field generated by the rationals
and−5{\displaystyle {\sqrt {-5}}}, the number6{\displaystyle 6}can be factorised both as6=2⋅3{\displaystyle 6=2\cdot 3}and6=(1+−5)(1−−5){\displaystyle 6=(1+{\sqrt {-5}})(1-{\sqrt {-5}})}; all of2{\displaystyle 2},3{\displaystyle 3},1+−5{\displaystyle 1+{\sqrt {-5}}}and1−−5{\displaystyle 1-{\sqrt {-5}}}are irreducible, and thus, in a naïve sense, analogous to primes among the integers.) The initial impetus for the development of ideal numbers (byKummer) seems to have come from the study of higher reciprocity laws,[84]that is, generalisations ofquadratic reciprocity.
Number fields are often studied as extensions of smaller number fields: a fieldLis said to be anextensionof a fieldKifLcontainsK.
(For example, the complex numbersCare an extension of the realsR, and the realsRare an extension of the rationalsQ.)
Classifying the possible extensions of a given number field is a difficult and partially open problem. Abelian extensions—that is, extensionsLofKsuch that theGalois group[note 10]Gal(L/K) ofLoverKis anabelian group—are relatively well understood.
Their classification was the object of the programme ofclass field theory, which was initiated in the late nineteenth century (partly byKroneckerandEisenstein) and carried out largely in 1900–1950.
An example of an active area of research in algebraic number theory isIwasawa theory. TheLanglands program, one of the main current large-scale research plans in mathematics, is sometimes described as an attempt to generalise class field theory to non-abelian extensions of number fields.
The central problem of Diophantine geometry is to determine when aDiophantine equationhas integer or rational solutions, and if it does, how many. The approach taken is to think of the solutions of an equation as a geometric object.
For example, an equation in two variables defines a curve in the plane. More generally, an equation or system of equations in two or more variables defines acurve, asurface, or some other such object inn-dimensional space. In Diophantine geometry, one asks whether there are anyrational points(points all of whose coordinates are rationals) orintegral points(points all of whose coordinates are integers) on the curve or surface. If there are any such points, the next step is to ask how many there are and how they are distributed. A basic question in this direction is whether there are finitely
or infinitely many rational points on a given curve or surface.
Consider, for instance, thePythagorean equationx2+y2=1{\displaystyle x^{2}+y^{2}=1}. One would like to know its rational solutions, namely(x,y){\displaystyle (x,y)}such thatxandyare both rational. This is the same as asking for all integer solutions
toa2+b2=c2{\displaystyle a^{2}+b^{2}=c^{2}}; any solution to the latter equation gives us a solutionx=a/c{\displaystyle x=a/c},y=b/c{\displaystyle y=b/c}to the former. It is also the
same as asking for all points with rational coordinates on the curve described byx2+y2=1{\displaystyle x^{2}+y^{2}=1}(a circle of radius 1 centered on the origin).
The rephrasing of questions on equations in terms of points on curves is felicitous. The finiteness or not of the number of rational or integer points on an algebraic curve (that is, rational or integer solutions to an equationf(x,y)=0{\displaystyle f(x,y)=0}, wheref{\displaystyle f}is a polynomial in two variables) depends crucially on thegenusof the curve.[note 11]A major achievement of this approach isWiles's proof of Fermat's Last Theorem, for which other geometrical notions are just as crucial.
There is also the closely linked area ofDiophantine approximations: given a numberx{\displaystyle x}, determine how well it can be approximated by rational numbers. One seeks approximations that are good relative to the amount of space required to write the rational number: calla/q{\displaystyle a/q}(withgcd(a,q)=1{\displaystyle \gcd(a,q)=1}) a good approximation tox{\displaystyle x}if|x−a/q|<1qc{\displaystyle |x-a/q|<{\frac {1}{q^{c}}}}, wherec{\displaystyle c}is large. This question is of special interest ifx{\displaystyle x}is an algebraic number. Ifx{\displaystyle x}cannot be approximated well, then some equations do not have integer or rational solutions. Moreover, several concepts (especially that ofheight) are critical both in Diophantine geometry and in the study of Diophantine approximations. This question is also of special interest intranscendental number theory: if a number can be approximated better than any algebraic number, then it is atranscendental number. It is by this argument thatπandehave been shown to be transcendental.
Diophantine geometry should not be confused with thegeometry of numbers, which is a collection of graphical methods for answering certain questions in algebraic number theory.Arithmetic geometryis a contemporary term for the same domain covered by Diophantine geometry, particularly when one wishes to emphasize the connections to modern algebraic geometry (for example, inFaltings's theorem) rather than to techniques in Diophantine approximations.
The areas below date from no earlier than the mid-twentieth century, even if they are based on older material. For example, although algorithms in number theory have a long history, the modern study ofcomputabilitybegan only in the 1930s and 1940s, whilecomputational complexity theoryemerged in the 1970s.
Probabilistic number theory starts with questions such as the following: Take an integernat random between one and a million. How likely is it to be prime? (this is just another way of asking how many primes there are between one and a million). How many prime divisors willnhave on average? What is the probability that it will have many more or many fewer divisors or prime divisors than the average?
Much of probabilistic number theory can be seen as an important special case of the study of variables that are almost, but not quite, mutuallyindependent. For example, the event that a random integer between one and a million be divisible by two and the event that it be divisible by three are almost independent, but not quite.
It is sometimes said thatprobabilistic combinatoricsuses the fact that whatever happens with probability greater than0{\displaystyle 0}must happen sometimes; one may say with equal justice that many applications of probabilistic number theory hinge on the fact that whatever is unusual must be rare. If certain algebraic objects (say, rational or integer solutions to certain equations) can be shown to be in the tail of certain sensibly defined distributions, it follows that there must be few of them; this is a very concrete non-probabilistic statement following from a probabilistic one.
At times, a non-rigorous, probabilistic approach leads to a number ofheuristicalgorithms and open problems, notablyCramér's conjecture.
Combinatorics in number theory starts with questions like the following: Does a fairly "thick"infinite setA{\displaystyle A}contain many elements in arithmetic progression:a{\displaystyle a},a+b,a+2b,a+3b,…,a+10b{\displaystyle a+b,a+2b,a+3b,\ldots ,a+10b}? Should it be possible to write large integers as sums of elements ofA{\displaystyle A}?
These questions are characteristic ofarithmetic combinatorics,a coalescing field that subsumesadditive number theory(which concerns itself with certain very specific setsA{\displaystyle A}of arithmetic significance, such as the primes or the squares), some of thegeometry of numbers, as well as some rapidly developing new material. Its focus on issues of growth and distribution accounts in part for its developing links withergodic theory,finite group theory,model theory, and other fields.
The termadditive combinatoricsis also used; however, the setsA{\displaystyle A}being studied need not be sets of integers, but rather subsets of non-commutativegroups, for which the multiplication symbol, not the addition symbol, is traditionally used; they can also be subsets ofrings, in which case the growth ofA+A{\displaystyle A+A}andA{\displaystyle A}·A{\displaystyle A}may be compared.
While the wordalgorithmgoes back only to certain readers ofal-Khwārizmī, careful descriptions of methods of solution are older than proofs: such methods (that is, algorithms) are as old as any recognisable mathematics—ancient Egyptian, Babylonian, Vedic, Chinese—whereas proofs appeared only with the Greeks of the classical period.
An early case is that of what is now called the Euclidean algorithm. In its basic form (namely, as an algorithm for computing thegreatest common divisor) it appears as Proposition 2 of Book VII inElements, together with a proof of correctness. However, in the form that is often used in number theory (namely, as an algorithm for finding integer solutions to an equationax+by=c{\displaystyle ax+by=c}, or, what is the same, for finding the quantities whose existence is assured by theChinese remainder theorem) it first appears in the works ofĀryabhaṭa(fifth to sixth centuries) as an algorithm calledkuṭṭaka("pulveriser"), without a proof of correctness.
There are two main questions: "Can this be computed?" and "Can it be computed rapidly?" Anyone can test whether a number is prime or, if it is not, split it into prime factors; doing so rapidly is another matter. Fast algorithms fortesting primalityare now known, but, in spite of much work (both theoretical and practical), no truly fast algorithm for factoring.
The difficulty of a computation can be useful: modern protocols forencrypting messages(for example,RSA) depend on functions that are known to all, but whose inverses are known only to a chosen few, and would take one too long a time to figure out on one's own. For example, these functions can be such that their inverses can be computed only if certain large integers are factorized. While many difficult computational problems outside number theory are known, most working encryption protocols nowadays are based on the difficulty of a few number-theoretical problems.
Some things may not be computable at all; in fact, this can be proven in some instances. For instance, in 1970, it was proven, as a solution toHilbert's tenth problem, that there is noTuring machinewhich can solve all Diophantine equations.[85]In particular, this means that, given acomputably enumerableset of axioms, there are Diophantine equations for which there is no proof, starting from the axioms, of whether the set of equations has or does not have integer solutions. (i.e., Diophantine equations for which there are no integer solutions, since, given a Diophantine equation with at least one solution, the solution itself provides a proof of the fact that a solution exists. It cannot be proven that a particular Diophantine equation is of this kind, since this would imply that it has no solutions.)
The number-theoristLeonard Dickson(1874–1954) said "Thank God that number theory is unsullied by any application". Such a view is no longer applicable to number theory.[86]In 1974,Donald Knuthsaid "virtually every theorem in elementary number theory arises in a natural, motivated way in connection with the problem of making computers do high-speed numerical calculations".[87]Elementary number theory is taught indiscrete mathematicscourses forcomputer scientists. It also has applications to the continuous innumerical analysis.[88]
Number theory has now several modern applications spanning diverse areas such as:
TheAmerican Mathematical Societyawards theCole Prizein Number Theory. Moreover, number theory is one of the three mathematical subdisciplines rewarded by theFermat Prize.
[...] the question "how was the tablet calculated?" does not have to have the same answer as the question "what problems does the tablet set?" The first can be answered most satisfactorily by reciprocal pairs, as first suggested half a century ago, and the second by some sort of right-triangle problems (Robson 2001, p. 202).
Robson takes issue with the notion that the scribe who produced Plimpton 322 (who had to "work for a living", and would not have belonged to a "leisured middle class") could have been motivated by his own "idle curiosity" in the absence of a "market for new mathematics".(Robson 2001, pp. 199–200)
[26] Now there are an unknown number of things. If we count by threes, there is a remainder 2; if we count by fives, there is a remainder 3; if we count by sevens, there is a remainder 2. Find the number of things.Answer: 23.
Method: If we count by threes and there is a remainder 2, put down 140. If we count by fives and there is a remainder 3, put down 63. If we count by sevens and there is a remainder 2, put down 30. Add them to obtain 233 and subtract 210 to get the answer. If we count by threes and there is a remainder 1, put down 70. If we count by fives and there is a remainder 1, put down 21. If we count by sevens and there is a remainder 1, put down 15. When [a number] exceeds 106, the result is obtained by subtracting 105.
[36] Now there is a pregnant woman whose age is 29. If the gestation period is 9 months, determine the sex of the unborn child.Answer: Male.
Method: Put down 49, add the gestation period and subtract the age. From the remainder take away 1 representing the heaven, 2 the earth, 3 the man, 4 the four seasons, 5 the five phases, 6 the six pitch-pipes, 7 the seven stars [of the Dipper], 8 the eight winds, and 9 the nine divisions [of China under Yu the Great]. If the remainder is odd, [the sex] is male and if the remainder is even, [the sex] is female.
This is the last problem in Sunzi's otherwise matter-of-fact treatise.
Two of the most popular introductions to the subject are:
Hardy and Wright's book is a comprehensive classic, though its clarity sometimes suffers due to the authors' insistence on elementary methods (Apostol 1981).
Vinogradov's main attraction consists in its set of problems, which quickly lead to Vinogradov's own research interests; the text itself is very basic and close to minimal. Other popular first introductions are:
Probability theoryorprobability calculusis the branch ofmathematicsconcerned withprobability. Although there are several differentprobability interpretations, probability theory treats the concept in a rigorous mathematical manner by expressing it through a set ofaxioms. Typically these axioms formalise probability in terms of aprobability space, which assigns ameasuretaking values between 0 and 1, termed theprobability measure, to a set of outcomes called thesample space. Any specified subset of the sample space is called anevent.
Central subjects in probability theory include discrete and continuousrandom variables,probability distributions, andstochastic processes(which provide mathematical abstractions ofnon-deterministicor uncertain processes or measuredquantitiesthat may either be single occurrences or evolve over time in a random fashion).
Although it is not possible to perfectly predict random events, much can be said about their behavior. Two major results in probability theory describing such behaviour are thelaw of large numbersand thecentral limit theorem.
As a mathematical foundation forstatistics, probability theory is essential to many human activities that involve quantitative analysis of data.[1]Methods of probability theory also apply to descriptions of complex systems given only partial knowledge of their state, as instatistical mechanicsorsequential estimation. A great discovery of twentieth-centuryphysicswas the probabilistic nature of physical phenomena at atomic scales, described inquantum mechanics.[2]
The modern mathematical theory ofprobabilityhas its roots in attempts to analyzegames of chancebyGerolamo Cardanoin the sixteenth century, and byPierre de FermatandBlaise Pascalin the seventeenth century (for example the "problem of points").[3]Christiaan Huygenspublished a book on the subject in 1657.[4]In the 19th century, what is considered theclassical definition of probabilitywas completed byPierre Laplace.[5]
Initially, probability theory mainly considereddiscreteevents, and its methods were mainlycombinatorial. Eventually,analyticalconsiderations compelled the incorporation ofcontinuousvariables into the theory.
This culminated in modern probability theory, on foundations laid byAndrey Nikolaevich Kolmogorov. Kolmogorov combined the notion ofsample space, introduced byRichard von Mises, andmeasure theoryand presented hisaxiom systemfor probability theory in 1933. This became the mostly undisputedaxiomatic basisfor modern probability theory; but, alternatives exist, such as the adoption of finite rather than countable additivity byBruno de Finetti.[6]
Most introductions to probability theory treat discrete probability distributions and continuous probability distributions separately. The measure theory-based treatment of probability covers the discrete, continuous, a mix of the two, and more.
Consider anexperimentthat can produce a number of outcomes. The set of all outcomes is called thesample spaceof the experiment. Thepower setof the sample space (or equivalently, the event space) is formed by considering all different collections of possible results. For example, rolling an honest die produces one of six possible results. One collection of possible results corresponds to getting an odd number. Thus, the subset {1,3,5} is an element of the power set of the sample space of dice rolls. These collections are calledevents. In this case, {1,3,5} is the event that the die falls on some odd number. If the results that actually occur fall in a given event, that event is said to have occurred.
Probability is away of assigningevery "event" a value between zero and one, with the requirement that the event made up of all possible results (in our example, the event {1,2,3,4,5,6}) be assigned a value of one. To qualify as aprobability distribution, the assignment of values must satisfy the requirement that if you look at a collection of mutually exclusive events (events that contain no common results, e.g., the events {1,6}, {3}, and {2,4} are all mutually exclusive), the probability that any of these events occurs is given by the sum of the probabilities of the events.[7]
The probability that any one of the events {1,6}, {3}, or {2,4} will occur is 5/6. This is the same as saying that the probability of event {1,2,3,4,6} is 5/6. This event encompasses the possibility of any number except five being rolled. The mutually exclusive event {5} has a probability of 1/6, and the event {1,2,3,4,5,6} has a probability of 1, that is, absolute certainty.
When doing calculations using the outcomes of an experiment, it is necessary that all thoseelementary eventshave a number assigned to them. This is done using arandom variable. A random variable is a function that assigns to each elementary event in the sample space areal number. This function is usually denoted by a capital letter.[8]In the case of a die, the assignment of a number to certain elementary events can be done using theidentity function. This does not always work. For example, whenflipping a cointhe two possible outcomes are "heads" and "tails". In this example, the random variableXcould assign to the outcome "heads" the number "0" (X(heads)=0{\textstyle X({\text{heads}})=0}) and to the outcome "tails" the number "1" (X(tails)=1{\displaystyle X({\text{tails}})=1}).
Discrete probability theorydeals with events that occur incountablesample spaces.
Examples: Throwingdice, experiments withdecks of cards,random walk, and tossingcoins.
Classical definition:
Initially the probability of an event to occur was defined as the number of cases favorable for the event, over the number of total outcomes possible in an equiprobable sample space: seeClassical definition of probability.
For example, if the event is "occurrence of an even number when a dice is rolled", the probability is given by36=12{\displaystyle {\tfrac {3}{6}}={\tfrac {1}{2}}}, since 3 faces out of the 6 have even numbers and each face has the same probability of appearing.
Modern definition:
The modern definition starts with afinite or countable setcalled thesample space, which relates to the set of allpossible outcomesin classical sense, denoted byΩ{\displaystyle \Omega }. It is then assumed that for each elementx∈Ω{\displaystyle x\in \Omega \,}, an intrinsic "probability" valuef(x){\displaystyle f(x)\,}is attached, which satisfies the following properties:
That is, the probability functionf(x) lies between zero and one for every value ofxin the sample spaceΩ, and the sum off(x) over all valuesxin the sample spaceΩis equal to 1. Aneventis defined as anysubsetE{\displaystyle E\,}of the sample spaceΩ{\displaystyle \Omega \,}. Theprobabilityof the eventE{\displaystyle E\,}is defined as
So, the probability of the entire sample space is 1, and the probability of the null event is 0.
The functionf(x){\displaystyle f(x)\,}mapping a point in the sample space to the "probability" value is called aprobability mass functionabbreviated aspmf.
Continuous probability theorydeals with events that occur in a continuous sample space.
Classical definition:
The classical definition breaks down when confronted with the continuous case. SeeBertrand's paradox.
Modern definition:
If the sample space of a random variableXis the set ofreal numbers(R{\displaystyle \mathbb {R} }) or a subset thereof, then a function called thecumulative distribution function(CDF)F{\displaystyle F\,}exists, defined byF(x)=P(X≤x){\displaystyle F(x)=P(X\leq x)\,}. That is,F(x) returns the probability thatXwill be less than or equal tox.
The CDF necessarily satisfies the following properties.
The random variableX{\displaystyle X}is said to have a continuous probability distribution if the corresponding CDFF{\displaystyle F}is continuous. IfF{\displaystyle F\,}isabsolutely continuous, then its derivative exists almost everywhere and integrating the derivative gives us the CDF back again. In this case, the random variableXis said to have aprobability density function(PDF) or simplydensityf(x)=dF(x)dx.{\displaystyle f(x)={\frac {dF(x)}{dx}}\,.}
For a setE⊆R{\displaystyle E\subseteq \mathbb {R} }, the probability of the random variableXbeing inE{\displaystyle E\,}is
Whereas thePDFexists only for continuous random variables, theCDFexists for all random variables (including discrete random variables) that take values inR.{\displaystyle \mathbb {R} \,.}
These concepts can be generalized formultidimensionalcases onRn{\displaystyle \mathbb {R} ^{n}}and other continuous sample spaces.
The utility of the measure-theoretic treatment of probability is that it unifies the discrete and the continuous cases, and makes the difference a question of which measure is used. Furthermore, it covers distributions that are neither discrete nor continuous nor mixtures of the two.
An example of such distributions could be a mix of discrete and continuous distributions—for example, a random variable that is 0 with probability 1/2, and takes a random value from a normal distribution with probability 1/2. It can still be studied to some extent by considering it to have a PDF of(δ[x]+φ(x))/2{\displaystyle (\delta [x]+\varphi (x))/2}, whereδ[x]{\displaystyle \delta [x]}is theDirac delta function.
Other distributions may not even be a mix, for example, theCantor distributionhas no positive probability for any single point, neither does it have a density. The modern approach to probability theory solves these problems usingmeasure theoryto define theprobability space:
Given any setΩ{\displaystyle \Omega \,}(also calledsample space) and aσ-algebraF{\displaystyle {\mathcal {F}}\,}on it, ameasureP{\displaystyle P\,}defined onF{\displaystyle {\mathcal {F}}\,}is called aprobability measureifP(Ω)=1.{\displaystyle P(\Omega )=1.\,}
IfF{\displaystyle {\mathcal {F}}\,}is theBorel σ-algebraon the set of real numbers, then there is a unique probability measure onF{\displaystyle {\mathcal {F}}\,}for any CDF, and vice versa. The measure corresponding to a CDF is said to beinducedby the CDF. This measure coincides with the pmf for discrete variables and PDF for continuous variables, making the measure-theoretic approach free of fallacies.
Theprobabilityof a setE{\displaystyle E\,}in the σ-algebraF{\displaystyle {\mathcal {F}}\,}is defined as
where the integration is with respect to the measureμF{\displaystyle \mu _{F}\,}induced byF.{\displaystyle F\,.}
Along with providing better understanding and unification of discrete and continuous probabilities, measure-theoretic treatment also allows us to work on probabilities outsideRn{\displaystyle \mathbb {R} ^{n}}, as in the theory ofstochastic processes. For example, to studyBrownian motion, probability is defined on a space of functions.
When it is convenient to work with a dominating measure, theRadon-Nikodym theoremis used to define a density as the Radon-Nikodym derivative of the probability distribution of interest with respect to this dominating measure.  Discrete densities are usually defined as this derivative with respect to acounting measureover the set of all possible outcomes.  Densities forabsolutely continuousdistributions are usually defined as this derivative with respect to theLebesgue measure.  If a theorem can be proved in this general setting, it holds for both discrete and continuous distributions as well as others;  separate proofs are not required for discrete and continuous distributions.
Certain random variables occur very often in probability theory because they well describe many natural or physical processes. Their distributions, therefore, have gainedspecial importancein probability theory. Some fundamentaldiscrete distributionsare thediscrete uniform,Bernoulli,binomial,negative binomial,Poissonandgeometric distributions. Importantcontinuous distributionsinclude thecontinuous uniform,normal,exponential,gammaandbeta distributions.
In probability theory, there are several notions of convergence forrandom variables. They are listed below in the order of strength, i.e., any subsequent notion of convergence in the list implies convergence according to all of the preceding notions.
As the names indicate, weak convergence is weaker than strong convergence. In fact, strong convergence implies convergence in probability, and convergence in probability implies weak convergence. The reverse statements are not always true.
Common intuition suggests that if a fair coin is tossed many times, thenroughlyhalf of the time it will turn upheads, and the other half it will turn uptails. Furthermore, the more often the coin is tossed, the more likely it should be that the ratio of the number ofheadsto the number oftailswill approach unity. Modern probability theory provides a formal version of this intuitive idea, known as thelaw of large numbers. This law is remarkable because it is not assumed in the foundations of probability theory, but instead emerges from these foundations as a theorem. Since it links theoretically derived probabilities to their actual frequency of occurrence in the real world, the law of large numbers is considered as a pillar in the history of statistical theory and has had widespread influence.[9]
Thelaw of large numbers(LLN) states that the sample average
of asequenceofindependent and identically distributed random variablesXk{\displaystyle X_{k}}converges towards their commonexpectation(expected value)μ{\displaystyle \mu }, provided that the expectation of|Xk|{\displaystyle |X_{k}|}is finite.
It is in the different forms ofconvergence of random variablesthat separates theweakand thestronglaw of large numbers[10]
It follows from the LLN that if an event of probabilitypis observed repeatedly during independent experiments, the ratio of the observed frequency of that event to the total number of repetitions converges towardsp.
For example, ifY1,Y2,...{\displaystyle Y_{1},Y_{2},...\,}are independentBernoulli random variablestaking values 1 with probabilitypand 0 with probability 1-p, thenE(Yi)=p{\displaystyle {\textrm {E}}(Y_{i})=p}for alli, so thatY¯n{\displaystyle {\bar {Y}}_{n}}converges topalmost surely.
The central limit theorem (CLT) explains the ubiquitous occurrence of thenormal distributionin nature, and this theorem, according to David Williams, "is one of the great results of mathematics."[11]
The theorem states that theaverageof many independent and identically distributed random variables with finite variance tends towards a normal distributionirrespectiveof the distribution followed by the original random variables. Formally, letX1,X2,…{\displaystyle X_{1},X_{2},\dots \,}be independent random variables withmeanμ{\displaystyle \mu }andvarianceσ2>0.{\displaystyle \sigma ^{2}>0.\,}Then the sequence of random variables
converges in distribution to astandard normalrandom variable.
For some classes of random variables, the classic central limit theorem works rather fast, as illustrated in theBerry–Esseen theorem. For example, the distributions with finite  first, second, and third moment from theexponential family; on the other hand, for some random variables of theheavy tailandfat tailvariety, it works very slowly or may not work at all: in such cases one may use theGeneralized Central Limit Theorem(GCLT).
Statistics(fromGerman:Statistik,orig."description of astate, a country"[1]) is the discipline that concerns the collection, organization, analysis, interpretation, and presentation ofdata.[2]In applying statistics to a scientific, industrial, or social problem, it is conventional to begin with astatistical populationor astatistical modelto be studied. Populations can be diverse groups of people or objects such as "all people living in a country" or "every atom composing a crystal". Statistics deals with every aspect of data, including the planning of data collection in terms of the design ofsurveysandexperiments.[3]
Whencensusdata (comprising every member of the target population) cannot be collected,statisticianscollect data by developing specific experiment designs and surveysamples. Representative sampling assures that inferences and conclusions can reasonably extend from the sample to the population as a whole. Anexperimental studyinvolves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, anobservational studydoes not involve experimental manipulation.
Two main statistical methods are used indata analysis:descriptive statistics, which summarize data from a sample usingindexessuch as themeanorstandard deviation, andinferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, sampling variation).[4]Descriptive statistics are most often concerned with two sets of properties of adistribution(sample or population):central tendency(orlocation) seeks to characterize the distribution's central or typical value, whiledispersion(orvariability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences made usingmathematical statisticsemploy the framework ofprobability theory, which deals with the analysis of random phenomena.
A standard statistical procedure involves the collection of data leading to atest of the relationshipbetween two statistical data sets, or a data set and synthetic data drawn from an idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, analternativeto an idealizednull hypothesisof no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized:Type I errors(null hypothesis is rejected when it is in fact true, giving a "false positive") andType II errors(null hypothesis fails to be rejected when it is in fact false, giving a "false negative"). Multiple problems have come to be associated with this framework, ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis.[4]
Statistical measurement processes are also prone to error in regards to the data that they generate. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also occur. The presence ofmissing dataorcensoringmay result in biased estimates and specific techniques have been developed to address these problems.
"Statistics is both the science of uncertainty and the technology of extracting information from data." - featured in the International Encyclopedia of Statistical Science.[5]
Statistics is the discipline that deals withdata, facts and figures with which meaningful information is inferred. Data may represent a numerical value, in form of quantitative data, or a label, as with qualitative data. Data may be collected, presented and summarised, in one of two methods called descriptive statistics. Two elementary summaries of data, singularly called a statistic, are the mean and dispersion. Whereas inferential statistics interprets data from a population sample to induce statements and predictions about a population.[6][7][5]
Statistics is regarded as a body of science[8]or a branch of mathematics.[9]It is based on probability, a branch of mathematics that studies random events. Statistics is considered the science of uncertainty. This arises from the ways to cope with measurement and sampling error as well as dealing with uncertanties in modelling. Although probability and statistics was once paired together as a single subject, they are conceptually distinct from one another. The former is based on deducing answers to specific situations from a general theory of probability, meanwhile statistics induces statements about a population based on a data set. Statistics serves to bridge the gap between probability and applied mathematical fields.[10][5][11]
Some consider statistics to be a distinctmathematical sciencerather than a branch of mathematics. While many scientific investigations make use of data, statistics is generally concerned with the use of data in the context of uncertainty and decision-making in the face of uncertainty.[12][13]Statistics is indexed at 62, a subclass of probability theory and stochastic processes, in the Mathematics Subject Classification.[14]Mathematical statistics is covered in the range 276-280 of subclass QA (science > mathematics) in the Library of Congress Classification.[15]
The word statistics ultimately comes from the Latin word Status, meaning "situation" or "condition" in society, which in late Latin adopted the meaning "state". Derived from this, political scientist Gottfried Achenwall, coined the German word statistik (a summary of how things stand). In 1770, the term entered the English language through German and referred to the study of political arrangements. The term gained its modern meaning in the 1790s in John Sinclair's works.[16][17]In modern German, the term statistik is synonymous with mathematical statistics. The term statistic, in singular form, is used to describe a function that returns its value of the same name.[18]
When full census data cannot be collected, statisticians collect sample data by developing specificexperiment designsandsurvey samples. Statistics itself also provides tools for prediction and forecasting throughstatistical models.
To use a sample as a guide to an entire population, it is important that it truly represents the overall population. Representativesamplingassures that inferences and conclusions can safely extend from the sample to the population as a whole. A major problem lies in determining the extent that the sample chosen is actually representative. Statistics offers methods to estimate and correct for any bias within the sample and data collection procedures. There are also methods of experimental design that can lessen these issues at the outset of a study, strengthening its capability to discern truths about the population.
Sampling theory is part of themathematical disciplineofprobability theory. Probability is used inmathematical statisticsto study thesampling distributionsofsample statisticsand, more generally, the properties ofstatistical procedures. The use of any statistical method is valid when the system or population under consideration satisfies the assumptions of the method. The difference in point of view between classic probability theory and sampling theory is, roughly, that probability theory starts from the given parameters of a total population todeduceprobabilities that pertain to samples. Statistical inference, however, moves in the opposite direction—inductively inferringfrom samples to the parameters of a larger or total population.
A common goal for a statistical research project is to investigatecausality, and in particular to draw a conclusion on the effect of changes in the values of predictors orindependent variables on dependent variables. There are two major types of causal statistical studies:experimental studiesandobservational studies. In both types of studies, the effect of differences of an independent variable (or variables) on the behavior of the dependent variable are observed. The difference between the two types lies in how the study is actually conducted. Each can be very effective. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additionalmeasurements with different levelsusing the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involveexperimental manipulation. Instead, data are gathered and correlations between predictors and response are investigated. While the tools of data analysis work best on data fromrandomized studies, they are also applied to other kinds of data—likenatural experimentsandobservational studies[19]—for which a statistician would use a modified, more structured estimation method (e.g.,difference in differences estimationandinstrumental variables, among many others) that produceconsistent estimators.
Experiments on human behavior have special concerns. The famousHawthorne studyexamined changes to the working environment at the Hawthorne plant of theWestern Electric Company. The researchers were interested in determining whether increased illumination would increase the productivity of theassembly lineworkers. The researchers first measured the productivity in the plant, then modified the illumination in an area of the plant and checked if the changes in illumination affected productivity. It turned out that productivity indeed improved (under the experimental conditions). However, the study is heavily criticized today for errors in experimental procedures, specifically for the lack of acontrol groupandblindness. TheHawthorne effectrefers to finding that an outcome (in this case, worker productivity) changed due to observation itself. Those in the Hawthorne study became more productive not because the lighting was changed but because they were being observed.[20]
An example of an observational study is one that explores the association between smoking and lung cancer. This type of study typically uses a survey to collect observations about the area of interest and then performs statistical analysis. In this case, the researchers would collect observations of both smokers and non-smokers, perhaps through acohort study, and then look for the number of cases of lung cancer in each group.[21]Acase-control studyis another type of observational study in which people with and without the outcome of interest (e.g. lung cancer) are invited to participate and their exposure histories are collected.
Various attempts have been made to produce a taxonomy oflevels of measurement. The psychophysicistStanley Smith Stevensdefined nominal, ordinal, interval, and ratio scales. Nominal measurements do not have meaningful rank order among values, and permit any one-to-one (injective) transformation. Ordinal measurements have imprecise differences between consecutive values, but have a meaningful order to those values, and permit any order-preserving transformation. Interval measurements have meaningful distances between measurements defined, but the zero value is arbitrary (as in the case withlongitudeandtemperaturemeasurements inCelsiusorFahrenheit), and permit any linear transformation. Ratio measurements have both a meaningful zero value and the distances between different measurements defined, and permit any rescaling transformation.
Because variables conforming only to nominal or ordinal measurements cannot be reasonably measured numerically, sometimes they are grouped together ascategorical variables, whereas ratio and interval measurements are grouped together asquantitative variables, which can be eitherdiscreteorcontinuous, due to their numerical nature. Such distinctions can often be loosely correlated withdata typein computer science, in that dichotomous categorical variables may be represented with theBoolean data type, polytomous categorical variables with arbitrarily assignedintegersin theintegral data type, and continuous variables with thereal data typeinvolvingfloating-point arithmetic. But the mapping of computer science data types to statistical data types depends on which categorization of the latter is being implemented.
Other categorizations have been proposed. For example, Mosteller and Tukey (1977)[22]distinguished grades, ranks, counted fractions, counts, amounts, and balances. Nelder (1990)[23]described continuous counts, continuous ratios, count ratios, and categorical modes of data. (See also: Chrisman (1998),[24]van den Berg (1991).[25])
The issue of whether or not it is appropriate to apply different kinds of statistical methods to data obtained from different kinds of measurement procedures is complicated by issues concerning the transformation of variables and the precise interpretation of research questions. "The relationship between the data and what they describe merely reflects the fact that certain kinds of statistical statements may have truth values which are not invariant under some transformations. Whether or not a transformation is sensible to contemplate depends on the question one is trying to answer."[26]: 82
Adescriptive statistic(in thecount nounsense) is asummary statisticthat quantitatively describes or summarizes features of a collection ofinformation,[27]whiledescriptive statisticsin themass nounsense is the process of using and analyzing those statistics. Descriptive statistics is distinguished frominferential statistics(or inductive statistics), in that descriptive statistics aims to summarize asample, rather than use the data to learn about thepopulationthat the sample of data is thought to represent.[28]
Statistical inferenceis the process of usingdata analysisto deduce properties of an underlyingprobability distribution.[29]Inferential statistical analysis infers properties of apopulation, for example by testing hypotheses and deriving estimates.  It is assumed that the observed data set issampledfrom a larger population. Inferential statistics can be contrasted withdescriptive statistics. Descriptive statistics is solely concerned with properties of the observed data, and it does not rest on the assumption that the data come from a larger population.[30]
Considerindependent identically distributed (IID) random variableswith a givenprobability distribution: standardstatistical inferenceandestimation theorydefines arandom sampleas therandom vectorgiven by thecolumn vectorof these IID variables.[31]Thepopulationbeing examined is described by a probability distribution that may have unknown parameters.
A statistic is a random variable that is a function of the random sample, butnot a function of unknown parameters. The probability distribution of the statistic, though, may have unknown parameters. Consider now a function of the unknown parameter: anestimatoris a statistic used to estimate such function. Commonly used estimators includesample mean, unbiasedsample varianceandsample covariance.
A random variable that is a function of the random sample and of the unknown parameter, but whose probability distributiondoes not depend on the unknown parameteris called apivotal quantityor pivot. Widely used pivots include thez-score, thechi square statisticand Student'st-value.
Between two estimators of a given parameter, the one with lowermean squared erroris said to be moreefficient. Furthermore, an estimator is said to beunbiasedif itsexpected valueis equal to thetrue valueof the unknown parameter being estimated, and asymptotically unbiased if its expected value converges at thelimitto the true value of such parameter.
Other desirable properties for estimators include:UMVUEestimators that have the lowest variance for all possible values of the parameter to be estimated (this is usually an easier property to verify than efficiency) andconsistent estimatorswhichconverges in probabilityto the true value of such parameter.
This still leaves the question of how to obtain estimators in a given situation and carry the computation, several methods have been proposed: themethod of moments, themaximum likelihoodmethod, theleast squaresmethod and the more recent method ofestimating equations.
Interpretation of statistical information can often involve the development of anull hypothesiswhich is usually (but not necessarily) that no relationship exists among variables or that no change occurred over time.[32][33]
The best illustration for a novice is the predicament encountered by a criminal trial. The null hypothesis, H0, asserts that the defendant is innocent, whereas the alternative hypothesis, H1, asserts that the defendant is guilty. The indictment comes because of suspicion of the guilt. The H0(status quo) stands in opposition to H1and is maintained unless H1is supported by evidence "beyond a reasonable doubt". However, "failure to reject H0" in this case does not imply innocence, but merely that the evidence was insufficient to convict. So the jury does not necessarilyacceptH0butfails to rejectH0. While one can not "prove" a null hypothesis, one can test how close it is to being true with apower test, which tests fortype II errors.
Whatstatisticianscall analternative hypothesisis simply a hypothesis that contradicts the null hypothesis.
Working from anull hypothesis, two broad categories of error are recognized:
Standard deviationrefers to the extent to which individual observations in a sample differ from a central value, such as the sample or population mean, whileStandard errorrefers to an estimate of difference between sample mean and population mean.
Astatistical erroris the amount by which an observation differs from itsexpected value. Aresidualis the amount an observation differs from the value the estimator of the expected value assumes on a given sample (also called prediction).
Mean squared erroris used for obtainingefficient estimators, a widely used class of estimators.Root mean square erroris simply the square root of mean squared error.
Many statistical methods seek to minimize theresidual sum of squares, and these are called "methods of least squares" in contrast toLeast absolute deviations. The latter gives equal weight to small and big errors, while the former gives more weight to large errors. Residual sum of squares is alsodifferentiable, which provides a handy property for doingregression. Least squares applied tolinear regressionis calledordinary least squaresmethod and least squares applied tononlinear regressionis callednon-linear least squares. Also in a linear regression model the non deterministic part of the model is called error term, disturbance or more simply noise. Both linear regression and non-linear regression are addressed inpolynomial least squares, which also describes the variance in a prediction of the dependent variable (y axis) as a function of the independent variable (x axis) and the deviations (errors, noise, disturbances) from the estimated (fitted) curve.
Measurement processes that generate statistical data are also subject to error.  Many of these errors are classified asrandom(noise) orsystematic(bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence ofmissing dataorcensoringmay result inbiased estimatesand specific techniques have been developed to address these problems.[34]
Most studies only sample part of a population, so results do not fully represent the whole population. Any estimates obtained from the sample only approximate the population value.Confidence intervalsallow statisticians to express how closely the sample estimate matches the true value in the whole population. Often they are expressed as 95% confidence intervals. Formally, a 95% confidence interval for a value is a range where, if the sampling and analysis were repeated under the same conditions (yielding a different dataset), the interval would include the true (population) value in 95% of all possible cases. This doesnotimply that the probability that the true value is in the confidence interval is 95%. From thefrequentistperspective, such a claim does not even make sense, as the true value is not arandom variable.  Either the true value is or is not within the given interval. However, it is true that, before any data are sampled and given a plan for how to construct the confidence interval, the probability is 95% that the yet-to-be-calculated interval will cover the true value: at this point, the limits of the interval are yet-to-be-observedrandom variables. One approach that does yield an interval that can be interpreted as having a given probability of containing the true value is to use acredible intervalfromBayesian statistics: this approach depends on a different way ofinterpreting what is meant by "probability", that is as aBayesian probability.
In principle confidence intervals can be symmetrical or asymmetrical. An interval can be asymmetrical because it works as lower or upper bound for a parameter (left-sided interval or right sided interval), but it can also be asymmetrical because the two sided interval is built violating symmetry around the estimate. Sometimes the bounds for a confidence interval are reached asymptotically and these are used to approximate the true bounds.
Statistics rarely give a simple Yes/No type answer to the question under analysis. Interpretation often comes down to the level of statistical significance applied to the numbers and often refers to the probability of a value accurately rejecting the null hypothesis (sometimes referred to as thep-value).
The standard approach[31]is to test a null hypothesis against an alternative hypothesis. Acritical regionis the set of values of the estimator that leads to refuting the null hypothesis. The probability of type I error is therefore the probability that the estimator belongs to the critical region given that null hypothesis is true (statistical significance) and the probability of type II error is the probability that the estimator does not belong to the critical region given that the alternative hypothesis is true. Thestatistical powerof a test is the probability that it correctly rejects the null hypothesis when the null hypothesis is false.
Referring to statistical significance does not necessarily mean that the overall result is significant in real world terms. For example, in a large study of a drug it may be shown that the drug has a statistically significant but very small beneficial effect, such that the drug is unlikely to help the patient noticeably.
Although in principle the acceptable level of statistical significance may be subject to debate, thesignificance levelis the largest p-value that allows the test to reject the null hypothesis. This test is logically equivalent to saying that the p-value is the probability, assuming the null hypothesis is true, of observing a result at least as extreme as thetest statistic. Therefore, the smaller the significance level, the lower the probability of committing type I error.
Some problems are usually associated with this framework (Seecriticism of hypothesis testing):
Some well-known statisticaltestsand procedures are:
An alternative paradigm to the popularfrequentistparadigm is to useBayes' theoremto update theprior probabilityof the hypotheses in consideration based on therelative likelihoodof the evidence gathered to obtain aposterior probability. Bayesian methods have been aided by the increase in available computing power to compute theposterior probabilityusing numerical approximation techniques likeMarkov Chain Monte Carlo.
For statistically modelling purposes, Bayesian models tend to behierarchical, for example, one could model eachYoutubechannel as having video views distributed as a normal distribution with channel dependent mean and varianceN(μi,σi){\displaystyle {\mathcal {N}}(\mu _{i},\sigma _{i})}, while modeling the channel means as themselves coming from a normal distribution representing the distribution of average video view counts per channel, and the variances as coming from another distribution.
The concept of usinglikelihood ratiocan also be prominently seen inmedical diagnostic testing.
Exploratory data analysis(EDA) is an approach toanalyzingdata setsto summarize their main characteristics, often with visual methods. Astatistical modelcan be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task.
Mathematical statistics is the application of mathematics to statistics. Mathematical techniques used for this includemathematical analysis,linear algebra,stochastic analysis,differential equations, andmeasure-theoretic probability theory.[1][7]All statistical analyses make use of at least some mathematics, and mathematical statistics can therefore be regarded as a fundamental component of general statistics.[8]
Formal discussions on inference date back to themathematiciansandcryptographersof theIslamic Golden Agebetween the 8th and 13th centuries.Al-Khalil(717–786) wrote theBook of Cryptographic Messages, which contains one of the first uses ofpermutationsandcombinations, to list all possible Arabic words with and without vowels.[36]Al-Kindi'sManuscript on Deciphering Cryptographic Messagesgave a detailed description of how to usefrequency analysisto decipherencryptedmessages, providing an early example ofstatistical inferencefordecoding.Ibn Adlan(1187–1268) later made an important contribution on the use ofsample sizein frequency analysis.[36]
Although the termstatisticwas introduced by the Italian scholarGirolamo Ghiliniin 1589 with reference to a collection of facts and information about a state, it was the GermanGottfried Achenwallin 1749 who started using the term as a collection of quantitative information, in the modern use for this science.[37][38]The earliest writing containing statistics in Europe dates back to 1663, with the publication ofNatural and Political Observations upon the Bills of MortalitybyJohn Graunt.[39]Early applications of statistical thinking revolved around the needs of states to base policy on demographic and economic data, hence itsstat-etymology. The scope of the discipline of statistics broadened in the early 19th century to include the collection and analysis of data in general. Today, statistics is widely employed in government, business, and natural and social sciences.
The mathematical foundations of statistics developed from discussions concerninggames of chanceamong mathematicians such asGerolamo Cardano,Blaise Pascal,Pierre de Fermat, andChristiaan Huygens. Although the idea of probability was already examined in ancient and medieval law and philosophy (such as the work ofJuan Caramuel),probability theoryas a mathematical discipline only took shape at the very end of the 17th century, particularly inJacob Bernoulli's posthumous workArs Conjectandi.[40]This was the first book where the realm of games of chance and the realm of the probable (which concerned opinion, evidence, and argument) were combined and submitted to mathematical analysis.[41]Themethod of least squareswas first described byAdrien-Marie Legendrein 1805, thoughCarl Friedrich Gausspresumably made use of it a decade earlier in 1795.[42]
The modern field of statistics emerged in the late 19th and early 20th century in three stages.[43]The first wave, at the turn of the century, was led by the work ofFrancis GaltonandKarl Pearson, who transformed statistics into a rigorous mathematical discipline used for analysis, not just in science, but in industry and politics as well. Galton's contributions included introducing the concepts ofstandard deviation,correlation,regression analysisand the application of these methods to the study of the variety of human characteristics—height, weight and eyelash length among others.[44]Pearson developed thePearson product-moment correlation coefficient, defined as a product-moment,[45]themethod of momentsfor the fitting of distributions to samples and thePearson distribution, among many other things.[46]Galton and Pearson foundedBiometrikaas the first journal of mathematical statistics andbiostatistics(then calledbiometry), and the latter founded the world's first university statistics department atUniversity College London.[47]
The second wave of the 1910s and 20s was initiated byWilliam Sealy Gosset, and reached its culmination in the insights ofRonald Fisher, who wrote the textbooks that were to define the academic discipline in universities around the world. Fisher's most important publications were his 1918 seminal paperThe Correlation between Relatives on the Supposition of Mendelian Inheritance(which was the first to use the statistical term,variance), his classic 1925 workStatistical Methods for Research Workersand his 1935The Design of Experiments,[48][49][50]where he developed rigorousdesign of experimentsmodels. He originated the concepts ofsufficiency,ancillary statistics,Fisher's linear discriminatorandFisher information.[51]He also coined the termnull hypothesisduring theLady tasting teaexperiment, which "is never proved or established, but is possibly disproved, in the course of experimentation".[52][53]In his 1930 bookThe Genetical Theory of Natural Selection, he applied statistics to variousbiologicalconcepts such asFisher's principle[54](whichA. W. F. Edwardscalled "probably the most celebrated argument inevolutionary biology") andFisherian runaway,[55][56][57][58][59][60]a concept insexual selectionabout a positive feedback runaway effect found inevolution.
The final wave, which mainly saw the refinement and expansion of earlier developments, emerged from the collaborative work betweenEgon PearsonandJerzy Neymanin the 1930s. They introduced the concepts of "Type II" error,power of a testandconfidence intervals. Jerzy Neyman in 1934 showed that stratified random sampling was in general a better method of estimation than purposive (quota) sampling.[61]
Today, statistical methods are applied in all fields that involve decision making, for making accurate inferences from a collated body of data and for making decisions in the face of uncertainty based on statistical methodology. The use of moderncomputershas expedited large-scale statistical computations and has also made possible new methods that are impractical to perform manually. Statistics continues to be an area of active research, for example on the problem of how to analyzebig data.[62]
Applied statistics,sometimes referred to asStatistical science,[63]comprises descriptive statistics and the application of inferential statistics.[64][65]Theoretical statisticsconcerns the logical arguments underlying justification of approaches tostatistical inference, as well as encompassingmathematical statistics. Mathematical statistics includes not only the manipulation ofprobability distributionsnecessary for deriving results related to methods of estimation and inference, but also various aspects ofcomputational statisticsand thedesign of experiments.
Statistical consultantscan help organizations and companies that do not have in-house expertise relevant to their particular questions.
Machine learningmodels are statistical and probabilistic models that capture patterns in the data through use of computational algorithms.
Statistics is applicable to a wide variety ofacademic disciplines, includingnaturalandsocial sciences, government, and business. Business statistics applies statistical methods ineconometrics,auditingand production and operations, including services improvement and marketing research.[66]A study of two journals in tropical biology found that the 12 most frequent statistical tests are:analysis of variance(ANOVA),chi-squared test,Student's t-test,linear regression,Pearson's correlation coefficient,Mann-Whitney U test,Kruskal-Wallis test,Shannon's diversity index,Tukey's range test,cluster analysis,Spearman's rank correlation coefficientandprincipal component analysis.[67]
A typical statistics course covers descriptive statistics, probability, binomial andnormal distributions, test of hypotheses and confidence intervals,linear regression, and correlation.[68]Modern fundamental statistical courses for undergraduate students focus on correct test selection, results interpretation, and use offree statistics software.[67]
The rapid and sustained increases in computing power starting from the second half of the 20th century have had a substantial impact on the practice of statistical science. Early statistical models were almost always from the class oflinear models, but powerful computers, coupled with suitable numericalalgorithms, caused an increased interest innonlinear models(such asneural networks) as well as the creation of new types, such asgeneralized linear modelsandmultilevel models.
Increased computing power has also led to the growing popularity of computationally intensive methods based onresampling, such aspermutation testsand thebootstrap, while techniques such asGibbs samplinghave made use ofBayesian modelsmore feasible. The computer revolution has implications for the future of statistics with a new emphasis on "experimental" and "empirical" statistics. A large number of both general and special purposestatistical softwareare now available. Examples of available software capable of complex statistical computation include programs such asMathematica,SAS,SPSS, andR.
In business, "statistics" is a widely usedmanagement-anddecision supporttool. It is particularly applied infinancial management,marketing management, andproduction,servicesandoperations management.[69][70]Statistics is also heavily used inmanagement accountingandauditing. The discipline ofManagement Scienceformalizes the use of statistics, and other mathematics, in business. (Econometricsis the application of statistical methods toeconomic datain order to give empirical content toeconomic relationships.)
A typical "Business Statistics" course is intended forbusiness majors, and covers[71]descriptive statistics(collection, description, analysis, and summary of data), probability (typically thebinomialandnormal distributions), test of hypotheses and confidence intervals,linear regression, and correlation; (follow-on) courses may includeforecasting,time series,decision trees,multiple linear regression, and other topics frombusiness analyticsmore generally.Professional certification programs, such as theCFA, often include topics in statistics.
Statistical techniques are used in a wide range of types of scientific and social research, including:biostatistics,computational biology,computational sociology,network biology,social science,sociologyandsocial research. Some fields of inquiry use applied statistics so extensively that they havespecialized terminology. These disciplines include:
In addition, there are particular types of statistical analysis that have also developed their own specialised terminology and methodology:
Statistics form a key basis tool in business and manufacturing as well. It is used to understand measurement systems variability, control processes (as instatistical process controlor SPC), for summarizing data, and to make data-driven decisions.
Misuse of statisticscan produce subtle but serious errors in description and interpretation—subtle in the sense that even experienced professionals make such errors, and serious in the sense that they can lead to devastating decision errors. For instance, social policy, medical practice, and the reliability of structures like bridges all rely on the proper use of statistics.
Even when statistical techniques are correctly applied, the results can be difficult to interpret for those lacking expertise. Thestatistical significanceof a trend in the data—which measures the extent to which a trend could be caused by random variation in the sample—may or may not agree with an intuitive sense of its significance. The set of basic statistical skills (and skepticism) that people need to deal with information in their everyday lives properly is referred to asstatistical literacy.
There is a general perception that statistical knowledge is all-too-frequently intentionallymisusedby finding ways to interpret only the data that are favorable to the presenter.[72]A mistrust and misunderstanding of statistics is associated with the quotation, "There are three kinds of lies: lies, damned lies, and statistics". Misuse of statistics can be both inadvertent and intentional, and the bookHow to Lie with Statistics,[72]byDarrell Huff, outlines a range of considerations. In an attempt to shed light on the use and misuse of statistics, reviews of statistical techniques used in particular fields are conducted (e.g. Warne, Lazo, Ramos, and Ritter (2012)).[73]
Ways to avoid misuse of statistics include using proper diagrams and avoidingbias.[74]Misuse can occur when conclusions areovergeneralizedand claimed to be representative of more than they really are, often by either deliberately or unconsciously overlooking sampling bias.[75]Bar graphs are arguably the easiest diagrams to use and understand, and they can be made either by hand or with simple computer programs.[74]Most people do not look for bias or errors, so they are not noticed. Thus, people may often believe that something is true even if it is not wellrepresented.[75]To make data gathered from statistics believable and accurate, the sample taken must be representative of the whole.[76]According to Huff, "The dependability of a sample can be destroyed by [bias]... allow yourself some degree of skepticism."[77]
To assist in the understanding of statistics Huff proposed a series of questions to be asked in each case:[72]
The concept ofcorrelationis particularly noteworthy for the potential confusion it can cause. Statistical analysis of adata setoften reveals that two variables (properties) of the population under consideration tend to vary together, as if they were connected. For example, a study of annual income that also looks at age of death, might find that poor people tend to have shorter lives than affluent people. The two variables are said to be correlated; however, they may or may not be the cause of one another. The correlation phenomena could be caused by a third, previously unconsidered phenomenon, called a lurking variable orconfounding variable. For this reason, there is no way to immediately infer the existence of a causal relationship between the two variables.
Set theoryis the branch ofmathematical logicthat studiessets, which can be informally described as collections of objects. Although objects of any kind can be collected into a set, set theory – as a branch ofmathematics– is mostly concerned with those that are relevant to mathematics as a whole.
The modern study of set theory was initiated by the German mathematiciansRichard DedekindandGeorg Cantorin the 1870s. In particular, Georg Cantor is commonly considered the founder of set theory. The non-formalized systems investigated during this early stage go under the name ofnaive set theory. After the discovery ofparadoxes within naive set theory(such asRussell's paradox,Cantor's paradoxand theBurali-Forti paradox), variousaxiomatic systemswere proposed in the early twentieth century, of whichZermelo–Fraenkel set theory(with or without theaxiom of choice) is still the best-known and most studied.
Set theory is commonly employed as a foundational system for the whole of mathematics, particularly in the form of Zermelo–Fraenkel set theory with the axiom of choice. Besides its foundational role, set theory also provides the framework to develop a mathematical theory ofinfinity, and has various applications incomputer science(such as in the theory ofrelational algebra),philosophy,formal semantics, andevolutionary dynamics. Its foundational appeal, together with itsparadoxes, and its implications for the concept of infinity and its multiple applications have made set theory an area of major interest forlogiciansandphilosophers of mathematics. Contemporary research into set theory covers a vast array of topics, ranging from the structure of thereal numberline to the study of theconsistencyoflarge cardinals.
The basic notion of grouping objects has existed since at least theemergence of numbers, and the notion of treating sets as their own objects has existed since at least theTree of Porphyry, 3rd-century AD. The simplicity and ubiquity of sets makes it hard to determine the origin of sets as now used in mathematics, however,Bernard Bolzano'sParadoxes of the Infinite(Paradoxien des Unendlichen, 1851) is generally considered the first rigorous introduction of sets to mathematics. In his work, he (among other things) expanded onGalileo's paradox, and introducedone-to-one correspondenceof infinite sets, for example between theintervals[0,5]{\displaystyle [0,5]}and[0,12]{\displaystyle [0,12]}by the relation5y=12x{\displaystyle 5y=12x}. However, he resisted saying these sets wereequinumerous, and his work is generally considered to have been uninfluential in mathematics of his time.[1][2]
Before mathematical set theory, basic concepts ofinfinitywere considered to be solidly in the domain of philosophy (see:Infinity (philosophy)andInfinity § History). Since the 5th century BC, beginning with Greek philosopherZeno of Eleain the West (and earlyIndian mathematiciansin the East), mathematicians had struggled with the concept of infinity. With thedevelopment of calculusin the late 17th century, philosophers began to generally distinguish betweenactual and potential infinity, wherein mathematics was only considered in the latter.[3]Carl Friedrich Gaussfamously stated: "Infinity is nothing more than a figure of speech which helps us talk about limits. The notion of a completed infinity doesn't belong in mathematics."[4]
Development of mathematical set theory was motivated by several mathematicians.Bernhard Riemann's lectureOn the Hypotheses which lie at the Foundations of Geometry(1854) proposed new ideas abouttopology, and about basing mathematics (especially geometry) in terms of sets ormanifoldsin the sense of aclass(which he calledMannigfaltigkeit) now calledpoint-set topology. The lecture was published byRichard Dedekindin 1868, along with Riemann's paper ontrigonometric series(which presented theRiemann integral), The latter was a starting point a movement inreal analysisfor the study of “seriously”discontinuous functions. A youngGeorg Cantorentered into this area, which led him to the study ofpoint-sets. Around 1871, influenced by Riemann, Dedekind began working with sets in his publications, which dealt very clearly and precisely withequivalence relations,partitions of sets, andhomomorphisms. Thus, many of the usual set-theoretic procedures of twentieth-century mathematics go back to his work. However, he did not publish a formal explanation of his set theory until 1888.
Set theory, as understood by modern mathematicians, is generally considered to be founded by a single paper in 1874 byGeorg CantortitledOn a Property of the Collection of All Real Algebraic Numbers.[5][6][7]In his paper, he developed the notion ofcardinality, comparing the sizes of two sets by setting them in one-to-one correspondence. His "revolutionary discovery" was that the set of allreal numbersisuncountable, that is, one cannot put all real numbers in a list. This theorem is proved usingCantor's first uncountability proof, which differs from the more familiar proof using hisdiagonal argument.
Cantor introduced fundamental constructions in set theory, such as thepower setof a setA, which is the set of all possiblesubsetsofA. He later proved that the size of the power set ofAis strictly larger than the size ofA, even whenAis an infinite set; this result soon became known asCantor's theorem. Cantor developed a theory oftransfinite numbers, calledcardinalsandordinals, which extended the arithmetic of the natural numbers. His notation for the cardinal numbers was the Hebrew letterℵ{\displaystyle \aleph }(ℵ,aleph) with a natural number subscript; for the ordinals he employed the Greek letterω{\displaystyle \omega }(ω,omega).
Set theory was beginning to become an essential ingredient of the new “modern” approach to mathematics. Originally, Cantor's theory of transfinite numbers was regarded as counter-intuitive – even shocking. This caused it to encounter resistance from mathematical contemporaries such asLeopold KroneckerandHenri Poincaréand later fromHermann WeylandL. E. J. Brouwer, whileLudwig Wittgensteinraisedphilosophical objections(see:Controversy over Cantor's theory).[a]Dedekind's algebraic style only began to find followers in the 1890s
Despite the controversy, Cantor's set theory gained remarkable ground around the turn of the 20th century with the work of several notable mathematicians and philosophers. Richard Dedekind, around the same time, began working with sets in his publications, and famously constructing the real numbers usingDedekind cuts. He also worked withGiuseppe Peanoin developing thePeano axioms, which formalized natural-number arithmetic, using set-theoretic ideas, which also introduced theepsilonsymbol forset membership. Possibly most prominently,Gottlob Fregebegan to develop hisFoundations of Arithmetic.
In his work, Frege tries to ground all mathematics in terms of logical axioms using Cantor's cardinality. For example, the sentence "the number of horses in the barn is four" means that four objects fall under the concepthorse in the barn. Frege attempted to explain our grasp of numbers through cardinality ('the number of...', orNx:Fx{\displaystyle Nx:Fx}), relying onHume's principle.
However, Frege's work was short-lived, as it was found byBertrand Russellthat his axioms lead to acontradiction. Specifically, Frege'sBasic Law V(now known as theaxiom schema of unrestricted comprehension). According toBasic Law V, for any sufficiently well-definedproperty, there is the set of all and only the objects that have that property. The contradiction, calledRussell's paradox, is shown as follows:
LetRbe the set of all sets that are not members of themselves. (This set is sometimes called "the Russell set".)  IfRis not a member of itself, then its definition entails that it is a member of itself; yet, if it is a member of itself, then it is not a member of itself, since it is the set of all sets that are not members of themselves. The resulting contradiction is Russell's paradox. In symbols:
This came around a time of severalparadoxesor counter-intuitive results. For example, that theparallel postulatecannot be proved, the existence ofmathematical objectsthat cannot be computed or explicitly described, and the existence of theorems of arithmetic that cannot be proved withPeano arithmetic. The result was afoundational crisis of mathematics.
Set theory begins with a fundamentalbinary relationbetween an objectoand a setA. Ifois amember(orelement) ofA, the notationo∈Ais used. A set is described by listing elements separated by commas, or by a characterizing property of its elements, within braces { }.[8]Since sets are objects, the membership relation can relate sets as well, i.e., sets themselves can be members of other sets.
A derived binary relation between two sets is the subset relation, also calledset inclusion. If all the members of setAare also members of setB, thenAis asubsetofB, denotedA⊆B. For example,{1, 2}is a subset of{1, 2, 3}, and so is{2}but{1, 4}is not. As implied by this definition, a set is a subset of itself. For cases where this possibility is unsuitable or would make sense to be rejected, the termproper subsetis defined, variously denotedA⊂B{\displaystyle A\subset B},A⊊B{\displaystyle A\subsetneq B}, orA⫋B{\displaystyle A\subsetneqq B}(note however that the notationA⊂B{\displaystyle A\subset B}is sometimes used synonymously withA⊆B{\displaystyle A\subseteq B}; that is, allowing the possibility thatAandBare equal).  We callAaproper subsetofBif and only ifAis a subset ofB, butAis not equal toB. Also, 1, 2, and 3 are members (elements) of the set{1, 2, 3}, but are not subsets of it; and in turn, the subsets, such as{1}, are not members of the set{1, 2, 3}. More complicated relations can exist; for example, the set{1}is both a member and a proper subset of the set{1, {1}}.
Just asarithmeticfeaturesbinary operationsonnumbers, set theory features binary operations on sets.[9]The following is a partial list of them:
Some basic sets of central importance are the set ofnatural numbers, the set ofreal numbersand theempty set– the unique set containing no elements. The empty set is also occasionally called thenull set,[15]though this name is ambiguous and can lead to several interpretations. The empty set can be denoted with empty braces "{}{\displaystyle \{\}}" or the symbol "∅{\displaystyle \varnothing }" or "∅{\displaystyle \emptyset }".
Thepower setof a setA, denotedP(A){\displaystyle {\mathcal {P}}(A)}, is the set whose members are all of the possible subsets ofA. For example, the power set of{1, 2}is{ {}, {1}, {2}, {1, 2} }. Notably,P(A){\displaystyle {\mathcal {P}}(A)}contains bothAand the empty set.
A set ispureif all of its members are sets, all members of its members are sets, and so on. For example, the set containing only the empty set is a nonempty pure set. In modern set theory, it is common to restrict attention to thevon Neumann universeof pure sets, and many systems of axiomatic set theory are designed to axiomatize the pure sets only. There are many technical advantages to this restriction, and little generality is lost, because essentially all mathematical concepts can be modeled by pure sets. Sets in the von Neumann universe are organized into acumulative hierarchy, based on how deeply their members, members of members, etc. are nested. Each set in this hierarchy is assigned (bytransfinite recursion) anordinal numberα{\displaystyle \alpha }, known as itsrank.The rank of a pure setX{\displaystyle X}is defined to be the least ordinal that is strictly greater than the rank of any of its elements. For example, the empty set is assigned rank 0, while the setcontaining only the empty set is assigned rank 1. For each ordinalα{\displaystyle \alpha }, the setVα{\displaystyle V_{\alpha }}is defined to consist of all pure sets with rank less thanα{\displaystyle \alpha }. The entire von Neumann universe is denotedV{\displaystyle V}.
Elementary set theory can be studied informally and intuitively, and so can be taught in primary schools usingVenn diagrams. The intuitive approach tacitly assumes that a set may be formed from the class of all objects satisfying any particular defining condition. This assumption gives rise to paradoxes, the simplest and best known of which areRussell's paradoxand theBurali-Forti paradox.Axiomatic set theorywas originally devised to rid set theory of such paradoxes.[note 1]
The most widely studied systems of axiomatic set theory imply that all sets form acumulative hierarchy.[b]Such systems come in two flavors, those whoseontologyconsists of:
The above systems can be modified to allowurelements, objects that can be members of sets but that are not themselves sets and do not have any members.
TheNew Foundationssystems ofNFU(allowingurelements) andNF(lacking them), associate withWillard Van Orman Quine, are not based on a cumulative hierarchy. NF and NFU include a "set of everything", relative to which every set has a complement. In these systems urelements matter, because NF, but not NFU, produces sets for which theaxiom of choicedoes not hold. Despite NF's ontology not reflecting the traditional cumulative hierarchy and violating well-foundedness,Thomas Forsterhas argued that it does reflect aniterative conception of set.[16]
Systems ofconstructive set theory, such as CST, CZF, and IZF, embed their set axioms inintuitionisticinstead ofclassical logic. Yet other systems accept classical logic but feature a nonstandard membership relation. These includerough set theoryandfuzzy set theory, in which the value of anatomic formulaembodying the membership relation is not simplyTrueorFalse. TheBoolean-valued modelsofZFCare a related subject.
An enrichment of ZFC calledinternal set theorywas proposed byEdward Nelsonin 1977.[17]
Many mathematical concepts can be defined precisely using only set theoretic concepts. For example, mathematical structures as diverse asgraphs,manifolds,rings,vector spaces, andrelational algebrascan all be defined as sets satisfying various (axiomatic) properties.Equivalenceandorder relationsare ubiquitous in mathematics, and the theory of mathematicalrelationscan be described in set theory.[18][19]
Set theory is also a promising foundational system for much of mathematics. Since the publication of the first volume ofPrincipia Mathematica, it has been claimed that most (or even all) mathematical theorems can be derived using an aptly designed set of axioms for set theory, augmented with many definitions, usingfirstorsecond-order logic. For example, properties of thenaturalandreal numberscan be derived within set theory, as each of these number systems can be defined by representing their elements as sets of specific forms.[20]
Set theory as a foundation formathematical analysis,topology,abstract algebra, anddiscrete mathematicsis likewise uncontroversial; mathematicians accept (in principle) that theorems in these areas can be derived from the relevant definitions and the axioms of set theory. However, it remains that few full derivations of complex mathematical theorems from set theory have been formally verified, since such formal derivations are often much longer than the natural language proofs mathematicians commonly present. One verification project,Metamath, includes human-written, computer-verified derivations of more than 12,000 theorems starting fromZFCset theory,first-order logicandpropositional logic.[21]
Set theory is a major area of research in mathematics with many interrelated subfields:
Combinatorial set theoryconcerns extensions of finitecombinatoricsto infinite sets. This includes the study ofcardinal arithmeticand the study of extensions ofRamsey's theoremsuch as theErdős–Rado theorem.
Descriptive set theoryis the study of subsets of thereal lineand, more generally, subsets ofPolish spaces. It begins with the study ofpointclassesin theBorel hierarchyand extends to the study of more complex hierarchies such as theprojective hierarchyand theWadge hierarchy. Many properties ofBorel setscan be established in ZFC, but proving these properties hold for more complicated sets requires additional axioms related to determinacy and large cardinals.
The field ofeffective descriptive set theoryis between set theory andrecursion theory. It includes the study oflightface pointclasses, and is closely related tohyperarithmetical theory. In many cases, results of classical descriptive set theory have effective versions; in some cases, new results are obtained by proving the effective version first and then extending ("relativizing") it to make it more broadly applicable.
A recent area of research concernsBorel equivalence relationsand more complicated definableequivalence relations. This has important applications to the study ofinvariantsin many fields of mathematics.
In set theory as Cantor defined and Zermelo and Fraenkel axiomatized, an object is either a member of a set or not. Infuzzy set theorythis condition was relaxed byLotfi A. Zadehso an object has adegree of membershipin a set, a number between 0 and 1. For example, the degree of membership of a person in the set of "tall people" is more flexible than a simple yes or no answer and can be a real number such as 0.75.
Aninner modelof Zermelo–Fraenkel set theory (ZF) is a transitiveclassthat includes all the ordinals and satisfies all the axioms of ZF. The canonical example is theconstructible universeLdeveloped by Gödel.
One reason that the study of inner models is of interest is that it can be used to prove consistency results. For example, it can be shown that regardless of whether a modelVof ZF satisfies thecontinuum hypothesisor theaxiom of choice, the inner modelLconstructed inside the original model will satisfy both the generalized continuum hypothesis and the axiom of choice. Thus the assumption that ZF is consistent (has at least one model) implies that ZF together with these two principles is consistent.
The study of inner models is common in the study ofdeterminacyandlarge cardinals, especially when considering axioms such as the axiom of determinacy that contradict the axiom of choice. Even if a fixed model of set theory satisfies the axiom of choice, it is possible for an inner model to fail to satisfy the axiom of choice. For example, the existence of sufficiently large cardinals implies that there is an inner model satisfying the axiom of determinacy (and thus not satisfying the axiom of choice).[22]
Alarge cardinalis a cardinal number with an extra property. Many such properties are studied, includinginaccessible cardinals,measurable cardinals, and many more. These properties typically imply the cardinal number must be very large, with the existence of a cardinal with the specified property unprovable inZermelo–Fraenkel set theory.
Determinacyrefers to the fact that, under appropriate assumptions, certain two-player games of perfect information are determined from the start in the sense that one player must have a winning strategy. The existence of these strategies has important consequences in descriptive set theory, as the assumption that a broader class of games is determined often implies that a broader class of sets will have a topological property. Theaxiom of determinacy(AD) is an important object of study; although incompatible with the axiom of choice, AD implies that all subsets of the real line are well behaved (in particular, measurable and with the perfect set property). AD can be used to prove that theWadge degreeshave an elegant structure.
Paul Coheninvented the method offorcingwhile searching for amodelofZFCin which thecontinuum hypothesisfails, or a model of ZF in which theaxiom of choicefails. Forcing adjoins to some given model of set theory additional sets in order to create a larger model with properties determined (i.e. "forced") by the construction and the original model. For example, Cohen's construction adjoins additional subsets of thenatural numberswithout changing any of thecardinal numbersof the original model. Forcing is also one of two methods for provingrelative consistencyby finitistic methods, the other method beingBoolean-valued models.
Acardinal invariantis a property of the real line measured by a cardinal number. For example, a well-studied invariant is the smallest cardinality of a collection ofmeagre setsof reals whose union is the entire real line. These are invariants in the sense that any two isomorphic models of set theory must give the same cardinal for each invariant. Many cardinal invariants have been studied, and the relationships between them are often complex and related to axioms of set theory.
Set-theoretic topologystudies questions ofgeneral topologythat are set-theoretic in nature or that require advanced methods of set theory for their solution. Many of these theorems are independent of ZFC, requiring stronger axioms for their proof. A famous problem is thenormal Moore space question, a question in general topology that was the subject of intense research. The answer to the normal Moore space question was eventually proved to be independent of ZFC.
From set theory's inception, some mathematicians have objected to it as afoundation for mathematics. The most common objection to set theory, oneKroneckervoiced in set theory's earliest years, starts from theconstructivistview that mathematics is loosely related to computation. If this view is granted, then the treatment of infinite sets, both innaiveand in axiomatic set theory, introduces into mathematics methods and objects that are not computable even in principle. The feasibility of constructivism as a substitute foundation for mathematics was greatly increased byErrett Bishop's influential bookFoundations of Constructive Analysis.[23]
A different objection put forth byHenri Poincaréis that defining sets using the axiom schemas ofspecificationandreplacement, as well as theaxiom of power set, introducesimpredicativity, a type ofcircularity, into the definitions of mathematical objects. The scope of predicatively founded mathematics, while less than that of the commonly accepted Zermelo–Fraenkel theory, is much greater than that of constructive mathematics, to the point thatSolomon Fefermanhas said that "all of scientifically applicable analysis can be developed [using predicative methods]".[24]
Ludwig Wittgensteincondemned set theory philosophically for its connotations ofmathematical platonism.[25]He wrote that "set theory is wrong", since it builds on the "nonsense" of fictitious symbolism, has "pernicious idioms", and that it is nonsensical to talk about "all numbers".[26]Wittgenstein identified mathematics with algorithmic human deduction;[27]the need for a secure foundation for mathematics seemed, to him, nonsensical.[28]Moreover, since human effort is necessarily finite, Wittgenstein's philosophy required an ontological commitment to radicalconstructivismandfinitism.  Meta-mathematical statements – which, for Wittgenstein, included any statement quantifying over infinite domains, and thus almost all modern set theory – are not mathematics.[29]Few modern philosophers have adopted Wittgenstein's views after a spectacular blunder inRemarks on the Foundations of Mathematics: Wittgenstein attempted to refuteGödel's incompleteness theoremsafter having only read the abstract. As reviewersKreisel,Bernays,Dummett, andGoodsteinall pointed out, many of his critiques did not apply to the paper in full. Only recently have philosophers such asCrispin Wrightbegun to rehabilitate Wittgenstein's arguments.[30]
Category theoristshave proposedtopos theoryas an alternative to traditional axiomatic set theory. Topos theory can interpret various alternatives to that theory, such asconstructivism, finite set theory, andcomputableset theory.[31][32]Topoi also give a natural setting for forcing and discussions of the independence of choice from ZF, as well as providing the framework forpointless topologyandStone spaces.[33]
An active area of research is theunivalent foundationsand related to ithomotopy type theory. Within homotopy type theory, a set may be regarded as a homotopy 0-type, withuniversal propertiesof sets arising from the inductive and recursive properties ofhigher inductive types. Principles such as theaxiom of choiceand thelaw of the excluded middlecan be formulated in a manner corresponding to the classical formulation in set theory or perhaps in a spectrum of distinct ways unique to type theory. Some of these principles may be proven to be a consequence of other principles. The variety of formulations of these axiomatic principles allows for a detailed analysis of the formulations required in order to derive various mathematical results.[34][35]
As set theory gained popularity as a foundation for modern mathematics, there has been support for the idea of introducing the basics ofnaive set theoryearly inmathematics education.
In the US in the 1960s, theNew Mathexperiment aimed to teach basic set theory, among other abstract concepts, toprimary schoolstudents but was met with much criticism.[36]The math syllabus in European schools followed this trend and currently includes the subject at different levels in all grades.Venn diagramsare widely employed to explain basic set-theoretic relationships to primary school students (even thoughJohn Vennoriginally devised them as part of a procedure to assess thevalidityofinferencesinterm logic).
Set theory is used to introduce students tological operators(NOT, AND, OR), and semantic or rule description (technicallyintensional definition)[37]of sets (e.g. "months starting with the letterA"), which may be useful when learningcomputer programming, sinceBoolean logicis used in variousprogramming languages. Likewise, sets and other collection-like objects, such asmultisetsandlists, are commondatatypesin computer science and programming.[38]
In addition to that, certain sets are commonly used in mathematical teaching, such as the setsN{\displaystyle \mathbb {N} }ofnatural numbers,Z{\displaystyle \mathbb {Z} }ofintegers,R{\displaystyle \mathbb {R} }ofreal numbers, etc.). These are commonly used when defining amathematical functionas a relation from one set (thedomain) to another set (therange).[39]
Linear algebrais the branch ofmathematicsconcerninglinear equationssuch as
and their representations invector spacesand throughmatrices.[1][2][3]
Linear algebra is central to almost all areas of mathematics. For instance, linear algebra is fundamental in modern presentations ofgeometry, including for defining basic objects such aslines,planesandrotations. Also,functional analysis, a branch ofmathematical analysis, may be viewed as the application of linear algebra tofunction spaces.
Linear algebra is also used in most sciences and fields ofengineeringbecause it allowsmodelingmany natural phenomena, and computing efficiently with such models. Fornonlinear systems, which cannot be modeled with linear algebra, it is often used for dealing withfirst-order approximations, using the fact that thedifferentialof amultivariate functionat a point is the linear map that best approximates the function near that point.
The procedure (using counting rods) for solving simultaneous linear equations now calledGaussian eliminationappears in the ancient Chinese mathematical textChapter Eight:Rectangular ArraysofThe Nine Chapters on the Mathematical Art. Its use is illustrated in eighteen problems, with two to five equations.[4]
Systems of linear equationsarose in Europe with the introduction in 1637 byRené Descartesofcoordinatesingeometry. In fact, in this new geometry, now calledCartesian geometry, lines and planes are represented by linear equations, and computing their intersections amounts to solving systems of linear equations.
The first systematic methods for solving linear systems useddeterminantsand were first considered byLeibnizin 1693. In 1750,Gabriel Cramerused them for giving explicit solutions of linear systems, now calledCramer's rule. Later,Gaussfurther described the method of elimination, which was initially listed as an advancement ingeodesy.[5]
In 1844Hermann Grassmannpublished his "Theory of Extension" which included foundational new topics of what is today called linear algebra. In 1848,James Joseph Sylvesterintroduced the termmatrix, which is Latin forwomb.
Linear algebra grew with ideas noted in thecomplex plane. For instance, two numberswandzinC{\displaystyle \mathbb {C} }have a differencew–z, and the line segmentswzand0(w−z)are of the same length and direction. The segments areequipollent. The four-dimensional systemH{\displaystyle \mathbb {H} }ofquaternionswas discovered byW.R. Hamiltonin 1843.[6]The termvectorwas introduced asv=xi+yj+zkrepresenting a point in space. The quaternion differencep–qalso produces a segment equipollent topq. Otherhypercomplex numbersystems also used the idea of a linear space with abasis.
Arthur Cayleyintroducedmatrix multiplicationand theinverse matrixin 1856, making possible thegeneral linear group. The mechanism ofgroup representationbecame available for describing complex and hypercomplex numbers. Crucially, Cayley used a single letter to denote a matrix, thus treating a matrix as an aggregate object. He also realized the connection between matrices and determinants and wrote "There would be many things to say about this theory of matrices which should, it seems to me, precede the theory of determinants".[5]
Benjamin Peircepublished hisLinear Associative Algebra(1872), and his sonCharles Sanders Peirceextended the work later.[7]
Thetelegraphrequired an explanatory system, and the 1873 publication byJames Clerk MaxwellofA Treatise on Electricity and Magnetisminstituted afield theoryof forces and requireddifferential geometryfor expression. Linear algebra is flat differential geometry and serves in tangent spaces tomanifolds. Electromagnetic symmetries of spacetime are expressed by theLorentz transformations, and much of the history of linear algebra is thehistory of Lorentz transformations.
The first modern and more precise definition of a vector space was introduced byPeanoin 1888;[5]by 1900, a theory of linear transformations of finite-dimensional vector spaces had emerged. Linear algebra took its modern form in the first half of the twentieth century when many ideas and methods of previous centuries were generalized asabstract algebra. The development of computers led to increased research in efficientalgorithmsfor Gaussian elimination and matrix decompositions, and linear algebra became an essential tool for modeling and simulations.[5]
Until the 19th century, linear algebra was introduced throughsystems of linear equationsandmatrices. In modern mathematics, the presentation throughvector spacesis generally preferred, since it is moresynthetic, more general (not limited to the finite-dimensional case), and conceptually simpler, although more abstract.
A vector space over afieldF(often the field of thereal numbers) is asetVequipped with twobinary operations.ElementsofVare calledvectors, and elements ofFare calledscalars. The first operation,vector addition, takes any two vectorsvandwand outputs a third vectorv+w. The second operation,scalar multiplication, takes any scalaraand any vectorvand outputs a newvectorav. The axioms that addition and scalar multiplication must satisfy are the following. (In the list below,u,vandware arbitrary elements ofV, andaandbare arbitrary scalars in the fieldF.)[8]
The first four axioms mean thatVis anabelian groupunder addition.
An element of a specific vector space may have various natures; for example, it could be asequence, afunction, apolynomial, or amatrix. Linear algebra is concerned with the properties of such objects that are common to all vector spaces.
Linear mapsaremappingsbetween vector spaces that preserve the vector-space structure. Given two vector spacesVandWover a fieldF, a linear map (also called, in some contexts, linear transformation or linear mapping) is amap
That is compatible with addition and scalar multiplication, that is
This implies that for any vectorsu,vinVand scalarsa,binF, one has
WhenV=Ware the same vector space, a linear mapT:V→Vis also known as alinear operatoronV.
Abijectivelinear map between two vector spaces (that is, every vector from the second space is associated with exactly one in the first) is anisomorphism. Because an isomorphism preserves linear structure, two isomorphic vector spaces are "essentially the same" from the linear algebra point of view, in the sense that they cannot be distinguished by using vector space properties. An essential question in linear algebra is testing whether a linear map is an isomorphism or not, and, if it is not an isomorphism, finding itsrange(or image) and the set of elements that are mapped to the zero vector, called thekernelof the map. All these questions can be solved by usingGaussian eliminationor some variant of thisalgorithm.
The study of those subsets of vector spaces that are in themselves vector spaces under the induced operations is fundamental, similarly as for many mathematical structures. These subsets are calledlinear subspaces. More precisely, a linear subspace of a vector spaceVover a fieldFis asubsetWofVsuch thatu+vandauare inW, for everyu,vinW, and everyainF. (These conditions suffice for implying thatWis a vector space.)
For example, given a linear mapT:V→W, theimageT(V)ofV, and theinverse imageT−1(0)of0(calledkernelor null space), are linear subspaces ofWandV, respectively.
Another important way of forming a subspace is to considerlinear combinationsof a setSof vectors: the set of all sums
wherev1,v2, ...,vkare inS, anda1,a2, ...,akare inFform a linear subspace called thespanofS. The span ofSis also the intersection of all linear subspaces containingS. In other words, it is the smallest (for the inclusion relation) linear subspace containingS.
A set of vectors islinearly independentif none is in the span of the others. Equivalently, a setSof vectors is linearly independent if the only way to express the zero vector as a linear combination of elements ofSis to take zero for every coefficientai.
A set of vectors that spans a vector space is called aspanning setorgenerating set. If a spanning setSislinearly dependent(that is not linearly independent), then some elementwofSis in the span of the other elements ofS, and the span would remain the same if one were to removewfromS. One may continue to remove elements ofSuntil getting alinearly independent spanning set. Such a linearly independent set that spans a vector spaceVis called abasisofV. The importance of bases lies in the fact that they are simultaneously minimal-generating sets and maximal independent sets. More precisely, ifSis a linearly independent set, andTis a spanning set such thatS⊆T, then there is a basisBsuch thatS⊆B⊆T.
Any two bases of a vector spaceVhave the samecardinality, which is called thedimensionofV; this is thedimension theorem for vector spaces. Moreover, two vector spaces over the same fieldFareisomorphicif and only if they have the same dimension.[9]
If any basis ofV(and therefore every basis) has a finite number of elements,Vis afinite-dimensional vector space. IfUis a subspace ofV, thendimU≤ dimV. In the case whereVis finite-dimensional, the equality of the dimensions impliesU=V.
Matrices allow explicit manipulation of finite-dimensional vector spaces andlinear maps. Their theory is thus an essential part of linear algebra.
LetVbe a finite-dimensional vector space over a fieldF, and(v1,v2, ...,vm)be a basis ofV(thusmis the dimension ofV). By definition of a basis, the map
is abijectionfromFm, the set of thesequencesofmelements ofF, ontoV. This is anisomorphismof vector spaces, ifFmis equipped with its standard structure of vector space, where vector addition and scalar multiplication are done component by component.
This isomorphism allows representing a vector by itsinverse imageunder this isomorphism, that is by thecoordinate vector(a1, ...,am)or by thecolumn matrix
IfWis another finite dimensional vector space (possibly the same), with a basis(w1, ...,wn), a linear mapffromWtoVis well defined by its values on the basis elements, that is(f(w1), ...,f(wn)). Thus,fis well represented by the list of the corresponding column matrices. That is, if
Matrix multiplicationis defined in such a way that the product of two matrices is the matrix of thecompositionof the corresponding linear maps, and the product of a matrix and a column matrix is the column matrix representing the result of applying the represented linear map to the represented vector. It follows that the theory of finite-dimensional vector spaces and the theory of matrices are two different languages for expressing the same concepts.
Two matrices that encode the same linear transformation in different bases are calledsimilar. It can be proved that two matrices are similar if and only if one can transform one into the other byelementary row and column operations. For a matrix representing a linear map fromWtoV, the row operations correspond to change of bases inVand the column operations correspond to change of bases inW. Every matrix is similar to anidentity matrixpossibly bordered by zero rows and zero columns. In terms of vector spaces, this means that, for any linear map fromWtoV, there are bases such that a part of the basis ofWis mapped bijectively on a part of the basis ofV, and that the remaining basis elements ofW, if any, are mapped to zero.Gaussian eliminationis the basic algorithm for finding these elementary operations, and proving these results.
A finite set of linear equations in a finite set of variables, for example,x1,x2, ...,xn, orx,y, ...,zis called asystem of linear equationsor alinear system.[11][12][13][14][15]
Systems of linear equations form a fundamental part of linear algebra. Historically, linear algebra and matrix theory have been developed for solving such systems. In the modern presentation of linear algebra through vector spaces and matrices, many problems may be interpreted in terms of linear systems.
LetTbe the linear transformation associated with the matrixM. A solution of the system (S) is a vector
Let (S′) be the associatedhomogeneous system, where the right-hand sides of the equations are put to zero:
The solutions of (S′) are exactly the elements of thekernelofTor, equivalently,M.
TheGaussian-eliminationconsists of performingelementary row operationson theaugmented matrix
for putting it inreduced row echelon form. These row operations do not change the set of solutions of the system of equations. In the example, the reduced echelon form is
showing that the system (S) has the unique solution
It follows from this matrix interpretation of linear systems that the same methods can be applied for solving linear systems and for many operations on matrices and linear transformations, which include the computation of theranks,kernels,matrix inverses.
A linearendomorphismis a linear map that maps a vector spaceVto itself. 
IfVhas a basis ofnelements, such an endomorphism is represented by a square matrix of sizen.
Concerning general linear maps, linear endomorphisms, and square matrices have some specific properties that make their study an important part of linear algebra, which is used in many parts of mathematics, includinggeometric transformations,coordinate changes,quadratic forms, and many other parts of mathematics.
Thedeterminantof a square matrixAis defined to be[16]
whereSnis thegroup of all permutationsofnelements,σis a permutation, and(−1)σtheparityof the permutation. A matrix isinvertibleif and only if the determinant is invertible (i.e., nonzero if the scalars belong to a field).
Cramer's ruleis aclosed-form expression, in terms of determinants, of the solution of asystem ofnlinear equations innunknowns. Cramer's rule is useful for reasoning about the solution, but, except forn= 2or3, it is rarely used for computing a solution, sinceGaussian eliminationis a faster algorithm.
Thedeterminant of an endomorphismis the determinant of the matrix representing the endomorphism in terms of some ordered basis. This definition makes sense since this determinant is independent of the choice of the basis.
Iffis a linear endomorphism of a vector spaceVover a fieldF, aneigenvectoroffis a nonzero vectorvofVsuch thatf(v) =avfor some scalarainF. This scalarais aneigenvalueoff.
If the dimension ofVis finite, and a basis has been chosen,fandvmay be represented, respectively, by a square matrixMand a column matrixz; the equation defining eigenvectors and eigenvalues becomes
Using theidentity matrixI, whose entries are all zero, except those of the main diagonal, which are equal to one, this may be rewritten
Aszis supposed to be nonzero, this means thatM–aIis asingular matrix, and thus that its determinantdet (M−aI)equals zero. The eigenvalues are thus therootsof thepolynomial
IfVis of dimensionn, this is amonic polynomialof degreen, called thecharacteristic polynomialof the matrix (or of the endomorphism), and there are, at most,neigenvalues.
If a basis exists that consists only of eigenvectors, the matrix offon this basis has a very simple structure: it is adiagonal matrixsuch that the entries on themain diagonalare eigenvalues, and the other entries are zero. In this case, the endomorphism and the matrix are said to bediagonalizable. More generally, an endomorphism and a matrix are also said diagonalizable, if they become diagonalizable afterextendingthe field of scalars. In this extended sense, if the characteristic polynomial issquare-free, then the matrix is diagonalizable.
Asymmetric matrixis always diagonalizable. There are non-diagonalizable matrices, the simplest being
(it cannot be diagonalizable since its square is thezero matrix, and the square of a nonzero diagonal matrix is never zero).
When an endomorphism is not diagonalizable, there are bases on which it has a simple form, although not as simple as the diagonal form. TheFrobenius normal formdoes not need to extend the field of scalars and makes the characteristic polynomial immediately readable on the matrix. TheJordan normal formrequires to extension of the field of scalar for containing all eigenvalues and differs from the diagonal form only by some entries that are just above the main diagonal and are equal to 1.
Alinear formis a linear map from a vector spaceVover a fieldFto the field of scalarsF, viewed as a vector space over itself. Equipped bypointwiseaddition and multiplication by a scalar, the linear forms form a vector space, called thedual spaceofV, and usually denotedV*[17]orV′.[18][19]
Ifv1, ...,vnis a basis ofV(this implies thatVis finite-dimensional), then one can define, fori= 1, ...,n, a linear mapvi*such thatvi*(vi) = 1andvi*(vj) = 0ifj≠i. These linear maps form a basis ofV*, called thedual basisofv1, ...,vn. (IfVis not finite-dimensional, thevi*may be defined similarly; they are linearly independent, but do not form a basis.)
is a linear form onV*. This defines thecanonical linear mapfromVinto(V*)*, the dual ofV*, called thedouble dualorbidualofV. This canonical map is anisomorphismifVis finite-dimensional, and this allows identifyingVwith its bidual. (In the infinite-dimensional case, the canonical map is injective, but not surjective.)
There is thus a complete symmetry between a finite-dimensional vector space and its dual. This motivates the frequent use, in this context, of thebra–ket notation
be a linear map. For every linear formhonW, thecomposite functionh∘fis a linear form onV. This defines a linear map
between the dual spaces, which is called thedualor thetransposeoff.
IfVandWare finite-dimensional, andMis the matrix offin terms of some ordered bases, then the matrix off*over the dual bases is thetransposeMTofM, obtained by exchanging rows and columns.
If elements of vector spaces and their duals are represented by column vectors, this duality may be expressed inbra–ket notationby
To highlight this symmetry, the two members of this equality are sometimes written
Besides these basic concepts, linear algebra also studies vector spaces with additional structure, such as aninner product. The inner product is an example of abilinear form, and it gives the vector space a geometric structure by allowing for the definition of length and angles. Formally, aninner productis a map.
that satisfies the following threeaxiomsfor all vectorsu,v,winVand all scalarsainF:[20][21]
and so we can call this quantity the cosine of the angle between the two vectors.
Two vectors are orthogonal if⟨u,v⟩ = 0. An orthonormal basis is a basis where all basis vectors have length 1 and are orthogonal to each other. Given any finite-dimensional vector space, an orthonormal basis could be found by theGram–Schmidtprocedure. Orthonormal bases are particularly easy to deal with, since ifv=a1v1+ ⋯ +anvn, then
The inner product facilitates the construction of many useful concepts. For instance, given a transformT, we can define itsHermitian conjugateT*as the linear transform satisfying
IfTsatisfiesTT*=T*T, we callTnormal. It turns out that normal matrices are precisely the matrices that have an orthonormal system of eigenvectors that spanV.
There is a strong relationship between linear algebra andgeometry, which started with the introduction byRené Descartes, in 1637, ofCartesian coordinates. In this new (at that time) geometry, now calledCartesian geometry, points are represented byCartesian coordinates, which are sequences of three real numbers (in the case of the usualthree-dimensional space). The basic objects of geometry, which arelinesandplanesare represented by linear equations. Thus, computing intersections of lines and planes amounts to solving systems of linear equations. This was one of the main motivations for developing linear algebra.
Mostgeometric transformation, such astranslations,rotations,reflections,rigid motions,isometries, andprojectionstransform lines into lines. It follows that they can be defined, specified, and studied in terms of linear maps. This is also the case ofhomographiesandMöbius transformationswhen considered as transformations of aprojective space.
Until the end of the 19th century, geometric spaces were defined byaxiomsrelating points, lines, and planes (synthetic geometry). Around this date, it appeared that one may also define geometric spaces by constructions involving vector spaces (see, for example,Projective spaceandAffine space). It has been shown that the two approaches are essentially equivalent.[22]In classical geometry, the involved vector spaces are vector spaces over the reals, but the constructions may be extended to vector spaces over any field, allowing considering geometry over arbitrary fields, includingfinite fields.
Presently, most textbooks introduce geometric spaces from linear algebra, and geometry is often presented, at the elementary level, as a subfield of linear algebra.
Linear algebra is used in almost all areas of mathematics, thus making it relevant in almost all scientific domains that use mathematics. These applications may be divided into several wide categories.
Functional analysisstudiesfunction spaces. These are vector spaces with additional structure, such asHilbert spaces. Linear algebra is thus a fundamental part of functional analysis and its applications, which include, in particular,quantum mechanics(wave functions) andFourier analysis(orthogonal basis).
Nearly allscientific computationsinvolve linear algebra. Consequently, linear algebra algorithms have been highly optimized.BLASandLAPACKare the best known implementations. For improving efficiency, some of them configure the algorithms automatically, at run time, to adapt them to the specificities of the computer (cachesize, number of availablecores, ...).
Since the 1960s there have been processors with specialized instructions[23]for optimizing the operations of linear algebra, optional array processors[24]under the control of a conventional processor, supercomputers[25][26][27]designed for array processing and conventional processors augmented[28]with vector registers.
Some contemporaryprocessors, typicallygraphics processing units(GPU), are designed with a matrix structure, for optimizing the operations of linear algebra.[29]
Themodelingofambient spaceis based ongeometry. Sciences concerned with this space use geometry widely. This is the case withmechanicsandrobotics, for describingrigid body dynamics;geodesyfor describingEarth shape;perspectivity,computer vision, andcomputer graphics, for describing the relationship between a scene and its plane representation; and many other scientific domains.
In all these applications,synthetic geometryis often used for general descriptions and a qualitative approach, but for the study of explicit situations, one must compute withcoordinates. This requires the heavy use of linear algebra.
Most physical phenomena are modeled bypartial differential equations. To solve them, one usually decomposes the space in which the solutions are searched into small, mutually interactingcells. Forlinear systemsthis interaction involveslinear functions. Fornonlinear systems, this interaction is often approximated by linear functions.[b]This is called a linear model or first-order approximation. Linear models are frequently used for complex nonlinear real-world systems because they makeparametrizationmore manageable.[30]In both cases, very large matrices are generally involved.Weather forecasting(or more specifically,parametrization for atmospheric modeling) is a typical example of a real-world application, where the whole Earthatmosphereis divided into cells of, say, 100 km of width and 100 km of height.
Linear algebra, a branch of mathematics dealing withvector spacesandlinear mappingsbetween these spaces, plays a critical role in various engineering disciplines, includingfluid mechanics,fluid dynamics, andthermal energysystems. Its application in these fields is multifaceted and indispensable for solving complex problems.
Influid mechanics, linear algebra is integral to understanding and solving problems related to the behavior of fluids. It assists in the modeling and simulation of fluid flow, providing essential tools for the analysis offluid dynamicsproblems. For instance, linear algebraic techniques are used to solve systems ofdifferential equationsthat describe fluid motion. These equations, often complex andnon-linear, can be linearized using linear algebra methods, allowing for simpler solutions and analyses.
In the field of fluid dynamics, linear algebra finds its application incomputational fluid dynamics(CFD), a branch that usesnumerical analysisanddata structuresto solve and analyze problems involving fluid flows. CFD relies heavily on linear algebra for the computation of fluid flow andheat transferin various applications. For example, theNavier–Stokes equations, fundamental influid dynamics, are often solved using techniques derived from linear algebra. This includes the use ofmatricesandvectorsto represent and manipulate fluid flow fields.
Furthermore, linear algebra plays a crucial role inthermal energysystems, particularly inpower systemsanalysis. It is used to model and optimize the generation,transmission, anddistributionof electric power. Linear algebraic concepts such as matrix operations andeigenvalueproblems are employed to enhance the efficiency, reliability, and economic performance ofpower systems. The application of linear algebra in this context is vital for the design and operation of modernpower systems, includingrenewable energysources andsmart grids.
Overall, the application of linear algebra influid mechanics,fluid dynamics, andthermal energysystems is an example of the profound interconnection betweenmathematicsandengineering. It provides engineers with the necessary tools to model, analyze, and solve complex problems in these domains, leading to advancements in technology and industry.
This section presents several related topics that do not appear generally in elementary textbooks on linear algebra but are commonly considered, in advanced mathematics, as parts of linear algebra.
The existence of multiplicative inverses in fields is not involved in the axioms defining a vector space. One may thus replace the field of scalars by aringR, and this gives the structure called amoduleoverR, orR-module.
The concepts of linear independence, span, basis, and linear maps (also calledmodule homomorphisms) are defined for modules exactly as for vector spaces, with the essential difference that, ifRis not a field, there are modules that do not have any basis. The modules that have a basis are thefree modules, and those that are spanned by a finite set are thefinitely generated modules. Module homomorphisms between finitely generated free modules may be represented by matrices. The theory of matrices over a ring is similar to that of matrices over a field, except thatdeterminantsexist only if the ring iscommutative, and that a square matrix over a commutative ring isinvertibleonly if its determinant has amultiplicative inversein the ring.
Vector spaces are completely characterized by their dimension (up to an isomorphism). In general, there is not such a complete classification for modules, even if one restricts oneself to finitely generated modules. However, every module is acokernelof a homomorphism of free modules.
Modules over the integers can be identified withabelian groups, since the multiplication by an integer may be identified as a repeated addition. Most of the theory of abelian groups may be extended to modules over aprincipal ideal domain. In particular, over a principal ideal domain, every submodule of a free module is free, and thefundamental theorem of finitely generated abelian groupsmay be extended straightforwardly to finitely generated modules over a principal ring.
There are many rings for which there are algorithms for solving linear equations and systems of linear equations. However, these algorithms have generally acomputational complexitythat is much higher than similar algorithms over a field. For more details, seeLinear equation over a ring.
Inmultilinear algebra, one considers multivariable linear transformations, that is, mappings that are linear in each of several different variables. This line of inquiry naturally leads to the idea of thedual space, the vector spaceV*consisting of linear mapsf:V→FwhereFis the field of scalars. Multilinear mapsT:Vn→Fcan be described viatensor productsof elements ofV*.
If, in addition to vector addition and scalar multiplication, there is a bilinear vector productV×V→V, the vector space is called analgebra; for instance, associative algebras are algebras with an associate vector product (like the algebra of square matrices, or the algebra of polynomials).
Vector spaces that are not finite-dimensional often require additional structure to be tractable. Anormed vector spaceis a vector space along with a function called anorm, which measures the "size" of elements. The norm induces ametric, which measures the distance between elements, and induces atopology, which allows for a definition of continuous maps. The metric also allows for a definition oflimitsandcompleteness– a normed vector space that is complete is known as aBanach space. A complete metric space along with the additional structure of aninner product(a conjugate symmetricsesquilinear form) is known as aHilbert space, which is in some sense a particularly well-behaved Banach space.Functional analysisapplies the methods of linear algebra alongside those ofmathematical analysisto study various function spaces; the central objects of study in functional analysis areLpspaces, which are Banach spaces, and especially theL2space of square-integrable functions, which is the only Hilbert space among them. Functional analysis is of particular importance to quantum mechanics, the theory of partial differential equations, digital signal processing, and electrical engineering. It also provides the foundation and theoretical framework that underlies the Fourier transform and related methods.
Inmathematics, adifferential equationis anequationthat relates one or more unknownfunctionsand theirderivatives.[1]In applications, the functions generally represent physical quantities, the derivatives represent their rates of change, and the differential equation defines a relationship between the two. Such relations are common inmathematical modelsandscientific laws; therefore, differential equations play a prominent role in many disciplines includingengineering,physics,economics, andbiology.
The study of differential equations consists mainly of the study of their solutions (the set of functions that satisfy each equation), and of the properties of their solutions. Only the simplest differential equations are solvable by explicit formulas; however, many properties of solutions of a given differential equation may be determined without computing them exactly.
Often when aclosed-form expressionfor the solutions is not available, solutions may be approximated numerically using computers, and manynumerical methodshave been developed to determine solutions with a given degree of accuracy. Thetheory of dynamical systemsanalyzes thequalitativeaspects of solutions, such as theiraverage behaviorover a long time interval.
Differential equations came into existence with theinvention of calculusbyIsaac NewtonandGottfried Leibniz. In Chapter 2 of his 1671 workMethodus fluxionum et Serierum Infinitarum,[2]Newton listed three kinds of differential equations:
In all these cases,yis an unknown function ofx(or ofx1andx2), andfis a given function.
He solves these examples and others using infinite series and discusses the non-uniqueness of solutions.
Jacob Bernoulliproposed theBernoulli differential equationin 1695.[3]This is anordinary differential equationof the form
for which the following year Leibniz obtained solutions by simplifying it.[4]
Historically, the problem of a vibrating string such as that of amusical instrumentwas studied byJean le Rond d'Alembert,Leonhard Euler,Daniel Bernoulli, andJoseph-Louis Lagrange.[5][6][7][8]In 1746, d’Alembert discovered the one-dimensionalwave equation, and within ten years Euler discovered the three-dimensional wave equation.[9]
TheEuler–Lagrange equationwas developed in the 1750s by Euler and Lagrange in connection with their studies of thetautochroneproblem. This is the problem of determining a curve on which a weighted particle will fall to a fixed point in a fixed amount of time, independent of the starting point. Lagrange solved this problem in 1755 and sent the solution to Euler. Both further developed Lagrange's method and applied it tomechanics, which led to the formulation ofLagrangian mechanics.
In 1822,Fourierpublished his work onheat flowinThéorie analytique de la chaleur(The Analytic Theory of Heat),[10]in which he based his reasoning onNewton's law of cooling, namely, that the flow of heat between two adjacent molecules is proportional to the extremely small difference of their temperatures. Contained in this book was Fourier's proposal of hisheat equationfor conductive diffusion of heat. This partial differential equation is now a common part of mathematical physics curriculum.
Inclassical mechanics, the motion of a body is described by its position and velocity as the time value varies.Newton's lawsallow these variables to be expressed dynamically (given the position, velocity, acceleration and various forces acting on the body) as a differential equation for the unknown position of the body as a function of time.
In some cases, this differential equation (called anequation of motion) may be solved explicitly.
An example of modeling a real-world problem using differential equations is the determination of the velocity of a ball falling through the air, considering only gravity and air resistance. The ball's acceleration towards the ground is the acceleration due to gravity minus the deceleration due to air resistance. Gravity is considered constant, and air resistance may be modeled as proportional to the ball's velocity. This means that the ball's acceleration, which is a derivative of its velocity, depends on the velocity (and the velocity depends on time). Finding the velocity as a function of time involves solving a differential equation and verifying its validity.
Differential equations can be classified several different ways. Besides describing the properties of the equation itself, these classes of differential equations can help inform the choice of approach to a solution. Commonly used distinctions include whether the equation is ordinary or partial, linear or non-linear, and homogeneous or heterogeneous. This list is far from exhaustive; there are many other properties and subclasses of differential equations which can be very useful in specific contexts.
Anordinary differential equation(ODE) is an equation containing an unknownfunction of one real or complex variablex, its derivatives, and some given functions ofx. The unknown function is generally represented by avariable(often denotedy), which, therefore,dependsonx. Thusxis often called theindependent variableof the equation. The term "ordinary" is used in contrast with the termpartial differential equation, which may be with respect tomore thanone independent variable.
Linear differential equationsare the differential equations that arelinearin the unknown function and its derivatives. Their theory is well developed, and in many cases one may express their solutions in terms ofintegrals.
Most ODEs that are encountered inphysicsare linear. Therefore, mostspecial functionsmay be defined as solutions of linear differential equations (seeHolonomic function).
As, in general, the solutions of a differential equation cannot be expressed by aclosed-form expression,numerical methodsare commonly used for solving differential equations on a computer.
Apartial differential equation(PDE) is a differential equation that contains unknownmultivariable functionsand theirpartial derivatives. (This is in contrast toordinary differential equations, which deal with functions of a single variable and their derivatives.) PDEs are used to formulate problems involving functions of several variables, and are either solved in closed form, or used to create a relevantcomputer model.
PDEs can be used to describe a wide variety of phenomena in nature such assound,heat,electrostatics,electrodynamics,fluid flow,elasticity, orquantum mechanics. These seemingly distinct physical phenomena can be formalized similarly in terms of PDEs. Just as ordinary differential equations often model one-dimensionaldynamical systems, partial differential equations often modelmultidimensional systems.Stochastic partial differential equationsgeneralize partial differential equations for modelingrandomness.
Anon-linear differential equationis a differential equation that is not alinear equationin the unknown function and its derivatives (the linearity or non-linearity in the arguments of the function are not considered here). There are very few methods of solving nonlinear differential equations exactly; those that are known typically depend on the equation having particularsymmetries. Nonlinear differential equations can exhibit very complicated behaviour over extended time intervals, characteristic ofchaos. Even the fundamental questions of existence, uniqueness, and extendability of solutions for nonlinear differential equations, and well-posedness of initial and boundary value problems for nonlinear PDEs are hard problems and their resolution in special cases is considered to be a significant advance in the mathematical theory (cf.Navier–Stokes existence and smoothness). However, if the differential equation is a correctly formulated representation of a meaningful physical process, then one expects it to have a solution.[11]
Linear differential equations frequently appear asapproximationsto nonlinear equations. These approximations are only valid under restricted conditions. For example, theharmonic oscillatorequation is an approximation to the nonlinear pendulum equation that is valid for small amplitude oscillations.
Theorder of the differential equationis the highestorder of derivativeof the unknown function that appears in the differential equation. 
For example, an equation containing onlyfirst-order derivativesis afirst-order differential equation, an equation containing thesecond-order derivativeis asecond-order differential equation, and so on.[12][13]
When it is written as apolynomial equationin the unknown function and its derivatives, itsdegree of the differential equationis, depending on the context, thepolynomial degreein the highest derivative of the unknown function,[14]or itstotal degreein the unknown function and its derivatives. In particular, alinear differential equationhas degree one for both meanings, but the non-linear differential equationy′+y2=0{\displaystyle y'+y^{2}=0}is of degree one for the first meaning but not for the second one.
Differential equations that describe natural phenomena almost always have only first and second order derivatives in them, but there are some exceptions, such as thethin-film equation, which is a fourth order partial differential equation.
In the first group of examplesuis an unknown function ofx, andcandωare constants that are supposed to be known. Two broad classifications of both ordinary and partial differential equations consist of distinguishing betweenlinearandnonlineardifferential equations, and betweenhomogeneousdifferential equationsandheterogeneousones.
In the next group of examples, the unknown functionudepends on two variablesxandtorxandy.
Solvingdifferential equations is not like solvingalgebraic equations. Not only are their solutions often unclear, but whether solutions are unique or exist at all are also notable subjects of interest.
For first order initial value problems, thePeano existence theoremgives one set of circumstances in which a solution exists. Given any point(a,b){\displaystyle (a,b)}in the xy-plane, define some rectangular regionZ{\displaystyle Z}, such thatZ=[l,m]×[n,p]{\displaystyle Z=[l,m]\times [n,p]}and(a,b){\displaystyle (a,b)}is in the interior ofZ{\displaystyle Z}. If we are given a differential equationdydx=g(x,y){\textstyle {\frac {dy}{dx}}=g(x,y)}and the condition thaty=b{\displaystyle y=b}whenx=a{\displaystyle x=a}, then there is locally a solution to this problem ifg(x,y){\displaystyle g(x,y)}and∂g∂x{\textstyle {\frac {\partial g}{\partial x}}}are both continuous onZ{\displaystyle Z}. This solution exists on some interval with its center ata{\displaystyle a}. The solution may not be unique. (SeeOrdinary differential equationfor other results.)
However, this only helps us with first orderinitial value problems. Suppose we had a linear initial value problem of the nth order:
For any nonzerofn(x){\displaystyle f_{n}(x)}, if{f0,f1,…}{\displaystyle \{f_{0},f_{1},\ldots \}}andg{\displaystyle g}are continuous on some interval containingx0{\displaystyle x_{0}},y{\displaystyle y}exists and is unique.[15]
The theory of differential equations is closely related to the theory ofdifference equations, in which the coordinates assume only discrete values, and the relationship involves values of the unknown function or functions and values at nearby coordinates. Many methods to compute numerical solutions of differential equations or study the properties of differential equations involve the approximation of the solution of a differential equation by the solution of a corresponding difference equation.
The study of differential equations is a wide field inpureandapplied mathematics,physics, andengineering. All of these disciplines are concerned with the properties of differential equations of various types. Pure mathematics focuses on the existence and uniqueness of solutions, while applied mathematics emphasizes the rigorous justification of the methods for approximating solutions. Differential equations play an important role in modeling virtually every physical, technical, or biological process, from celestial motion, to bridge design, to interactions between neurons. Differential equations such as those used to solve real-life problems may not necessarily be directly solvable, i.e. do not haveclosed formsolutions. Instead, solutions can be approximated usingnumerical methods.
Many fundamental laws ofphysicsandchemistrycan be formulated as differential equations. Inbiologyandeconomics, differential equations are used tomodelthe behavior ofcomplex systems. The mathematical theory of differential equations first developed together with the sciences where the equations had originated and where the results found application. However, diverse problems, sometimes originating in quite distinct scientific fields, may give rise to identical differential equations. Whenever this happens, mathematical theory behind the equations can be viewed as a unifying principle behind diverse phenomena. As an example, consider the propagation of light and sound in the atmosphere, and of waves on the surface of a pond. All of them may be described by the same second-orderpartial differential equation, thewave equation, which allows us to think of light and sound as forms of waves, much like familiar waves in the water. Conduction of heat, the theory of which was developed byJoseph Fourier, is governed by another second-order partial differential equation, theheat equation. It turns out that manydiffusionprocesses, while seemingly different, are described by the same equation; theBlack–Scholesequation in finance is, for instance, related to the heat equation.
The number of differential equations that have received a name, in various scientific areas is a witness of the importance of the topic. SeeList of named differential equations.
SomeCASsoftware can solve differential equations. These are the commands used in the leading programs:
Game theoryis the study ofmathematical modelsof strategic interactions.[1]It has applications in many fields ofsocial science, and is used extensively ineconomics,logic,systems scienceandcomputer science.[2]Initially, game theory addressed two-personzero-sum games, in which a participant's gains or losses are exactly balanced by the losses and gains of the other participant. In the 1950s, it was extended to the study of non zero-sum games, and was eventually applied to a wide range ofbehavioral relations. It is now anumbrella termfor thescienceof rationaldecision makingin humans, animals, and computers.
Modern game theory began with the idea of mixed-strategy equilibria in two-person zero-sum games and its proof byJohn von Neumann. Von Neumann's original proof used theBrouwer fixed-point theoremon continuous mappings into compactconvex sets, which became a standard method in game theory andmathematical economics. His paper was followed byTheory of Games and Economic Behavior(1944), co-written withOskar Morgenstern, which consideredcooperative gamesof several players.[3]The second edition provided anaxiomatic theoryofexpected utility, which allowed mathematical statisticians and economists to treat decision-making under uncertainty.[4]
Game theory was developed extensively in the 1950s, and was explicitly applied toevolutionin the 1970s, although similar developments go back at least as far as the 1930s. Game theory has been widely recognized as an important tool in many fields.John Maynard Smithwas awarded theCrafoord Prizefor his application ofevolutionary game theoryin 1999, and fifteen game theorists have won theNobel Prize in economicsas of 2020, including most recentlyPaul MilgromandRobert B. Wilson.
In 1713, a letter attributed to Charles Waldegrave, an activeJacobiteand uncle to British diplomatJames Waldegrave, analyzed a game called "le her". Waldegrave provided aminimaxmixed strategysolution to a two-person version of the card game, and the problem is now known as theWaldegrave problem.[5][6]
In 1838,Antoine Augustin Cournotprovided amodel of competitioninoligopolies. Though he did not refer to it as such, he presented a solution that is theNash equilibriumof the game in hisRecherches sur les principes mathématiques de la théorie des richesses(Researches into the Mathematical Principles of the Theory of Wealth). In 1883,Joseph Bertrandcritiqued Cournot's model as unrealistic, providing an alternative model of price competition[7]which would later be formalized byFrancis Ysidro Edgeworth.[8]
In 1913,Ernst ZermelopublishedÜber eine Anwendung der Mengenlehre auf die Theorie des Schachspiels(On an Application of Set Theory to the Theory of the Game of Chess), which proved that the optimal chess strategy isstrictly determined.[9]
The work ofJohn von Neumannestablished game theory as its own independent field in the early-to-mid 20th century, with von Neumann publishing his paperOn the Theory of Games of Strategyin 1928.[10][11]Von Neumann's original proof usedBrouwer's fixed-point theoremon continuousmappingsinto compactconvex sets, which became a standard method in game theory andmathematical economics. Von Neumann's work in game theory culminated in his 1944 bookTheory of Games and Economic Behavior, co-authored withOskar Morgenstern.[12]The second edition of this book provided anaxiomatic theory of utility, which reincarnatedDaniel Bernoulli'sold theory of utility (of money) as an independent discipline. This foundational work contains the method for finding mutually consistent solutions for two-person zero-sum games. Subsequent work focused primarily oncooperative gametheory, which analyzes optimal strategies for groups of individuals, presuming that they can enforce agreements between them about proper strategies.[13]
In his 1938 bookApplications aux Jeux de Hasardand earlier notes,Émile Borelproved aminimax theoremfor two-person zero-sum matrix games only when the pay-off matrix is symmetric and provided a solution to a non-trivial infinite game (known in English asBlotto game). Borel conjectured the non-existence of mixed-strategy equilibria infinite two-person zero-sum games, a conjecture that was proved false by von Neumann.[14]
In 1950,John Nashdeveloped a criterion for mutual consistency of players' strategies known as theNash equilibrium, applicable to a wider variety of games than the criterion proposed by von Neumann and Morgenstern. Nash proved that every finite n-player, non-zero-sum (not just two-player zero-sum)non-cooperative gamehas what is now known as a Nash equilibrium in mixed strategies.
Game theory experienced a flurry of activity in the 1950s, during which the concepts of thecore, theextensive form game,fictitious play,repeated games, and theShapley valuewere developed. The 1950s also saw the first applications of game theory tophilosophyandpolitical science. The first mathematical discussion of theprisoner's dilemmaappeared, and an experiment was undertaken by mathematiciansMerrill M. FloodandMelvin Dresher, as part of theRAND Corporation's investigations into game theory. RAND pursued the studies because of possible applications to globalnuclear strategy.[15]
In 1965,Reinhard Seltenintroduced hissolution conceptofsubgame perfect equilibria, which further refined the Nash equilibrium. Later he would introducetrembling hand perfectionas well. In 1994 Nash, Selten andHarsanyibecameEconomics Nobel Laureatesfor their contributions to economic game theory.
In the 1970s, game theory was extensively applied inbiology, largely as a result of the work ofJohn Maynard Smithand hisevolutionarily stable strategy. In addition, the concepts ofcorrelated equilibrium,trembling hand perfectionandcommon knowledge[a]were introduced and analyzed.
In 1994, John Nash was awarded the Nobel Memorial Prize in the Economic Sciences for his contribution to game theory. Nash's most famous contribution to game theory is the concept of the Nash equilibrium, which is a solution concept fornon-cooperative games, published in 1951. A Nash equilibrium is a set of strategies, one for each player, such that no player can improve their payoff by unilaterally changing their strategy.
In 2005, game theoristsThomas SchellingandRobert Aumannfollowed Nash, Selten, and Harsanyi as Nobel Laureates. Schelling worked on dynamic models, early examples ofevolutionary game theory. Aumann contributed more to the equilibrium school, introducing equilibrium coarsening and correlated equilibria, and developing an extensive formal analysis of the assumption of common knowledge and of its consequences.
In 2007,Leonid Hurwicz,Eric Maskin, andRoger Myersonwere awarded the Nobel Prize in Economics "for having laid the foundations ofmechanism designtheory". Myerson's contributions include the notion ofproper equilibrium, and an important graduate text:Game Theory, Analysis of Conflict.[1]Hurwicz introduced and formalized the concept ofincentive compatibility.
In 2012,Alvin E. RothandLloyd S. Shapleywere awarded the Nobel Prize in Economics "for the theory of stable allocations and the practice of market design". In 2014, the Nobel went to game theoristJean Tirole.
A game iscooperativeif the players are able to form binding commitments externally enforced (e.g. throughcontract law). A game isnon-cooperativeif players cannot form alliances or if all agreements need to beself-enforcing(e.g. throughcredible threats).[16]
Cooperative games are often analyzed through the framework ofcooperative game theory, which focuses on predicting which coalitions will form, the joint actions that groups take, and the resulting collective payoffs. It is different fromnon-cooperative game theorywhich focuses on predicting individual players' actions and payoffs by analyzingNash equilibria.[17][18]
Cooperative game theory provides a high-level approach as it describes only the structure and payoffs of coalitions, whereas non-cooperative game theory also looks at how strategic interaction will affect the distribution of payoffs. As non-cooperative game theory is more general, cooperative games can be analyzed through the approach of non-cooperative game theory (the converse does not hold) provided that sufficient assumptions are made to encompass all the possible strategies available to players due to the possibility of external enforcement of cooperation.
A symmetric game is a game where each player earns the same payoff when making the same choice. In other words, the identity of the player does not change the resulting game facing the other player.[19]Many of the commonly studied 2×2 games are symmetric. The standard representations ofchicken, the prisoner's dilemma, and thestag huntare all symmetric games.
The most commonly studied asymmetric games are games where there are not identical strategy sets for both players. For instance, theultimatum gameand similarly thedictator gamehave different strategies for each player. It is possible, however, for a game to have identical strategies for both players, yet be asymmetric. For example, the game pictured in this section's graphic is asymmetric despite having identical strategy sets for both players.
Zero-sum games (more generally, constant-sum games) are games in which choices by players can neither increase nor decrease the available resources. In zero-sum games, the total benefit goes to all players in a game, for every combination of strategies, and always adds to zero (more informally, a player benefits only at the equal expense of others).[20]Pokerexemplifies a zero-sum game (ignoring the possibility of the house's cut), because one wins exactly the amount one's opponents lose. Other zero-sum games includematching penniesand most classical board games includingGoandchess.
Many games studied by game theorists (including the famed prisoner's dilemma) are non-zero-sum games, because theoutcomehas net results greater or less than zero. Informally, in non-zero-sum games, a gain by one player does not necessarily correspond with a loss by another.
Furthermore,constant-sum gamescorrespond to activities like theft and gambling, but not to the fundamental economic situation in which there are potentialgains from trade. It is possible to transform any constant-sum game into a (possibly asymmetric) zero-sum game by adding a dummy player (often called "the board") whose losses compensate the players' net winnings.
Simultaneous gamesare games where both players move simultaneously, or instead the later players are unaware of the earlier players' actions (making themeffectivelysimultaneous).Sequential games(a type of dynamic games) are games where players do not make decisions simultaneously, and player's earlier actions affect the outcome and decisions of other players.[21]This need not beperfect informationabout every action of earlier players; it might be very little knowledge. For instance, a player may know that an earlier player did not perform one particular action, while they do not know which of the other available actions the first player actually performed.
The difference between simultaneous and sequential games is captured in the different representations discussed above. Often,normal formis used to represent simultaneous games, whileextensive formis used to represent sequential ones. The transformation of extensive to normal form is one way, meaning that multiple extensive form games correspond to the same normal form. Consequently, notions of equilibrium for simultaneous games are insufficient for reasoning about sequential games; seesubgame perfection.
In short, the differences between sequential and simultaneous games are as follows:
An important subset of sequential games consists of games of perfect information. A game with perfect information means that all players, at every move in the game, know the previous history of the game and the moves previously made by all other players. An imperfect information game is played when the players do not know all moves already made by the opponent such as a simultaneous move game.[22]Examples of perfect-information games includetic-tac-toe,checkers,chess, andGo.[23][24][25]
Many card games are games of imperfect information, such aspokerandbridge.[26]Perfect information is often confused withcomplete information, which is a similar concept pertaining to the common knowledge of each player's sequence, strategies, and payoffs throughout gameplay.[27]Complete information requires that every player know the strategies and payoffs available to the other players but not necessarily the actions taken, whereas perfect information is knowledge of all aspects of the game and players.[28]Games ofincomplete informationcan be reduced, however, to games of imperfect information by introducing "moves by nature".[29]
One of the assumptions of the Nash equilibrium is that every player has correct beliefs about the actions of the other players. However, there are many situations in game theory where participants do not fully understand the characteristics of their opponents. Negotiators may be unaware of their opponent's valuation of the object of negotiation, companies may be unaware of their opponent's cost functions, combatants may be unaware of their opponent's strengths, and jurors may be unaware of their colleague's interpretation of the evidence at trial. In some cases, participants may know the character of their opponent well, but may not know how well their opponent knows his or her own character.[30]
Bayesian gamemeans a strategic game with incomplete information. For a strategic game, decision makers are players, and every player has a group of actions. A core part of the imperfect information specification is the set of states. Every state completely describes a collection of characteristics relevant to the player such as their preferences and details about them. There must be a state for every set of features that some player believes may exist.[31]
For example, where Player 1 is unsure whether Player 2 would rather date her or get away from her, while Player 2 understands Player 1's preferences as before. To be specific, supposing that Player 1 believes that Player 2 wants to date her under a probability of 1/2 and get away from her under a probability of 1/2 (this evaluation comes from Player 1's experience probably: she faces players who want to date her half of the time in such a case and players who want to avoid her half of the time). Due to the probability involved, the analysis  of this situation requires to understand the player's preference for the draw, even though people are only interested in pure strategic equilibrium.
Games in which the difficulty of finding an optimal strategy stems from the multiplicity of possible moves are called combinatorial games. Examples include chess andGo. Games that involveimperfect informationmay also have a strong combinatorial character, for instancebackgammon. There is no unified theory addressing combinatorial elements in games. There are, however, mathematical tools that can solve some particular problems and answer some general questions.[32]
Games of perfect information have been studied incombinatorial game theory, which has developed novel representations, e.g.surreal numbers, as well ascombinatorialandalgebraic(andsometimes non-constructive) proof methods tosolve gamesof certain types, including "loopy" games that may result in infinitely long sequences of moves. These methods address games with higher combinatorial complexity than those usually considered in traditional (or "economic") game theory.[33][34]A typical game that has been solved this way isHex. A related field of study, drawing fromcomputational complexity theory, isgame complexity, which is concerned with estimating the computational difficulty of finding optimal strategies.[35]
Research inartificial intelligencehas addressed both perfect and imperfect information games that have very complex combinatorial structures (like chess, go, or backgammon) for which no provable optimal strategies have been found. The practical solutions involve computational heuristics, likealpha–beta pruningor use ofartificial neural networkstrained byreinforcement learning, which make games more tractable in computing practice.[32][36]
Much of game theory is concerned with finite, discrete games that have a finite number of players, moves, events, outcomes, etc. Many concepts can be extended, however.Continuous gamesallow players to choose a strategy from a continuous strategy set. For instance,Cournot competitionis typically modeled with players' strategies being any non-negative quantities, including fractional quantities.
Differential games such as the continuouspursuit and evasion gameare continuous games where the evolution of the players' state variables is governed bydifferential equations. The problem of finding an optimal strategy in a differential game is closely related to theoptimal controltheory. In particular, there are two types of strategies: the open-loop strategies are found using thePontryagin maximum principlewhile the closed-loop strategies are found usingBellman's Dynamic Programmingmethod.
A particular case of differential games are the games with a randomtime horizon.[37]In such games, the terminal time is a random variable with a givenprobability distributionfunction. Therefore, the players maximize themathematical expectationof the cost function. It was shown that the modified optimization problem can be reformulated as a discounted differential game over an infinite time interval.
Evolutionary game theory studies players who adjust their strategies over time according to rules that are not necessarily rational or farsighted.[38]In general, the evolution of strategies over time according to such rules is modeled as aMarkov chainwith a state variable such as the current strategy profile or how the game has been played in the recent past. Such rules may feature imitation, optimization, or survival of the fittest.
In biology, such models can representevolution, in which offspring adopt their parents' strategies and parents who play more successful strategies (i.e. corresponding to higher payoffs) have a greater number of offspring. In the social sciences, such models typically represent strategic adjustment by players who play a game many times within their lifetime and, consciously or unconsciously, occasionally adjust their strategies.[39]
Individual decision problems with stochastic outcomes are sometimes considered "one-player games". They may be modeled using similar tools within the related disciplines ofdecision theory,operations research, and areas ofartificial intelligence, particularlyAI planning(with uncertainty) andmulti-agent system. Although these fields may have different motivators, the mathematics involved are substantially the same, e.g. usingMarkov decision processes(MDP).[40]
Stochastic outcomes can also be modeled in terms of game theory by adding a randomly acting player who makes "chance moves" ("moves by nature").[41]This player is not typically considered a third player in what is otherwise a two-player game, but merely serves to provide a roll of the dice where required by the game.
For some problems, different approaches to modeling stochastic outcomes may lead to different solutions. For example, the difference in approach between MDPs and theminimax solutionis that the latter considers the worst-case over a set of adversarial moves, rather than reasoning in expectation about these moves given a fixed probability distribution. The minimax approach may be advantageous where stochastic models of uncertainty are not available, but may also be overestimating extremely unlikely (but costly) events, dramatically swaying the strategy in such scenarios if it is assumed that an adversary can force such an event to happen.[42](SeeBlack swan theoryfor more discussion on this kind of modeling issue, particularly as it relates to predicting and limiting losses in investment banking.)
General models that include all elements of stochastic outcomes, adversaries, and partial or noisy observability (of moves by other players) have also been studied. The "gold standard" is considered to be partially observablestochastic game(POSG), but few realistic problems are computationally feasible in POSG representation.[42]
These are games the play of which is the development of the rules for another game, the target or subject game.Metagamesseek to maximize the utility value of the rule set developed. The theory of metagames is related tomechanism designtheory.
The termmetagame analysisis also used to refer to a practical approach developed by Nigel Howard,[43]whereby a situation is framed as a strategic game in which stakeholders try to realize their objectives by means of the options available to them. Subsequent developments have led to the formulation ofconfrontation analysis.
Mean field game theory is the study of strategic decision making in very large populations of small interacting agents. This class of problems was considered in the economics literature byBoyan JovanovicandRobert W. Rosenthal, in the engineering literature byPeter E. Caines, and by mathematiciansPierre-Louis Lionsand Jean-Michel Lasry.
The games studied in game theory are well-defined mathematical objects. To be fully defined, a game must specify the following elements: theplayersof the game, theinformationandactionsavailable to each player at each decision point, and thepayoffsfor each outcome. (Eric Rasmusen refers to these four "essential elements" by the acronym "PAPI".)[44][45][46][47]A game theorist typically uses these elements, along with asolution conceptof their choosing, to deduce a set of equilibriumstrategiesfor each player such that, when these strategies are employed, no player can profit by unilaterally deviating from their strategy. These equilibrium strategies determine anequilibriumto the game—a stable state in which either one outcome occurs or a set of outcomes occur with known probability.
Most cooperative games are presented in the characteristic function form, while the extensive and the normal forms are used to define noncooperative games.
The extensive form can be used to formalize games with a time sequencing of moves. Extensive form games can be visualized using gametrees(as pictured here). Here eachvertex(or node) represents a point of choice for a player. The player is specified by a number listed by the vertex. The lines out of the vertex represent a possible action for that player. The payoffs are specified at the bottom of the tree. The extensive form can be viewed as a multi-player generalization of adecision tree.[48]To solve any extensive form game,backward inductionmust be used. It involves working backward up the game tree to determine what a rational player would do at the last vertex of the tree, what the player with the previous move would do given that the player with the last move is rational, and so on until the first vertex of the tree is reached.[49]
The game pictured consists of two players.  The way this particular game is structured (i.e., with sequential decision making and perfect information),Player 1"moves" first by choosing eitherForU(fair or unfair). Next in the sequence,Player 2, who has now observedPlayer 1's move, can choose to play eitherAorR(accept or reject). OncePlayer 2has made their choice, the game is considered finished and each player gets their respective payoff, represented in the image as two numbers, where the first number represents Player 1's payoff, and the second number represents Player 2's payoff.  Suppose thatPlayer 1choosesUand thenPlayer 2choosesA:Player 1then gets a payoff of "eight" (which in real-world terms can be interpreted in many ways, the simplest of which is in terms of money but could mean things such as eight days of vacation or eight countries conquered or even eight more opportunities to play the same game against other players) andPlayer 2gets a payoff of "two".
The extensive form can also capture simultaneous-move games and games with imperfect information. To represent it, either a dotted line connects different vertices to represent them as being part of the same information set (i.e. the players do not know at which point they are), or a closed line is drawn around them. (See example in theimperfect information section.)
The normal (or strategic form) game is usually represented by amatrixwhich shows the players, strategies, and payoffs (see the example to the right). More generally it can be represented by any function that associates a payoff for each player with every possible combination of actions. In the accompanying example there are two players; one chooses the row and the other chooses the column. Each player has two strategies, which are specified by the number of rows and the number of columns. The payoffs are provided in the interior. The first number is the payoff received by the row player (Player 1 in our example); the second is the payoff for the column player (Player 2 in our example). Suppose that Player 1 playsUpand that Player 2 playsLeft. Then Player 1 gets a payoff of 4, and Player 2 gets 3.
When a game is presented in normal form, it is presumed that each player acts simultaneously or, at least, without knowing the actions of the other. If players have some information about the choices of other players, the game is usually presented in extensive form.
Every extensive-form game has an equivalent normal-form game, however, the transformation to normal form may result in an exponential blowup in the size of the representation, making it computationally impractical.[50]
In cooperative game theory the characteristic function lists the payoff of each coalition. The origin of this formulation is in John von Neumann and Oskar Morgenstern's book.[51]
Formally, a characteristic function is a functionv:2N→R{\displaystyle v:2^{N}\to \mathbb {R} }[52]from the set of all possible coalitions of players to a set of payments, and also satisfiesv(∅)=0{\displaystyle v(\emptyset )=0}. The function describes how much collective payoff a set of players can gain by forming a coalition.
Alternative game representation forms are used for some subclasses of games or adjusted to the needs of interdisciplinary research.[53]In addition to classical game representations, some of the alternative representations also encode time related aspects.
As a method ofapplied mathematics, game theory has been used to study a wide variety of human and animal behaviors. It was initially developed ineconomicsto understand a large collection of economic behaviors, including behaviors of firms, markets, and consumers. The first use of game-theoretic analysis was byAntoine Augustin Cournotin 1838 with his solution of theCournot duopoly. The use of game theory in the social sciences has expanded, and game theory has been applied to political, sociological, and psychological behaviors as well.[68]
Although pre-twentieth-centurynaturalistssuch asCharles Darwinmade game-theoretic kinds of statements, the use of game-theoretic analysis in biology began withRonald Fisher's studies of animal behavior during the 1930s. This work predates the name "game theory", but it shares many important features with this field. The developments in economics were later applied to biology largely by John Maynard Smith in his 1982 bookEvolution and the Theory of Games.[69]
In addition to being used to describe, predict, and explain behavior, game theory has also been used to develop theories of ethical or normative behavior and toprescribesuch behavior.[70]Ineconomics and philosophy, scholars have applied game theory to help in the understanding of good or proper behavior. Game-theoretic approaches have also been suggested in thephilosophy of languageandphilosophy of science.[71]Game-theoretic arguments of this type can be found as far back asPlato.[72]An alternative version of game theory, calledchemical game theory, represents the player's choices as metaphorical chemical reactant molecules called "knowlecules".[73]Chemical game theory then calculates the outcomes as equilibrium solutions to a system of chemical reactions.
The primary use of game theory is to describe andmodelhow human populations behave.[citation needed]Some[who?]scholars believe that by finding the equilibria of games they can predict how actual human populations will behave when confronted with situations analogous to the game being studied. This particular view of game theory has been criticized. It is argued that the assumptions made by game theorists are often violated when applied to real-world situations. Game theorists usually assume players act rationally, but in practice, human rationality and/or behavior often deviates from the model of rationality as used in game theory. Game theorists respond by comparing their assumptions to those used inphysics. Thus while their assumptions do not always hold, they can treat game theory as a reasonable scientificidealakin to the models used byphysicists. However, empirical work has shown that in some classic games, such as thecentipede game,guess 2/3 of the averagegame, and thedictator game, people regularly do not play Nash equilibria. There is an ongoing debate regarding the importance of these experiments and whether the analysis of the experiments fully captures all aspects of the relevant situation.[b]
Some game theorists, following the work of John Maynard Smith andGeorge R. Price, have turned to evolutionary game theory in order to resolve these issues. These models presume either no rationality orbounded rationalityon the part of players. Despite the name, evolutionary game theory does not necessarily presumenatural selectionin the biological sense. Evolutionary game theory includes both biological as well as cultural evolution and also models of individual learning (for example,fictitious playdynamics).
Some scholars see game theory not as a predictive tool for the behavior of human beings, but as a suggestion for how people ought to behave. Since a strategy, corresponding to a Nash equilibrium of a game constitutes one'sbest responseto the actions of the other players – provided they are in (the same) Nash equilibrium – playing a strategy that is part of a Nash equilibrium seems appropriate. This normative use of game theory has also come under criticism.[75]
Game theory is a major method used in mathematical economics and business formodelingcompeting behaviors of interactingagents.[c][76][77][78]Applications include a wide array of economic phenomena and approaches, such asauctions,bargaining,mergers and acquisitionspricing,[79]fair division,duopolies,oligopolies,social networkformation,agent-based computational economics,[80][81]general equilibrium, mechanism design,[82][83][84][85][86]andvoting systems;[87]and across such broad areas as experimental economics,[88][89][90][91][92]behavioral economics,[93][94][95][96][97][98]information economics,[44][45][46][47]industrial organization,[99][100][101][102]andpolitical economy.[103][104][105][46]
This research usually focuses on particular sets of strategies known as"solution concepts" or "equilibria". A common assumption is that players act rationally. In non-cooperative games, the most famous of these is the Nash equilibrium. A set of strategies is a Nash equilibrium if each represents a best response to the other strategies. If all the players are playing the strategies in a Nash equilibrium, they have no unilateral incentive to deviate, since their strategy is the best they can do given what others are doing.[106][107]
The payoffs of the game are generally taken to represent theutilityof individual players.
A prototypical paper on game theory in economics begins by presenting a game that is an abstraction of a particular economic situation. One or more solution concepts are chosen, and the author demonstrates which strategy sets in the presented game are equilibria of the appropriate type. Economists and business professors suggest two primary uses (noted above):descriptiveandprescriptive.[70]
Game theory also has an extensive use in a specific branch or stream of economics –Managerial Economics. One important usage of it in the field of managerial economics is in analyzing strategic interactions between firms.[108]For example, firms may be competing in a market with limited resources, and game theory can help managers understand how their decisions impact their competitors and the overall market outcomes. Game theory can also be used to analyze cooperation between firms, such as in forming strategic alliances or joint ventures. Another use of game theory in managerial economics is in analyzing pricing strategies. For example, firms may use game theory to determine the optimalpricing strategybased on how they expect their competitors to respond to their pricing decisions. Overall, game theory serves as a useful tool for analyzing strategic interactions and decision making in the context of managerial economics.
TheChartered Institute of Procurement & Supply(CIPS) promotes knowledge and use of game theory within the context of businessprocurement.[109]CIPS and TWS Partners have conducted a series of surveys designed to explore the understanding, awareness and application of game theory amongprocurementprofessionals. Some of the main findings in their third annual survey (2019) include:
Sensible decision-making is critical for the success of projects.  In project management, game theory is used to model the decision-making process of players, such as investors, project managers, contractors, sub-contractors, governments and customers.  Quite often, these players have competing interests, and sometimes their interests are directly detrimental to other players, making project management scenarios well-suited to be modeled by game theory.
Piraveenan (2019)[111]in his review provides several examples where game theory is used to model project management scenarios. For instance, an investor typically has several investment options, and each option will likely result in a different project, and thus one of the investment options has to be chosen before the project charter can be produced. Similarly, any large project involving subcontractors, for instance, a construction project, has a complex interplay between the main contractor (the project manager) and subcontractors, or among the subcontractors themselves, which typically has several decision points. For example, if there is an ambiguity in the contract between the contractor and subcontractor, each must decide how hard to push their case without jeopardizing the whole project, and thus their own stake in it. Similarly, when projects from competing organizations are launched, the marketing personnel have to decide what is the best timing and strategy to market the project, or its resultant product or service, so that it can gain maximum traction in the face of competition. In each of these scenarios, the required decisions depend on the decisions of other players who, in some way, have competing interests to the interests of the decision-maker, and thus can ideally be modeled using game theory.
Piraveenan[111]summarizes that two-player games are predominantly used to model project management scenarios, and based on the identity of these players, five distinct types of games are used in project management.
In terms of types of games, both cooperative as well as non-cooperative, normal-form as well as extensive-form, and zero-sum as well as non-zero-sum  are used to model various project management scenarios.
The application of game theory topolitical scienceis focused in the overlapping areas offair division,political economy,public choice,war bargaining,positive political theory, andsocial choice theory. In each of these areas, researchers have developed game-theoretic models in which the players are often voters, states, special interest groups, and politicians.[112]
Early examples of game theory applied to political science are provided byAnthony Downs. In his 1957 bookAn Economic Theory of Democracy,[113]he applies theHotelling firm location modelto the political process. In the Downsian model, political candidates commit to ideologies on a one-dimensional policy space. Downs first shows how the political candidates will converge to the ideology preferred by the median voter if voters are fully informed, but then argues that voters choose to remain rationally ignorant which allows for candidate divergence. Game theory was applied in 1962 to theCuban Missile Crisisduring the presidency of John F. Kennedy.[114]
It has also been proposed that game theory explains the stability of any form of political government.  Taking the simplest case of a monarchy, for example, the king, being only one person, does not and cannot maintain his authority by personally exercising physical control over all or even any significant number of his subjects.  Sovereign control is instead explained by the recognition by each citizen that all other citizens expect each other to view the king (or other established government) as the person whose orders will be followed.  Coordinating communication among citizens to replace the sovereign is effectively barred, since conspiracy to replace the sovereign is generally punishable as a crime.[115]Thus, in a process that can be modeled by variants of the prisoner's dilemma, during periods of stability no citizen will find it rational to move to replace the sovereign, even if all the citizens know they would be better off if they were all to act collectively.[citation needed]
A game-theoretic explanation fordemocratic peaceis that public and open debate in democracies sends clear and reliable information regarding their intentions to other states. In contrast, it is difficult to know the intentions of nondemocratic leaders, what effect concessions will have, and if promises will be kept. Thus there will be mistrust and unwillingness to make concessions if at least one of the parties in a dispute is a non-democracy.[116]
However, game theory predicts that two countries may still go to war even if their leaders are cognizant of the costs of fighting. War may result from asymmetric information; two countries may have incentives to mis-represent the amount of military resources they have on hand, rendering them unable to settle disputes agreeably without resorting to fighting. Moreover, war may arise because of commitment problems: if two countries wish to settle a dispute via peaceful means, but each wishes to go back on the terms of that settlement, they may have no choice but to resort to warfare. Finally, war may result from issue indivisibilities.[117]
Game theory could also help predict a nation's responses when there is a new rule or law to be applied to that nation. One example is Peter John Wood's (2013) research looking into what nations could do to help reduce climate change. Wood thought this could be accomplished by making treaties with other nations to reducegreenhouse gas emissions. However, he concluded that this idea could not work because it would create a prisoner's dilemma for the nations.[118]
Game theory has been used extensively to model decision-making scenarios relevant to defence applications.[119]Most studies that has applied game theory in defence settings are concerned with Command and Control Warfare, and can be further classified into studies dealing with (i) Resource Allocation Warfare (ii) Information Warfare (iii) Weapons Control Warfare, and (iv) Adversary Monitoring Warfare.[119]Many of the problems studied are concerned with sensing and tracking, for example a surface ship trying to track a hostile submarine and the submarine trying to evade being tracked, and the interdependent decision making that takes place with regards to bearing, speed, and the sensor technology activated by both vessels.
The tool,[120]for example, automates the transformation of public vulnerability data into models, allowing defenders to synthesize optimal defence strategies through Stackelberg equilibrium analysis. This approach enhances cyber resilience by enabling defenders to anticipate and counteract attackers’ best responses, making game theory increasingly relevant in adversarial cybersecurity environments.
Ho et al. provide a broad summary of game theory applications in defence, highlighting its advantages and limitations across both physical and cyber domains.
Unlike those in economics, the payoffs for games inbiologyare often interpreted as corresponding tofitness. In addition, the focus has been less on equilibria that correspond to a notion of rationality and more on ones that would be maintained by evolutionary forces. The best-known equilibrium in biology is known as theevolutionarily stable strategy(ESS), first introduced in (Maynard Smith & Price 1973). Although its initial motivation did not involve any of the mental requirements of the Nash equilibrium, every ESS is a Nash equilibrium.
In biology, game theory has been used as a model to understand many different phenomena. It was first used to explain the evolution (and stability) of the approximate 1:1sex ratios. (Fisher 1930) suggested that the 1:1 sex ratios are a result of evolutionary forces acting on individuals who could be seen as trying to maximize their number of grandchildren.
Additionally, biologists have used evolutionary game theory and the ESS to explain the emergence ofanimal communication.[121]The analysis ofsignaling gamesandother communication gameshas provided insight into the evolution of communication among animals. For example, themobbing behaviorof many species, in which a large number of prey animals attack a larger predator, seems to be an example of spontaneous emergent organization. Ants have also been shown to exhibit feed-forward behavior akin to fashion (seePaul Ormerod'sButterfly Economics).
Biologists have used thegame of chickento analyze fighting behavior and territoriality.[122]
According to Maynard Smith, in the preface toEvolution and the Theory of Games, "paradoxically, it has turned out that game theory is more readily applied to biology than to the field of economic behaviour for which it was originally designed". Evolutionary game theory has been used to explain many seemingly incongruous phenomena in nature.[123]
One such phenomenon is known asbiological altruism. This is a situation in which an organism appears to act in a way that benefits other organisms and is detrimental to itself. This is distinct from traditional notions of altruism because such actions are not conscious, but appear to be evolutionary adaptations to increase overall fitness. Examples can be found in species ranging from vampire bats that regurgitate blood they have obtained from a night's hunting and give it to group members who have failed to feed, to worker bees that care for the queen bee for their entire lives and never mate, tovervet monkeysthat warn group members of a predator's approach, even when it endangers that individual's chance of survival.[124]All of these actions increase the overall fitness of a group, but occur at a cost to the individual.
Evolutionary game theory explains this altruism with the idea ofkin selection. Altruists discriminate between the individuals they help and favor relatives.Hamilton's ruleexplains the evolutionary rationale behind this selection with the equationc < b × r, where the costcto the altruist must be less than the benefitbto the recipient multiplied by the coefficient of relatednessr. The more closely related two organisms are causes the incidences of altruism to increase because they share many of the same alleles. This means that the altruistic individual, by ensuring that the alleles of its close relative are passed on through survival of its offspring, can forgo the option of having offspring itself because the same number of alleles are passed on. For example, helping a sibling (in diploid animals) has a coefficient of1⁄2, because (on average) an individual shares half of the alleles in its sibling's offspring. Ensuring that enough of a sibling's offspring survive to adulthood precludes the necessity of the altruistic individual producing offspring.[124]The coefficient values depend heavily on the scope of the playing field; for example if the choice of whom to favor includes all genetic living things, not just all relatives, we assume the discrepancy between all humans only accounts for approximately 1% of the diversity in the playing field, a coefficient that was1⁄2in the smaller field becomes 0.995. Similarly if it is considered that information other than that of a genetic nature (e.g. epigenetics, religion, science, etc.) persisted through time the playing field becomes larger still, and the discrepancies smaller.
Game theory has come to play an increasingly important role inlogicand incomputer science. Several logical theories have a basis ingame semantics. In addition, computer scientists have used games to modelinteractive computations. Also, game theory provides a theoretical basis to the field ofmulti-agent systems.[125]
Separately, game theory has played a role inonline algorithms; in particular, thek-server problem, which has in the past been referred to asgames with moving costsandrequest-answer games.[126]Yao's principleis a game-theoretic technique for provinglower boundson thecomputational complexityofrandomized algorithms, especially online algorithms.
The emergence of the Internet has motivated the development of algorithms for finding equilibria in games, markets, computational auctions, peer-to-peer systems, and security and information markets.Algorithmic game theory[86]and within italgorithmic mechanism design[85]combine computationalalgorithm designand analysis ofcomplex systemswith economic theory.[127][128][129]
Game theory has multiple applications in the field of artificial intelligence and machine learning. It is often used in developing autonomous systems that can make complex decisions in uncertain environment.[130]Some other areas of application of game theory in AI/ML context are as follows - multi-agent system formation, reinforcement learning,[131]mechanism design etc.[132]By using game theory to model the behavior of other agents and anticipate their actions, AI/ML systems can make better decisions and operate more effectively.[133]
Game theory has been put to several uses inphilosophy. Responding to two papers byW.V.O. Quine(1960,1967),Lewis (1969)used game theory to develop a philosophical account ofconvention. In so doing, he provided the first analysis ofcommon knowledgeand employed it in analyzing play incoordination games. In addition, he first suggested that one can understandmeaningin terms ofsignaling games. This later suggestion has been pursued by several philosophers since Lewis.[134][135]FollowingLewis (1969)game-theoretic account of conventions, Edna Ullmann-Margalit (1977) andBicchieri(2006) have developed theories ofsocial normsthat define them as Nash equilibria that result from transforming a mixed-motive game into a coordination game.[136][137]
Game theory has also challenged philosophers to think in terms of interactiveepistemology: what it means for a collective to have common beliefs or knowledge, and what are the consequences of this knowledge for the social outcomes resulting from the interactions of agents. Philosophers who have worked in this area include Bicchieri (1989, 1993),[138][139]Skyrms(1990),[140]andStalnaker(1999).[141]
The synthesis of game theory withethicswas championed byR. B. Braithwaite.[142]The hope was that rigorous mathematical analysis of game theory might help formalize the more imprecise philosophical discussions. However, this expectation was only materialized to a limited extent.[143]
Inethics, some (most notably David Gauthier, Gregory Kavka, and Jean Hampton)[who?]authors have attempted to pursueThomas Hobbes' project of deriving morality from self-interest. Since games like theprisoner's dilemmapresent an apparent conflict between morality and self-interest, explaining why cooperation is required by self-interest is an important component of this project. This general strategy is a component of the generalsocial contractview inpolitical philosophy(for examples, seeGauthier (1986)andKavka (1986)).[d]
Other authors have attempted to use evolutionary game theory in order to explain the emergence of human attitudes about morality and corresponding animal behaviors. These authors look at several games including the prisoner's dilemma,stag hunt, and theNash bargaining gameas providing an explanation for the emergence of attitudes about morality (see, e.g., Skyrms (1996,2004) and Sober and Wilson (1998)).
Since the decision to take a vaccine for a particular disease is often made by individuals, who may consider a range of factors and parameters in making this decision (such as the incidence and prevalence of the disease, perceived and real risks associated with contracting the disease,  mortality rate, perceived and real risks associated with vaccination, and financial cost of vaccination), game theory has been used to model and predict vaccination uptake in a society.[144][145]
William Poundstonedescribed the game in his 1993 book Prisoner's Dilemma:[146]
Two members of a criminal gang, A and B, are arrested and imprisoned. Each prisoner is in solitary confinement with no means of communication with their partner. The principal charge would lead to a sentence of ten years in prison; however, the police do not have the evidence for a conviction. They plan to sentence both to two years in prison on a lesser charge but offer each prisoner a Faustian bargain: If one of them confesses to the crime of the principal charge, betraying the other, they will be pardoned and free to leave while the other must serve the entirety of the sentence instead of just two years for the lesser charge.
Thedominant strategy(and therefore the best response to any possible opponent strategy), is to betray the other, which aligns with thesure-thing principle.[147]However, both prisoners staying silent would yield a greater reward for both of them than mutual betrayal.
The "battle of the sexes" is a term used to describe the perceived conflict between men and women in various areas of life, such as relationships, careers, and social roles. This conflict is often portrayed in popular culture, such as movies and television shows, as a humorous or dramatic competition between the genders. This conflict can be depicted in a game theory framework. This is an example of non-cooperative games.
An example of the "battle of the sexes" can be seen in the portrayal of relationships in popular media, where men and women are often depicted as being fundamentally different and in conflict with each other. For instance, in some romantic comedies, the male and female protagonists are shown as having opposing views on love and relationships, and they have to overcome these differences in order to be together.[148]
In this game, there are two pure strategy Nash equilibria: one where both the players choose the same strategy and the other where the players choose different options. If the game is played in mixed strategies, where each player chooses their strategy randomly, then there is an infinite number of Nash equilibria. However, in the context of the "battle of the sexes" game, the assumption is usually made that the game is played in pure strategies.[149]
The ultimatum game is a game that has become a popular instrument ofeconomic experiments. An early description is by Nobel laureateJohn Harsanyiin 1961.[150]
One player, the proposer, is endowed with a sum of money. The proposer is tasked with splitting it with another player, the responder (who knows what the total sum is). Once the proposer communicates his decision, the responder may accept it or reject it. If the responder accepts, the money is split per the proposal; if the responder rejects, both players receive nothing.  Both players know in advance the consequences of the responder accepting or rejecting the offer. The game demonstrates how social acceptance, fairness, and generosity influence the players decisions.[151]
Ultimatum game has a variant, that is the dictator game. They are mostly identical, except in dictator game the responder has no power to reject the proposer's offer.
The Trust Game is an experiment designed to measure trust in economic decisions. It is also called "the investment game" and is designed to investigate trust and demonstrate its importance rather than "rationality" of self-interest. The game was designed by Berg Joyce, John Dickhaut and Kevin McCabe in 1995.[152]
In the game, one player (the investor) is given a sum of money and must decide how much of it to give to another player (the trustee). The amount given is then tripled by the experimenter. The trustee then decides how much of the tripled amount to return to the investor. If the recipient is completely self interested, then he/she should return nothing. However that is not true as the experiment conduct. The outcome suggest that people are willing to place a trust, by risking some amount of money, in the belief that there would be reciprocity.[153]
The Cournot competition model involves players choosing quantity of a homogenous product to produce independently and simultaneously, wheremarginal costcan be different for each firm and the firm's payoff is profit. The production costs are public information and the firm aims to find their profit-maximizing quantity based on what they believe the other firm will produce and behave like monopolies. In this game firms want to produce at the monopoly quantity but there is a high incentive to deviate and produce more, which decreases the market-clearing price.[22]For example, firms may be tempted to deviate from the monopoly quantity if there is a low monopoly quantity and high price, with the aim of increasing production to maximize  profit.[22]However this option does not provide the highest payoff, as a firm's ability to maximize profits depends on its market share and the elasticity of the market demand.[154]The Cournot equilibrium is reached when each firm operates on their reaction function with no incentive to deviate, as they have the best response based on the other firms output.[22]Within the game, firms reach the Nash equilibrium when the Cournot equilibrium is achieved.
The Bertrand competition assumes homogenous products and a constant marginal cost and players choose the prices.[22]The equilibrium of price competition is where the price is equal to marginal costs, assuming complete information about the competitors' costs. Therefore, the firms have an incentive to deviate from the equilibrium because a homogenous product with a lower price will gain all of the market share, known as a cost advantage.[155]
Topology(from theGreekwordsτόπος, 'place, location', andλόγος, 'study') is the branch ofmathematicsconcerned with the properties of ageometric objectthat are preserved undercontinuousdeformations, such asstretching,twisting, crumpling, and bending; that is, without closing holes, opening holes, tearing, gluing, or passing through itself.
Atopological spaceis asetendowed with a structure, called atopology, which allows defining continuous deformation of subspaces, and, more generally, all kinds ofcontinuity.Euclidean spaces, and, more generally,metric spacesare examples of topological spaces, as any distance or metric defines a topology. The deformations that are considered in topology arehomeomorphismsandhomotopies. A property that is invariant under such deformations is atopological property. The following are basic examples of topological properties: thedimension, which allows distinguishing between alineand asurface;compactness, which allows distinguishing between a line and a circle;connectedness, which allows distinguishing a circle from two non-intersecting circles.
The ideas underlying topology go back toGottfried Wilhelm Leibniz, who in the 17th century envisioned thegeometria situsandanalysis situs.Leonhard Euler'sSeven Bridges of Königsbergproblem andpolyhedron formulaare arguably the field's first theorems. The termtopologywas introduced byJohann Benedict Listingin the 19th century, although, it was not until the first decades of the 20th century that the idea of a topological space was developed.
The motivating insight behind topology is that some geometric problems depend not on the exact shape of the objects involved, but rather on the way they are put together. For example, the square and the circle have many properties in common: they are both one-dimensional objects (from a topological point of view) and both separate the plane into two parts, the part inside and the part outside.
In one of the first papers in topology,Leonhard Eulerdemonstrated that it was impossible to find a route through the town of Königsberg (nowKaliningrad) that would cross each of its seven bridges exactly once. This result did not depend on the lengths of the bridges or on their distance from one another, but only on connectivity properties: which bridges connect to which islands or riverbanks. ThisSeven Bridges of Königsbergproblem led to the branch of mathematics known asgraph theory.
Similarly, thehairy ball theoremof algebraic topology says that "one cannot comb the hair flat on a hairy ball without creating acowlick." This fact is immediately convincing to most people, even though they might not recognize the more formal statement of the theorem, that there is no nonvanishing continuoustangent vector fieldon the sphere. As with theBridges of Königsberg, the result does not depend on the shape of the sphere; it applies to any kind of smooth blob, as long as it has no holes.
To deal with these problems that do not rely on the exact shape of the objects, one must be clear about just what properties these problems do rely on. From this need arises the notion ofhomeomorphism. The impossibility of crossing each bridge just once applies to any arrangement of bridgeshomeomorphicto those in Königsberg, and the hairy ball theorem applies to any space homeomorphic to a sphere.
Intuitively, two spaces are homeomorphic if one can be deformed into the other without cutting or gluing. A famous example, known as the "Topologist's Breakfast", is that a topologist cannot distinguish a coffee mug from a doughnut; a sufficiently pliable doughnut could be reshaped to a coffee cup by creating a dimple and progressively enlarging it while shrinking the hole into a handle.[1]
Homeomorphism can be considered the most basictopological equivalence. Another ishomotopy equivalence. This is harder to describe without getting technical, but the essential notion is that two objects are homotopy equivalent if they both result from "squishing" some larger object.
Topology, as a well-defined mathematical discipline, originates in the early part of the twentieth century, but some isolated results can be traced back several centuries.[2]Among these are certain questions in geometry investigated byLeonhard Euler. His 1736 paper on theSeven Bridges of Königsbergis regarded as one of the first practical applications of topology.[2]On 14 November 1750, Euler wrote to a friend that he had realized the importance of theedgesof apolyhedron. This led to hispolyhedron formula,V−E+F= 2(whereV,E, andFrespectively indicate the number of vertices, edges, and faces of the polyhedron). Some authorities regard this analysis as the first theorem, signaling the birth of topology.[3]
Further contributions were made byAugustin-Louis Cauchy,Ludwig Schläfli,Johann Benedict Listing,Bernhard RiemannandEnrico Betti.[4]Listing introduced the term "Topologie" inVorstudien zur Topologie, written in his native German, in 1847, having used the word for ten years in correspondence before its first appearance in print.[5]The English form "topology" was used in 1883 in Listing's obituary in the journalNatureto distinguish "qualitative geometry from the ordinary geometry in which quantitative relations chiefly are treated".[6]
Their work was corrected, consolidated and greatly extended byHenri Poincaré. In 1895, he published his ground-breaking paper onAnalysis Situs, which introduced the concepts now known ashomotopyandhomology, which are now considered part ofalgebraic topology.[4]
The development of topology in the 20th century was marked by significant advances in both foundational theory and its application to other fields of mathematics. Unifying the work on function spaces ofGeorg Cantor,Vito Volterra,Cesare Arzelà,Jacques Hadamard,Giulio Ascoliand others,Maurice Fréchetintroduced themetric spacein 1906.[7]A metric space is now considered a special case of a general topological space, with any given topological space potentially giving rise to many distinct metric spaces. In 1914,Felix Hausdorffcoined the term "topological space" and defined what is now called aHausdorff space.[8]Currently, a topological space is a slight generalization of Hausdorff spaces, given in 1922 byKazimierz Kuratowski.[9]
Modern topology depends strongly on the ideas of set theory, developed by Georg Cantor in the later part of the 19th century. In addition to establishing the basic ideas of set theory, Cantor considered point sets inEuclidean spaceas part of his study ofFourier series. For further developments, seepoint-set topologyand algebraic topology.
The 2022Abel Prizewas awarded toDennis Sullivan"for his groundbreaking contributions to topology in its broadest sense, and in particular its algebraic, geometric and dynamical aspects".[10]
The termtopologyalso refers to a specific mathematical idea central to the area of mathematics called topology. Informally, a topology describes how elements of a set relate spatially to each other. The same set can have different topologies. For instance, thereal line, thecomplex plane, and theCantor setcan be thought of as the same set with different topologies.
Formally, letXbe a set and letτbe afamilyof subsets ofX. Thenτis called a topology onXif:
Ifτis a topology onX, then the pair(X,τ)is called a topological space. The notationXτmay be used to denote a setXendowed with the particular topologyτ. By definition, every topology is aπ-system.
The members ofτare calledopen setsinX. A subset ofXis said to be closed if its complement is inτ(that is, its complement is open). A subset ofXmay be open, closed, both (aclopen set), or neither. The empty set andXitself are always both closed and open. An open subset ofXwhich contains a pointxis called an openneighborhoodofx.
Afunctionor map from one topological space to another is calledcontinuousif the inverseimageof any open set is open. If the function maps thereal numbersto the real numbers (both spaces with the standard topology), then this definition of continuous is equivalent to the definition of continuous incalculus. If a continuous function isone-to-oneandonto, and if the inverse of the function is also continuous, then the function is called a homeomorphism and the domain of the function is said to be homeomorphic to the range. Another way of saying this is that the function has a natural extension to the topology. If two spaces are homeomorphic, they have identical topological properties and are considered topologically the same. The cube and the sphere are homeomorphic, as are the coffee cup and the doughnut. However, the sphere is not homeomorphic to the doughnut.
While topological spaces can be extremely varied and exotic, many areas of topology focus on the more familiar class of spaces known as manifolds. Amanifoldis a topological space that resembles Euclidean space near each point.  More precisely, each point of ann-dimensional manifold has aneighborhoodthat ishomeomorphicto the Euclidean space of dimensionn.Linesandcircles, but notfigure eights, are one-dimensional manifolds.  Two-dimensional manifolds are also calledsurfaces, although not allsurfacesare manifolds.  Examples include theplane, the sphere, and the torus, which can all be realized without self-intersection in three dimensions, and theKlein bottleandreal projective plane, which cannot (that is, all their realizations are surfaces that are not manifolds).
General topology is the branch of topology dealing with the basic set-theoretic definitions and constructions used in topology.[11][12]It is the foundation of most other branches of topology, including differential topology, geometric topology, and algebraic topology. Another name for general topology is point-set topology.
The basic object of study istopological spaces, which are sets equipped with atopology, that is, a family ofsubsets, calledopen sets, which isclosedunder finiteintersectionsand (finite or infinite)unions. The fundamental concepts of topology, such ascontinuity,compactness, andconnectedness, can be defined in terms of open sets. Intuitively, continuous functions take nearby points to nearby points. Compact sets are those that can be covered by finitely many sets of arbitrarily small size. Connected sets are sets that cannot be divided into two pieces that are far apart. The wordsnearby,arbitrarily small, andfar apartcan all be made precise by using open sets. Several topologies can be defined on a given space. Changing a topology consists of changing the collection of open sets. This changes which functions are continuous and which subsets are compact or connected.
Metric spacesare an important class of topological spaces where the distance between any two points is defined by a function called ametric. In a metric space, an open set is a union of open disks, where an open disk of radiusrcentered atxis the set of all points whose distance toxis less thanr. Many common spaces are topological spaces whose topology can be defined by a metric. This is the case of thereal line, thecomplex plane, real and complexvector spacesandEuclidean spaces. Having a metric simplifies many proofs.
Algebraic topology is a branch of mathematics that uses tools fromalgebrato study topological spaces.[13]The basic goal is to find algebraic invariants thatclassifytopological spacesup tohomeomorphism, though usually most classify up to homotopy equivalence.
The most important of these invariants arehomotopy groups, homology, andcohomology.
Although algebraic topology primarily uses algebra to study topological problems, using topology to solve algebraic problems is sometimes also possible. Algebraic topology, for example, allows for a convenient proof that any subgroup of afree groupis again a free group.
Differential topology is the field dealing withdifferentiable functionsondifferentiable manifolds.[14]It is closely related todifferential geometryand together they make up the geometric theory of differentiable manifolds.
More specifically, differential topology considers the properties and structures that require only asmooth structureon a manifold to be defined. Smooth manifolds are "softer" than manifolds with extra geometric structures, which can act as obstructions to certain types of equivalences anddeformationsthat exist in differential topology. For instance, volume andRiemannian curvatureare invariants that can distinguish different geometric structures on the same smooth manifold – that is, one can smoothly "flatten out" certain manifolds, but it might require distorting the space and affecting the curvature or volume.
Geometric topology is a branch of topology that primarily focuses on low-dimensionalmanifolds(that is, spaces of dimensions 2, 3, and 4) and their interaction with geometry, but it also includes some higher-dimensional topology.[15]Some examples of topics in geometric topology areorientability,handle decompositions,local flatness, crumpling and the planar and higher-dimensionalSchönflies theorem.
In high-dimensional topology,characteristic classesare a basic invariant, andsurgery theoryis a key theory.
Low-dimensional topology is strongly geometric, as reflected in theuniformization theoremin 2 dimensions – every surface admits a constant curvature metric; geometrically, it has one of 3 possible geometries: positivecurvature/spherical, zero curvature/flat, and negative curvature/hyperbolic – and thegeometrization conjecture(now theorem) in 3 dimensions – every 3-manifold can be cut into pieces, each of which has one of eight possible geometries.
2-dimensional topology can be studied ascomplex geometryin one variable (Riemannsurfaces are complex curves) – by the uniformization theorem everyconformal classofmetricsis equivalent to a unique complex one, and 4-dimensional topology can be studied from the point of view of complex geometry in two variables (complex surfaces), though not every 4-manifold admits a complex structure.
Occasionally, one needs to use the tools of topology but a "set of points" is not available. Inpointless topologyone considers instead thelatticeof open sets as the basic notion of the theory,[16]whileGrothendieck topologiesare structures defined on arbitrarycategoriesthat allow the definition ofsheaveson those categories and with that the definition of general cohomology theories.[17]
Topology has been used to study various biological systems including molecules and nanostructure (e.g., membraneous objects). In particular,circuit topologyandknot theoryhave been extensively applied to classify and compare the topology of folded proteins and nucleic acids.Circuit topologyclassifies folded molecular chains based on the pairwise arrangement of their intra-chain contacts and chain crossings.Knot theory, a branch of topology, is used in biology to study the effects of certain enzymes on DNA. These enzymes cut, twist, and reconnect the DNA, causing knotting with observable effects such as slowerelectrophoresis.[18]
Topological data analysisuses techniques from algebraic topology to determine the large-scale structure of a set (for instance, determining if a cloud of points is spherical ortoroidal). The main method used by topological data analysis is to:
Several branches ofprogramming language semantics, such asdomain theory, are formalized using topology. In this context,Steve Vickers, building on work bySamson Abramskyand Michael B. Smyth, characterizes topological spaces asBooleanorHeyting algebrasover open sets, which are characterized assemidecidable(equivalently, finitely observable) properties.[20]
Topology is relevant to physics in areas such ascondensed matter physics,[21]quantum field theory,quantum computingandphysical cosmology.
The topological dependence of mechanical properties in solids is of interest in the disciplines ofmechanical engineeringandmaterials science. Electrical and mechanical properties depend on the arrangement and network structures ofmoleculesand elementary units in materials.[22]Thecompressive strengthofcrumpledtopologies is studied in attempts to understand the high strength to weight of such structures that are mostly empty space.[23]Topology is of further significance inContact mechanicswhere the dependence of stiffness and friction on thedimensionalityof surface structures is the subject of interest with applications in multi-body physics.
Atopological quantum field theory(or topological field theory or TQFT) is a quantum field theory that computestopological invariants. Although TQFTs were invented by physicists, they are also of mathematical interest, being related to, among other things,knot theory, the theory offour-manifoldsin algebraic topology, and the theory ofmoduli spacesin algebraic geometry.Donaldson,Jones,Witten, andKontsevichhave all wonFields Medalsfor work related to topological field theory.
The topological classification ofCalabi–Yau manifoldshas important implications instring theory, as different manifolds can sustain different kinds of strings.[24]
Intopological quantum computers, the qubits are stored intopological properties, that are by definition invariant with respect tohomotopies.[25]
In cosmology, topology can be used to describe the overallshape of the universe.[26]This area of research is commonly known asspacetime topology.
In condensed matter, a relevant application to topological physics comes from the possibility of obtaining a one-way current, which is a current protected from backscattering. It was first discovered in electronics with the famousquantum Hall effect, and then generalized in other areas of physics, for instance in photonics[27]byF.D.M Haldane.
The possible positions of arobotcan be described by amanifoldcalledconfiguration space.[28]In the area ofmotion planning, one finds paths between two points in configuration space. These paths represent a motion of the robot'sjointsand other parts into the desired pose.[29]
Disentanglement puzzlesare based on topological aspects of the puzzle's shapes and components.[30][31][32]
In order to create a continuous join of pieces in a modular construction, it is necessary to create an unbroken path in an order that surrounds each piece and traverses each edge only once. This process is an application of theEulerian path.[33]
Chaos theoryis aninterdisciplinaryarea ofscientific studyand branch ofmathematics. It focuses on underlying patterns anddeterministiclawsofdynamical systemsthat are highly sensitive toinitial conditions. These were once thought to have completely random states of disorder and irregularities.[1]Chaos theory states that within the apparent randomness ofchaotic complex systems, there are underlying patterns, interconnection, constantfeedback loops, repetition,self-similarity,fractalsandself-organization.[2]Thebutterfly effect, an underlying principle of chaos, describes how a small change in one state of a deterministicnonlinear systemcan result in large differences in a later state (meaning there is sensitive dependence on initial conditions).[3]A metaphor for this behavior is that a butterfly flapping its wings inBrazilcan cause or prevent atornadoinTexas.[4][5]: 181–184[6]
Small differences in initial conditions, such as those due to errors in measurements or due to rounding errors innumerical computation, can yield widely diverging outcomes for such dynamical systems, rendering long-term prediction of their behavior impossible in general.[7]This can happen even though these systems aredeterministic, meaning that their future behavior follows a unique evolution[8]and is fully determined by their initial conditions, with norandomelements involved.[9]In other words, the deterministic nature of these systems does not make them predictable.[10][11]This behavior is known asdeterministic chaos, or simplychaos. The theory was summarized byEdward Lorenzas:[12]
Chaos: When the present determines the future but the approximate present does not approximately determine the future.
Chaotic behavior exists in many natural systems, including fluid flow, heartbeat irregularities, weather and climate.[13][14][8]It also occurs spontaneously in some systems with artificial components, such asroad traffic.[2]This behavior can be studied through the analysis of a chaoticmathematical modelor through analytical techniques such asrecurrence plotsandPoincaré maps. Chaos theory has applications in a variety of disciplines, includingmeteorology,[8]anthropology,[15]sociology,environmental science,computer science,engineering,economics,ecology, andpandemiccrisis management.[16][17]The theory formed the basis for such fields of study ascomplex dynamical systems,edge of chaostheory andself-assemblyprocesses.
Chaos theory concerns deterministic systems whose behavior can, in principle, be predicted. Chaotic systems are predictable for a while and then 'appear' to become random. The amount of time for which the behavior of a chaotic system can be effectively predicted depends on three things: how much uncertainty can be tolerated in the forecast, how accurately its current state can be measured, and a time scale depending on the dynamics of the system, called theLyapunov time. Some examples of Lyapunov times are: chaotic electrical circuits, about 1 millisecond; weather systems, a few days (unproven); the inner solar system, 4 to 5 million years.[18]In chaotic systems, the uncertainty in a forecast increasesexponentiallywith elapsed time. Hence, mathematically, doubling the forecast time more than squares the proportional uncertainty in the forecast. This means, in practice, a meaningful prediction cannot be made over an interval of more than two or three times the Lyapunov time. When meaningful predictions cannot be made, the system appears random.[19]
In common usage, "chaos" means "a state of disorder".[20][21]However, in chaos theory, the term is defined more precisely. Although no universally accepted mathematical definition of chaos exists, a commonly used definition, originally formulated byRobert L. Devaney, says that to classify a dynamical system as chaotic, it must have these properties:[22]
In some cases, the last two properties above have been shown to actually imply sensitivity to initial conditions.[23][24]In the discrete-time case, this is true for allcontinuousmapsonmetric spaces.[25]In these cases, while it is often the most practically significant property, "sensitivity to initial conditions" need not be stated in the definition.
If attention is restricted tointervals, the second property implies the other two.[26]An alternative and a generally weaker definition of chaos uses only the first two properties in the above list.[27]
Sensitivity to initial conditionsmeans that each point in a chaotic system is arbitrarily closely approximated by other points that have significantly different future paths or trajectories. Thus, an arbitrarily small change or perturbation of the current trajectory may lead to significantly different future behavior.[2]
Sensitivity to initial conditions is popularly known as the "butterfly effect", so-called because of the title of a paper given byEdward Lorenzin 1972 to theAmerican Association for the Advancement of Sciencein Washington, D.C., entitledPredictability: Does the Flap of a Butterfly's Wings in Brazil set off a Tornado in Texas?.[28]The flapping wing represents a small change in the initial condition of the system, which causes a chain of events that prevents the predictability of large-scale phenomena. Had the butterfly not flapped its wings, the trajectory of the overall system could have been vastly different.
As suggested in Lorenz's book entitledThe Essence of Chaos, published in 1993,[5]: 8"sensitive dependence can serve as an acceptable definition of chaos". In the same book, Lorenz defined the butterfly effect as: "The phenomenon that a small alteration in the state of a dynamical system will cause subsequent states to differ greatly from the states that would have followed without the alteration."[5]: 23The above definition is consistent with the sensitive dependence of solutions on initial conditions (SDIC). An idealized skiing model was developed to illustrate the sensitivity of time-varying paths to initial positions.[5]: 189–204A predictability horizon can be determined before the onset of SDIC (i.e., prior to significant separations of initial nearby trajectories).[29]
A consequence of sensitivity to initial conditions is that if we start with a limited amount of information about the system (as is usually the case in practice), then beyond a certain time, the system would no longer be predictable. This is most prevalent in the case of weather, which is generally predictable only about a week ahead.[30]This does not mean that one cannot assert anything about events far in the future—only that some restrictions on the system are present. For example, we know that the temperature of the surface of the earth will not naturally reach 100 °C (212 °F) or fall below −130 °C (−202 °F) on earth (during the currentgeologic era), but we cannot predict exactly which day will have the hottest temperature of the year.
In more mathematical terms, theLyapunov exponentmeasures the sensitivity to initial conditions, in the form of rate of exponential divergence from the perturbed initial conditions.[31]More specifically, given two startingtrajectoriesin thephase spacethat are infinitesimally close, with initial separationδZ0{\displaystyle \delta \mathbf {Z} _{0}}, the two trajectories end up diverging at a rate given by
wheret{\displaystyle t}is the time andλ{\displaystyle \lambda }is the Lyapunov exponent. The rate of separation depends on the orientation of the initial separation vector, so a whole spectrum of Lyapunov exponents can exist. The number of Lyapunov exponents is equal to the number of dimensions of the phase space, though it is common to just refer to the largest one. For example, the maximal Lyapunov exponent (MLE) is most often used, because it determines the overall predictability of the system. A positive MLE, coupled with the solution’s boundedness, is usually taken as an indication that the system is chaotic.[8]
In addition to the above property, other properties related to sensitivity of initial conditions also exist. These include, for example,measure-theoreticalmixing(as discussed inergodictheory) and properties of aK-system.[11]
A chaotic system may have sequences of values for the evolving variable that exactly repeat themselves, giving periodic behavior starting from any point in that sequence. However, such periodic sequences are repelling rather than attracting, meaning that if the evolving variable is outside the sequence, however close, it will not enter the sequence and in fact, will diverge from it. Thus foralmost allinitial conditions, the variable evolves chaotically with non-periodic behavior.
Topological mixing(or the weaker condition of topological transitivity) means that the system evolves over time so that any given region oropen setof itsphase spaceeventually overlaps with any other given region. This mathematical concept of "mixing" corresponds to the standard intuition, and the mixing of coloreddyesor fluids is an example of a chaotic system.
Topological mixing is often omitted from popular accounts of chaos, which equate chaos with only sensitivity to initial conditions. However, sensitive dependence on initial conditions alone does not give chaos. For example, consider the simple dynamical system produced by repeatedly doubling an initial value. This system has sensitive dependence on initial conditions everywhere, since any pair of nearby points eventually becomes widely separated. However, this example has no topological mixing, and therefore has no chaos. Indeed, it has extremely simple behavior: all points except 0 tend to positive or negative infinity.
A mapf:X→X{\displaystyle f:X\to X}is said to be topologically transitive if for any pair of non-emptyopen setsU,V⊂X{\displaystyle U,V\subset X}, there existsk>0{\displaystyle k>0}such thatfk(U)∩V≠∅{\displaystyle f^{k}(U)\cap V\neq \emptyset }. Topological transitivity is a weaker version oftopological mixing. Intuitively, if a map is topologically transitive then given a pointxand a regionV, there exists a pointynearxwhose orbit passes throughV. This implies that it is impossible to decompose the system into two open sets.[32]
An important related theorem is the Birkhoff Transitivity Theorem. It is easy to see that the existence of a dense orbit implies topological transitivity. The Birkhoff Transitivity Theorem states that ifXis asecond countable,complete metric space, then topological transitivity implies the existence of adense setof points inXthat have dense orbits.[33]
For a chaotic system to havedenseperiodic orbitsmeans that every point in the space is approached arbitrarily closely by periodic orbits.[32]The one-dimensionallogistic mapdefined byx→ 4x(1 –x)is one of the simplest systems with density of periodic orbits. For example,5−58{\displaystyle {\tfrac {5-{\sqrt {5}}}{8}}}→5+58{\displaystyle {\tfrac {5+{\sqrt {5}}}{8}}}→5−58{\displaystyle {\tfrac {5-{\sqrt {5}}}{8}}}(or approximately 0.3454915 → 0.9045085 → 0.3454915) is an (unstable) orbit of period 2, and similar orbits exist for periods 4, 8, 16, etc. (indeed, for all the periods specified bySharkovskii's theorem).[34]
Sharkovskii's theorem is the basis of the Li and Yorke[35](1975) proof that any continuous one-dimensional system that exhibits a regular cycle of period three will also display regular cycles of every other length, as well as completely chaotic orbits.
Some dynamical systems, like the one-dimensionallogistic mapdefined byx→ 4x(1 –x),are chaotic everywhere, but in many cases chaotic behavior is found only in a subset of phase space. The cases of most interest arise when the chaotic behavior takes place on anattractor, since then a large set of initial conditions leads to orbits that converge to this chaotic region.[36]
An easy way to visualize a chaotic attractor is to start with a point in thebasin of attractionof the attractor, and then simply plot its subsequent orbit. Because of the topological transitivity condition, this is likely to produce a picture of the entire final attractor, and indeed both orbits shown in the figure on the right give a picture of the general shape of the Lorenz attractor. This attractor results from a simple three-dimensional model of theLorenzweather system. The Lorenz attractor is perhaps one of the best-known chaotic system diagrams, probably because it is not only one of the first, but it is also one of the most complex, and as such gives rise to a very interesting pattern that, with a little imagination, looks like the wings of a butterfly.
Unlikefixed-point attractorsandlimit cycles, the attractors that arise from chaotic systems, known asstrange attractors, have great detail and complexity. Strange attractors occur in bothcontinuousdynamical systems (such as the Lorenz system) and in somediscretesystems (such as theHénon map). Other discrete dynamical systems have a repelling structure called aJulia set, which forms at the boundary between basins of attraction of fixed points. Julia sets can be thought of as strange repellers. Both strange attractors and Julia sets typically have afractalstructure, and thefractal dimensioncan be calculated for them.
In contrast to single type chaotic solutions, studies using Lorenz models[40][41]have emphasized the importance of considering various types of solutions. For example, coexisting chaotic and non-chaotic may appear within the same model (e.g., the double pendulum system) using the same modeling configurations but different initial conditions. The findings of attractor coexistence, obtained from classical and generalized Lorenz models,[37][38][39]suggested a revised view that "the entirety of weather possesses a dual nature of chaos and order with distinct predictability", in contrast to the conventional view of "weather is chaotic".
Discrete chaotic systems, such as thelogistic map, can exhibit strange attractors whatever theirdimensionality. In contrast, forcontinuousdynamical systems, thePoincaré–Bendixson theoremshows that a strange attractor can only arise in three or more dimensions.Finite-dimensionallinear systemsare never chaotic; for a dynamical system to display chaotic behavior, it must be eithernonlinearor infinite-dimensional.
ThePoincaré–Bendixson theoremstates that a two-dimensional differential equation has very regular behavior. The Lorenz attractor discussed below is generated by a system of threedifferential equationssuch as:
wherex{\displaystyle x},y{\displaystyle y}, andz{\displaystyle z}make up thesystem state,t{\displaystyle t}is time, andσ{\displaystyle \sigma },ρ{\displaystyle \rho },β{\displaystyle \beta }are the systemparameters. Five of the terms on the right hand side are linear, while two are quadratic; a total of seven terms. Another well-known chaotic attractor is generated by theRössler equations, which have only one nonlinear term out of seven. Sprott[42]found a three-dimensional system with just five terms, that had only one nonlinear term, which exhibits chaos for certain parameter values. Zhang and Heidel[43][44]showed that, at least for dissipative and conservative quadratic systems, three-dimensional quadratic systems with only three or four terms on the right-hand side cannot exhibit chaotic behavior. The reason is, simply put, that solutions to such systems areasymptoticto a two-dimensional surface and therefore solutions are well behaved.
While the Poincaré–Bendixson theorem shows that a continuous dynamical system on the Euclideanplanecannot be chaotic, two-dimensional continuous systems withnon-Euclidean geometrycan still exhibit some chaotic properties.[45]Perhaps surprisingly, chaos may occur also in linear systems, provided they are infinite dimensional.[46]A theory of linear chaos is being developed in a branch of mathematical analysis known asfunctional analysis.
The above set of three ordinary differential equations has been referred to as the three-dimensional Lorenz model.[47]Since 1963, higher-dimensional Lorenz models have been developed in numerous studies[48][49][37][38]for examining the impact of an increased degree of nonlinearity, as well as its collective effect with heating and dissipations, on solution stability.
The straightforward generalization of coupled discrete maps[50]is based upon convolution integral which mediates interaction between spatially distributed maps:ψn+1(r→,t)=∫K(r→−r→,,t)f[ψn(r→,,t)]dr→,{\displaystyle \psi _{n+1}({\vec {r}},t)=\int K({\vec {r}}-{\vec {r}}^{,},t)f[\psi _{n}({\vec {r}}^{,},t)]d{\vec {r}}^{,}},
where kernelK(r→−r→,,t){\displaystyle K({\vec {r}}-{\vec {r}}^{,},t)}is propagator derived as Green function of a relevant physical system,[51]
f[ψn(r→,t)]{\displaystyle f[\psi _{n}({\vec {r}},t)]}might be logistic map alikeψ→Gψ[1−tanh⁡(ψ)]{\displaystyle \psi \rightarrow G\psi [1-\tanh(\psi )]}orcomplex map. For examples of complex maps theJulia setf[ψ]=ψ2{\displaystyle f[\psi ]=\psi ^{2}}orIkeda mapψn+1=A+Bψnei(|ψn|2+C){\displaystyle \psi _{n+1}=A+B\psi _{n}e^{i(|\psi _{n}|^{2}+C)}}may serve. When wave propagation problems at distanceL=ct{\displaystyle L=ct}with wavelengthλ=2π/k{\displaystyle \lambda =2\pi /k}are considered the kernelK{\displaystyle K}may have a form of Green function forSchrödinger equation:.[52][53]
K(r→−r→,,L)=ikexp⁡[ikL]2πLexp⁡[ik|r→−r→,|22L]{\displaystyle K({\vec {r}}-{\vec {r}}^{,},L)={\frac {ik\exp[ikL]}{2\pi L}}\exp[{\frac {ik|{\vec {r}}-{\vec {r}}^{,}|^{2}}{2L}}]}.
Under the right conditions, chaos spontaneously evolves into a lockstep pattern. In theKuramoto model, four conditions suffice to produce synchronization in a chaotic system.
Examples include thecoupled oscillationofChristiaan Huygens' pendulums, fireflies,neurons, theLondon Millennium Bridgeresonance, and large arrays ofJosephson junctions.[54]
Moreover, from the theoretical physics standpoint, dynamical chaos itself, in its most general manifestation, is a spontaneous order. The essence here is that most orders in nature arise from thespontaneous breakdownof various symmetries. This large family of phenomena includes elasticity, superconductivity, ferromagnetism, and many others. According to thesupersymmetric theory of stochastic dynamics, chaos, or more precisely, its stochastic generalization, is also part of this family. The corresponding symmetry being broken is thetopological supersymmetrywhich is hidden in allstochastic (partial) differential equations, and the correspondingorder parameteris afield-theoreticembodiment of the butterfly effect.[55]
James Clerk Maxwellfirst emphasized the "butterfly effect", and is seen as being one of the earliest to discuss chaos theory, with work in the 1860s and 1870s.[56][57][58]An early proponent of chaos theory wasHenri Poincaré. In the 1880s, while studying thethree-body problem, he found that there can be orbits that are nonperiodic, and yet not forever increasing nor approaching a fixed point.[59][60][61]In 1898,Jacques Hadamardpublished an influential study of the chaotic motion of a free particle gliding frictionlessly on a surface of constant negative curvature, called "Hadamard's billiards".[62]Hadamard was able to show that all trajectories are unstable, in that all particle trajectories diverge exponentially from one another, with a positiveLyapunov exponent.
Chaos theory began in the field ofergodic theory. Later studies, also on the topic of nonlineardifferential equations, were carried out byGeorge David Birkhoff,[63]Andrey Nikolaevich Kolmogorov,[64][65][66]Mary Lucy CartwrightandJohn Edensor Littlewood,[67]andStephen Smale.[68]Although chaotic planetary motion had not been observed, experimentalists had encountered turbulence in fluid motion and nonperiodic oscillation in radio circuits without the benefit of a theory to explain what they were seeing.
Despite initial insights in the first half of the twentieth century, chaos theory became formalized as such only after mid-century, when it first became evident to some scientists thatlinear theory, the prevailing system theory at that time, simply could not explain the observed behavior of certain experiments like that of thelogistic map. What had been attributed to measure imprecision and simple "noise" was considered by chaos theorists as a full component of the studied systems. In 1959Boris Valerianovich Chirikovproposed a criterion for the emergence of classical chaos in Hamiltonian systems (Chirikov criterion). He applied this criterion to explain some experimental results onplasma confinementin open mirror traps.[69][70]This is regarded as the very first physical theory of chaos, which succeeded in explaining a concrete experiment. And Boris Chirikov himself is considered as a pioneer in classical and quantum chaos.[71][72][73]
The main catalyst for the development of chaos theory was the electronic computer. Much of the mathematics of chaos theory involves the repeatediterationof simple mathematical formulas, which would be impractical to do by hand. Electronic computers made these repeated calculations practical, while figures and images made it possible to visualize these systems. As a graduate student in Chihiro Hayashi's laboratory at Kyoto University, Yoshisuke Ueda was experimenting with analog computers and noticed, on November 27, 1961, what he called "randomly transitional phenomena". Yet his advisor did not agree with his conclusions at the time, and did not allow him to report his findings until 1970.[74][75]
Edward Lorenzwas an early pioneer of the theory. His interest in chaos came about accidentally through his work onweather predictionin 1961.[76][13]Lorenz and his collaboratorEllen FetterandMargaret Hamilton[77]were using a simple digital computer, aRoyal McBeeLGP-30, to run weather simulations. They wanted to see a sequence of data again, and to save time they started the simulation in the middle of its course. They did this by entering a printout of the data that corresponded to conditions in the middle of the original simulation. To their surprise, the weather the machine began to predict was completely different from the previous calculation. They tracked this down to the computer printout. The computer worked with 6-digit precision, but the printout rounded variables off to a 3-digit number, so a value like 0.506127 printed as 0.506. This difference is tiny, and the consensus at the time would have been that it should have no practical effect. However, Lorenz discovered that small changes in initial conditions produced large changes in long-term outcome.[78]Lorenz's discovery, which gave its name toLorenz attractors, showed that even detailed atmospheric modeling cannot, in general, make precise long-term weather predictions.
In 1963,Benoit Mandelbrot, studyinginformation theory, discovered that noise in many phenomena (includingstock pricesandtelephonecircuits) was patterned like aCantor set, a set of points with infinite roughness and detail.[79]Mandelbrot described both the "Noah effect" (in which sudden discontinuous changes can occur) and the "Joseph effect" (in which persistence of a value can occur for a while, yet suddenly change afterwards).[80][81]In 1967, he published "How long is the coast of Britain? Statistical self-similarity and fractional dimension", showing that a coastline's length varies with the scale of the measuring instrument, resembles itself at all scales, and is infinite in length for aninfinitesimallysmall measuring device.[82]Arguing that a ball of twine appears as a point when viewed from far away (0-dimensional), a ball when viewed from fairly near (3-dimensional), or a curved strand (1-dimensional), he argued that the dimensions of an object are relative to the observer and may be fractional. An object whose irregularity is constant over different scales ("self-similarity") is afractal(examples include theMenger sponge, theSierpiński gasket, and theKoch curveorsnowflake, which is infinitely long yet encloses a finite space and has afractal dimensionof circa 1.2619). In 1982, Mandelbrot publishedThe Fractal Geometry of Nature, which became a classic of chaos theory.[83]
In December 1977, theNew York Academy of Sciencesorganized the first symposium on chaos, attended by David Ruelle,Robert May,James A. Yorke(coiner of the term "chaos" as used in mathematics),Robert Shaw, and the meteorologist Edward Lorenz. The following year Pierre Coullet and Charles Tresser published "Itérations d'endomorphismes et groupe de renormalisation", andMitchell Feigenbaum's article "Quantitative Universality for a Class of Nonlinear Transformations" finally appeared in a journal, after 3 years of referee rejections.[84][85]Thus Feigenbaum (1975) and Coullet & Tresser (1978) discovered theuniversalityin chaos, permitting the application of chaos theory to many different phenomena.
In 1979,Albert J. Libchaber, during a symposium organized in Aspen byPierre Hohenberg, presented his experimental observation of thebifurcationcascade that leads to chaos and turbulence inRayleigh–Bénard convectionsystems. He was awarded theWolf Prize in Physicsin 1986 along withMitchell J. Feigenbaumfor their inspiring achievements.[86]
In 1986, the New York Academy of Sciences co-organized with theNational Institute of Mental Healthand theOffice of Naval Researchthe first important conference on chaos in biology and medicine. There,Bernardo Hubermanpresented a mathematical model of theeye trackingdysfunction among people withschizophrenia.[87]This led to a renewal ofphysiologyin the 1980s through the application of chaos theory, for example, in the study of pathologicalcardiac cycles.
In 1987,Per Bak,Chao TangandKurt Wiesenfeldpublished a paper inPhysical Review Letters[88]describing for the first timeself-organized criticality(SOC), considered one of the mechanisms by whichcomplexityarises in nature.
Alongside largely lab-based approaches such as theBak–Tang–Wiesenfeld sandpile, many other investigations have focused on large-scale natural or social systems that are known (or suspected) to displayscale-invariantbehavior. Although these approaches were not always welcomed (at least initially) by specialists in the subjects examined, SOC has nevertheless become established as a strong candidate for explaining a number of natural phenomena, includingearthquakes, (which, long before SOC was discovered, were known as a source of scale-invariant behavior such as theGutenberg–Richter lawdescribing the statistical distribution of earthquake sizes, and theOmori law[89]describing the frequency of aftershocks),solar flares, fluctuations in economic systems such asfinancial markets(references to SOC are common ineconophysics), landscape formation,forest fires,landslides,epidemics, andbiological evolution(where SOC has been invoked, for example, as the dynamical mechanism behind the theory of "punctuated equilibria" put forward byNiles EldredgeandStephen Jay Gould). Given the implications of a scale-free distribution of event sizes, some researchers have suggested that another phenomenon that should be considered an example of SOC is the occurrence ofwars. These investigations of SOC have included both attempts at modelling (either developing new models or adapting existing ones to the specifics of a given natural system), and extensive data analysis to determine the existence and/or characteristics of natural scaling laws.
Also in 1987James GleickpublishedChaos: Making a New Science, which became a best-seller and introduced the general principles of chaos theory as well as its history to the broad public.[90]Initially the domain of a few, isolated individuals, chaos theory progressively emerged as a transdisciplinary and institutional discipline, mainly under the name ofnonlinear systemsanalysis. Alluding toThomas Kuhn's concept of aparadigm shiftexposed inThe Structure of Scientific Revolutions(1962), many "chaologists" (as some described themselves) claimed that this new theory was an example of such a shift, a thesis upheld by Gleick.
The availability of cheaper, more powerful computers broadens the applicability of chaos theory. Currently, chaos theory remains an active area of research,[91]involving many different disciplines such asmathematics,topology,physics,[92]social systems,[93]population modeling,biology,meteorology,astrophysics,information theory,computational neuroscience,pandemiccrisis management,[16][17]etc.
The sensitive dependence on initial conditions (i.e., butterfly effect) has been illustrated using the following folklore:[90]
For want of a nail, the shoe was lost.For want of a shoe, the horse was lost.For want of a horse, the rider was lost.For want of a rider, the battle was lost.For want of a battle, the kingdom was lost.And all for the want of a horseshoe nail.
Based on the above, many people mistakenly believe that the impact of a tiny initial perturbation monotonically increases with time and that any tiny perturbation can eventually produce a large impact on numerical integrations. However, in 2008, Lorenz stated that he did not feel that this verse described true chaos but that it better illustrated the simpler phenomenon of instability and that the verse implicitly suggests that subsequent small events will not reverse the outcome.[94]Based on the analysis, the verse only indicates divergence, not boundedness.[6]Boundedness is important for the finite size of a butterfly pattern.[6]The characteristic of the aforementioned verse was described as "finite-time sensitive dependence".[95]
Although chaos theory was born from observing weather patterns, it has become applicable to a variety of other situations. Some areas benefiting from chaos theory today aregeology,mathematics,biology,computer science,economics,[97][98][99]engineering,[100][101]finance,[102][103][104][105][106]meteorology,philosophy,anthropology,[15]physics,[107][108][109]politics,[110][111]population dynamics,[112]androbotics. A few categories are listed below with examples, but this is by no means a comprehensive list as new applications are appearing.
Chaos theory has been used for many years incryptography. In the past few decades, chaos and nonlinear dynamics have been used in the design of hundreds ofcryptographic primitives. These algorithms include imageencryption algorithms,hash functions,secure pseudo-random number generators,stream ciphers,watermarking, andsteganography.[113]The majority of these algorithms are based on uni-modal chaotic maps and a big portion of these algorithms use the control parameters and the initial condition of the chaotic maps as their keys.[114]From a wider perspective, without loss of generality, the similarities between the chaotic maps and the cryptographic systems is the main motivation for the design of chaos based cryptographic algorithms.[113]One type of encryption, secret key orsymmetric key, relies ondiffusion and confusion, which is modeled well by chaos theory.[115]Another type of computing,DNA computing, when paired with chaos theory, offers a way to encrypt images and other information.[116]Many of the DNA-Chaos cryptographic algorithms are proven to be either not secure, or the technique applied is suggested to be not efficient.[117][118][119]
Robotics is another area that has recently benefited from chaos theory. Instead of robots acting in a trial-and-error type of refinement to interact with their environment, chaos theory has been used to build apredictive model.[120]Chaotic dynamics have been exhibited bypassive walkingbiped robots.[121]
For over a hundred years, biologists have been keeping track of populations of different species withpopulation models. Most models arecontinuous, but recently scientists have been able to implement chaotic models in certain populations.[122]For example, a study on models ofCanadian lynxshowed there was chaotic behavior in the population growth.[123]Chaos can also be found in ecological systems, such ashydrology. While a chaotic model for hydrology has its shortcomings, there is still much to learn from looking at the data through the lens of chaos theory.[124]Another biological application is found incardiotocography. Fetal surveillance is a delicate balance of obtaining accurate information while being as noninvasive as possible. Better models of warning signs offetal hypoxiacan be obtained through chaotic modeling.[125]
As Perry points out,modelingof chaotictime seriesinecologyis helped by constraint.[126]: 176, 177There is always potential difficulty in distinguishing real chaos from chaos that is only in the model.[126]: 176, 177Hence both constraint in the model and or duplicate time series data for comparison will be helpful in constraining the model to something close to the reality, for example Perry & Wall 1984.[126]: 176, 177Gene-for-geneco-evolution sometimes shows chaotic dynamics inallele frequencies.[127]Adding variables exaggerates this: Chaos is more common inmodelsincorporating additional variables to reflect additional facets of real populations.[127]Robert M. Mayhimself did some of these foundational crop co-evolution studies, and this in turn helped shape the entire field.[127]Even for a steady environment, merely combining onecropand onepathogenmay result inquasi-periodic-orchaotic-oscillations in pathogenpopulation.[128]: 169
It is possible that economic models can also be improved through an application of chaos theory, but predicting the health of an economic system and what factors influence it most is an extremely complex task.[129]Economic and financial systems are fundamentally different from those in the classical natural sciences since the former are inherently stochastic in nature, as they result from the interactions of people, and thus pure deterministic models are unlikely to provide accurate representations of the data. The empirical literature that tests for chaos in economics and finance presents very mixed results, in part due to confusion between specific tests for chaos and more general tests for non-linear relationships.[130]
Chaos could be found in economics by the means ofrecurrence quantification analysis. In fact, Orlando et al.[131]by the means of the so-called recurrence quantification correlation index were able to detect hidden changes in time series. Then, the same technique was employed to detect transitions from laminar (regular) to turbulent (chaotic) phases as well as differences between macroeconomic variables and highlight hidden features of economic dynamics.[132]Finally, chaos theory could help in modeling how an economy operates as well as in embedding shocks due to external events such as COVID-19.[133]
Due to the sensitive dependence of solutions on initial conditions (SDIC), also known as the butterfly effect, chaotic systems like the Lorenz 1963 model imply a finite predictability horizon. This means that while accurate predictions are possible over a finite time period, they are not feasible over an infinite time span. Considering the nature of Lorenz's chaotic solutions, the committee led by Charney et al. in 1966[134]extrapolated a doubling time of five days from a general circulation model, suggesting a predictability limit of two weeks. This connection between the five-day doubling time and the two-week predictability limit was also recorded in a 1969 report by the Global Atmospheric Research Program (GARP).[135]To acknowledge the combined direct and indirect influences from the Mintz and Arakawa model and Lorenz's models, as well as the leadership of Charney et al., Shen et al.[136]refer to the two-week predictability limit as the "Predictability Limit Hypothesis," drawing an analogy to Moore's Law.
In AI-driven large language models, responses can exhibit sensitivities to factors like alterations in formatting and variations in prompts. These sensitivities are akin to butterfly effects.[137]Although classifying AI-powered large language models as classical deterministic chaotic systems poses challenges, chaos-inspired approaches and techniques (such as ensemble modeling) may be employed to extract reliable information from these expansive language models (see also "Butterfly Effect in Popular Culture").
In chemistry, predicting gas solubility is essential to manufacturingpolymers, but models usingparticle swarm optimization(PSO) tend to converge to the wrong points. An improved version of PSO has been created by introducing chaos, which keeps the simulations from getting stuck.[138]Incelestial mechanics, especially when observing asteroids, applying chaos theory leads to better predictions about when these objects will approach Earth and other planets.[139]Four of the fivemoons of Plutorotate chaotically. Inquantum physicsandelectrical engineering, the study of large arrays ofJosephson junctionsbenefitted greatly from chaos theory.[140]Closer to home, coal mines have always been dangerous places where frequent natural gas leaks cause many deaths. Until recently, there was no reliable way to predict when they would occur. But these gas leaks have chaotic tendencies that, when properly modeled, can be predicted fairly accurately.[141]
Chaos theory can be applied outside of the natural sciences, but historically nearly all such studies have suffered from lack of reproducibility; poor external validity; and/or inattention to cross-validation, resulting in poor predictive accuracy (if out-of-sample prediction has even been attempted). Glass[142]and Mandell and Selz[143]have found that no EEG study has as yet indicated the presence of strange attractors or other signs of chaotic behavior.
Redington and Reidbord (1992) attempted to demonstrate that the human heart could display chaotic traits. They monitored the changes in between-heartbeat intervals for a single psychotherapy patient as she moved through periods of varying emotional intensity during a therapy session. Results were admittedly inconclusive. Not only were there ambiguities in the various plots the authors produced to purportedly show evidence of chaotic dynamics (spectral analysis, phase trajectory, and autocorrelation plots), but also when they attempted to compute a Lyapunov exponent as more definitive confirmation of chaotic behavior, the authors found they could not reliably do so.[144]
In their 1995 paper, Metcalf and Allen[145]maintained that they uncovered in animal behavior a pattern of period doubling leading to chaos. The authors examined a well-known response called schedule-induced polydipsia, by which an animal deprived of food for certain lengths of time will drink unusual amounts of water when the food is at last presented. The control parameter (r) operating here was the length of the interval between feedings, once resumed. The authors were careful to test a large number of animals and to include many replications, and they designed their experiment so as to rule out the likelihood that changes in response patterns were caused by different starting places for r.
Time series and first delay plots provide the best support for the claims made, showing a fairly clear march from periodicity to irregularity as the feeding times were increased. The various phase trajectory plots and spectral analyses, on the other hand, do not match up well enough with the other graphs or with the overall theory to lead inexorably to a chaotic diagnosis. For example, the phase trajectories do not show a definite progression towards greater and greater complexity (and away from periodicity); the process seems quite muddied. Also, where Metcalf and Allen saw periods of two and six in their spectral plots, there is room for alternative interpretations. All of this ambiguity necessitate some serpentine, post-hoc explanation to show that results fit a chaotic model.
By adapting a model of career counseling to include a chaotic interpretation of the relationship between employees and the job market, Amundson and Bright found that better suggestions can be made to people struggling with career decisions.[146]Modern organizations are increasingly seen as opencomplex adaptive systemswith fundamental natural nonlinear structures, subject to internal and external forces that may contribute chaos. For instance,team buildingandgroup developmentis increasingly being researched as an inherently unpredictable system, as the uncertainty of different individuals meeting for the first time makes the trajectory of the team unknowable.[147]
Traffic forecasting may benefit from applications of chaos theory. Better predictions of when a congestion will occur would allow measures to be taken to disperse it before it would have occurred. Combining chaos theory principles with a few other methods has led to a more accurate short-term prediction model (see the plot of theBML traffic modelat right).[148]
Chaos theory has been applied to environmentalwater cycledata (alsohydrologicaldata), such as rainfall and streamflow.[149]These studies have yielded controversial results, because the methods for detecting a chaotic signature are often relatively subjective. Early studies tended to "succeed" in finding chaos, whereas subsequent studies and meta-analyses called those studies into question and provided explanations for why these datasets are not likely to have low-dimension chaotic dynamics.[150]
Inmathematicsandcomputer science,graph theoryis the study ofgraphs, which aremathematical structuresused to model pairwise relations between objects. A graph in this context is made up ofvertices(also callednodesorpoints) which are connected byedges(also calledarcs,linksorlines). A distinction is made betweenundirected graphs, where edges link two vertices symmetrically, anddirected graphs, where edges link two vertices asymmetrically. Graphs are one of the principal objects of study indiscrete mathematics.
Definitions in graph theory vary. The following are some of the more basic ways of defining graphs and relatedmathematical structures.
In one restricted but very common sense of the term,[1][2]agraphis anordered pairG=(V,E){\displaystyle G=(V,E)}comprising:
To avoid ambiguity, this type of object may be called anundirected simple graph.
In the edge{x,y}{\displaystyle \{x,y\}}, the verticesx{\displaystyle x}andy{\displaystyle y}are called theendpointsof the edge. The edge is said tojoinx{\displaystyle x}andy{\displaystyle y}and to beincidentonx{\displaystyle x}and ony{\displaystyle y}. A vertex may exist in a graph and not belong to an edge. Under this definition,multiple edges, in which two or more edges connect the same vertices, are not allowed.
In one more general sense of the term allowing multiple edges,[3][4]agraphis an ordered tripleG=(V,E,ϕ){\displaystyle G=(V,E,\phi )}comprising:
To avoid ambiguity, this type of object may be called anundirectedmultigraph.
Aloopis an edge that joins a vertex to itself. Graphs as defined in the two definitions above cannot have loops, because a loop joining a vertexx{\displaystyle x}to itself is the edge (for an undirected simple graph) or is incident on (for an undirected multigraph){x,x}={x}{\displaystyle \{x,x\}=\{x\}}which is not in{{x,y}∣x,y∈Vandx≠y}{\displaystyle \{\{x,y\}\mid x,y\in V\;{\textrm {and}}\;x\neq y\}}. To allow loops, the definitions must be expanded. For undirected simple graphs, the definition ofE{\displaystyle E}should be modified toE⊆{{x,y}∣x,y∈V}{\displaystyle E\subseteq \{\{x,y\}\mid x,y\in V\}}. For undirected multigraphs, the definition ofϕ{\displaystyle \phi }should be modified toϕ:E→{{x,y}∣x,y∈V}{\displaystyle \phi :E\to \{\{x,y\}\mid x,y\in V\}}. To avoid ambiguity, these types of objects may be calledundirected simple graph permitting loopsandundirected multigraph permitting loops(sometimes alsoundirectedpseudograph), respectively.
V{\displaystyle V}andE{\displaystyle E}are usually taken to be finite, and many of the well-known results are not true (or are rather different) for infinite graphs because many of the arguments fail in theinfinite case. Moreover,V{\displaystyle V}is often assumed to be non-empty, butE{\displaystyle E}is allowed to be the empty set. Theorderof a graph is|V|{\displaystyle |V|}, its number of vertices. Thesizeof a graph is|E|{\displaystyle |E|}, its number of edges. Thedegreeorvalencyof a vertex is the number of edges that are incident to it, where a loop is counted twice. Thedegreeof a graph is the maximum of the degrees of its vertices.
In an undirected simple graph of ordern, the maximum degree of each vertex isn− 1and the maximum size of the graph is⁠n(n− 1)/2⁠.
The edges of an undirected simple graph permitting loopsG{\displaystyle G}induce a symmetrichomogeneous relation∼{\displaystyle \sim }on the vertices ofG{\displaystyle G}that is called theadjacency relationofG{\displaystyle G}. Specifically, for each edge(x,y){\displaystyle (x,y)}, its endpointsx{\displaystyle x}andy{\displaystyle y}are said to beadjacentto one another, which is denotedx∼y{\displaystyle x\sim y}.
Adirected graphordigraphis a graph in which edges have orientations.
In one restricted but very common sense of the term,[5]adirected graphis an ordered pairG=(V,E){\displaystyle G=(V,E)}comprising:
To avoid ambiguity, this type of object may be called adirected simple graph. In set theory and graph theory,Vn{\displaystyle V^{n}}denotes the set ofn-tuplesof elements ofV,{\displaystyle V,}that is, ordered sequences ofn{\displaystyle n}elements that are not necessarily distinct.
In the edge(x,y){\displaystyle (x,y)}directed fromx{\displaystyle x}toy{\displaystyle y}, the verticesx{\displaystyle x}andy{\displaystyle y}are called theendpointsof the edge,x{\displaystyle x}thetailof the edge andy{\displaystyle y}theheadof the edge. The edge is said tojoinx{\displaystyle x}andy{\displaystyle y}and to beincidentonx{\displaystyle x}and ony{\displaystyle y}. A vertex may exist in a graph and not belong to an edge. The edge(y,x){\displaystyle (y,x)}is called theinverted edgeof(x,y){\displaystyle (x,y)}.Multiple edges, not allowed under the definition above, are two or more edges with both the same tail and the same head.
In one more general sense of the term allowing multiple edges,[5]adirected graphis an ordered tripleG=(V,E,ϕ){\displaystyle G=(V,E,\phi )}comprising:
To avoid ambiguity, this type of object may be called adirected multigraph.
Aloopis an edge that joins a vertex to itself. Directed graphs as defined in the two definitions above cannot have loops, because a loop joining a vertexx{\displaystyle x}to itself is the edge (for a directed simple graph) or is incident on (for a directed multigraph)(x,x){\displaystyle (x,x)}which is not in{(x,y)∣(x,y)∈V2andx≠y}{\displaystyle \left\{(x,y)\mid (x,y)\in V^{2}\;{\textrm {and}}\;x\neq y\right\}}. So to allow loops the definitions must be expanded. For directed simple graphs, the definition ofE{\displaystyle E}should be modified toE⊆{(x,y)∣(x,y)∈V2}{\displaystyle E\subseteq \left\{(x,y)\mid (x,y)\in V^{2}\right\}}. For directed multigraphs, the definition ofϕ{\displaystyle \phi }should be modified toϕ:E→{(x,y)∣(x,y)∈V2}{\displaystyle \phi :E\to \left\{(x,y)\mid (x,y)\in V^{2}\right\}}. To avoid ambiguity, these types of objects may be called precisely adirected simple graph permitting loopsand adirected multigraph permitting loops(or aquiver) respectively.
The edges of a directed simple graph permitting loopsG{\displaystyle G}is ahomogeneous relation~ on the vertices ofG{\displaystyle G}that is called theadjacency relationofG{\displaystyle G}. Specifically, for each edge(x,y){\displaystyle (x,y)}, its endpointsx{\displaystyle x}andy{\displaystyle y}are said to beadjacentto one another, which is denotedx{\displaystyle x}~y{\displaystyle y}.
Graphs can be used to model many types of relations and processes in physical, biological,[7][8]social and information systems.[9]Many practical problems can be represented by graphs. Emphasizing their application to real-world systems, the termnetworkis sometimes defined to mean a graph in which attributes (e.g. names) are associated with the vertices and edges, and the subject that expresses and understands real-world systems as a network is callednetwork science.
Withincomputer science, 'causal' and 'non-causal' linked structures are graphs that are used to represent networks of communication, data organization, computational devices, the flow of computation, etc. For instance, the link structure of awebsitecan be represented by a directed graph, in which the vertices represent web pages and directed edges representlinksfrom one page to another. A similar approach can be taken to problems in social media,[10]travel, biology, computer chip design, mapping the progression of neuro-degenerative diseases,[11][12]and many other fields. The development ofalgorithmstohandle graphsis therefore of major interest in computer science. Thetransformation of graphsis often formalized and represented bygraph rewrite systems. Complementary tograph transformationsystems focusing on rule-based in-memory manipulation of graphs aregraph databasesgeared towardstransaction-safe,persistentstoring and querying ofgraph-structured data.
Graph-theoretic methods, in various forms, have proven particularly useful inlinguistics, since natural language often lends itself well to discrete structure. Traditionally,syntaxand compositional semantics follow tree-based structures, whose expressive power lies in theprinciple of compositionality, modeled in a hierarchical graph. More contemporary approaches such ashead-driven phrase structure grammarmodel the syntax of natural language usingtyped feature structures, which aredirected acyclic graphs. 
Withinlexical semantics, especially as applied to computers, modeling word meaning is easier when a given word is understood in terms of related words;semantic networksare therefore important incomputational linguistics. Still, other methods in phonology (e.g.optimality theory, which useslattice graphs) and morphology (e.g. finite-state morphology, usingfinite-state transducers) are common in the analysis of language as a graph. Indeed, the usefulness of this area of mathematics to linguistics has borne organizations such asTextGraphs, as well as various 'Net' projects, such asWordNet,VerbNet, and others.
Graph theory is also used to study molecules inchemistryandphysics. Incondensed matter physics, the three-dimensional structure of complicated simulated atomic structures can be studied quantitatively by gathering statistics on graph-theoretic properties related to the topology of the atoms. Also, "theFeynman graphs and rules of calculationsummarizequantum field theoryin a form in close contact with the experimental numbers one wants to understand."[13]In chemistry a graph makes a natural model for a molecule, where vertices representatomsand edgesbonds. This approach is especially used in computer processing of molecular structures, ranging fromchemical editorsto database searching. Instatistical physics, graphs can represent local connections between interacting parts of a system, as well as the dynamics of a physical process on such
systems. Similarly, incomputational neurosciencegraphs can be used to represent functional connections between brain areas that interact to give rise to various cognitive processes, where the vertices represent different areas of the brain and the edges represent the connections between those areas. Graph theory plays an important role in electrical modeling of electrical networks, here, weights are associated with resistance of the wire segments to obtain electrical properties of network structures.[14]Graphs are also used to represent the micro-scale channels ofporous media, in which the vertices represent the pores and the edges represent the smaller channels connecting the pores.Chemical graph theoryuses themolecular graphas a means to model molecules.
Graphs and networks are excellent models to study and understand phase transitions and critical phenomena.
Removal of nodes or edges leads to a critical transition where the network breaks into small clusters which is studied as a phase transition. This breakdown is studied viapercolation theory.[15]
Graph theory is also widely used insociologyas a way, for example, tomeasure actors' prestigeor to explorerumor spreading, notably through the use ofsocial network analysissoftware. Under the umbrella of social networks are many different types of graphs.[17]Acquaintanceship and friendship graphs describe whether people know each other. Influence graphs model whether certain people can influence the behavior of others. Finally, collaboration graphs model whether two people work together in a particular way, such as acting in a movie together.
Likewise, graph theory is useful inbiologyand conservation efforts where a vertex can represent regions where certain species exist (or inhabit) and the edges represent migration paths or movement between the regions. This information is important when looking at breeding patterns or tracking the spread of disease, parasites or how changes to the movement can affect other species.
Graphs are also commonly used inmolecular biologyandgenomicsto model and analyse datasets with complex relationships. For example, graph-based methods are often used to 'cluster' cells together into cell-types insingle-cell transcriptome analysis. Another use is to model genes or proteins in apathwayand study the relationships between them, such as metabolic pathways and gene regulatory networks.[18]Evolutionary trees, ecological networks, and hierarchical clustering of gene expression patterns are also represented as graph structures.
Graph theory is also used inconnectomics;[19]nervous systems can be seen as a graph, where the nodes are neurons and the edges are the connections between them.
In mathematics, graphs are useful in geometry and certain parts oftopologysuch asknot theory.Algebraic graph theoryhas close links withgroup theory. Algebraic graph theory has been applied to many areas including dynamic systems and complexity.
A graph structure can be extended by assigning a weight to each edge of the graph. Graphs with weights, orweighted graphs, are used to represent structures in which pairwise connections have some numerical values. For example, if a graph represents a road network, the weights could represent the length of each road. There may be several weights associated with each edge, including distance (as in the previous example), travel time, or monetary cost. Such weighted graphs are commonly used to program GPS's, and travel-planning search engines that compare flight times and costs.
The paper written byLeonhard Euleron theSeven Bridges of Königsbergand published in 1736 is regarded as the first paper in the history of graph theory.[20]This paper, as well as the one written byVandermondeon theknight problem,carried on with theanalysis situsinitiated byLeibniz. Euler's formula relating the number of edges, vertices, and faces of a convex polyhedron was studied and generalized byCauchy[21]andL'Huilier,[22]and represents the beginning of the branch of mathematics known astopology.
More than one century after Euler's paper on the bridges ofKönigsbergand whileListingwas introducing the concept of topology,Cayleywas led by an interest in particular analytical forms arising fromdifferential calculusto study a particular class of graphs, thetrees.[23]This study had many implications for theoreticalchemistry. The techniques he used mainly concern theenumeration of graphswith particular properties. Enumerative graph theory then arose from the results of Cayley and the fundamental results published byPólyabetween 1935 and 1937. These were generalized byDe Bruijnin 1959. Cayley linked his results on trees with contemporary studies of chemical composition.[24]The fusion of ideas from mathematics with those from chemistry began what has become part of the standard terminology of graph theory.
In particular, the term "graph" was introduced bySylvesterin a paper published in 1878 inNature, where he draws an analogy between "quantic invariants" and "co-variants" of algebra and molecular diagrams:[25]
The first textbook on graph theory was written byDénes Kőnig, and published in 1936.[26]Another book byFrank Harary, published in 1969, was "considered the world over to be the definitive textbook on the subject",[27]and enabled mathematicians, chemists, electrical engineers and social scientists to talk to each other. Harary donated all of the royalties to fund thePólya Prize.[28]
One of the most famous and stimulating problems in graph theory is thefour color problem: "Is it true that any map drawn in the plane may have its regions colored with four colors, in such a way that any two regions having a common border have different colors?" This problem was first posed byFrancis Guthriein 1852 and its first written record is in a letter ofDe Morganaddressed toHamiltonthe same year. Many incorrect proofs have been proposed, including those by Cayley,Kempe, and others. The study and the generalization of this problem byTait,Heawood,RamseyandHadwigerled to the study of the colorings of the graphs embedded on surfaces with arbitrarygenus. Tait's reformulation generated a new class of problems, thefactorization problems, particularly studied byPetersenandKőnig. The works of Ramsey on colorations and more specially the results obtained byTuránin 1941 was at the origin of another branch of graph theory,extremal graph theory.
The four color problem remained unsolved for more than a century. In 1969Heinrich Heeschpublished a method for solving the problem using computers.[29]A computer-aided proof produced in 1976 byKenneth AppelandWolfgang Hakenmakes fundamental use of the notion of "discharging" developed by Heesch.[30][31]The proof involved checking the properties of 1,936 configurations by computer, and was not fully accepted at the time due to its complexity. A simpler proof considering only 633 configurations was given twenty years later byRobertson,Seymour,SandersandThomas.[32]
The autonomous development of topology from 1860 and 1930 fertilized graph theory back through the works ofJordan,KuratowskiandWhitney. Another important factor of common development of graph theory andtopologycame from the use of the techniques of modern algebra. The first example of such a use comes from the work of the physicistGustav Kirchhoff, who published in 1845 hisKirchhoff's circuit lawsfor calculating thevoltageandcurrentinelectric circuits.
The introduction of probabilistic methods in graph theory, especially in the study ofErdősandRényiof the asymptotic probability of graph connectivity, gave rise to yet another branch, known asrandom graph theory, which has been a fruitful source of graph-theoretic results.
A graph is an abstraction of relationships that emerge in nature; hence, it cannot be coupled to a certain representation. The way it is represented depends on the degree of convenience such representation provides for a certain application. The most common representations are the visual, in which, usually, vertices are drawn and connected by edges, and the tabular, in which rows of a table provide information about the relationships between the vertices within the graph.
Graphs are usually represented visually by drawing a point or circle for every vertex, and drawing a line between two vertices if they are connected by an edge. If the graph is directed, the direction is indicated by drawing an arrow. If the graph is weighted, the weight is added on the arrow.
A graph drawing should not be confused with the graph itself (the abstract, non-visual structure) as there are several ways to structure the graph drawing. All that matters is which vertices are connected to which others by how many edges and not the exact layout. In practice, it is often difficult to decide if two drawings represent the same graph. Depending on the problem domain some layouts may be better suited and easier to understand than others.
The pioneering work ofW. T. Tuttewas very influential on the subject of graph drawing. Among other achievements, he introduced the use of linear algebraic methods to obtain graph drawings.
Graph drawing also can be said to encompass problems that deal with thecrossing numberand its various generalizations. The crossing number of a graph is the minimum number of intersections between edges that a drawing of the graph in the plane must contain. For aplanar graph, the crossing number is zero by definition. Drawings on surfaces other than the plane are also studied.
There are other techniques to visualize a graph away from vertices and edges, includingcircle packings,intersection graph, and other visualizations of theadjacency matrix.
The tabular representation lends itself well to computational applications. There are different ways to store graphs in a computer system. Thedata structureused depends on both the graph structure and thealgorithmused for manipulating the graph. Theoretically one can distinguish between list and matrix structures but in concrete applications the best structure is often a combination of both. List structures are often preferred forsparse graphsas they have smaller memory requirements.Matrixstructures on the other hand provide faster access for some applications but can consume huge amounts of memory. Implementations of sparse matrix structures that are efficient on modern parallel computer architectures are an object of current investigation.[33]
List structures include theedge list, an array of pairs of vertices, and theadjacency list, which separately lists the neighbors of each vertex: Much like the edge list, each vertex has a list of which vertices it is adjacent to.
Matrix structures include theincidence matrix, a matrix of 0's and 1's whose rows represent vertices and whose columns represent edges, and theadjacency matrix, in which both the rows and columns are indexed by vertices. In both cases a 1 indicates two adjacent objects and a 0 indicates two non-adjacent objects. Thedegree matrixindicates the degree of vertices. TheLaplacian matrixis a modified form of the adjacency matrix that incorporates information about thedegreesof the vertices, and is useful in some calculations such asKirchhoff's theoremon the number ofspanning treesof a graph.
Thedistance matrix, like the adjacency matrix, has both its rows and columns indexed by vertices, but rather than containing a 0 or a 1 in each cell it contains the length of ashortest pathbetween two vertices.
There is a large literature ongraphical enumeration: the problem of counting graphs meeting specified conditions. Some of this work is found in Harary and Palmer (1973).
A common problem, called thesubgraph isomorphism problem, is finding a fixed graph as asubgraphin a given graph. One reason to be interested in such a question is that manygraph propertiesarehereditaryfor subgraphs, which means that a graph has the property if and only if all subgraphs have it too.
Finding maximal subgraphs of a certain kind is often anNP-complete problem. For example:
One special case of subgraph isomorphism is thegraph isomorphism problem. It asks whether two graphs are isomorphic. It is not known whether this problem is NP-complete, nor whether it can be solved in polynomial time.
A similar problem is findinginduced subgraphsin a given graph. Again, some important graph properties are hereditary with respect to induced subgraphs, which means that a graph has a property if and only if all induced subgraphs also have it. Finding maximal induced subgraphs of a certain kind is also often NP-complete. For example:
Still another such problem, the minor containment problem, is to find a fixed graph as a minor of a given graph. Aminoror subcontraction of a graph is any graph obtained by taking a subgraph and contracting some (or no) edges. Many graph properties are hereditary for minors, which means that a graph has a property if and only if all minors have it too. For example,Wagner's Theoremstates:
A similar problem, the subdivision containment problem, is to find a fixed graph as asubdivisionof a given graph. Asubdivisionorhomeomorphismof a graph is any graph obtained by subdividing some (or no) edges. Subdivision containment is related to graph properties such asplanarity. For example,Kuratowski's Theoremstates:
Another problem in subdivision containment is theKelmans–Seymour conjecture:
Another class of problems has to do with the extent to which various species and generalizations of graphs are determined by theirpoint-deleted subgraphs. For example:
Many problems and theorems in graph theory have to do with various ways of coloring graphs. Typically, one is interested in coloring a graph so that no two adjacent vertices have the same color, or with other similar restrictions. One may also consider coloring edges (possibly so that no two coincident edges are the same color), or other variations. Among the famous results and conjectures concerning graph coloring are the following:
Constraint modeling theories concern families of directed graphs related by apartial order. In these applications, graphs are ordered by specificity, meaning that more constrained graphs—which are more specific and thus contain a greater amount of information—are subsumed by those that are more general. Operations between graphs include evaluating the direction of a subsumption relationship between two graphs, if any, and computing graph unification. The unification of two argument graphs is defined as the most general graph (or the computation thereof) that is consistent with (i.e. contains all of the information in) the inputs, if such a graph exists; efficient unification algorithms are known.
For constraint frameworks which are strictlycompositional, graph unification is the sufficient satisfiability and combination function. Well-known applications includeautomatic theorem provingand modeling theelaboration of linguistic structure.
There are numerous problems arising especially from applications that have to do with various notions offlows in networks, for example:
Covering problemsin graphs may refer to variousset cover problemson subsets of vertices/subgraphs.
Decomposition, defined as partitioning the edge set of a graph (with as many vertices as necessary accompanying the edges of each part of the partition), has a wide variety of questions. Often, the problem is to decompose a graph into subgraphs isomorphic to a fixed graph; for instance, decomposing a complete graph into Hamiltonian cycles. Other problems specify a family of graphs into which a given graph should be decomposed, for instance, a family of cycles, or decomposing a complete graphKninton− 1specified trees having, respectively, 1, 2, 3, ...,n− 1edges.
Some specific decomposition problems and similar problems that have been studied include:
Many problems involve characterizing the members of various classes of graphs. Some examples of such questions are below:
Quantum mechanicsis the fundamental physicaltheorythat describes the behavior of matter and of light; its unusual characteristics typically occur at and below the scale ofatoms.[2]: 1.1It is the foundation of allquantum physics, which includesquantum chemistry,quantum field theory,quantum technology, andquantum information science.
Quantum mechanics can describe many systems thatclassical physicscannot. Classical physics can describe many aspects of nature at an ordinary (macroscopicand(optical) microscopic) scale, but is not sufficient for describing them at very smallsubmicroscopic(atomic andsubatomic) scales. Classical mechanics can be derived from quantum mechanics as an approximation that is valid at ordinary scales.[3]
Quantum systems haveboundstates that arequantizedtodiscrete valuesofenergy,momentum,angular momentum, and other quantities, in contrast to classical systems where these quantities can be measured continuously. Measurements of quantum systems show characteristics of bothparticlesandwaves(wave–particle duality), and there are limits to how accurately the value of a physical quantity can be predicted prior to its measurement, given a complete set of initial conditions (theuncertainty principle).
Quantum mechanicsarose graduallyfrom theories to explain observations that could not be reconciled with classical physics, such asMax Planck's solution in 1900 to theblack-body radiationproblem, and the correspondence between energy and frequency inAlbert Einstein's1905 paper, which explained thephotoelectric effect. These early attempts to understand microscopic phenomena, now known as the "old quantum theory", led to the full development of quantum mechanics in the mid-1920s byNiels Bohr,Erwin Schrödinger,Werner Heisenberg,Max Born,Paul Diracand others. The modern theory is formulated in variousspecially developed mathematical formalisms. In one of them, a mathematical entity called thewave functionprovides information, in the form ofprobability amplitudes, about what measurements of a particle's energy, momentum, and other physical properties may yield.
Quantum mechanics allows the calculation of properties and behaviour ofphysical systems. It is typically applied to microscopic systems:molecules,atomsandsubatomic particles. It has been demonstrated to hold for complex molecules with thousands of atoms,[4]but its application to human beings raises philosophical problems, such asWigner's friend, and its application to the universe as a whole remains speculative.[5]Predictions of quantum mechanics have been verified experimentally to an extremely high degree ofaccuracy. For example, the refinement of quantum mechanics for the interaction of light and matter, known asquantum electrodynamics(QED), has beenshown to agree with experimentto within 1 part in 1012when predicting the magnetic properties of an electron.[6]
A fundamental feature of the theory is that it usually cannot predict with certainty what will happen, but only give probabilities. Mathematically, a probability is found by taking the square of the absolute value of acomplex number, known as a probability amplitude. This is known as theBorn rule, named after physicistMax Born. For example, a quantum particle like anelectroncan be described by a wave function, which associates to each point in space a probability amplitude. Applying the Born rule to these amplitudes gives aprobability density functionfor the position that the electron will be found to have when an experiment is performed to measure it. This is the best the theory can do; it cannot say for certain where the electron will be found. TheSchrödinger equationrelates the collection of probability amplitudes that pertain to one moment of time to the collection of probability amplitudes that pertain to another.[7]: 67–87
One consequence of the mathematical rules of quantum mechanics is a tradeoff in predictability between measurable quantities. The most famous form of thisuncertainty principlesays that no matter how a quantum particle is prepared or how carefully experiments upon it are arranged, it is impossible to have a precise prediction for a measurement of its position and also at the same time for a measurement of itsmomentum.[7]: 427–435
Another consequence of the mathematical rules of quantum mechanics is the phenomenon ofquantum interference, which is often illustrated with thedouble-slit experiment. In the basic version of this experiment, acoherent light source, such as alaserbeam, illuminates a plate pierced by two parallel slits, and the light passing through the slits is observed on a screen behind the plate.[8]: 102–111[2]: 1.1–1.8The wave nature of light causes the light waves passing through the two slits tointerfere, producing bright and dark bands on the screen – a result that would not be expected if light consisted of classical particles.[8]However, the light is always found to be absorbed at the screen at discrete points, as individual particles rather than waves; the interference pattern appears via the varying density of these particle hits on the screen. Furthermore, versions of the experiment that include detectors at the slits find that each detectedphotonpasses through one slit (as would a classical particle), and not through both slits (as would a wave).[8]: 109[9][10]However,such experimentsdemonstrate that particles do not form the interference pattern if one detects which slit they pass through.  This behavior is known aswave–particle duality. In addition to light,electrons,atoms, andmoleculesare all found to exhibit the same dual behavior when fired towards a double slit.[2]
Another non-classical phenomenon predicted by quantum mechanics isquantum tunnelling: a particle that goes up against apotential barriercan cross it, even if its kinetic energy is smaller than the maximum of the potential.[11]In classical mechanics this particle would be trapped. Quantum tunnelling has several important consequences, enablingradioactive decay,nuclear fusionin stars, and applications such asscanning tunnelling microscopy,tunnel diodeandtunnel field-effect transistor.[12][13]
When quantum systems interact, the result can be the creation ofquantum entanglement: their properties become so intertwined that a description of the whole solely in terms of the individual parts is no longer possible. Erwin Schrödinger called entanglement "...thecharacteristic trait of quantum mechanics, the one that enforces its entire departure from classical lines of thought".[14]Quantum entanglement enablesquantum computingand is part of quantum communication protocols, such asquantum key distributionandsuperdense coding.[15]Contrary to popular misconception, entanglement does not allow sending signalsfaster than light, as demonstrated by theno-communication theorem.[15]
Another possibility opened by entanglement is testing for "hidden variables", hypothetical properties more fundamental than the quantities addressed in quantum theory itself, knowledge of which would allow more exact predictions than quantum theory provides. A collection of results, most significantlyBell's theorem, have demonstrated that broad classes of such hidden-variable theories are in fact incompatible with quantum physics. According to Bell's theorem, if nature actually operates in accord with any theory oflocalhidden variables, then the results of aBell testwill be constrained in a particular, quantifiable way. Many Bell tests have been performed and they have shown results incompatible with the constraints imposed by local hidden variables.[16][17]
It is not possible to present these concepts in more than a superficial way without introducing the mathematics involved; understanding quantum mechanics requires not only manipulating complex numbers, but alsolinear algebra,differential equations,group theory, and other more advanced subjects.[18][19]Accordingly, this article will present a mathematical formulation of quantum mechanics and survey its application to some useful and oft-studied examples.
In the mathematically rigorous formulation of quantum mechanics, the state of a quantum mechanical system is a vectorψ{\displaystyle \psi }belonging to a (separable) complexHilbert spaceH{\displaystyle {\mathcal {H}}}. This vector is postulated to be normalized under the Hilbert space inner product, that is, it obeys⟨ψ,ψ⟩=1{\displaystyle \langle \psi ,\psi \rangle =1}, and it is well-defined up to a complex number of modulus 1 (the global phase), that is,ψ{\displaystyle \psi }andeiαψ{\displaystyle e^{i\alpha }\psi }represent the same physical system. In other words, the possible states are points in theprojective spaceof a Hilbert space, usually called thecomplex projective space. The exact nature of this Hilbert space is dependent on the system – for example, for describing position and momentum the Hilbert space is the space of complexsquare-integrablefunctionsL2(C){\displaystyle L^{2}(\mathbb {C} )}, while the Hilbert space for thespinof a single proton is simply the space of two-dimensional complex vectorsC2{\displaystyle \mathbb {C} ^{2}}with the usual inner product.
Physical quantities of interest – position, momentum, energy, spin – are represented by observables, which areHermitian(more precisely,self-adjoint) linearoperatorsacting on the Hilbert space. A quantum state can be aneigenvectorof an observable, in which case it is called aneigenstate, and the associatedeigenvaluecorresponds to the value of the observable in that eigenstate. More generally, a quantum state will be a linear combination of the eigenstates, known as aquantum superposition. When an observable is measured, the result will be one of its eigenvalues with probability given by theBorn rule: in the simplest case the eigenvalueλ{\displaystyle \lambda }is non-degenerate and the probability is given by|⟨λ→,ψ⟩|2{\displaystyle |\langle {\vec {\lambda }},\psi \rangle |^{2}}, whereλ→{\displaystyle {\vec {\lambda }}}is its associated unit-length eigenvector. More generally, the eigenvalue is degenerate and the probability is given by⟨ψ,Pλψ⟩{\displaystyle \langle \psi ,P_{\lambda }\psi \rangle }, wherePλ{\displaystyle P_{\lambda }}is the projector onto its associated eigenspace. In the continuous case, these formulas give instead theprobability density.
After the measurement, if resultλ{\displaystyle \lambda }was obtained, the quantum state is postulated tocollapsetoλ→{\displaystyle {\vec {\lambda }}}, in the non-degenerate case, or toPλψ/⟨ψ,Pλψ⟩{\textstyle P_{\lambda }\psi {\big /}\!{\sqrt {\langle \psi ,P_{\lambda }\psi \rangle }}}, in the general case. Theprobabilisticnature of quantum mechanics thus stems from the act of measurement. This is one of the most difficult aspects of quantum systems to understand. It was the central topic in the famousBohr–Einstein debates, in which the two scientists attempted to clarify these fundamental principles by way ofthought experiments. In the decades after the formulation of quantum mechanics, the question of what constitutes a "measurement" has been extensively studied. Newerinterpretations of quantum mechanicshave been formulated that do away with the concept of "wave function collapse" (see, for example, themany-worlds interpretation). The basic idea is that when a quantum system interacts with a measuring apparatus, their respective wave functions becomeentangledso that the original quantum system ceases to exist as an independent entity (seeMeasurement in quantum mechanics[20]).
The time evolution of a quantum state is described by the Schrödinger equation:iℏ∂∂tψ(t)=Hψ(t).{\displaystyle i\hbar {\frac {\partial }{\partial t}}\psi (t)=H\psi (t).}HereH{\displaystyle H}denotes theHamiltonian, the observable corresponding to thetotal energyof the system, andℏ{\displaystyle \hbar }is the reducedPlanck constant. The constantiℏ{\displaystyle i\hbar }is introduced so that the Hamiltonian is reduced to theclassical Hamiltonianin cases where the quantum system can be approximated by a classical system; the ability to make such an approximation in certain limits is called thecorrespondence principle.
The solution of this differential equation is given byψ(t)=e−iHt/ℏψ(0).{\displaystyle \psi (t)=e^{-iHt/\hbar }\psi (0).}The operatorU(t)=e−iHt/ℏ{\displaystyle U(t)=e^{-iHt/\hbar }}is known as the time-evolution operator, and has the crucial property that it isunitary. This time evolution isdeterministicin the sense that – given an initial quantum stateψ(0){\displaystyle \psi (0)}– it makes a definite prediction of what the quantum stateψ(t){\displaystyle \psi (t)}will be at any later time.[21]
Some wave functions produce probability distributions that are independent of time, such aseigenstatesof the Hamiltonian.[7]: 133–137Many systems that are treated dynamically in classical mechanics are described by such "static" wave functions. For example, a single electron in an unexcited atom is pictured classically as a particle moving in a circular trajectory around theatomic nucleus, whereas in quantum mechanics, it is described by a static wave function surrounding the nucleus. For example, the electron wave function for an unexcited hydrogen atom is a spherically symmetric function known as ansorbital(Fig. 1).
Analytic solutions of the Schrödinger equation are known forvery few relatively simple model Hamiltoniansincluding thequantum harmonic oscillator, theparticle in a box, thedihydrogen cation, and thehydrogen atom. Even theheliumatom – which contains just two electrons – has defied all attempts at a fully analytic treatment, admitting no solution inclosed form.[22][23][24]
However, there are techniques for finding approximate solutions. One method, calledperturbation theory, uses the analytic result for a simple quantum mechanical model to create a result for a related but more complicated model by (for example) the addition of a weakpotential energy.[7]: 793Another approximation method applies to systems for which quantum mechanics produces only small deviations from classical behavior. These deviations can then be computed based on the classical motion.[7]: 849
One consequence of the basic quantum formalism is the uncertainty principle. In its most familiar form, this states that no preparation of a quantum particle can imply simultaneously precise predictions both for a measurement of its position and for a measurement of its momentum.[25][26]Both position and momentum are observables, meaning that they are represented byHermitian operators. The position operatorX^{\displaystyle {\hat {X}}}and momentum operatorP^{\displaystyle {\hat {P}}}do not commute, but rather satisfy thecanonical commutation relation:[X^,P^]=iℏ.{\displaystyle [{\hat {X}},{\hat {P}}]=i\hbar .}Given a quantum state, the Born rule lets us compute expectation values for bothX{\displaystyle X}andP{\displaystyle P}, and moreover for powers of them. Defining the uncertainty for an observable by astandard deviation, we haveσX=⟨X2⟩−⟨X⟩2,{\displaystyle \sigma _{X}={\textstyle {\sqrt {\left\langle X^{2}\right\rangle -\left\langle X\right\rangle ^{2}}}},}and likewise for the momentum:σP=⟨P2⟩−⟨P⟩2.{\displaystyle \sigma _{P}={\sqrt {\left\langle P^{2}\right\rangle -\left\langle P\right\rangle ^{2}}}.}The uncertainty principle states thatσXσP≥ℏ2.{\displaystyle \sigma _{X}\sigma _{P}\geq {\frac {\hbar }{2}}.}Either standard deviation can in principle be made arbitrarily small, but not both simultaneously.[27]This inequality generalizes to arbitrary pairs of self-adjoint operatorsA{\displaystyle A}andB{\displaystyle B}. Thecommutatorof these two operators is[A,B]=AB−BA,{\displaystyle [A,B]=AB-BA,}and this provides the lower bound on the product of standard deviations:σAσB≥12|⟨[A,B]⟩|.{\displaystyle \sigma _{A}\sigma _{B}\geq {\tfrac {1}{2}}\left|{\bigl \langle }[A,B]{\bigr \rangle }\right|.}
Another consequence of the canonical commutation relation is that the position and momentum operators areFourier transformsof each other, so that a description of an object according to its momentum is the Fourier transform of its description according to its position. The fact that dependence in momentum is the Fourier transform of the dependence in position means that the momentum operator is equivalent (up to ani/ℏ{\displaystyle i/\hbar }factor) to taking the derivative according to the position, since in Fourier analysisdifferentiation corresponds to multiplication in the dual space. This is why in quantum equations in position space, the momentumpi{\displaystyle p_{i}}is replaced by−iℏ∂∂x{\displaystyle -i\hbar {\frac {\partial }{\partial x}}}, and in particular in thenon-relativistic Schrödinger equation in position spacethe momentum-squared term is replaced with a Laplacian times−ℏ2{\displaystyle -\hbar ^{2}}.[25]
When two different quantum systems are considered together, the Hilbert space of the combined system is thetensor productof the Hilbert spaces of the two components. For example, letAandBbe two quantum systems, with Hilbert spacesHA{\displaystyle {\mathcal {H}}_{A}}andHB{\displaystyle {\mathcal {H}}_{B}}, respectively. The Hilbert space of the composite system is thenHAB=HA⊗HB.{\displaystyle {\mathcal {H}}_{AB}={\mathcal {H}}_{A}\otimes {\mathcal {H}}_{B}.}If the state for the first system is the vectorψA{\displaystyle \psi _{A}}and the state for the second system isψB{\displaystyle \psi _{B}}, then the state of the composite system isψA⊗ψB.{\displaystyle \psi _{A}\otimes \psi _{B}.}Not all states in the joint Hilbert spaceHAB{\displaystyle {\mathcal {H}}_{AB}}can be written in this form, however, because the superposition principle implies that linear combinations of these "separable" or "product states" are also valid. For example, ifψA{\displaystyle \psi _{A}}andϕA{\displaystyle \phi _{A}}are both possible states for systemA{\displaystyle A}, and likewiseψB{\displaystyle \psi _{B}}andϕB{\displaystyle \phi _{B}}are both possible states for systemB{\displaystyle B}, then12(ψA⊗ψB+ϕA⊗ϕB){\displaystyle {\tfrac {1}{\sqrt {2}}}\left(\psi _{A}\otimes \psi _{B}+\phi _{A}\otimes \phi _{B}\right)}is a valid joint state that is not separable. States that are not separable are calledentangled.[28][29]
If the state for a composite system is entangled, it is impossible to describe either component systemAor systemBby a state vector. One can instead definereduced density matricesthat describe the statistics that can be obtained by making measurements on either component system alone. This necessarily causes a loss of information, though: knowing the reduced density matrices of the individual systems is not enough to reconstruct the state of the composite system.[28][29]Just as density matrices specify the state of a subsystem of a larger system, analogously,positive operator-valued measures(POVMs) describe the effect on a subsystem of a measurement performed on a larger system. POVMs are extensively used in quantum information theory.[28][30]
As described above, entanglement is a key feature of models of measurement processes in which an apparatus becomes entangled with the system being measured. Systems interacting with the environment in which they reside generally become entangled with that environment, a phenomenon known asquantum decoherence. This can explain why, in practice, quantum effects are difficult to observe in systems larger than microscopic.[31]
There are many mathematically equivalent formulations of quantum mechanics. One of the oldest and most common is the "transformation theory" proposed byPaul Dirac, which unifies and generalizes the two earliest formulations of quantum mechanics –matrix mechanics(invented byWerner Heisenberg) and wave mechanics (invented byErwin Schrödinger).[32]An alternative formulation of quantum mechanics isFeynman'spath integral formulation, in which a quantum-mechanical amplitude is considered as a sum over all possible classical and non-classical paths between the initial and final states. This is the quantum-mechanical counterpart of theaction principlein classical mechanics.[33]
The HamiltonianH{\displaystyle H}is known as thegeneratorof time evolution, since it defines a unitary time-evolution operatorU(t)=e−iHt/ℏ{\displaystyle U(t)=e^{-iHt/\hbar }}for each value oft{\displaystyle t}. From this relation betweenU(t){\displaystyle U(t)}andH{\displaystyle H}, it follows that any observableA{\displaystyle A}that commutes withH{\displaystyle H}will beconserved: its expectation value will not change over time.[7]: 471This statement generalizes, as mathematically, any Hermitian operatorA{\displaystyle A}can generate a family of unitary operators parameterized by a variablet{\displaystyle t}. Under the evolution generated byA{\displaystyle A}, any observableB{\displaystyle B}that commutes withA{\displaystyle A}will be conserved. Moreover, ifB{\displaystyle B}is conserved by evolution underA{\displaystyle A}, thenA{\displaystyle A}is conserved under the evolution generated byB{\displaystyle B}. This implies a quantum version of the result proven byEmmy Noetherin classical (Lagrangian) mechanics: for everydifferentiablesymmetryof a Hamiltonian, there exists a correspondingconservation law.
The simplest example of a quantum system with a position degree of freedom is a free particle in a single spatial dimension. A free particle is one which is not subject to external influences, so that its Hamiltonian consists only of its kinetic energy:H=12mP2=−ℏ22md2dx2.{\displaystyle H={\frac {1}{2m}}P^{2}=-{\frac {\hbar ^{2}}{2m}}{\frac {d^{2}}{dx^{2}}}.}The general solution of the Schrödinger equation is given byψ(x,t)=12π∫−∞∞ψ^(k,0)ei(kx−ℏk22mt)dk,{\displaystyle \psi (x,t)={\frac {1}{\sqrt {2\pi }}}\int _{-\infty }^{\infty }{\hat {\psi }}(k,0)e^{i(kx-{\frac {\hbar k^{2}}{2m}}t)}\mathrm {d} k,}which is a superposition of all possibleplane wavesei(kx−ℏk22mt){\displaystyle e^{i(kx-{\frac {\hbar k^{2}}{2m}}t)}}, which are eigenstates of the momentum operator with momentump=ℏk{\displaystyle p=\hbar k}. The coefficients of the superposition areψ^(k,0){\displaystyle {\hat {\psi }}(k,0)}, which is the Fourier transform of the initial quantum stateψ(x,0){\displaystyle \psi (x,0)}.
It is not possible for the solution to be a single momentum eigenstate, or a single position eigenstate, as these are not normalizable quantum states.[note 1]Instead, we can consider a Gaussianwave packet:ψ(x,0)=1πa4e−x22a{\displaystyle \psi (x,0)={\frac {1}{\sqrt[{4}]{\pi a}}}e^{-{\frac {x^{2}}{2a}}}}which has Fourier transform, and therefore momentum distributionψ^(k,0)=aπ4e−ak22.{\displaystyle {\hat {\psi }}(k,0)={\sqrt[{4}]{\frac {a}{\pi }}}e^{-{\frac {ak^{2}}{2}}}.}We see that as we makea{\displaystyle a}smaller the spread in position gets smaller, but the spread in momentum gets larger. Conversely, by makinga{\displaystyle a}larger we make the spread in momentum smaller, but the spread in position gets larger. This illustrates the uncertainty principle.
As we let the Gaussian wave packet evolve in time, we see that its center moves through space at a constant velocity (like a classical particle with no forces acting on it). However, the wave packet will also spread out as time progresses, which means that the position becomes more and more uncertain. The uncertainty in momentum, however, stays constant.[34]
The particle in a one-dimensional potential energy box is the most mathematically simple example where restraints lead to the quantization of energy levels. The box is defined as having zero potential energy everywhereinsidea certain region, and therefore infinite potential energy everywhereoutsidethat region.[25]: 77–78For the one-dimensional case in thex{\displaystyle x}direction, the time-independent Schrödinger equation may be written−ℏ22md2ψdx2=Eψ.{\displaystyle -{\frac {\hbar ^{2}}{2m}}{\frac {d^{2}\psi }{dx^{2}}}=E\psi .}
With the differential operator defined byp^x=−iℏddx{\displaystyle {\hat {p}}_{x}=-i\hbar {\frac {d}{dx}}}the previous equation is evocative of theclassic kinetic energy analogue,12mp^x2=E,{\displaystyle {\frac {1}{2m}}{\hat {p}}_{x}^{2}=E,}with stateψ{\displaystyle \psi }in this case having energyE{\displaystyle E}coincident with the kinetic energy of the particle.
The general solutions of the Schrödinger equation for the particle in a box areψ(x)=Aeikx+Be−ikxE=ℏ2k22m{\displaystyle \psi (x)=Ae^{ikx}+Be^{-ikx}\qquad \qquad E={\frac {\hbar ^{2}k^{2}}{2m}}}or, fromEuler's formula,ψ(x)=Csin⁡(kx)+Dcos⁡(kx).{\displaystyle \psi (x)=C\sin(kx)+D\cos(kx).\!}
The infinite potential walls of the box determine the values ofC,D,{\displaystyle C,D,}andk{\displaystyle k}atx=0{\displaystyle x=0}andx=L{\displaystyle x=L}whereψ{\displaystyle \psi }must be zero. Thus, atx=0{\displaystyle x=0},ψ(0)=0=Csin⁡(0)+Dcos⁡(0)=D{\displaystyle \psi (0)=0=C\sin(0)+D\cos(0)=D}andD=0{\displaystyle D=0}. Atx=L{\displaystyle x=L},ψ(L)=0=Csin⁡(kL),{\displaystyle \psi (L)=0=C\sin(kL),}in whichC{\displaystyle C}cannot be zero as this would conflict with the postulate thatψ{\displaystyle \psi }has norm 1. Therefore, sincesin⁡(kL)=0{\displaystyle \sin(kL)=0},kL{\displaystyle kL}must be an integer multiple ofπ{\displaystyle \pi },k=nπLn=1,2,3,….{\displaystyle k={\frac {n\pi }{L}}\qquad \qquad n=1,2,3,\ldots .}
This constraint onk{\displaystyle k}implies a constraint on the energy levels, yieldingEn=ℏ2π2n22mL2=n2h28mL2.{\displaystyle E_{n}={\frac {\hbar ^{2}\pi ^{2}n^{2}}{2mL^{2}}}={\frac {n^{2}h^{2}}{8mL^{2}}}.}
Afinite potential wellis the generalization of the infinite potential well problem to potential wells having finite depth. The finite potential well problem is mathematically more complicated than the infinite particle-in-a-box problem as the wave function is not pinned to zero at the walls of the well. Instead, the wave function must satisfy more complicated mathematical boundary conditions as it is nonzero in regions outside the well. Another related problem is that of therectangular potential barrier, which furnishes a model for thequantum tunnelingeffect that plays an important role in the performance of modern technologies such asflash memoryandscanning tunneling microscopy.
As in the classical case, the potential for the quantum harmonic oscillator is given by[7]: 234V(x)=12mω2x2.{\displaystyle V(x)={\frac {1}{2}}m\omega ^{2}x^{2}.}
This problem can either be treated by directly solving the Schrödinger equation, which is not trivial, or by using the more elegant "ladder method" first proposed by Paul Dirac. Theeigenstatesare given byψn(x)=12nn!⋅(mωπℏ)1/4⋅e−mωx22ℏ⋅Hn(mωℏx),{\displaystyle \psi _{n}(x)={\sqrt {\frac {1}{2^{n}\,n!}}}\cdot \left({\frac {m\omega }{\pi \hbar }}\right)^{1/4}\cdot e^{-{\frac {m\omega x^{2}}{2\hbar }}}\cdot H_{n}\left({\sqrt {\frac {m\omega }{\hbar }}}x\right),\qquad }n=0,1,2,….{\displaystyle n=0,1,2,\ldots .}whereHnare theHermite polynomialsHn(x)=(−1)nex2dndxn(e−x2),{\displaystyle H_{n}(x)=(-1)^{n}e^{x^{2}}{\frac {d^{n}}{dx^{n}}}\left(e^{-x^{2}}\right),}and the corresponding energy levels areEn=ℏω(n+12).{\displaystyle E_{n}=\hbar \omega \left(n+{1 \over 2}\right).}
This is another example illustrating the discretization of energy forbound states.
TheMach–Zehnder interferometer(MZI) illustrates the concepts of superposition and interference with linear algebra in dimension 2, rather than differential equations. It can be seen as a simplified version of the double-slit experiment, but it is of interest in its own right, for example in thedelayed choice quantum eraser, theElitzur–Vaidman bomb tester, and in studies of quantum entanglement.[35][36]
We can model a photon going through the interferometer by considering that at each point it can be in a superposition of only two paths: the "lower" path which starts from the left, goes straight through both beam splitters, and ends at the top, and the "upper" path which starts from the bottom, goes straight through both beam splitters, and ends at the right. The quantum state of the photon is therefore a vectorψ∈C2{\displaystyle \psi \in \mathbb {C} ^{2}}that is a superposition of the "lower" pathψl=(10){\displaystyle \psi _{l}={\begin{pmatrix}1\\0\end{pmatrix}}}and the "upper" pathψu=(01){\displaystyle \psi _{u}={\begin{pmatrix}0\\1\end{pmatrix}}}, that is,ψ=αψl+βψu{\displaystyle \psi =\alpha \psi _{l}+\beta \psi _{u}}for complexα,β{\displaystyle \alpha ,\beta }. In order to respect the postulate that⟨ψ,ψ⟩=1{\displaystyle \langle \psi ,\psi \rangle =1}we require that|α|2+|β|2=1{\displaystyle |\alpha |^{2}+|\beta |^{2}=1}.
Bothbeam splittersare modelled as the unitary matrixB=12(1ii1){\displaystyle B={\frac {1}{\sqrt {2}}}{\begin{pmatrix}1&i\\i&1\end{pmatrix}}}, which means that when a photon meets the beam splitter it will either stay on the same path with a probability amplitude of1/2{\displaystyle 1/{\sqrt {2}}}, or be reflected to the other path with a probability amplitude ofi/2{\displaystyle i/{\sqrt {2}}}. The phase shifter on the upper arm is modelled as the unitary matrixP=(100eiΔΦ){\displaystyle P={\begin{pmatrix}1&0\\0&e^{i\Delta \Phi }\end{pmatrix}}}, which means that if the photon is on the "upper" path it will gain a relative phase ofΔΦ{\displaystyle \Delta \Phi }, and it will stay unchanged if it is in the lower path.
A photon that enters the interferometer from the left will then be acted upon with a beam splitterB{\displaystyle B}, a phase shifterP{\displaystyle P}, and another beam splitterB{\displaystyle B}, and so end up in the stateBPBψl=ieiΔΦ/2(−sin⁡(ΔΦ/2)cos⁡(ΔΦ/2)),{\displaystyle BPB\psi _{l}=ie^{i\Delta \Phi /2}{\begin{pmatrix}-\sin(\Delta \Phi /2)\\\cos(\Delta \Phi /2)\end{pmatrix}},}and the probabilities that it will be detected at the right or at the top are given respectively byp(u)=|⟨ψu,BPBψl⟩|2=cos2⁡ΔΦ2,{\displaystyle p(u)=|\langle \psi _{u},BPB\psi _{l}\rangle |^{2}=\cos ^{2}{\frac {\Delta \Phi }{2}},}p(l)=|⟨ψl,BPBψl⟩|2=sin2⁡ΔΦ2.{\displaystyle p(l)=|\langle \psi _{l},BPB\psi _{l}\rangle |^{2}=\sin ^{2}{\frac {\Delta \Phi }{2}}.}One can therefore use the Mach–Zehnder interferometer to estimate thephase shiftby estimating these probabilities.
It is interesting to consider what would happen if the photon were definitely in either the "lower" or "upper" paths between the beam splitters. This can be accomplished by blocking one of the paths, or equivalently by removing the first beam splitter (and feeding the photon from the left or the bottom, as desired). In both cases, there will be no interference between the paths anymore, and the probabilities are given byp(u)=p(l)=1/2{\displaystyle p(u)=p(l)=1/2}, independently of the phaseΔΦ{\displaystyle \Delta \Phi }. From this we can conclude that the photon does not take one path or another after the first beam splitter, but rather that it is in a genuine quantum superposition of the two paths.[37]
Quantum mechanics has had enormous success in explaining many of the features of our universe, with regard to small-scale and discrete quantities and interactions which cannot be explained byclassical methods.[note 2]Quantum mechanics is often the only theory that can reveal the individual behaviors of the subatomic particles that make up all forms of matter (electrons,protons,neutrons,photons, and others).Solid-state physicsandmaterials scienceare dependent upon quantum mechanics.[38]
In many aspects, modern technology operates at a scale where quantum effects are significant. Important applications of quantum theory includequantum chemistry,quantum optics,quantum computing,superconducting magnets,light-emitting diodes, theoptical amplifierand the laser, thetransistorandsemiconductorssuch as themicroprocessor,medical and research imagingsuch asmagnetic resonance imagingandelectron microscopy.[39]Explanations for many biological and physical phenomena are rooted in the nature of the chemical bond, most notably the macro-moleculeDNA.
The rules of quantum mechanics assert that the state space of a system is a Hilbert space and that observables of the system are Hermitian operators acting on vectors in that space – although they do not tell us which Hilbert space or which operators. These can be chosen appropriately in order to obtain a quantitative description of a quantum system, a necessary step in making physical predictions. An important guide for making these choices is thecorrespondence principle, a heuristic which states that the predictions of quantum mechanics reduce to those ofclassical mechanicsin the regime of largequantum numbers.[40]One can also start from an established classical model of a particular system, and then try to guess the underlying quantum model that would give rise to the classical model in the correspondence limit. This approach is known asquantization.[41]: 299[42]
When quantum mechanics was originally formulated, it was applied to models whose correspondence limit wasnon-relativisticclassical mechanics. For instance, the well-known model of thequantum harmonic oscillatoruses an explicitly non-relativistic expression for thekinetic energyof the oscillator, and is thus a quantum version of theclassical harmonic oscillator.[7]: 234
Complications arise withchaotic systems, which do not have good quantum numbers, andquantum chaosstudies the relationship between classical and quantum descriptions in these systems.[41]: 353
Quantum decoherenceis a mechanism through which quantum systems losecoherence, and thus become incapable of displaying many typically quantum effects:quantum superpositionsbecome simply probabilistic mixtures, and quantum entanglement becomes simply classical correlations.[7]: 687–730Quantum coherence is not typically evident at macroscopic scales, though at temperatures approachingabsolute zeroquantum behavior may manifest macroscopically.[note 3]
Many macroscopic properties of a classical system are a direct consequence of the quantum behavior of its parts. For example, the stability of bulk matter (consisting of atoms andmoleculeswhich would quickly collapse under electric forces alone), the rigidity of solids, and the mechanical, thermal, chemical, optical and magnetic properties of matter are all results of the interaction ofelectric chargesunder the rules of quantum mechanics.[43]
Early attempts to merge quantum mechanics withspecial relativityinvolved the replacement of the Schrödinger equation with a covariant equation such as theKlein–Gordon equationor theDirac equation. While these theories were successful in explaining many experimental results, they had certain unsatisfactory qualities stemming from their neglect of the relativistic creation and annihilation of particles. A fully relativistic quantum theory required the development of quantum field theory, which applies quantization to a field (rather than a fixed set of particles). The first complete quantum field theory,quantum electrodynamics, provides a fully quantum description of theelectromagnetic interaction. Quantum electrodynamics is, along withgeneral relativity, one of the most accurate physical theories ever devised.[44][45]
The full apparatus of quantum field theory is often unnecessary for describing electrodynamic systems. A simpler approach, one that has been used since the inception of quantum mechanics, is to treatchargedparticles as quantum mechanical objects being acted on by a classicalelectromagnetic field. For example, the elementary quantum model of thehydrogen atomdescribes theelectric fieldof the hydrogen atom using a classical−e2/(4πϵ0r){\displaystyle \textstyle -e^{2}/(4\pi \epsilon _{_{0}}r)}Coulomb potential.[7]: 285Likewise, in aStern–Gerlach experiment, a charged particle is modeled as a quantum system, while the background magnetic field is described classically.[41]: 26This "semi-classical" approach fails if quantum fluctuations in the electromagnetic field play an important role, such as in the emission of photons bycharged particles.
Quantum fieldtheories for thestrong nuclear forceand theweak nuclear forcehave also been developed. The quantum field theory of the strong nuclear force is calledquantum chromodynamics, and describes the interactions of subnuclear particles such asquarksandgluons. The weak nuclear force and the electromagnetic force were unified, in their quantized forms, into a single quantum field theory (known aselectroweak theory), by the physicistsAbdus Salam,Sheldon GlashowandSteven Weinberg.[46]
Even though the predictions of both quantum theory and general relativity have been supported by rigorous and repeatedempirical evidence, their abstract formalisms contradict each other and they have proven extremely difficult to incorporate into one consistent, cohesive model. Gravity is negligible in many areas of particle physics, so that unification between general relativity and quantum mechanics is not an urgent issue in those particular applications. However, the lack of a correct theory ofquantum gravityis an important issue inphysical cosmologyand the search by physicists for an elegant "Theory of Everything" (TOE). Consequently, resolving the inconsistencies between both theories has been a major goal of 20th- and 21st-century physics. This TOE would combine not only the models of subatomic physics but also derive the four fundamental forces of nature from a single force or phenomenon.[47]
One proposal for doing so isstring theory, which posits that thepoint-like particlesofparticle physicsare replaced byone-dimensionalobjects calledstrings. String theory describes how these strings propagate through space and interact with each other. On distance scales larger than the string scale, a string looks just like an ordinary particle, with itsmass,charge, and other properties determined by thevibrationalstate of the string. In string theory, one of the many vibrational states of the string corresponds to thegraviton, a quantum mechanical particle that carries gravitational force.[48][49]
Another popular theory isloop quantum gravity(LQG), which describes quantum properties of gravity and is thus a theory ofquantum spacetime. LQG is an attempt to merge and adapt standard quantum mechanics and standard general relativity. This theory describes space as an extremely fine fabric "woven" of finite loops calledspin networks. The evolution of a spin network over time is called aspin foam. The characteristic length scale of a spin foam is thePlanck length, approximately 1.616×10−35m, and so lengths shorter than the Planck length are not physically meaningful in LQG.[50]
Since its inception, the many counter-intuitive aspects and results of quantum mechanics have provoked strongphilosophicaldebates and manyinterpretations. The arguments centre on the probabilistic nature of quantum mechanics, the difficulties withwavefunction collapseand the relatedmeasurement problem, andquantum nonlocality. Perhaps the only consensus that exists about these issues is that there is no consensus.Richard Feynmanonce said, "I think I can safely say that nobody understands quantum mechanics."[51]According toSteven Weinberg, "There is now in my opinion no entirely satisfactory interpretation of quantum mechanics."[52]
The views ofNiels Bohr, Werner Heisenberg and other physicists are often grouped together as the "Copenhagen interpretation".[53][54]According to these views, the probabilistic nature of quantum mechanics is not atemporaryfeature which will eventually be replaced by a deterministic theory, but is instead afinalrenunciation of the classical idea of "causality". Bohr in particular emphasized that any well-defined application of the quantum mechanical formalism must always make reference to the experimental arrangement, due to thecomplementarynature of evidence obtained under different experimental situations. Copenhagen-type interpretations were adopted by Nobel laureates in quantum physics, including Bohr,[55]Heisenberg,[56]Schrödinger,[57]Feynman,[2]and Zeilinger[58]as well as 21st-century researchers in quantum foundations.[59]
Albert Einstein, himself one of the founders ofquantum theory, was troubled by its apparent failure to respect some cherished metaphysical principles, such asdeterminismandlocality. Einstein's long-running exchanges with Bohr about the meaning and status of quantum mechanics are now known as theBohr–Einstein debates. Einstein believed that underlying quantum mechanics must be a theory that explicitly forbidsaction at a distance. He argued that quantum mechanics was incomplete, a theory that was valid but not fundamental, analogous to howthermodynamicsis valid, but the fundamental theory behind it isstatistical mechanics. In 1935, Einstein and his collaboratorsBoris PodolskyandNathan Rosenpublished an argument that the principle of locality implies the incompleteness of quantum mechanics, athought experimentlater termed theEinstein–Podolsky–Rosen paradox.[note 4]In 1964,John Bellshowed that EPR's principle of locality, together with determinism, was actually incompatible with quantum mechanics: they implied constraints on the correlations produced by distance systems, now known asBell inequalities, that can be violated by entangled particles.[64]Since thenseveral experimentshave been performed to obtain these correlations, with the result that they do in fact violate Bell inequalities, and thus falsify the conjunction of locality with determinism.[16][17]
Bohmian mechanicsshows that it is possible to reformulate quantum mechanics to make it deterministic, at the price of making it explicitly nonlocal. It attributes not only a wave function to a physical system, but in addition a real position, that evolves deterministically under a nonlocal guiding equation. The evolution of a physical system is given at all times by the Schrödinger equation together with the guiding equation; there is never a collapse of the wave function. This solves the measurement problem.[65]
Everett'smany-worlds interpretation, formulated in 1956, holds thatallthe possibilities described by quantum theorysimultaneouslyoccur in a multiverse composed of mostly independent parallel universes.[66]This is a consequence of removing the axiom of the collapse of the wave packet. All possible states of the measured system and the measuring apparatus, together with the observer, are present in a real physical quantum superposition. While the multiverse is deterministic, we perceive non-deterministic behavior governed by probabilities, because we do not observe the multiverse as a whole, but only one parallel universe at a time. Exactly how this is supposed to work has been the subject of much debate. Several attempts have been made to make sense of this and derive the Born rule,[67][68]with no consensus on whether they have been successful.[69][70][71]
Relational quantum mechanicsappeared in the late 1990s as a modern derivative of Copenhagen-type ideas,[72]andQuantum Bayesianismwas developed some years later.[73]
Quantum mechanics was developed in the early decades of the 20th century, driven by the need to explain phenomena that, in some cases, had been observed in earlier times. Scientific inquiry into the wave nature of light began in the 17th and 18th centuries, when scientists such asRobert Hooke,Christiaan HuygensandLeonhard Eulerproposed a wave theory of light based on experimental observations.[74]In 1803 EnglishpolymathThomas Youngdescribed the famousdouble-slit experiment.[75]This experiment played a major role in the general acceptance of thewave theory of light.
During the early 19th century,chemicalresearch byJohn DaltonandAmedeo Avogadrolent weight to theatomic theoryof matter, an idea thatJames Clerk Maxwell,Ludwig Boltzmannand others built upon to establish thekinetic theory of gases. The successes of kinetic theory gave further credence to the idea that matter is composed of atoms, yet the theory also had shortcomings that would only be resolved by the development of quantum mechanics.[76]While the early conception of atoms fromGreek philosophyhad been that they were indivisible units – the word "atom" deriving from theGreekfor 'uncuttable' – the 19th century saw the formulation of hypotheses about subatomic structure. One important discovery in that regard wasMichael Faraday's 1838 observation of a glow caused by an electrical discharge inside a glass tube containing gas at low pressure.Julius Plücker,Johann Wilhelm HittorfandEugen Goldsteincarried on and improved upon Faraday's work, leading to the identification ofcathode rays, whichJ. J. Thomsonfound to consist of subatomic particles that would be called electrons.[77][78]
Theblack-body radiationproblem was discovered byGustav Kirchhoffin 1859. In 1900, Max Planck proposed the hypothesis that energy is radiated and absorbed in discrete "quanta" (or energy packets), yielding a calculation that precisely matched the observed patterns of black-body radiation.[79]The wordquantumderives from theLatin, meaning "how great" or "how much".[80]According to Planck, quantities of energy could be thought of as divided into "elements" whose size (E) would be proportional to theirfrequency(ν):E=hν{\displaystyle E=h\nu \ },
wherehis thePlanck constant. Planck cautiously insisted that this was only an aspect of the processes of absorption and emission of radiation and was not thephysical realityof the radiation.[81]In fact, he considered his quantum hypothesis a mathematical trick to get the right answer rather than a sizable discovery.[82]However, in 1905 Albert Einstein interpreted Planck's quantum hypothesisrealisticallyand used it to explain thephotoelectric effect, in which shining light on certain materials can eject electrons from the material. Niels Bohr then developed Planck's ideas about radiation into amodel of the hydrogen atomthat successfully predicted thespectral linesof hydrogen.[83]Einstein further developed this idea to show that anelectromagnetic wavesuch as light could also be described as a particle (later called the photon), with a discrete amount of energy that depends on its frequency.[84]In his paper "On the Quantum Theory of Radiation", Einstein expanded on the interaction between energy and matter to explain the absorption and emission of energy by atoms. Although overshadowed at the time by his general theory of relativity, this paper articulated the mechanism underlying the stimulated emission of radiation,[85]which became the basis of the laser.[86]
This phase is known as theold quantum theory. Never complete or self-consistent, the old quantum theory was rather a set ofheuristiccorrections to classical mechanics.[87][88]The theory is now understood as asemi-classical approximationto modern quantum mechanics.[89][90]Notable results from this period include, in addition to the work of Planck, Einstein and Bohr mentioned above, Einstein andPeter Debye's work on thespecific heatof solids, Bohr andHendrika Johanna van Leeuwen'sproofthat classical physics cannot account fordiamagnetism, andArnold Sommerfeld's extension of the Bohr model to include special-relativistic effects.[87][91]
In the mid-1920s quantum mechanics was developed to become the standard formulation for atomic physics. In 1923, the French physicistLouis de Broglieput forward his theory of matter waves by stating that particles can exhibit wave characteristics and vice versa. Building on de Broglie's approach, modern quantum mechanics was born in 1925, when the German physicists Werner Heisenberg, Max Born, andPascual Jordan[92][93]developedmatrix mechanicsand the Austrian physicist Erwin Schrödinger inventedwave mechanics. Born introduced the probabilistic interpretation of Schrödinger's wave function in July 1926.[94]Thus, the entire field of quantum physics emerged, leading to its wider acceptance at the FifthSolvay Conferencein 1927.[95]
By 1930, quantum mechanics had been further unified and formalized byDavid Hilbert, Paul Dirac andJohn von Neumann[96]with greater emphasis onmeasurement, the statistical nature of our knowledge of reality, andphilosophical speculation about the 'observer'. It has since permeated many disciplines, including quantum chemistry,quantum electronics,quantum optics, andquantum information science. It also provides a useful framework for many features of the modernperiodic table of elements, and describes the behaviors ofatomsduringchemical bondingand the flow of electrons in computersemiconductors, and therefore plays a crucial role in many modern technologies. While quantum mechanics was constructed to describe the world of the very small, it is also needed to explain somemacroscopicphenomena such assuperconductors[97]andsuperfluids.[98]
The following titles, all by working physicists, attempt to communicate quantum theory to lay people, using a minimum of technical apparatus:
